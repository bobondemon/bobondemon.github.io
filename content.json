[{"title":"Introduction of Probably Approximately Correct (PAC) 林軒田課程筆記","date":"2024-03-19T11:55:05.000Z","path":"2024/03/19/Introduction-of-Probably-Approximately-Correct-PAC-林軒田課程筆記/","text":"這是林軒田教授在 Coursera 機器學習基石上 (Machine Learning Foundations)—Mathematical Foundations Week4 的課程筆記.說明了為什麼我們用 training data 學出來的 model 可以對沒看過的 data 有泛化能力, 因此機器學習才有可能真正應用上.課程單元的這句話總結得很好 “learning can be probably approximately correct when given enough statistical data and finite number of hypotheses”以下為筆記內容 怎麼說都你對, training data 外的 predition$\\mathcal{D}$ is training data, learning algorithm 找到的 function $g$ 對 $\\mathcal{D}$ 完美的 predict, 但 training data 外的資料也能保證很好嗎? 所以對 training data $\\mathcal{D}$ 找 hypothesis 沒問題嗎??由 Probably Approximately Correct (PAC) 出發, 告訴我們當 data 量夠大的時候是沒問題的. 大概差不多正確 Probably Approximately Correct (PAC)Hoeffding’s Inequality請看投影片 $\\mathbb{P}[|\\nu-\\mu|&gt;\\epsilon]$ 想成壞事發生的機率. 所以告訴我們, 壞事發生的機率有個 upper bound. 就是當 $N$ 夠大, 壞事發生的機率就很小. Wiki: Hoeffding’s inequalityLet $X_1,…,X_N$ be independent r.v.s such that $a_i\\leq X_i\\leq b_i$ almost surely. Consider $S_N=X_1+…+X_N$. Then for all $t&gt;0$,$$\\begin{align} P(S_N-\\mathbb{E}[S_N]\\geq t)\\leq \\exp\\left( -\\frac{2t^2}{\\sum_{i=1}^N(b_i-a_i)^2}\\right) \\\\ P(|S_N-\\mathbb{E}[S_N]|\\geq t)\\leq 2\\exp\\left( -\\frac{2t^2}{\\sum_{i=1}^N(b_i-a_i)^2}\\right) \\end{align}$$ 以老師的投影片來說, random variable $X_i$ 表示是不是橘球, 是的話就是 $1$, 不是就是 $0$. 所以 $0\\leq X_i \\leq 1$.因此 $S_N=X_1+…+X_N$ 就表示 $N$ 個球中是橘球的數量的 random variable. 然後帶入 wiki 的公式就可以跟老師的投影片結果對照起來. 注意 wiki 使用 $S_N$, 但投影片用的是 $\\nu=S_N/N, \\mu=\\mathbb{E}[S_N]/N$. 所以 $t=N\\epsilon$. 所以對 training data $\\mathcal{D}$ 不過 Hoeffding’s inequality 它的 bound 不是很 tight. Connection to Learning上面罐子抽彈珠的例子, 其實跟 ML 要怎麼確認找到的 $h$ 是不是跟 oracle $f$ 夠像是同一個問題 把整個 $\\mathcal{X}$ 當成是整個罐子的彈珠, 而 training dataset $\\mathcal{D}$ 是抽樣出來的 $N$-sample.對某一個 $x\\in\\mathcal{X}$, $h(x)\\neq f(x)$ 就是橘色彈珠, 否則就是綠色彈珠. 則 $\\mu$ 就是 $\\mathbb{E}{out}$, $\\nu$ 就是 $\\mathbb{E}{in}$.所以原來壞事發生的機率 $\\mathbb{P}[|\\nu-\\mu|&gt;\\epsilon]$ 在這裡變成 $\\mathbb{P}[|\\mathbb{E}_{in}-\\mathbb{E}_{out}|&gt;\\epsilon]$ (inside-test 和 outside-test 的錯誤率差太多的機率)則根據 Hoeffding’s Inequality, 對某一固定的 hypothesis $h$ 在 training data 上的表現和 out-of-sample 的 data 表現大概接近.$$\\mathbb{P}\\left[|E_{in}(h)-E_{out}(h)|&gt;\\epsilon\\right]\\leq 2\\exp\\left(-2\\epsilon^2N\\right)$$ 上圖中, 先不看 learning algorithm $\\mathcal{A}$ 以及它挑出來的 final hypothesis $g$ 那兩塊.注意到, 目前所說的是指已經固定一個 $h$ 了, 然後我們可以用 unknown probability $P$ 採樣出來的 set $\\mathcal{X}$ 得到 $\\mathbb{E}_{in}\\approx\\mathbb{E}_{out}$. 意思就是這個 $h$ 不能針對 $\\mathbb{E}_{in}$ 去學習找出來.再白話一點 “固定一個 $h$” 意思是罐子裡彈珠的顏色已經先固定了! Hoeffding’s 只是告訴我們抽樣的資料 (size $N$ 的彈珠, set $\\mathcal{D}$) 查看到的錯誤率跟整個罐子的錯誤率會很接近. 所以當然不能針對抽出來的資料再來挑選 $h$, 這樣等於事後改變彈珠顏色. 也就是說這時候的 $\\mathbb{E}_{in}(g)$ 指的是用 verification data 來看 error, 或許叫 $\\mathbb{E}_{ver}$ 比較好. 而真正的 training data 給 learning algorithm $\\mathcal{A}$ 用來挑 $g$.這就是 Verification dataset 在做的事情. 💡 下一段所講的 $\\mathbb{E}_{in}$ 其實指的是 $\\mathbb{E}_{ver}$, training data $\\mathcal{D}$ 其實是 verification data Connection to Real Learning一個固定的 hypothesis $h$ 會對應到一種彈珠顏色分布情形, 橘色代表它跟 oracle $f$ 不一樣的 input $x$.Hoeffding 告訴我們, 對這一個 $h$ 我們看 $\\mathbb{E}_{in}(h)$ 大概差不多等於 $\\mathbb{E}_{out}(h)$. 換句話說 BAD 的機率很小, 所以我們可以相信 $\\mathbb{E}_{in}(h)$ 的評估結果. $$\\mathbb{P}_{\\mathcal{D}}[{\\color{orange}{\\text{BAD }}} \\mathcal{D}] = \\sum_{D\\in\\mathcal{D}}{\\mathbb{P}(D)\\cdot 1[D \\text{ is}{\\color{orange}{\\text{ BAD}}}]}$$ Hypothesis set $\\mathcal{H}$ 通常會有無窮多個 hypothesis, 我們先假設它只有 $M$ 個就好. 無窮多的 case 之後課程會介紹. 結果對 sampling 出來的 data (training data), 我們發現其中有一個 hypothesis 表現全對, 我們可以選它嗎? 當然不能. 那不能的話, learning algorithm $\\mathcal{A}$ 根據 training data (這裡其實指的是 verification data) 挑最好的 $\\mathbb{E}_{in}$ 不就沒意義了? 別緊張, 我們先看一下所有 hypothesis 對應 training data 的表: 關鍵在 column, 因為 learning algorithm $\\mathcal{A}$, 對某一 sampling 出來的 training data $\\mathcal{D}_i$ (這裡其實指的是 verification data) 會挑一個 error 最小的 hypothesis. 但會挑到哪一個不知道 (因為 $\\mathcal{A}$ 是用真正的 training data 挑的, 不是用 $\\mathcal{D}_i$ 挑的, 別忘了 $\\mathcal{D}_i$ 是 verification data), 所以任何的 $h_i$ 都有可能被 $\\mathcal{A}$ 挑到.所以只要 column 中有某一個 hypothesis 是 BAD 的話, learning algorithm 就失去作用了. 對於 all 最後那個 row 來說, 只要有一個 hypothesis 是 BAD, all 就算 BAD. 如同 PAC 一樣, 我們希望最後一個 row BAD 的機率越低愈好.評估一下這樣情況下, 發生 BAD 的機率: $$\\mathbb{P}_{\\mathcal{D}}[{\\color{orange}{\\text{BAD }}} \\mathcal{D}] = 2{\\color{orange}{M}}\\exp\\left(-2\\epsilon^2N\\right)$$ 所以其實還是有 upper bound, 換句話說, learening algorithm 挑出來的 $g$ 仍然滿足 PAC!","tags":[{"name":"Probably Approximately Correct (PAC)","slug":"Probably-Approximately-Correct-PAC","permalink":"https://bobondemon.github.io/tags/Probably-Approximately-Correct-PAC/"}]},{"title":"量化技術路線","date":"2024-02-17T12:41:45.000Z","path":"2024/02/17/量化技術路線/","text":"總結一下 (up to 2024-02-17) 目前學習的量化技術和流程圖, 同時也記錄在 github 裡. Post Training Quantization (PTQ) 稱事後量化. Quantization Aware Training (QAT) 表示訓練時考慮量化造成的損失來做訓練為了得到 fixed point model 可以對事先訓練好的 float model 做 PTQ 或 QAT, 或是直接從 QAT 流程開始同時 QAT 也可以用 PTQ 來初始化訓練. 如果要從 float model 開始做量化的話, 可以考慮在訓練 float model 時就對之後量化能更加友善的技術 (如 R^2, KURE, PACT) 接著對每個技術點盡量以最簡單的方式解說. 如果對量化還不是那麼熟悉, 建議參考一下文章後半段的”簡單回顧量化” 量化技術和流程 Floating Model Training這階段主要是讓訓練出來的 floating model 有利於之後量化的技術 R^2 [paper]: 認為 outliers 愈少, 愈有利於後面的量化或壓縮. 提出了 3 種 regularization losses. KURE [paper]: 使用 4th moments Kurtosis (KURE, KUrtosis REgularization) 來當 regularization 讓分佈接近 uniform, 同樣會有利於後面的量化. PACT [paper]: 使得 $l,u$ 這兩個 clipping 上下界能被學習, 限制數值範圍 PTQPTQ 是針對 float model 做量化的技術, 不需要 training data, 通常只需要些許的 calibration data 即可, 有些技術仍會需要算 gradients, 而有些不用, 甚至連 calibration data 都不用.一般來說 PTQ 效果會比 QAT 差, 但速度比 QAT 快多了, 同時針對 LLM 這種大模型 QAT 成本太高都只能使用 PTQ. CLE, Bias Absorption, Bias Correction [paper]: Qaulcomm DFQ (Data Free Quantization 技術), 詳見 [blog] AdaRound [paper]: weight 量化時 (式 (1)) 的 rounding (四捨五入) 不一定是最佳的, 找出使用 floor 或 ceil 的最佳組合來取代全部都用 rounding 的方式 OBQ (Optimal Brain Quantization) [paper]: 對於 weights 在 quantize 其中一個元素後還要調整其他元素, 使得 quantization 對 output activatyions 的 re-construction error 最小. OCTAV (Optimally Clipped Tensors And Vectors)[paper]: 找出最佳的 scale $S$ 使得 quantization MSE 最小 (式 (4)), 詳見[blog1, blog2] Transformer GPTQ (WOQ) [paper]: 基於 OBQ 的技術來針對 Transformer 做些改進和加速. Weight-Only-Quantization (WOQ) Transformer AWQ (WOQ) [paper]: 對 input activations 值域特別大的那些 channels 做 scaling 處理, 這樣能維持 LLM 的效果, 詳見筆記 [blog]. Weight-Only-Quantization (WOQ) Transformer SmoothQuant [paper]: 透過一些等價的轉換將 activations 的 scale 縮小並放大 weights 的 scale, 使得 activations 變的較容易 quant 而 weights 仍然容易 quant. 詳見筆記 [blog] QAT一般來說透過插入 fake-quant op (不清楚的話參見 “簡單回顧量化” 裡的說明) 使得在訓練時能感知到量化的誤差 STE (Straight Through Estimator): 在做量化時 clip and round 這兩個運算不可微分, 為了能 back propagation 假裝沒有這兩個不可微分的 ops. 這是最常見和標準的 QAT 技巧. EWGS [paper]: 由於多個 floating 值會對應到同一個 quantized 值, 使得這些不同的 floating 值因為 STE 的原因都使用相同的 gradients, EWGS 改善了這點. 論文的 figure 1 圖示很清楚. MAD [in OCTAV paper]: 改善了 STE 對於 clipping op 的 under estimate 問題, 詳見論文裡的 figure 3 and appendix C. PACT [paper]: 使得 $l,u$ 這兩個 clipping 上下界能被學習, 限制數值範圍. 可以放在 QAT 過程中使用. LSQ+ [paper]: 使得 $S,Z$ 這兩個 qparam 能被學習, 詳見筆記 [blog] OCTAV (Optimally Clipped Tensors And Vectors)[paper]: 找出最佳的 scale $S$ 使得 quantization MSE 最小 (式 (4)), 詳見[blog1, blog2]. 除了上面 PTQ 做之外, 也可放在 QAT 過程中. K-means [paper], DKM [paper]: 屬於 nonlinear 量化, 利用 Kmeans 求出代表性的 codebook. DKM 為進一步改進的方法. N2UQ [paper]: 屬於 nonlinear 量化, 讓量化區間變成可學的 (固定的量化區間就是線性量化). 簡單回顧量化 量化就是將 float $X$ 用有限個點來表示, 如下圖 $\\tilde{X}$ 的 4 個點對應到原來的 $X$ 可以看到是很不規則的, 或是說非線性如果說這有限個點採用”線性”的對應方式, 則我們可以寫成下面式子對應關係: $$\\begin{align} \\hat{X}=\\text{clip}\\left(\\text{round}\\left(X\\over S\\right)+Z,l,u\\right) \\\\ \\tilde{X}=S(\\hat{X}-Z) \\end{align}$$ $Z,S$ 分別稱為 zero point 和 scale, 而 $l,u$ 是 clipping 的 lower and upper bound.所以量化參數 quantization parameters (用 qparam 簡稱) 就是$$\\begin{align} \\text{qparam}=\\{Z,S,l,u\\} \\end{align}$$ Quantization Meam Square Error (MSE): $$\\begin{align}\\mathbb{E}_X[(X-\\tilde{X})^2] \\end{align}$$ Symmetric: $Z=0$ 時為對稱量化 Dynamic: qparam 在 inference 時才去統計出 Static: qparam 在 inference 之前就事先統計好 Quantization Granuity [SongHan slide]: per-tensor: 整個 weight or activation tensor 共用同一組 qparam per-channel: 同一個 channel 共用同一組 qparam, 例如以 convolution kernel 來說, 同一個 output channel 的 weights 共用同一組 qparam per-group: 常用在 LLM 的 Transformer, 通常以 64, 128 為一組共用 qparam 另外, 我們常說的 quant, de-quant, re-quant, fake-quant 可以用下圖來表示: Model Compression Toolkits 以下蒐集一些重要的模型壓縮 repositories, 因此不限於量化, 有些還包含 pruning, NAS, distillation, 或圖優化等 Microsoft Olive Microsoft NNI (Neural Network Intelligence): with NAS, Pruning, Quantization, Distilling OpenVino Neural Network Compression Framework (NNCF) Intel Neural Compressor: with NAS, Pruning, Quantization, Distillation Qualcomm AIMET: Quantization (DFQ and AdaRound, QAT), Model Compression (Spatial SVD, Channel pruning) NVidia TensorRT-LLM: optimize LLM (Transformer-based) models on NVidia GPU, using techniques such as Multi-query Attention (MQA), Group-query Attention(GQA), Paged KV Cache, SmoothQuant, GPTQ, AWQ, Speculative decoding, … Sony Model Compression Toolkit (MCT): Quantization with PTQ, GPTQ, QAT, Enhanced Post-Training Quantization (EPTQ). Structured Pruning","tags":[{"name":"Post Training Quantization (PTQ)","slug":"Post-Training-Quantization-PTQ","permalink":"https://bobondemon.github.io/tags/Post-Training-Quantization-PTQ/"},{"name":"Activation-aware Weight Quantization (AWQ)","slug":"Activation-aware-Weight-Quantization-AWQ","permalink":"https://bobondemon.github.io/tags/Activation-aware-Weight-Quantization-AWQ/"},{"name":"Weight Only Quantization (WOQ)","slug":"Weight-Only-Quantization-WOQ","permalink":"https://bobondemon.github.io/tags/Weight-Only-Quantization-WOQ/"},{"name":"Straight Through Estimator (STE)","slug":"Straight-Through-Estimator-STE","permalink":"https://bobondemon.github.io/tags/Straight-Through-Estimator-STE/"},{"name":"Quantization Aware Training (QAT)","slug":"Quantization-Aware-Training-QAT","permalink":"https://bobondemon.github.io/tags/Quantization-Aware-Training-QAT/"},{"name":"Fake Quantization","slug":"Fake-Quantization","permalink":"https://bobondemon.github.io/tags/Fake-Quantization/"},{"name":"LSQ","slug":"LSQ","permalink":"https://bobondemon.github.io/tags/LSQ/"},{"name":"LSQ+","slug":"LSQ","permalink":"https://bobondemon.github.io/tags/LSQ/"},{"name":"Data Free Quantization (DFQ)","slug":"Data-Free-Quantization-DFQ","permalink":"https://bobondemon.github.io/tags/Data-Free-Quantization-DFQ/"},{"name":"Quantization Error","slug":"Quantization-Error","permalink":"https://bobondemon.github.io/tags/Quantization-Error/"},{"name":"Linear Quantization","slug":"Linear-Quantization","permalink":"https://bobondemon.github.io/tags/Linear-Quantization/"},{"name":"Nonlinear Quantization","slug":"Nonlinear-Quantization","permalink":"https://bobondemon.github.io/tags/Nonlinear-Quantization/"},{"name":"OCTAV","slug":"OCTAV","permalink":"https://bobondemon.github.io/tags/OCTAV/"},{"name":"Asymmetric Quantization","slug":"Asymmetric-Quantization","permalink":"https://bobondemon.github.io/tags/Asymmetric-Quantization/"},{"name":"Symmetric Quantization","slug":"Symmetric-Quantization","permalink":"https://bobondemon.github.io/tags/Symmetric-Quantization/"},{"name":"SmoothQuant","slug":"SmoothQuant","permalink":"https://bobondemon.github.io/tags/SmoothQuant/"},{"name":"AdaRound","slug":"AdaRound","permalink":"https://bobondemon.github.io/tags/AdaRound/"},{"name":"PACT","slug":"PACT","permalink":"https://bobondemon.github.io/tags/PACT/"},{"name":"Optimal Brain Quantization (OBQ)","slug":"Optimal-Brain-Quantization-OBQ","permalink":"https://bobondemon.github.io/tags/Optimal-Brain-Quantization-OBQ/"},{"name":"GPTQ","slug":"GPTQ","permalink":"https://bobondemon.github.io/tags/GPTQ/"},{"name":"EWGS","slug":"EWGS","permalink":"https://bobondemon.github.io/tags/EWGS/"},{"name":"N2UQ","slug":"N2UQ","permalink":"https://bobondemon.github.io/tags/N2UQ/"},{"name":"DKM","slug":"DKM","permalink":"https://bobondemon.github.io/tags/DKM/"}]},{"title":"高效率計算 Jacobian, Hessian, VJP, JVP, HVP","date":"2024-02-07T13:56:22.000Z","path":"2024/02/07/高效率計算-Jacobian-Hessian-VJP-JVP-HVP/","text":"⚠️ 可能寫的比較瑣碎和雜亂, 主要給自己筆記用 令 $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ 的 Jacobian matrix 為 $J_f(x)$ 是 $(m\\times n)$ 矩陣, 而 Hessian 為 $H_f(x)$ 是 $(m\\times n \\times n)$ 高維 tensor&emsp;$\\circ$ VJP 稱為 Vector-Jacobian Product, $vJ_f(x)$, 其中 $v$ 是 ($1\\times m$) 的 row vector&emsp;$\\circ$ JVP 稱為 Jacobian-Vector Product, $J_f(x)v$, 其中 $v$ 是 ($m\\times 1$) 的 column vector&emsp;$\\circ$ HVP 稱為 Hessian-Vector Product, $H_f(x)v$, 其中 $v$ 是 ($n\\times 1$) 的 column vector計算 $vJ_f(x)$ 不用先把矩陣 $J_f(x)$ 求出來再跟 $v$ 相乘, 而是可以直接得到相乘的結果(這樣做還更快), 聽起來有點矛盾對吧~同樣的 JVP 和 HVP 也是如此本文會說明怎麼高效率計算 VJP, JVP, Jacobian, Hessian, 以及 HVP 主要參考 PyTorch 文章: JACOBIANS, HESSIANS, HVP, VHP, AND MORE: COMPOSING FUNCTION TRANSFORMS HVP 可以用來有效率地計算 $tr(H_f(x))$, 而這個 term 有時候會被當作 loss 來用, 舉例來說:&emsp;$\\circ$ Sliced Score Matching (SSM) 會用到&emsp;$\\circ$ EWGS quantization (Network Quantization with Element-wise Gradient Scaling, arxiv) 會用到&emsp;$\\circ$ More and details see: Thoughts on Trace Estimation in Deep Learning, 更多例子且有非常深入的討論總結可以參考文末 Summary先把 function $f$ 定義好: (名字為predict) Vector-Jacobian Products (VJPs) $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$, $y=f(x)$, VJP 基本就是 $vJ_f(x)$.計算上就是一個 row vector ($1\\times m$) 乘上 Jacobian matrix, $J_f(x)=\\partial y/\\partial x:m\\times n$ 矩陣, 我們這麼寫: $$\\text{VJP }:(x,v)\\mapsto v J_f(x)$$ $$v J_f(x)= [v_1, v_2,...,v_m] \\left[ \\begin{array}{c} \\partial y_1/\\partial x \\\\ \\partial y_2/\\partial x \\\\ \\vdots \\\\ \\partial y_m/\\partial x \\end{array} \\right] = v_1\\frac{\\partial f_1(x)}{\\partial x}+\\dots+v_m\\frac{\\partial f_m(x)}{\\partial x}$$ PyTorch function torch.func.vjp(*func*, **primals*, ...) 的 primals 指的是 $x$, 會 return 一個 function 例如稱 $g$, 則 $g(v)=vJ_f(x)$. 這樣看起來要計算 $vJ_f(x)$ 還是要先把 $J_f(x)$ 這個 $m\\times n$ 矩陣先算出來再跟 $v$ 相乘. 但其實不用, 我們可以直接算結果, i.e. 省去顯式地先算 $J_f(x)$, 而這樣做會更有效率!怎麼做到呢? 我們可以這麼改寫: $$vJ_f(x)=v\\frac{\\partial f(x)}{\\partial x}=\\frac{\\partial (vf(x))}{\\partial x}$$ $v$ 是一個 ($1\\times m$) 的 row vector, $f(x)$ 是一個 ($m\\times 1$) column vector. $J_f(x)=\\partial f(x)/\\partial x:m\\times n$ 矩陣.這樣改寫的好處是 $vf(x)$ 已經是一個 scalar 了, 現在改成對 scalar 做 gradient 就可以得到答案, 並且是很有效率的, 所以不用先算出 $J_f(x)$ 這個 $m\\times n$ Jacobian 矩陣.對照一下 PyTorch 的 torch.autograd.grad1torch.autograd.grad(outputs, inputs, grad_outputs=None, ...) grad_outputs 其實就是上面的 $v$. 以 chainrule 來看, $${\\partial L \\over \\partial x} = {\\partial L \\over \\partial y} \\cdot {\\partial y \\over \\partial x}=v\\cdot J_f(x)$$ 因為 PyTorch 的 loss 一定是 $L:\\mathbb{R}^m\\rightarrow\\mathbb{R}$, 所以 $\\partial L / \\partial y: (1\\times m)$ 的 row vector, 以 VJP 的型式來看就是是指 $v$.或說利用 grad 計算 $\\partial L/\\partial x$ 的時候 grad_outputs 給的就是 $\\partial L / \\partial y: (1\\times m)$. 求 Jacobian Matrix PyTorch 介紹3種求 Jacobian 的方式:&emsp;1. For-loop 求 Jacobian&emsp;2. 用 vmap-vjp 求 Jacobian&emsp;3. 用 jacrev 求 Jacobian 1. For-loop 求 Jacobian如果 $v=e_i$, 則 $vJ_f(x)$ 為 i-th row of $J_f(x)$. 因此只要把 $i=1,…,m$ 都執行一次, 則能得到完整的 $J_f(x)$. 2. 用 vmap-vjp 求 Jacobian但想像上每一個 row 的計算可以並行, 因此使用 vjp and vmap 來並行計算. vjp 就是算一次 $vJ_f(x)$, 但這是一筆 sample, 如果要對一個 batch $V^T=[v_1^T,…,v_N^T]$ 計算 $VJ_f(x)$, 就套用 vmap 在 vjp 上, 讓他並行 vectorized 算. 解說一下 vmap, 以這個範例來說會回傳 vmap_vjp_fn 這個 function, 其 input argument 會跟 vjp_fn 一樣.差別是 vmap_vjp_fn 的 input argument unit_vectors 會比 vjp_fn 的 input argument x 多了一個 batch 的維度 (預設在維度0)即 x 是維度 (n, ), unit_vectors 是維度 (m, n) 這裡的 m 是 batch 維度. 3. 用 jacrev 求 Jacobian或直接使用 jacrev 直接幫忙做好 vmap-vjp 兩步驟 torch.func.jacrev(*func*, *argnums=0*, ...) 的說明:Returns a function that takes in the same inputs as func and returns the Jacobian of func with respect to the arg(s) at argnums 當然我們也可以針對 weight or bias 計算 Jacobian, 只需要對 argnums 改成 0 or 1 即可 Jacobian-Vector Products (JVPs) $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$, $y=f(x)$, JVP 基本就是 $J_f(x)v$, 計算上就是 Jacobian matrix, $J_f(x)=\\partial y/\\partial x:m\\times n$, 乘上一個 column vector ($n\\times 1$) 我們這麼寫: PyTorch function torch.func.jvp(*func*, *primals*, *tangents,* ...) 的 primals 指的是 $x$, tangents 指的是 $v$. 同樣的如果 $v=e_i$, 則 $J_f(x)v$ 為 i-th column of $J_f(x)$. 所以對於計算 Jacobian matrix:&emsp;$\\circ$ VJP 有 jacrev (稱 reverse-mode Jacobian)&emsp;$\\circ$ JVP 有 jacfwd (稱 forward-mode Jacobian) VJP and JVP 速度上的考量 Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$, VJP 使用 vmap 在 output 維度 $m$ 上, 反之 JVP 使用 vmap 在 input 維度 $n$ 上.使用 vmap 的那個維度如果比較大的話, 效率可能會比較差, 因此建議 vmap 作用在小的維度上.因此如果 Jacobian 是瘦高矩陣 $m&gt;n$ 建議使用 JVP jacfwd, 反之胖矮矩陣 $n&gt;m$ 建議使用 VJP jacrev. Hessian 計算 使用 torch.func.hessian 可以幫忙計算出 Hessian matrix.我們知道 Hessian matrix 是二次微分, 因此可以套用算 Jacobian 的 Jacobian matrix 得到.所以實際上底層運作為 hessian(f)=jacfwd(jacrev(f)). 也可以使用 jacfwd(jacfwd(f)) 或 jacrev(jacrev(f)) 根據矩陣寬高維度來增加效率. 計算 Batch Jacobian and Batch Hessian 說明一下 func = jacrev(predict, argnums=2) 和 vmap(func, in_dims) 這兩行:jacrev(predict, argnums=2) 會回傳一個 function 稱 func, 這個 func 的 input arguments 會跟 predict 一樣, 也就是 (weight, bias, x)然後 argnums=2 表示偏微分的變數為 index 2 即 x.執行 func 會 return Jacobian matrix, 即為一個 shape (Dout, Din) 的矩陣.然後 vmap 的 in_dims=(None, None, 0) 表示 func 的這3個 input arguments 要對哪一個 argument 的哪一個維度 index 當作執行 vectorized 並行運算. 這裡的例子是對第3個 argument 的 index 0, 即 argument x 的 batch_size 那一維度. 而 vmap 也是 return 一個 function 叫 compute_batch_jacobian 只是 output 會比原本的 func 回傳結果多了一個 batch 的維度.另外可以使用 sum trick 來避掉使用 vmap 這有點 tricky 注意到這個 function predict_with_output_summed 是 $\\mathbb{R}^b\\times \\mathbb{R}^n\\rightarrow\\mathbb{R}^{m}$ 所以這個 function 的 Jacobian matrix 維度是 $(m, (b, n))$, 實際上是 $(m, b, n)$ 這個正是 jacrev return 的 shape, 然後再 movedim 變成 $(b, m, n)$. 計算 Hessian-Vector Products (HVP) $$y=H_L(x)v$$ 其中 $x\\in\\mathbb{R}^n$, $L:\\mathbb{R}^n\\rightarrow \\mathbb{R}$, $H(x)=\\partial^2L/(\\partial x)^2:\\mathbb{R}^n\\rightarrow \\mathbb{R}^n$, $v:\\mathbb{R}^n$.如同我們在 VJP, $vJ_f(x)$, 提到不用先算出 $J_f(x)$ 這個 $m\\times n$ Jacobian 矩陣, 因此 VJP 可以很有效率計算. HVP 也一樣, 不用先算出 $H_L(x)$, 可以直接有效率地算出 $H_L(x)v$:$$H_L(x)v=\\frac{\\partial G_L^T(x)}{\\partial x}v=\\frac{\\partial G_L(x)^Tv}{\\partial x}$$ 其中 $G_L(x)$ 是 gradient, 為 $n\\times 1$ 的 column vector. 這樣做的好處是 $G_L(x)^Tv$ 已經是一個 scalar 了, 做偏微分很有效率, 也避開要算 $H_L(x)$.用 jvp 和 grad 來完成 HVP, primals 指的是 $x$, tangents 指的是 $v$.注意到 grad [link] (注意這裡說的是 torch.func.grad 不是 torch.autograd.grad 喔) 的 function 只能接受 output dimension 是 $\\mathbb{R}$ (f 只能 return scalar), 而 jacrev or jacfwd 可以處理 function 的 output 是 $\\mathbb{R}^m$.雖然都是算一次微分但有這個不同要注意! PyTorch 文件說使用 jvp 這種 forward-mode AD 不用建立 Autograd graph 所以會比較省 memory Benchmarking HVP 我們對比兩個方法:&emsp;1. Baseline: 先計算出 $H_L(x)$, 再和 $v$ 相乘&emsp;2. 上面的 hvp 高效率計算方式簡單實驗得到 hvp 所花的時間為 Baseline 的 84.4477%, 加速很有效! (不同機器可能會不同) Benchmark codes 這個 hvp 雖然有效率, 但有點麻煩是因為使用 torch.func.grad 這個 function 它的 input f (也就是上面範例的 predict) 必須 return scalar.而實際上我們都會是多維的結果, 至少會有一個 batch size 維度.考量到這種用法, 我想直接參考 Sliced score matching 的 toy example codes 這段, 可能這麼寫就好. 注意到裡面的 score 已經是 gradient 了, 請讀者再讀一下 codes 可以發現確實跟上述 hvp 的做法一樣. Summary 令 $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ 的 Jacobian matrix 為 $J_f(x)$ with shape $(m, n)$, 而 Hessian 為 $H_f(x)$ with shape $(m,n,n)$&emsp;$\\circ$ VJP: torch.func.vjp 可以有效率的來計算 $vJ_f(x)$, 不用真的把 $J_f(x)$ 先算出來, 就可以直接計算 vjp 的結果.&emsp;$\\circ$ JVP: torch.func.jvp 可以有效率的來計算 $J_f(x)v$, 不用真的把 $J_f(x)$ 先算出來, 就可以直接計算 jvp 的結果.&emsp;$\\circ$ Vectorized: 可利用 vmap 來做到 batch processing&emsp;$\\circ$ Jacobian: torch.func.jacrev 和 torch.func.jacfwd 可以有效率求出 $J_f(x)$: 用 vmap + jvp or vjp&emsp;$\\circ$ Hessian: torch.func.hessian=jacfwd(jacrev(f)) 可以有效率求出 $H_f(x)$&emsp;$\\circ$ HVP: 可以利用 jvp and grad 來有效率計算出 hvp: $H_f(x)v$, 不用真的把 Hessian matrix $H_f(x)$ 先算出來, 就可以直接計算 hvp 的結果. References JACOBIANS, HESSIANS, HVP, VHP, AND MORE: COMPOSING FUNCTION TRANSFORMS [link] JAX: Hessian-vector products with grad-of-grad [link] Sliced score matching 的 toy example codes [link] Thoughts on Trace Estimation in Deep Learning [link]","tags":[{"name":"Jacobian","slug":"Jacobian","permalink":"https://bobondemon.github.io/tags/Jacobian/"},{"name":"Hessian","slug":"Hessian","permalink":"https://bobondemon.github.io/tags/Hessian/"},{"name":"Vector Jacobian Product (VJP)","slug":"Vector-Jacobian-Product-VJP","permalink":"https://bobondemon.github.io/tags/Vector-Jacobian-Product-VJP/"},{"name":"Jacobian Vector Product (JVP)","slug":"Jacobian-Vector-Product-JVP","permalink":"https://bobondemon.github.io/tags/Jacobian-Vector-Product-JVP/"},{"name":"Hessian Vector Product (HVP)","slug":"Hessian-Vector-Product-HVP","permalink":"https://bobondemon.github.io/tags/Hessian-Vector-Product-HVP/"}]},{"title":"Speculative Decoding 詳讀 (下)","date":"2024-01-08T14:21:17.000Z","path":"2024/01/08/Speculative-Decoding-詳讀2/","text":"接續上一篇現在我們可以真正的來探討以下問題了:&emsp;A. 速度的分析: 加速到什麼程度? 跟小模型的速度和準確度有關聯嗎? (想像如果 draft 一直被拒絕, 則小模型都是多跑的)&emsp;B. 運算量的分析: Operation 數 (計算量) 也會減少嗎? 還是會增加?&emsp;C. Memory bandwidth 的分析: 會減少還是增加?&emsp;D. Performance 能維持住嗎 (PPL, WER, BLEU, … 端看 model task 是什麼): 還是會有 degrade? A. 速度的分析 將上一篇的公式 (10):$$\\mathbb{E}[\\#\\text{generated tokens}]=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$$ 代入上一篇的公式 (3)$$\\text{Walltime Improvement}=\\frac{\\mathbb{E}(\\#\\text{generated tokens})}{(c\\gamma+1)}$$ 我們得到$$\\begin{align} \\text{Walltime Improvement}=\\frac{1-\\alpha^{\\gamma+1}}{(c\\gamma+1)(1-\\alpha)} \\end{align}$$ 先分析一下 walltime, 我們設定 $c=[0.1,0.4,0.8]$, $\\gamma=[3, 5, 7]$ 觀察 walltime V.S. $\\alpha$ 的變化回顧一下 $c$ 表示 approx. 跟 target model 之間的速度比, 愈小表示 approx. model 速度愈快. $\\gamma$ 表示 proposal tokens 的數目. 而 $\\alpha$ 可以代表 approx. and target models 之間的匹配程度 (愈高表示愈匹配, proposal token 被接受的機率愈高)觀察到幾點 (注意到比黑色實線小, walltime improvement $&lt;1$, 代表沒有加速到):&emsp;1. 如果 $\\alpha$ 愈大, 表示大小模型之間愈匹配可以加速愈多&emsp;2. $c$ 愈小 (小模型速度愈快) 則加速愈多&emsp;3. $\\gamma$ 則不一定 (看$c=0.1$ 的 case), 所以可能要找出最佳值 那有沒有可能 $\\gamma$ 不管怎麼找都找不出 walltime improvement 至少 $&gt;1$ 呢? 這種情況就不用花力去氣找了.論文 Corollary 3.9. 說明 $\\alpha&gt;c$ 的情況則存在 $\\gamma$ 使得會有加速好處. 加速效果至少是 $(1+\\alpha)/(1+c)$ 倍.所以 approx. and target models 的選擇就先考慮 $\\alpha&gt;c$ 的配對, 然後對 $\\gamma$ 找出最佳值. 實務上我們可以用一個 calibration set 用 $\\alpha := \\mathbb{E}_t[\\beta_t]=\\sum_t\\sum_x\\min(p_t(x),q_t(x))$ 估計出來而 $c$ 則跑 $M_p,M_q$ 的 inference 測出來.接著選擇 approx. and target models 有 $\\alpha&gt;c$ 的配對, 最後 $\\gamma$ 則求解本篇公式 (1) 找出最佳值來. 給定 $\\alpha,c$ 對式 (1) 做數值最佳化找出最佳 $\\gamma$, 結果如下: 最後 walltime improvement 理論值式 (1) 和實際上量測出來的值有沒有差很多? 作者做了個比較EXP 是式(1) 計算的, EMP 是實際量測的, 雖然沒很準確, 但也算是正相關 B. 運算量的分析 將上一篇的公式 (10):$$\\mathbb{E}[\\#\\text{generated tokens}]=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$$ 代入上一篇的公式 (6)$$\\#\\text{OPs Increasing Ratio}= \\frac{\\hat{c}\\gamma+\\gamma+1}{\\mathbb{E}(\\#\\text{generated tokens})}$$ 我們得到$$\\begin{align} \\#\\text{OPs Increasing Ratio}=\\frac{(\\hat{c}\\gamma+\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}} \\end{align}$$ 我們一樣把圖畫出來觀察到幾點 (除了最後第4點的結論, 其他聽起來都像”每60秒就會有1分鐘過去”一樣地廢話):&emsp;1. 如果 $\\alpha$ 愈小 (大小模型愈不匹配), 則運算量增加愈多&emsp;2. $c$ 愈小 (小模型速度愈快) 則運算量增加的 overhead 愈少&emsp;3. $\\gamma$ 愈大則花愈多運算量&emsp;4. 比較需要注意的是, 不管怎樣都會花額外的計算量, 因為都比 baseline 高 是不是有點反直覺, 上面說可以加速, 但又說運算量會比較多. 其實原因就是 target model 可以並行算 C. Memory Bandwidth 的分析 這個理論分析比較單純, 由於 speculative 一個 run 的時候 target model 只會呼叫一次, 對比原本每產生一個 token 都要呼叫一次 target modelLoading 參數和 kv cache 這些 memory bandwidth 的次數就少非常多, 少的比例次數基本上就是 $\\mathbb{E}(\\# \\text{generated tokens})$ 上一篇的公式 (10) 的比例:$$\\mathbb{E}[\\#\\text{generated tokens}]=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$$ D. Performance 能維持住嗎? 回到一開始就破題說 performance 能維持住這件事. 如果不能維持, 上面所有分析都在做白工.論文的 Appendix A.1. 證明寫的很明白, 基本重複一遍而已回顧 $\\beta$ 表示時間 $t$ 的 accept probability (忽略下標 $t$)$$\\beta = \\sum_x\\min(p(x),q(x))$$ Modified distribution:$$p&apos;(x)=norm(\\max(0,p(x)-q(x))) \\\\ =\\frac{p(x)-\\min(q(x),p(x))}{\\sum_{x&apos;}(p(x&apos;)-\\min(q(x&apos;),p(x&apos;)))} \\\\ = \\frac{p(x)-\\min(q(x),p(x))}{\\sum_{x&apos;}p(x&apos;)-\\sum_{x&apos;}\\min(q(x&apos;),p(x&apos;))} \\\\ = \\frac{p(x)-\\min(q(x),p(x))}{1-\\beta}$$ 考慮 speculative decoding 最終採樣出 token $x’$ 的機率為:$$P(x=x&apos;)=P(\\text{guess accept},x=x&apos;) + P(\\text{guess reject},x=x&apos;)$$ 其中$$P(\\text{guess accept},x=x&apos;)=q(x&apos;)\\min\\left(1, \\frac{p(x&apos;)}{q(x&apos;)}\\right)=\\min(q(x&apos;),p(x&apos;))$$ 注意到 speculative decoding 接受的情況是:&emsp;1. 當 $p(x&apos;) \\geq q(x&apos;)$ 時會 accept&emsp;2. 否則有 $p(x&apos;)/q(x&apos;)$ 的機率 accept這樣寫起來就是 $\\min(1, p(x&apos;)/q(x&apos;))$ 的機率. 然後 accept 的話, token 是從 approx. model 採樣的, 因此是 $q(x’)$.另外$$P(\\text{guess reject},x=x&apos;)=(1-\\beta)p&apos;(x&apos;)=p(x&apos;)-\\min(q(x&apos;),p(x&apos;))$$ Reject 的話要從 modified distribution $p’(x)$ 去採樣.所以合在一起我們得到 $P(x=x’)=p(x’)$ References Google: Fast Inference from Transformers via Speculative Decoding [arvix] DeepMind: Accelerating Large Language Model Decoding with Speculative Sampling [arxiv] Speculative Decoding 詳讀 (上)","tags":[{"name":"Speculative Decoding","slug":"Speculative-Decoding","permalink":"https://bobondemon.github.io/tags/Speculative-Decoding/"},{"name":"Speculative Sampling","slug":"Speculative-Sampling","permalink":"https://bobondemon.github.io/tags/Speculative-Sampling/"},{"name":"Transformer","slug":"Transformer","permalink":"https://bobondemon.github.io/tags/Transformer/"}]},{"title":"Speculative Decoding 詳讀 (上)","date":"2024-01-08T13:29:43.000Z","path":"2024/01/08/Speculative-Decoding-詳讀/","text":"這是 Transformer inference 的加速, 有人猜測 GPT-4 也使用這個方法: https://archive.ph/2RQ8XSpeculative decoding 做到了不影響準確率情況下直接加速 (不改 model 架構, 不 fine tune, 不做 PTQ 等)這麼神奇的操作就是利用了一個小模型來先跑一些 tokens, 再由原來的大模型評估或修正.論文顯示 LLM 效果無損直接可提速 2~3 倍, 讓我們看下去 Motivation 使用 SongHan 教授的課程 slides. 利用 small model 先提出一些 draft tokens, 然後用 large model 來驗證. 如果大部分都接受, 直覺上可以省去很多 large model 的呼叫次數, 因此加速. 方法十分簡單, 不過其實魔鬼藏在細節裡, 跟原本只使用 large model 的方法比較有幾個問題要回答:&emsp;A. 速度的分析: 加速到什麼程度? 跟小模型的速度和準確度有關聯嗎? (想像如果 draft 一直被拒絕, 則小模型都是多跑的)&emsp;B. 運算量的分析: Operation 數 (計算量) 也會減少嗎? 還是會增加?&emsp;C. Memory bandwidth 的分析: 會減少還是增加?&emsp;D. Performance 能維持住嗎 (PPL, WER, BLEU, … 端看 model task 是什麼): 還是會有 degrade?Google 這篇論文很精彩的理論分析了以上所有問題, 並有實務驗證先破題, performance (PPL, WER, BLEU, …) 可以保證維持住! 我們等到本篇筆記最後在討論, 以下會先討論算法流程、加速和運算量的分析. Speculative Decoding 算法流程 使用論文的術語, 例如上面說的 small model 改稱 approximation model, large model 改稱 target model, draft 用 proposal tokens.Approx. model, $M_q$, 用 auto-regressive 方式產生 $\\gamma$ 個 proposal tokens $\\{x_1,...,x_\\gamma\\}$ 和機率分布 $\\{q_1(x),...,q_\\gamma(x)\\}$, 接著把 proposal token 結合上次一的 prefix tokens (但這裡我們為了簡化先忽略 prefix) 給 target model, $M_p$, 做一次 non-autoregressive forward (parallel) 跑出機率分布 $\\{p_1(x),...,p_\\gamma(x),p_{\\gamma+1}(x)\\}$.比較 $p(x),q(x)$ 來決定是否接受 proposal tokens, 如果 $p(x)\\geq q(x)$ 則採用 $M_q$ 的 proposal token, 否則有 $p(x)/q(x)$ 機率仍會接受 proposal token, 有 $1-p(x)/q(x)$ 的機率要根據修改的機率分布 $p&apos;(x)=norm(\\max(0,p(x)-q(x)))$ 重新採樣 token.另外如果所有 $\\gamma$ 個 proposal tokens 都被接受了, 則直接從 target model 的 $p_{\\gamma+1}(x)$ 採樣token.以上為一個 step or run, 重複直到句子產生結束.參考下圖: 注意到 $\\{p_1(x),...,p_\\gamma(x),p_{\\gamma+1}(x)\\}$ 是一次 forward 就跑出來的, 相比 auto-regressive 的方式要跑 $\\gamma$ 次 forward (load $\\gamma$ 次 model 參數), 現在只需要 load 一次參數(and kv-cache)因此可以節省 memory bandwidth. 但注意到這兩種方式的總計算量是不變的. 一般來說 $M_p$ 的輸入會結合上一次 decode 的 tokens (稱 prefix) 加上 $M_q$ 的 proposal tokens 當輸入, 但是這些 prefix 由於上一次 decode 時 forward 過, 在使用 kv-cache 的技巧下可以省略計算. 速度和運算量的初步分析 先定義 $\\mathbb{E}(\\# \\text{generated tokens})$ 表示 speculative decoding 平均一個 run 可以產生多少”有效” tokens (因為不是所有 proposal tokens 都會被接受) 推論速度 (Walltime) 變化?每一個 run 需要的時間為 $Tc\\gamma + T$, 其中 $T$ 是跑一次 target model 所花的時間, $c$ (cost coefficient) 是 approx. model 跟 target model 的時間比 (愈小表示 approx. model 跑愈快). 所以:&emsp;- speculative decoding 花了 $Tc\\gamma + T$ 的時間產生 $\\mathbb{E}(\\# \\text{generated tokens})$ 個 tokens&emsp;- 只用 target model 花了 $T$ 的時間產生 $1$ 個 token因此只要知道 $\\mathbb{E}(\\# \\text{generated tokens})$ 我們可推得使用 speculative decoding 的速度提升 (walltime improvement):$$\\begin{align} \\text{Walltime Improvement}=\\frac{\\text{Speculative decoding (tokens per time)}} {M_p\\text{ decoding (tokens per time)}}\\\\ =\\frac{\\mathbb{E}(\\#\\text{generated tokens})/(Tc\\gamma+T)} {1/T}\\\\ =\\frac{\\mathbb{E}(\\#\\text{generated tokens})}{(c\\gamma+1)} \\end{align}$$ 數值愈高表示使用 speculative decoding 加速愈多 運算量的變化?定義 $\\hat{T}$ 是 target model ”per token” 的運算量, 而 $\\hat{c}$ 是 approx. model 跟 target model 的運算量比. 每一次的 run, approx. model 會 auto-regressive $\\gamma$ 次, 所以是 $\\hat{T}\\hat{c}\\gamma$, 而 target model 會對 $\\gamma$ 個 proposal tokens parallel 去跑 $1$ 次, 注意到雖然是 parallel, 但總運算次數是正比於 proposal token 數量的 (只是並行跑), 所以花的運算量為 $\\hat{T}(\\gamma+1)$. 所以:&emsp;- speculative decoding 花了 $\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)$ 運算量產生 $\\mathbb{E}(\\# \\text{generated tokens})$ 個 tokens&emsp;- 只用 target model 花了 $\\hat{T}$ 的運算量產生 $1$ 個 token同樣只要知道 $\\mathbb{E}(\\# \\text{generated tokens})$ 我們可推得運算量的變化.PS: 注意到 prefix tokens 不會花到運算量因為 kv-cache 技巧, 所以考慮的時候可以忽略.$$\\begin{align} \\#\\text{OPs Increasing Ratio}=\\frac{\\text{Speculative decoding (\\#OPs per token)}}{M_p\\text{ decoding (\\#OPs per token)}} \\\\ =\\frac{(\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1))/\\mathbb{E}(\\#\\text{generated tokens})}{\\hat{T}/1} \\\\ = \\frac{\\hat{c}\\gamma+\\gamma+1}{\\mathbb{E}(\\#\\text{generated tokens})} \\end{align}$$ 數值愈高表示使用 speculative decoding 要花愈多 OPs 數 (運算量愈高) 平均生成的 Tokens 數 Proposal Tokens 被接受的機率 $\\beta_t,\\alpha$綜上所述, 需要先計算 $\\mathbb{E}(\\# \\text{generated tokens})$, 等同於要計算 token 的 accept 機率我們才能得知速度以及運算量的變化.將 proposal token $x_t\\sim q(x|x_1,...,x_{t-1})=:q_t(x)$ 被 speculative decoding 接受的機率定義為 $\\beta_t$.數學上可以麼表達 (為了清楚, 在沒有混淆情況下省略下標 $t$):$$\\begin{align} \\beta = \\mathbb{E}_{x\\sim q(x)} \\left\\{ \\begin{array}{rl} 1 &amp; q(x)\\leq p(x) \\\\ {p(x)\\over q(x)} &amp; q(x)&gt;p(x) \\end{array} \\right.\\\\ = \\mathbb{E}_{x\\sim q(x)} \\min\\left(1, {p(x)\\over q(x)}\\right)=\\sum_x\\min(p(x),q(x)) \\end{align}$$ 注意到 $\\beta_t$ 跟時間 $t$ 相關, 為了簡化, 論文假設 $\\beta_t,\\forall t$ 都是從一樣的 distribution sample 的 random variables.所以可以簡化為定義$$\\begin{align} \\alpha := \\mathbb{E}_t[\\beta_t]=\\sum_t\\sum_x\\min(p_t(x),q_t(x)) \\end{align}$$ 論文計算了不同 $M_q,M_p$ 之間的 $\\alpha$. 可以看到 $M_q$ model size 愈大 $\\alpha$ 愈高, 顯示愈匹配. 有趣的是, 以T5系列的 models 來看, $M_q$ 選擇 bi-gram 這種非常簡單的 LM $\\alpha$ 還有 $0.2$, 代表 bi-gram model 的 proposal tokens 平均5個有1個會被接受. 如果 approx. model 跟 target model 愈匹配的話, accept rate ($\\beta_t,\\alpha$) 就會愈高因此 $\\beta_t$ 或 $\\alpha$ 可以看成是小模型跟大模型的匹配程度. 但是再繼續之前, 我們必須先回顧一下幾何分佈 Geometric distribution with capped number of trails考慮一次測試 (trail) 的成功機率為 $\\theta$, 最多測試 $n$ 次 trails, random variable $X$ 代表要花多少次的 trails 才會至少成功一次. 注意到如果前 $n-1$ 次都 fail, 則強制最後第 $n$ 次一定成功.前 $n-1$ 次至少會 success 一次所需花的 trails 次數期望值為:&emsp;$1\\times\\text{第一次就成功的機率} + 2\\times\\text{第二次才就成功的機率} + ... + (n-1)\\times\\text{第}(n-1)\\text{次才成功的機率}$$$\\theta\\sum_{x=1}^{n-1}x(1-\\theta)^{x-1}=\\theta \\sum_{x=1}^{n-1}\\left( -\\frac{d}{d\\theta}(1-\\theta)^x \\right) \\\\ =-\\theta\\frac{d}{d\\theta}\\left(\\sum_{x=1}^{n-1}(1-\\theta)^x\\right) = -\\theta\\frac{d}{d\\theta}\\left( \\frac{(1-\\theta)(1-(1-\\theta)^{n-1})}{1-(1-\\theta)}\\right) \\\\ = -\\theta\\frac{d}{d\\theta}\\left( \\frac{(1-\\theta)-(1-\\theta)^n}{\\theta} \\right) \\\\ =-\\theta\\frac{\\theta(-1+n(1-\\theta)^{n-1})-(1-\\theta)+(1-\\theta)^n}{\\theta^2}\\\\ =\\frac{\\theta-n\\theta(1-\\theta)^{n-1}+(1-\\theta)-(1-\\theta)^n}{\\theta}$$ 加上 $n-1$ 次都 fail, 所以強制最後第 $n$ 次一定 success 的機率為 $(1-\\theta)^{n-1}$ 並乘上次數 $n$, 因此總體期望值為:$$\\mathbb{E}\\left[X\\right]= \\frac{\\theta-n\\theta(1-\\theta)^{n-1}+(1-\\theta)-(1-\\theta)^n}{\\theta} + n(1-\\theta)^{n-1} \\\\ =\\frac{1-(1-\\theta)^n}{\\theta}$$ 計算平均生成的 tokens 數$\\mathbb{E}(\\# \\text{generated tokens})$ 相當於要計算試驗次數有上限 (capped number of trails) 的 geometric distribution 的期望值.對應到 speculative decoding 的問題裡 $\\theta=1-\\alpha$, 且試驗次數最多 $\\gamma+1$ 次., 因此將 $\\theta = 1-\\alpha$, $n=\\gamma+1$ 代入得到:$$\\begin{align} \\mathbb{E}[\\#\\text{generated tokens}]=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha} \\end{align}$$ 論文把小模型與大模型的匹配程度 $\\alpha$ 跟 (10) 的關係畫出來: 我們發現 $M_q$ 與 $M_p$ 愈匹配的話, speculative decoding 一次 run 產生的 tokens 愈多 (很合理, 因為被接受的機率愈高)產生的 tokens 上限就是 $\\gamma+1$ ($\\gamma$ 個 proposal tokens 全被接受加上最後一個 $M_p$ 產生的 token) 待續 … References Google: Fast Inference from Transformers via Speculative Decoding [arvix] DeepMind: Accelerating Large Language Model Decoding with Speculative Sampling [arxiv] Speculative_sampling.drawio Speculative Decoding 詳讀 (下)","tags":[{"name":"Speculative Decoding","slug":"Speculative-Decoding","permalink":"https://bobondemon.github.io/tags/Speculative-Decoding/"},{"name":"Speculative Sampling","slug":"Speculative-Sampling","permalink":"https://bobondemon.github.io/tags/Speculative-Sampling/"},{"name":"Transformer","slug":"Transformer","permalink":"https://bobondemon.github.io/tags/Transformer/"}]},{"title":"AWQ 筆記","date":"2023-12-28T15:05:36.000Z","path":"2023/12/28/AWQ-筆記/","text":"如同 SmoothQuant 論文裡的圖, 在 memory size 已經跟不上算力和模型大小情況下, memory bandwidth 已經變成 bottleneck. 如何降低 memory 使用量將變的很關鍵, 因此 Activation-aware Weight Quantization (AWQ) 這篇文章就專注在 Weight Only Quantization (WOQ), 顧名思義就是 weight 使用 integer 4/3 bits, activations 仍維持 FP16.因為 computation is cheap, memory is expensive. Intel® Neural Compressor 有實作 WOQ 裡面有 AWQ 以下內容直接筆記 MIT SongHan 教授的課程內容[slides], [Video] 將 Weights quantize 到 4/3 bits 對 memory bandwidth 會有幫助, 但是直接使用 round-to-nearest (RTN) performance 會壞掉, 就算是使用 group-wise/block-wise 的方式也是沒用.作者發現如果保留特定的 $1\\%$ 的 weights 仍舊是 FP16 的話 (其餘都是 4/3 bits) 就可以保留住 performance. 如下圖顯示. 特定的 weights 是那些呢? 因為 output activations 是 input activations 乗上 weights, 所以應該要看 activations 不能只單獨考慮 weights 大小.還記得在 SmoothQuant 觀察到的現象嗎? activations 的 outliers 是以 per-channels 方式存在的, 也就是說 channels 之間差異可能很大, 但同一個 channel 內的值分佈都比較接近 圖中的 activation $X$ 的 row 表示 token (frame) 維度, column 表示 channel 維度. 所以對應到 weights 的話 input channel 就是 $W$ 的 row vectors. 要保留的那 $1\\%$ 的 row vectors 的 weights 就是找對應 $X$ 的 column vectors 總和 magnitude 比較大的那些來保留. 見下圖 (b) 但能不能連 FP16 都不要, 最好全部都是 INT 因為這樣對 HW 比較友好.作者發現透過一個簡單的 scaling 操作就有幫助 (其實概念一樣很像 SmoothQuant)類似 SmoothQuant 的方式, 先對 quantization 之前的 Weights 乘上 scale $s$, 對應的在 input activations $X$ 除上 $s$, 如果沒有做 quantization 數學上就是等價.下圖顯示對第 2 個 input channel 設定 $s=2$. 這麼做直接無損 performance. 但是為什麼呢? 原來 output activation 為 $$\\hat{Y}=Q(\\mathbf{w})\\cdot \\mathbf{x}=\\Delta\\cdot Round(\\mathbf{w}/\\Delta)\\cdot \\mathbf{x}$$ 現在改成:$$\\tilde{Y}=Q(\\mathbf{w}\\cdot s)\\cdot \\mathbf{x}/s=\\Delta\\cdot Round(s\\mathbf{w}/\\Delta)\\cdot \\mathbf{x}/s$$ 互相對比一下, 注意到由於 $\\mathbb{E}[Round(\\mathbf{w}/\\Delta)]=\\mathbb{E}[Round(s\\mathbf{w}/\\Delta)]=0.25$, 當 $s&gt;1$ 的時候 $\\tilde{Y}&lt;\\hat{Y}$, 使得 output activations 的 dynamic range 變小了, 等同於讓 outliers 變小更容易 quantization 了.注意到這裡有個假設: $\\Delta$ 不變的條件下. 這通常可以滿足, 因為實務上設定 $1 是 input activation 的 magnitude, $\\alpha\\in[0,1]$, $0$ 表示沒有 scale; $1$ 表示最強的 scale. Grid search 是對 $\\alpha$ 做. 最後實驗結果顯示對 LLMs, OpenFlamingo 做到 4/3bits 的 weights quantization 很有效: 這裡實驗如果是用 per-channel 會效果不好, 所以建議搭配 per-vector 或稱 per-group quantization. References AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, [arxiv] MIT HAN Lab, Course: TinyML and Efficient Deep Learning Computing [slides], [Video] Intel® Neural Compressor‘s WOQ","tags":[{"name":"Post Training Quantization (PTQ)","slug":"Post-Training-Quantization-PTQ","permalink":"https://bobondemon.github.io/tags/Post-Training-Quantization-PTQ/"},{"name":"Activation-aware Weight Quantization (AWQ)","slug":"Activation-aware-Weight-Quantization-AWQ","permalink":"https://bobondemon.github.io/tags/Activation-aware-Weight-Quantization-AWQ/"},{"name":"Weight Only Quantization (WOQ)","slug":"Weight-Only-Quantization-WOQ","permalink":"https://bobondemon.github.io/tags/Weight-Only-Quantization-WOQ/"}]},{"title":"SmoothQuant 筆記","date":"2023-12-28T12:59:28.000Z","path":"2023/12/28/SmoothQuant-筆記/","text":"這是 MIT SongHan 教授實驗室的論文, 使用 PTQ 對 LLM 做到 W8A8 的量化, 由於 activations 會有比較大的 outliers 導致 quantization 後損失較大, 而一般 weights 的 outliers 很少, 因此透過一些等價的轉換將 activations 的 scale 縮小並放大 weights 的 scale, 使得 activations 變的較容易 quant 而 weights 仍然容易 quant. 如論文的圖顯示: Quantization Granularity 先說明一下不同 quantization granularity, 其中&emsp;- Activation $X$ 的 row 是 token 維度, col 是 input channel 維度.&emsp;- Weight $W$ 的 row 是 input channel 維度, col 是 output channel 維度.&emsp;- $\\Delta_X$, $\\Delta_W$ 分別指的是 activation $X$ 和 weight $W$ 的量化參數 (scales, zero points) 可以看到 per-tensor 指的是整個 matrix 共享同一組量化參數而 per-token (per-frame) 則表示 $X$ 同一個 row 共享同一組量化參數; 同理 per-channel 是對 $W$ 的 output channel 同一個 column 共享同一組量化參數 GEMM 在對 $XW$ 矩陣乘法並行加速的時候, 對 $X$ 採用 row-major, $W$ 採用 col-major 則 output 每個 element 都可以獨立運算, 所以可以並行.這邊思考一個問題, 如果 $X$ 採用 per-channel (同一個 column 共享同一組量化參數), 在做 GEMM 時, $X$ 的一個 row 裡面每個元素都需要採用不同的量化參數, 這會破壞掉 GEMM 並行的好處.因此一般來說 $X$ 採用 per-token, 而 $W$ 採用 per-channel (output channel) 對 GEMM 比較友善. Motivation 實際觀察 $X$ 的分佈, 發現數值分佈的特性是 channel 內差異不大, 但 channel 之間的差異很大. 因此對 $X$ 來說採用 per-channel quantization 才是比較合適的, 但是從上一段我們知道 $X$ 採用 per-token 對 GEMM 才會比較友善. 那該怎麼辦? 這就是 SmoothQuant 要做的事, 降低 $X$ 的 outliers 使得可以仍採用 per-token. 如論文 Figure 4, 和講義說的 SmoothQuant 方法 這些 activations $X$ 的 outliers 都存在於某幾個特定的 channels, 跟哪一個 tokens 維度無關. 所以我們如果使用 per-channel quant, 則可以對 channels 的 scales 對應做個分配.其中 $X diag(s)^{-1}$ 可以把 $diag(s)^{-1}$ 融合進去前一層的 layer normalization 參數裡頭. 而 $diag(s)W$ 直接融進去 $W$ 的 scaling factor 裡.選擇 channel 的 re-scaling factor 如下: $$s_j=\\max(|X_j|)^\\alpha/\\max(|W_j|)^{1-\\alpha}$$ 通常 $\\alpha=0.5$ 是個很好的選擇 (控制 activation 還是 weight 量化難度的 trade-off), 但如果遇到 activation 的 outlier 比重占比較多的話 ($\\sim30\\%$), 可以選 $\\alpha=0.75$. 最後論文採用的 format 為: SmoothQuant 透過把 quantization 困難移轉到 weight 上, 所以 $X$ 仍可以使用 per-token(frame) 或甚至 per-tensor quant, 同時也不影響 GEMM 加速. 實驗結果 對 OPT-172B 能有效恢復 acc to FP16 水準, 同時需要的 GPU 減半, latency 也減少. 然後對更大的 model 也同樣有效, like MT-NLG 530B對 Llmma 同樣也是, 主要想看一下 SwishGLU, RoPE 這種不一樣的 op 對於 SmoothQuant 的假設是否一樣成立 References SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models, [arxiv] MIT HAN Lab, Course: TinyML and Efficient Deep Learning Computing [slides], [Video]","tags":[{"name":"Post Training Quantization (PTQ)","slug":"Post-Training-Quantization-PTQ","permalink":"https://bobondemon.github.io/tags/Post-Training-Quantization-PTQ/"},{"name":"SmoothQuant","slug":"SmoothQuant","permalink":"https://bobondemon.github.io/tags/SmoothQuant/"}]},{"title":"Qualcomm Data-Free Quantization 詳讀","date":"2023-11-24T15:20:05.000Z","path":"2023/11/24/Qualcomm-Data-Free-Quantization-詳讀/","text":"總歸來說 Data-Free Quantization (DFQ) 的目的是讓 floating model 做 weights 各種調整, 使得不管是 weights or activations 都變得適合 per tensor 量化.這樣理想上就不需用到 per channel 量化, 因為 per channel 雖然效果很好, 但硬體比較不友善, 且花的運算量較高. 另外 DFQ 屬於 Post-Training Quantization (PTQ) 方法. PTQ 對佈署到 edge 端很方便, 但一般來說 PTQ 都不如 Quantization-Aware Training (QAT) 的效果好, 因此 DFQ 嘗試提升效果. DFQ 共四步, 對照圖看, 需照順序: Cross-Layer Equalization (CLE): 輸入 fused BN 後的 float model $M_f^1$, floating 操作對 weights 做調整使得更均衡方便 per tensor 量化, 為 step 3 的前置作業, 輸出仍為 float model $M_f^2$. Bias Absorption (BA): 輸入 CLE 後的 float model $M_f^2$, floating 操作對 activations 做調整使得更均衡方便 per tensor 量化, 為 step 3 的前置作業, 輸出仍為 float model $M_f^3$. PTQ 量化: 輸入 CLE+BA 後的 float model $M_f^3$, 此時不管 weights or activations 都適合做 per-tensor 量化了, 所以直接 PTQ 輸出 int model $M_i^1$. Bias Correction (BC): 輸入 float model $M_f^1$ 和 step 3 的 $M_i^1$, 並且(option)給一些 unlabeled 的代表 data, BC 會對 $M_i^1$ 的 bias 參數補償因為量化造成的數值 mean 偏移, 輸出為最終 fixed point model $M_i^2$. Qualcomm AI Lab 的 tool AIMET 說 BC 這一步驟可以用 AdaRound (需要一小部分的 unlabelled training data) 取代 其實認真看完論文, 覺得限制有點多啊! 很多時候不能套 CLE, 有時 BA 也用不了. 把限制條列一下: ⚠️ CLE 限制:&emsp;1. Activation functions $f(\\cdot)$ 需為 piece-wise linear (e.g. ReLU, ReLU6, LeakyReLU, …)&emsp;2. 如果有 BN (Batch normalization) layer, 先把它 fuse 到 Conv 裡面, 所以第3點的限制才可以忽略 BN layer.&emsp;3. 相鄰的 layers 只能很單純是 $f(W^{(2)}f(W^{(1)}x+b^{(1)})+b^{(2)})$, 所以如果有 residual add 或 concat 才給 $W^{(2)}$ 作用的話就不行.⚠️ BA 限制:&emsp;1. activations 的每個維度是高斯分佈, 或能取得其分布, 例如透過 observer; 但在 AIMET 工具是假設有 BN 所以是高斯分布, 否則不用套用 BA&emsp;2. Activation functions $f(\\cdot)$ 需為 ReLU (or ReLU6), LeakyReLU 這種不行⚠️ BC 限制:&emsp;Empirical BC 需要給 representative data (可以是 unlabeled). 如果 Analytical BC (data-free) 則需有 BN —&gt; ReLU —&gt; Conv/FC 這樣順序的假設才能補償因 quantize 後 Conv/FC 這層輸出的 mean 偏移 接著我們描述一下 CLE, BA 和 BC 的動機, 然後再詳細介紹論文提出的這三個方法 Motivation CLE 動機Convolution kernels 在不同 output channels 來看, weights 的分佈有些很大有些很小, 這使得用統一一個 quantization parameter set 會不好. 所以如果能事先讓 weights 在不同 channel 的數值分佈接近, 這樣就適合用 per tensor quantization 了. 為此作者提出 Cross-Layer Equalizaiton (CLE) 方法. 圖來源為 AIMET Post-Training Quantization Techniques BA 動機不過做了 CLE 有個 side-effect 就是讓 activations 有可能反而不同 channels 分佈變的更不同, 為此作者提出 Bias Absorption (BA) 方法使得 activations 同樣適合 per-tensor quant. BC 動機另一方面, 其實 weights or input activations 經過 quantization 後, output activations 理想上希望是 un-biased, 但實際上都會有 bias, 如下圖 $$\\begin{align} \\mathbb{E}[\\tilde{y}_j-y_j]\\approx{1\\over N}\\sum_n\\left(\\tilde{W}x_n\\right)_j - \\left(Wx_n\\right)_j \\end{align}$$ 其中 $\\tilde{W},\\tilde{y}$ 分別是 quantized weight and output activation. 所以作者提出使用 Bias Correction (BC) 技巧來彌補. Data Free Quantization (DFQ) 詳細解釋 Cross-Layer Equalization (CLE), 幫助 weights per-tensor 量化對任何 $s&gt;0$, 且 $f(\\cdot)$ 是 piece-wise linear activation function:$$\\begin{align} f(x)=\\left\\{ \\begin{array}{rl} a_1x+b_1 &amp; \\text{if }x\\leq c_1 \\\\ a_2x+b_2 &amp; \\text{if }c_1&lt;x\\leq c_2 \\\\ \\vdots \\\\ a_nx+b_n &amp; \\text{if } c_{n-1}&lt;x \\\\ \\end{array} \\right. \\end{align}$$ 則我們可以找出等價的 $\\hat{f}(\\cdot)$ 使得 $f(sx)=s\\hat{f}(x)$: 設定 $\\hat{a}_i=a_i$, $\\hat{b}_i=b_i/s$ and $\\hat{c}_i=c_i/s$.這麼做有什麼好處呢? 考慮以下的情形給定兩個相鄰的 layers: $h=f(W^{(1)}x+b^{(1)})$ 和 $y=f(W^{(2)}h+b^{(2)})$, 其中 $f$ 是 piece-wise linear activation function.則我們有: $$\\begin{align} y=f(W^{(2)}f(W^{(1)}x+b^{(1)})+b^{(2)}) \\\\ =f(W^{(2)}S\\hat{f}(S^{-1}W^{(1)}x+S^{-1}b^{(1)})+b^{(2)}) \\\\ =f(\\hat{W}^{(2)}f(\\hat{W}^{(1)}x+\\hat{b}^{(1)})+b^{(2)}) \\end{align}$$ 其中 $S=\\text{diag}(s)$ 表示對角矩陣, $S_{ii}$ 是 neuron $i$ 的 scaling factor $s_i$. 就是靠這 $s$ 來調節 weights 分布.所以我們重新縮放了 weights: $\\hat{W}^{(2)}=W^{(2)}S$, $\\hat{W}^{(1)}=S^{-1}W^{(1)}$ and $\\hat{b}^{(1)}=S^{-1}b^{(1)}$.那麼怎麼設定最佳的 $S$ 呢? 理想上, 透過 $S$ 我們希望將 $\\hat{W}^{(1)}, \\hat{W}^{(2)}$ 變成適合 per tensor quantization.&emsp;- 定義 $r_i^{(1)}:=\\max(W_{i,:}^{(1)})$, 即為 $W^{(1)}$ 的 $i^{th}$ row vector 取 max.&emsp;- 同理 $\\hat{r}_i^{(1)}:=\\max(\\hat{W}_{i,:}^{(1)})=r_i^{(1)}/s_i$.類似地我們定義&emsp;- $r_j^{(2)}:=\\max(W_{:,j}^{(2)})$, 即為 $W^{(2)}$ 的 $j^{th}$ column vector 取 max.&emsp;- $\\hat{r}_j^{(2)}:=\\max(\\hat{W}_{:,j}^{(2)})=s_j\\cdot r_j^{(2)}$.注意到一個是 row vector 另一個是 column vector 這是因為 $W^{(1)}$ 的 row vector 對應的是 $W^{(2)}$ 的 column vector. 即第一層 layer 的 output channel 對應的是第二層 layer 的 input channel 的概念然後再令整個 weight matrix 的最大值為: $R^{(1)}:=\\max_i(r_i^{(1)})$ 和 $R^{(2)}:=\\max_j(r_j^{(2)})$大概示意圖長這樣子 最後就可以定義每一個 channel (1~m) 對於整個 weight matrix 的占比:$p_i^{(1)}=r_i^{(1)}/R^{(1)}$; $\\hat{p}_i^{(1)}=\\hat{r}_i^{(1)}/\\hat{R}^{(1)}$; 同理 $p_j^{(2)},\\hat{p}_j^{(2)}$到這裡不難理解, 只是很多 terms 要消化一下而已$p_i^{(1)}$ 表示 $i^{th}$ row vector 對整個 matrix $W^{(1)}$ 的佔比, 想像上如果每個 rows 的佔比都很大, 那就整體適合 per-tensor quantization.可以想像, 若 $\\hat{p}_i^{(1)}$ 比 $p_i^{(1)}$ 大表示 $i^{th}$ row vector 的佔比經過 $s_i$ 的調整變大, 但由於 $s_i$ 在 $W^{(1)}$ 用除的但在 $W^{(2)}$ 用乘的, 導致 $\\hat{p}_i^{(2)}$ 比 $p_i^{(2)}$ 小了, 意思是 $i^{th}$ column vector 的佔比反而變小. 所以一邊變大了但反而使另一邊變小了, 這一定是個 trade-off.所以我們希望兩邊都顧到 ($\\hat{p}_i^{(1)} \\hat{p}_i^{(2)}$ 一起考慮) , 作者就定義了這樣的目標函式: $$\\begin{align} \\max_S \\sum_i \\hat{p}_i^{(1)} \\hat{p}_i^{(2)} \\end{align}$$ 調整 $S$ 使兩邊 matrix $W^{(1)},W^{(2)}$ 的占比都要顧到, 找出使得總佔比量最大的 $S$.這個問題的最佳解在論文的 Appendix A 有證明, 我們先把解寫出來: $$\\begin{align} s_i=\\frac{1}{r_i^{(2)}}\\sqrt{r_i^{(1)}r_i^{(2)}} \\end{align}$$ 這樣的 $s_i$ 會使得 $\\hat{r}_i^{(1)}=\\hat{r}_i^{(2)}$, $\\forall i$. 把 $s_i$ 代到 $\\hat{r}_i^{(1)}$ and $\\hat{r}_i^{(2)}$ 就知道了. (這裡原論文寫 $r_i^{(1)}=r_i^{(2)}$ 應該是 typo)詳細證明記錄在最後的 Appendix (論文證明有些沒懂補充一下自己想法). Bias Absorption (BA), 幫助 activation per-tensor 量化再說之前, 先了解以下範例.首先對於 ReLU $r(\\cdot)$ 來說一定存在一個 non-negative vector $c$ 使得 $\\forall x$ $$r(Wx+b-c)=r(Wx+b)-c; \\quad \\forall x \\qquad\\qquad (\\star)$$ $c=0$ 就是一個 trivial 解.舉一個簡單範例, 考慮某一個 channel $i$, data $Wx_i$ 的機率分佈為直角三角形: 當 $b=3$ 的情況時, 則選 $c=0.5$ 滿足 $(\\star)$ 條件, 見下圖: 這個情況會滿足所有 $x$, 但如果 $Wx$ 的分布不像範例一定大於某一個值 (想像上面的直角三角形分布變成高斯分佈) 則我們只能選擇滿足大部份的值 如果是高斯分佈的話 (則 Batch norm 的 mean, std 就可拿來用), 論文選擇 3 個標準差所以保證 99.865% 滿足. 高斯分佈在 $\\mu\\pm3\\sigma$ 內的機率約為 $0.9973002$ [ref], 但由於我們要找的 $c$ 只會忽略 $&lt;\\mu-3\\sigma$ 的情況所以是 $1-(1-0.9973002)/2\\approx99.865$, 之後會有圖示比較清楚 有了以上概念後, 回頭過來看看經過 CLE 後還會發生什麼現象, 其中 $r(\\cdot)$ 是 ReLU.(突然渲染不出數學式子…煩阿) $\\hat{W}^{(1)}$ and $\\hat{W}^{(2)}$ 已經被 CLE 調整一波後數值分佈變得很接近 (適合 per-tensor quantization 👏🏻)但 $\\hat{b}^{(1)}=S^{-1}b^{(1)}$, 當 $s_i&lt;1$ 的時候會讓 channel $i$ 的 activation 放大導致 activations, $\\hat{W}^{(1)}x+\\hat{b}^{(1)}$, 的各 channel 之間分佈位置會不同, 因此也會讓 activations 不好做 quantization!利用上面說的概念我們這樣推導: 其中 $b^{\\star(1)}=\\hat{b}^{(1)}-c$ 和 $b^{\\star(2)}=\\hat{W}^{(2)}c+b^{(2)}$.💡 目的是把 $\\color{orange}{\\hat{W}^{(1)}x+\\hat{b}^{(1)}}$ 從不適合做 per-tensor quant 變成 $\\color{orange}{\\hat{W}^{(1)}x+b^{\\star(1)}}$ 容易做 per-tensor quant.則 $c$ 可以選擇盡量滿足所有 $\\hat{W}^{(1)}x+\\hat{b}^{(1)}$ 的值, 要這麼做最暴力的方式是餵所有 training data 去看資料分布, 選擇滿足大部分的情況, 例如滿足 99.99% 的數值.另外如果我們知道 $\\hat{W}^{(1)}x+\\hat{b}^{(1)}$ 會再經過 Batch normalization, i.e. $BN(\\hat{W}^{(1)}x+\\hat{b}^{(1)})$ 只是 BN 忽略不寫而已, 則令 $c=\\max(0,\\beta-3\\gamma)$, 其中 $\\beta,\\gamma$ 分別是 Batch normalization 的 shift and scale parameters, 這樣直接就滿足大於-3標準差的 99.865% 機率了. 開頭的 DFQ 流程圖有先做 BN folding, 所以此時的 $\\tilde{W}^{(1)}$ 已經是 folding 後的, 因此要事先把 $\\beta,\\gamma$ 存起來才能在這步驟用 我們來思考為啥 activations 從 $\\hat{W}^{(1)}x+\\hat{b}^{(1)}$ 變成 $\\hat{W}^{(1)}x+b^{\\star(1)}$ 後就會比較好做 per-tensor quantization, 這是因為我們選擇的這些 $c_i$ 會讓維度 $i$ 的 activation 對齊到剛好有 99.865% 大於 0, 而每個維度都依這樣的標準 align 自然就容易對整個 activations 做 quantization 了 (不需要 per-channel quant 了)!圖示一下上面的意思, 為了方便令 $\\hat{k}=\\hat{W}^{(1)}x+\\hat{b}^{(1)}$, 其中 $\\hat{k}_i$ 表示第 $i$ 維, 同理 $k^{\\star}=\\hat{W}^{(1)}x+b^{\\star(1)}$ 和 $k^\\star_i$: 注意到雖然 activations $k^\\star$ 適合 per-tensor quant 了, 但我們只是把這困難 pass 到 $b^{\\star(2)}$, 為啥這麼說呢? 因為 $b^{\\star(2)}$ 需要多加一項 $\\hat{W}^{(2)}c$, 但我們並不做任何保證 ,因此 activations $z$ (看式 (8))仍然有可能每個 channel 維度分佈位置也都不同, 所以實務上採取 layer 1 and 2 做完, 再做 layer 2 and 3, 依此列推下去. Bias Correction (BC)如同在 motivation 稍微提到的, 令 $\\epsilon=\\tilde{W}-W$ 是 quantization error, $\\tilde{W}$ 是 quant 後的參數. 且令 $y=Wx,\\tilde{y}=\\tilde{W}x$, 分別是 quant 前後的 output activations, 則我們有 $\\tilde{y}=y+\\epsilon x$.由於 quantization 後可能 activations 的分布 mean 值不會跟原來一樣, i.e. 可能會 $\\mathbb{E}[\\epsilon x]\\neq0$, 但可以透過下式被矯正回來: $\\mathbb{E}[y]=\\mathbb{E}[\\tilde{y}]-\\epsilon\\mathbb{E}[x]$所以只需要對 quant 完的 output 加上 $-\\epsilon\\mathbb{E}[x]$, 但實務上不會這麼做, 而是做在 bias parameter 裡 (bias 加上 $-\\epsilon\\mathbb{E}[x]$).不過我們怎麼會知道 input activation 的期望值, $\\mathbb{E}[x]$?做完上述 CLE + bias absorption 並得到量化 model 後跟原本 float model 比較可以得到 $\\epsilon$, 如果有 representative data (可以是 unlabeled) 情況下, 則丟 data 去計算 $\\mathbb{E}[x]$ 就可以了. 注意要按照 layer 做, 也就是做 $l^{th}$ layer 的 BC 項時, 假設 $1, 2,..,l-1$ layer 的 BC 項都 apply 上去了. 這叫做 Empirical Bias Correction, 詳見論文 Appendix D. (圖來源為 AIMET Post-Training Quantization Techniques)但論文標題是 “Data-free”, 怎麼辦呢? 此時論文要求要有這樣的 blocks 關聯: 已知目前要處理的 layer 是 $\\tilde{y}=\\tilde{W}x$. 論文假設此 layer 之前還有 BN and ReLU 兩個 blocks. 注意到需有這樣的關聯存在才可以.而 $\\mathbb{E}[x]$ 可以利用 BN 後 $x^{pre}$ 是 normal distribution 的特性來算. 注意到經過 ReLU 後的 $x$ 變成 clipped normal distribution, 而其 mean 可以利用 BN 的 shift and scale parameters 寫出 closed form 解.詳細直接參考論文, Appendix C 有推導. 這樣的做法稱 Analytical Bias Correction. (圖來源為 AIMET Post-Training Quantization Techniques) Experiments 由於 CLE and BA 目的是讓後面的 quantization 比較適合 per-tensor, 所以要觀察以下兩點:&emsp;1. 用了 CLE and/or BA 後, 由於輸出還是 float model, 那跟用之前的 float model 有無 performance 影響?&emsp;2. 用了 CLE and/or BA 後, 再用了 per-tensor 量化後, 能否逼近原本 float model (沒用 CLE/BA) 的 per-channel 量化?結果 Table 1 顯示以上兩點都沒問題. 再來如果加入 BC 則觀察能否補償因 quantization 造成的 mean 偏移損失? 其中可以看 quantization model 有無套用 CLE+BA.結果如 Table 2: Original model 直接硬做 PTQ to INT8 是慘不忍睹的 random 行為, 但直接加上 BC 補償後竟然就回到 52.02%!如果先用 CLE+BA 在量化到 INT8, performance 為 Table 1 的最佳 70.92%. 這種情況再加上 BC 還能提升一點點 (多少表示可能還是存在一點點的 mean 偏移)Clip@15 這個方法是直接對 weights 砍到 [-15, 15] 區間, 跟 CLE 目的一樣只是直接粗暴, 當然 BC 就能發揮更好的作用 (2.55% —&gt; 70.43%).剩下的實驗就不細說. AIMET Quantization Flow 以下為 AIMET AutoQuant 建議的量化流程, 總結得很不錯: 圖中的 CLE 我猜已經包含 BA 了, 然後可以看到沒有 BC, 因為被 AdaRound 取代掉也注意到在給 CLE 之前要先做 BatchNorm folding (如同我們在講 CLE 的限制 2) 流程就是建議先對 floating model 插好 fake quant op 來模擬 target HW 的 operators 行為 (QuantScheme Selection 那步). 先看看效果如何, 如果 OK 那 PTQ/QAT 都不需要.接著才確認 BN folding 是否能幫助提升效果? 不行的話套看看 PTQ 的 CLE (w/wo AdaRound). 再不行就要走 QAT 了. 到這終於紀錄完, 這篇初看感覺應該可以很快看完, 一讀才發現細節真的有夠多, 頗不容易. 也因為很認真細讀才發現其實有不少限制. 不過還是很有收穫拉~總之恭喜讀者(自己?)有耐心看完(寫完). ~~ 撒花收工 ~~ Appendix 證明 CLE 的最佳解 Render 爛掉了, 直接怒貼圖… References Data-Free Quantization Through Weight Equalization and Bias Correction, [arxiv] Up or Down? Adaptive Rounding for Post-Training Quantization, [arxiv] AI Model Efficiency Toolkit (AIMET)","tags":[{"name":"Post Training Quantization (PTQ)","slug":"Post-Training-Quantization-PTQ","permalink":"https://bobondemon.github.io/tags/Post-Training-Quantization-PTQ/"},{"name":"Quantization Aware Training (QAT)","slug":"Quantization-Aware-Training-QAT","permalink":"https://bobondemon.github.io/tags/Quantization-Aware-Training-QAT/"},{"name":"Data Free Quantization (DFQ)","slug":"Data-Free-Quantization-DFQ","permalink":"https://bobondemon.github.io/tags/Data-Free-Quantization-DFQ/"},{"name":"AIMET","slug":"AIMET","permalink":"https://bobondemon.github.io/tags/AIMET/"}]},{"title":"Quantization Error (Case with Clipping)","date":"2023-11-04T02:57:38.000Z","path":"2023/11/04/Quantization-Error-Case-with-Clipping/","text":"上一篇文章我們提到, uniformly constrained quantizer 有這樣的 quantization error:$$\\begin{align} J=s_{\\text max}^2{4^{-B}\\over 3} \\end{align}$$ 其中 $s_{\\text {max}}$ 表示 input $x$ 在 $[-s_{\\text {max}}, s_{\\text {max}}]$之間.這麼做雖然能確保所有 $x$ 都不會發生 clipping error, 但如果有一些 outlier 則會使得 quantization step 變很大 (quantization resolution 變低), 因此 quantization 的離散化誤差 (discretization error) 變大. Quantization error = (Discretization error) + (Clipping error) 舉例來說, 考慮下圖 (ref. from SongHan course EfficientML.ai Lecture 6):上圖左是 clipping scalar 設定很大, 上圖右則是設定很小. 可以看見 discretization error 跟 clipping error 互為 trade-off. 那麼問題來了, 怎麼設定 clipping scalar, 才會使得整體的 quantization error 最小?這篇文章 “Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training” [arxiv] 給出了理論值, 並使用 Newton’s method 幫助我們很快找到最佳解. Empirical Quantization Error Fake quantization 的過程為:$$\\begin{align} \\mathcal{Q}(x; s) = \\text{clip}\\left( s\\cdot 2^{1-B}\\cdot \\text{round}\\left(x\\cdot 2^{B-1}/s\\right), -s, s \\right) \\end{align}$$ 其中 $B$ 表示我們使用的 quantization bit 數, $s$ 為 clipping scalar. 這裡假設使用 symmetric quantization, i.e. zero point = 0.因此 empirical 的 error 可以直接計算如下:$$\\begin{align} J_{em}(s)=\\mathbb{E}\\left[(\\mathcal{Q}(X; s)-X)^2\\right] \\end{align}$$ 我們對 resnet-50 的 layer #17, #45 的 weights 計算 empirical quantization error. (為了驗證論文裡的 Figure 1 (a))先看一下 layers #17, #45 的 weight 分佈:計算 $J_{em}(s)$ 使用 $B=4$-bits 得到如下結果:這個 error 曲線看起來很棒啊, 如果是 convex function 則可以很有效率地找到 clipping scalar 的最佳解.理解一下這個曲線不難發現當 clipping scalar 很小, error 會上升是因為主要來源來自於 clipping error.但當 clipping scalar 變大, 則 clipping error 會變小但是 discretization error 變大, 因此才會有一個甜蜜點是最小值.計算 empirical error 主要函式如下: 12345678910111213141516def cal_qerror(w, qstepsize, zero, bit_num): quant_min, quant_max = -(2 ** (bit_num - 1)), 2 ** (bit_num - 1) - 1 w_q = torch.fake_quantize_per_tensor_affine(torch.as_tensor(w), qstepsize, zero, quant_min, quant_max).numpy() return w - w_qdef do_empirical_qerror_scanning(w, qstepsize, zero, bit_num=4, scalar_num=200, plot_ratio=7.0): # `qstepsize` stands for quantization step size qerrors = [] clipping_scalars = np.linspace(1e-2, qstepsize * plot_ratio, scalar_num) # for loop for each clipping scalar for cs in clipping_scalars: qerror = cal_qerror(w, 2 * cs / 2**bit_num, zero, bit_num) qerrors.append(np.mean(qerror**2)) return qerrors, clipping_scalars Theoretical Quantization Error Quantization 的 MSE 我們可以拆成兩部分:$$\\begin{align} J_{th}(s)={4^{-B}\\over 3}s^2\\int_0^s f_{|X|}(x)dx + \\int_s^\\infty (s-x)^2 f_{|X|}(x)dx \\end{align}$$ 其中 $f_{|X|}(\\cdot)$ 表示 $|X|$ 的 distribution 分佈.R.H.S. 的第一、二項分別是 discretization 和 clipping error, 應該算好理解.只是特別說明一下之前推導的 discretization error 是基於 error $(\\mathcal{Q}(x; s)-x)$ 為 uniformly distributed. (請參考上一篇文章). 如果不同的 $f_{|X|}(\\cdot)$, 是否對於 “$(\\mathcal{Q}(x; s)-x)$為 uniformly distributed” 這個假設就不成立呢?我認為如果 quantization resolution 夠高 (切得夠密), 則 error 的數值其分佈應該會接近 uniformly distributed. 論文裡有這麼一段話: For discretization noise, the term ${s^24^{-B}}/3$ does not require a priori knowledge of data distribution. It is obtained through sampling theory where quantization noise arises via approximating the neighborhood of a quantization level of any distribution as a local rectangle (Widrow &amp; Kollar´ , 2008, book: Quantization noise) Anyway, 我們可以對上式改寫如下:$$\\begin{align} J_{th}(s)= {4^{-B}\\over3}s^2\\mathbb{E}\\left[\\mathbf{1}_{\\{|X|\\leq s\\}}\\right] + \\mathbb{E}\\left[(s-|X|)^2\\mathbf{1}_{\\{|X|&gt;s\\}}\\right] \\\\ = J_1(s) + J_2(s)\\\\ \\end{align}$$ 其中 $\\mathbf{1}$ 是 indicator function, 注意到 $\\mathbf{1}_{\\{|X|\\leq s\\}}$ 的變數是 $s$, 對於固定的 $X$, $\\mathbf{1}_{\\{|X|\\leq s\\}}$ 是個 step function, 只有當 $s\\geq|X|$ 的時候 function 值才會是 $1$, 其他情況是 $0$. 此 step function 的微分為 0 almost everwhere. (數學語言是 “微分不為 0 的集合, 該集合的測度為 $0$”)我們使用 np.histogram 來畫出 theoretical quantization error, 來跟 empirical 比較:可以發現 theoretical and empirical error curves 很接近! 👏 數學真漂亮!計算 theoretical error 主要函式如下: 123456789101112131415def theoretical_mse_qerror(w, clipping_scalar, bit_num, bins=500): hist, bin_edges = np.histogram(np.abs(w), bins=bins, density=False) hist = hist / np.sum(hist) # turn into probability mass (note that it is different with density) clip_start_idx = np.where(np.diff(bin_edges &gt; clipping_scalar))[0] clip_start_idx = 0 if len(clip_start_idx) == 0 else clip_start_idx[0] J1 = np.sum(hist[:clip_start_idx]) * (clipping_scalar**2 / (3 * 4**bit_num)) J2 = 0.0 for idx in range(clip_start_idx, len(hist)): prob_x_mass = hist[idx] x = (bin_edges[idx + 1] + bin_edges[idx]) / 2 J2 += (clipping_scalar - x) ** 2 * prob_x_mass return J1 + J2 找出最佳 Clipping Scalar 我們計算一下 $J_1$ 的 gradient:$$\\begin{align} J_1&apos;(s)={4^{-B}\\over3}\\cdot2 s\\cdot\\mathbb{E}\\left[\\mathbf{1}_{\\{|X|\\leq s\\}}\\right] + {4^{-B}\\over3}s^2\\frac{\\partial}{\\partial s}\\mathbb{E}\\left[\\mathbf{1}_{\\{|X|\\leq s\\}}\\right] \\\\ = {4^{-B}\\over3}\\cdot2 s\\cdot\\mathbb{E}\\left[\\mathbf{1}_{\\{|X|\\leq s\\}}\\right] + {4^{-B}\\over3}s^2\\mathbb{E}\\left[\\frac{\\partial}{\\partial s}\\mathbf{1}_{\\{|X|\\leq s\\}}\\right] \\\\ ={4^{-B}\\over3}\\cdot2 s\\cdot\\mathbb{E}\\left[\\mathbf{1}_{\\{|X|\\leq s\\}}\\right] + 0 \\end{align}$$ (7) 到 (8) 是因為 expectation 的變數為 $X$ 跟 $s$ 無關, 所以微分跟 expectation 可以互換.(9) 是因為之前說過, 因為 $\\mathbf{1}_{\\{|X|\\leq s\\}}$ 是 step function, 所以其 gradient 為 $0$ almost everwhere.同理 $J_2$ 的 gradient:$$\\begin{align} J_2&apos;(s) = \\frac{\\partial}{\\partial s}\\mathbb{E}\\left[ (s-|X|)^2\\mathbf{1}_{\\{|X|&gt;s\\}} \\right] = \\mathbb{E}\\left[ \\frac{\\partial}{\\partial s} \\left( (s-|X|)^2\\mathbf{1}_{\\{|X|&gt;s\\}} \\right) \\right] \\\\ =\\mathbb{E}\\left[ 2(s-|X|)\\mathbf{1}_{\\{|X|&gt;s\\}} + (s-|X|)^2\\frac{\\partial}{\\partial s}\\mathbf{1}_{\\{|X|&gt;s\\}} \\right] \\\\ = \\mathbb{E}\\left[ 2(s-|X|)\\mathbf{1}_{\\{|X|&gt;s\\}} + 0 \\right] \\end{align}$$ (12) 是因為之前說過, 因為 $\\mathbf{1}_{\\{|X|&gt; s\\}}$ 是 step function, 所以其 gradient 為 $0$ almost everwhere.所以 $J_{th}$ 的 gradient:$$\\begin{align} J_{th}&apos;(s)= {4^{-B}\\over3}\\cdot2 s\\cdot\\mathbb{E}\\left[\\mathbf{1}_{\\{|X|\\leq s\\}}\\right] + \\mathbb{E}\\left[ 2(s-|X|)\\mathbf{1}_{\\{|X|&gt;s\\}} \\right] \\end{align}$$ 同樣的推導 $J_{th}&apos;&apos;$ 為:$$\\begin{align} J_{th}&apos;&apos;(s) = {4^{-B}\\over3}\\cdot2 \\cdot\\mathbb{E}\\left[\\mathbf{1}_{\\{|X|\\leq s\\}}\\right] + 2\\mathbb{E}\\left[\\mathbf{1}_{\\{|X|&gt;s\\}}\\right] \\end{align}$$ 因此根據 Newton’s method, $s_{n+1}=s_n-J_{th}&apos;(s)/J_{th}&apos;&apos;(s)$, 得到:$$\\begin{align} s_{n+1}=\\frac{\\mathbb{E}\\left[|X|\\cdot\\mathbf{1}_{\\{|X|&gt;s_n\\}}\\right]} { {4^{-B}\\over3}\\mathbb{E}\\left[\\mathbf{1}_{\\{|X|\\leq s_n\\}}\\right] + \\mathbb{E}\\left[\\mathbf{1}_{\\{|X|&gt;s_n\\}}\\right] } \\end{align}$$ 實務上 Newton’s method 很 robust, initial $s_1$ 選擇 $\\{0,s_{max},3\\sigma,4\\sigma,5\\sigma\\}$ 都可以有效收斂. 論文裡直接選擇 $s_1=({\\sum_x|x|})/(\\sum_x\\mathbf{1}_{|x|&gt;0})$, 相當於 $s_1=s_{max}$ iterates 到 $s_3$ 的情況. 我們實際用 Newton’s method 設定 s_init=0.0 和 10 次 iteration 的結果如下:確實能找出最佳的 clipping scalar. 到這裡算是複現了論文裡的 Figure 1 (a) 了.計算 optimal clipping scalar 主要函式如下: 1234567891011def find_opt_by_Newton_method(weights, bit_num, cs_init=0.0, iter_num=10): # `cs` stands for `clipping scalar` weights_abs = np.abs(weights) cs_cur = cs_init for itr in range(iter_num): indicator_larger = weights_abs &gt; cs_cur indicator_smaller = weights_abs &lt;= cs_cur # should we ignore case with `==0`? numerator = np.sum(weights_abs[indicator_larger]) denominator = np.sum(indicator_smaller) / (3 * 4**bit_num) + np.sum(indicator_larger) cs_cur = numerator / denominator return cs_cur Short Summary 常用的 uniform quantization 包含兩個參數 scale and zero_point, 一般可以使用 observer 來統計出數值分佈的最大最小值進而得到 clipping scalar (通常會搭配 moving averaging 來減緩 outlier 的影響).但這樣得到的 quantization error 沒有辦法保證是最小的.本文介紹的這篇論文把 quantization error 的理論值找出來, 並使用 Newton’s method 非常有效率的找出最佳 clipping scalar. 甚至可以鑲嵌在 QAT iteration 中.另一方面, 這篇論文找的最佳解跟任務的 loss function $\\mathcal{L}$ 無關. 如果希望跟 loss function 有關, 可以考慮使用 LSQ+ 或 PACT 的方式來學習出 scale and zero_point.總之這篇論文讓我們對 uniform quantization 的 error 有了更深入的理解, 也很漂亮得提供了高效求解方法. References Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training, [arxiv] SongHan course EfficientML.ai Lecture 6 複現論文 Figure 1 (a) 的 [Github] Learning Zero Point and Scale in Quantization Parameters [link]","tags":[{"name":"Quantization Error","slug":"Quantization-Error","permalink":"https://bobondemon.github.io/tags/Quantization-Error/"},{"name":"Linear Quantization","slug":"Linear-Quantization","permalink":"https://bobondemon.github.io/tags/Linear-Quantization/"},{"name":"Nonlinear Quantization","slug":"Nonlinear-Quantization","permalink":"https://bobondemon.github.io/tags/Nonlinear-Quantization/"},{"name":"OCTAV","slug":"OCTAV","permalink":"https://bobondemon.github.io/tags/OCTAV/"}]},{"title":"Quantization Error (Case without Clipping)","date":"2023-10-28T10:05:30.000Z","path":"2023/10/28/Quantization-Error-Case-without-Clipping/","text":"我在閱讀這篇論文: “Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training” [arxiv] 的時候, 看到這個式子說明 uniform constrained quantizer 有這樣的 quantization error:$$\\begin{align} J=s_{\\text max}^2{4^{-B}\\over 3} \\end{align}$$ 當下看得我一頭霧水, 後來查了資料才了解這個 quantization error 的推導, 因此筆記一下. [來源1], [來源2] 這裡要特別說明一下, 這邊的 quantization error 沒有考慮超過最大最小值造成的 clipping error. 將 clipping error 一起考慮是開頭說的那篇論文會探討的情況. 這樣的 quantization error 分析在傳統訊號處理可以看到, 例如 analog 訊號經過 ADC 變成 digital 訊號後會有 quantization 損失. 如果 quantization bit 增加 1 bit 則 SNR 增加約 6dB. 又如果採用 nonlinear quantization 則對音量較低的情況其 SNR 提昇會比 linear quantization 好. Nonlinear quantization 又分 $\\mu$-law (北美 and 日本) 和 A-law (歐洲 and 其他). 這些內容在下面的筆記都會解釋. Let’s go~ Uniform Quantization 令 quantization step size 為 $\\Delta v=s_{\\text max}/2^{B-1}$, 其中 $B$ 為 bit 數, 數值範圍在 $[-s_{\\text max},s_{\\text max}]$ 之間. 則 input $x$ 和 quantized $x_q$ 的關係如下圖 (圖片裡的 $m_p=s_{max}$): [來源1]我們將 quantization error $q=x-x_q$ 畫出來則如下圖: [來源1]所以 quantization error $q:=x-x_q$ 數值範圍分布在 $[-\\Delta v/2, \\Delta v/2]$ 之間. 到這邊應該都滿清楚的.此時做了一個假設, 假設 $q$ 的分布是 uniform distribution, 所以 power of $q$ 的期望值為:$$\\begin{align} P_q=\\int_{-{\\Delta v}/2}^{\\Delta v/2} q^2{1\\over \\Delta v}dq \\\\ = {1\\over\\Delta v}\\left[{q^3\\over3}\\right]_{-{\\Delta v}/2}^{\\Delta v /2}=...= {\\color{orange}{(\\Delta v)^2\\over 12}} \\\\ = \\frac{(s_{\\text max}/2^{B-1})^2}{12} = s_{\\text max}^2\\frac{1}{12\\cdot2^{2(B-1)}} = {\\color{orange}{s_{\\text max}^2\\frac{4^{-B}}{3}}} \\end{align}$$ 開頭那個奇怪的式子就是這麼來的. 另外 SNR 可以這麼表示:$$\\begin{align} \\text{SNR}=\\frac{\\text{Signal Power}}{\\text{Noise Power}} = \\frac{P_s}{P_q}=10\\log_{10}\\left(\\frac{3\\cdot4^B}{s_{max}^2}P_s\\right) \\\\ =10\\log_{10}\\left(3P_s/s_{max}^2\\right) + 20B\\log_{10}(2) \\approx \\alpha + 6B \\end{align}$$ 其中 $\\alpha$ 與 signal power 有關, 可以發現如果增加 1 bit 的表示能力, SNR 能提升約 6dB. Non-uniform Quantization 另外考慮到一般訊號數值大的只占少部分, $s_{max}$ 容易被 outlier 影響, 因此 quantization error 就會比較大. 如果說我們先將訊號做 nonlinear 壓縮 (compresser), i.e. 數值大的會被加比較多, 數值小的壓一點就好 (見下圖), 這樣數值間的差異變小後, 再經過 linear quantization 的話, quantization error 就不會那麼大了.相對的解碼的時候要做擴展 (expander).由於在 transmitter/receiver 端我們會做 compress/expand, 所以我們稱為 compander = compresser + expanderTelephone system (北美和日本): $\\mu=100$ for 7-bits (128 levels) $\\mu=255$ for 8-bits (256 levels) 而在歐洲和其他地方 $A=87.7$ or $87.6$. References Ali Muqaibel: 6.4.1 Quantization, Part I: Uniform Quantization and PCM Generation [YouTube] Ali Muqaibel: 6.4.2 Quantization, part II: Non Uniform Quantization [YouTube] Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training, [arxiv]","tags":[{"name":"Quantization Error","slug":"Quantization-Error","permalink":"https://bobondemon.github.io/tags/Quantization-Error/"},{"name":"Linear Quantization","slug":"Linear-Quantization","permalink":"https://bobondemon.github.io/tags/Linear-Quantization/"},{"name":"Nonlinear Quantization","slug":"Nonlinear-Quantization","permalink":"https://bobondemon.github.io/tags/Nonlinear-Quantization/"}]},{"title":"LoRAPrune, Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning 筆記","date":"2023-10-09T09:29:38.000Z","path":"2023/10/09/Pruning-Meets-Low-Rank-Parameter-Efficient-Fine-Tuning-筆記/","text":"本文是這篇論文 “LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning [arxiv]” 的筆記. 一般來說使用 first-order Taylor importance 的 pruning 方法 (下面會介紹此法) 需計算 gradients 來對每個 weight 計算重要性, 然後根據重要性剪枝. 但是現在模型已經愈來愈大, 對所有 weights 都須計算 gradient 的負擔太大. 另一方面, 在 LLM 中對於大模型的 fine tuning 使用 LoRA (PEFT, Parameter Efficient Fine Tuning, 的一種) 來計算 gradients 非常有效率, 原因是對原來的 weights 是 fixed 的, 只 train LoRA 外掛的”少量”參數, 因此只有少量的 gradients 需要計算. 不過我們思考一下, 如果要對已經 prune 的 weights 旁邊外掛 LoRA 的話, LoRA train 完後沒辦法 merge 回去原來的 weights, 因為有可能打亂原本要 prune 的位置. 但是反過來說, 如果先用 LoRA fine tune 完才進行剪枝, 又回到當模型太大而負擔太大沒效率的問題. 況且這樣分兩步驟可能不是很直接, 如果能在 LoRA fine tune 時就能一併考慮某些 weights 會被 prune 的情況下去 fine tune 可能會更好. 如何 pruning 原來的參數又能利用上 LoRA 的效率就是此篇論文的工作. $$\\begin{array}{|c |c |c |} \\hline &amp; 能否對原來的參數做剪枝? &amp; 是否很有效率? \\\\ \\hline \\text{1st order pruning} &amp; \\text{Yes} &amp; \\text{No} \\\\ \\hline \\text{LoRA} &amp; \\text{No} &amp; \\text{Yes} \\\\ \\hline \\text{LoRAPrune} &amp; \\text{Yes} &amp; \\text{Yes} \\\\ \\hline \\end{array}$$ 以下會先介紹 first-order Taylor importance 的 pruning 方法, 再來介紹 LoRA, 最後說明如何取兩者之優點得出此篇的方法: LoRAPrune First-order Taylor Importance Pruning 對 weight $w_{ij}$ 的 importance score 估計, 是以該 weight 被 prune 掉的話 ($w_{ij}=0$), 對 loss 有多少影響來當依據, 所以: $(W_0)_{ij}$ 用 $w_{ij}$ 表示 $$\\begin{align} \\mathcal{I}_{ij}=(\\mathcal{L}(x,y,W_0)-\\mathcal{L}(x,y,W_0|w_{ij}=0))^2 \\end{align}$$ 複習一下 Taylor expansion$$f(x)=f(a)+{f&apos;(a)\\over 1!}(x-a)+{f&apos;&apos;(a)\\over 2!}(x-a)^2+{f&apos;&apos;&apos;(a)\\over 3!}(x-a)^3+...$$ 所以$$\\mathcal{L}(W-\\delta W) = \\mathcal{L}(W) + \\nabla_W \\mathcal{L}^T\\cdot(-\\delta W) + {1\\over2}(-\\delta W)^T\\cdot(\\nabla_W^2 \\mathcal{L})\\cdot(-\\delta W) +... \\\\ \\Longrightarrow \\mathcal{L}(W)-\\mathcal{L}(W-\\delta W)= \\nabla_W \\mathcal{L}^T\\cdot(\\delta W) - {1\\over2}\\delta W^T\\cdot(\\nabla_W^2 \\mathcal{L})\\cdot\\delta W + ...$$ 假設二次項之後影響都比一次項小很多, 因此我們可以把參數 $w_{ij}$ 的 importance score 設定成一次項的 power:(這時的 $\\delta W=w_{ij}$)$$\\begin{align} \\mathcal{\\hat I}_{ij}=\\left( {\\partial\\mathcal{L}\\over\\partial w_{ij}}w_{ij} \\right)^2 \\end{align}$$ 我們就根據 $\\mathcal{\\hat I}_{ij}$ 來逐步剪枝不要的參數 LoRA LoRA (Low-Rank Adaptation) 公式為:$$\\begin{align} z=xW_0+xBA \\end{align}$$ 其中 $W_0\\in\\mathbf{R}^{d\\times k}$ 是原來 model 的參數, $A\\in\\mathbf{R}^{r\\times k}$ and $B\\in\\mathbf{R}^{d\\times r}$ 是 LoRA 的兩個 learnable low rank (rank $r$) 參數.會將 $W_0$ fixed 住, 只學 $A$ and $B$, 且由於 rank $r$ 通常都不大, 因此很有效率. 注意到為了保證 initial 的時候 performance (output) 跟原來一樣, 會將 $B$ initial 成 $0$ matrix ($A$ random Guassian 即可)學完之後, 可將 $A,B$ 的參數 merge 回 $W_0$, 所以 inference 不會增加額外計算量$$\\begin{align} W=W_0+BA \\end{align}$$ LoRAPrune 如果要將 $w_{ij}$ prune 掉的話, 相當於設定 $(BA)_{ij}=-w_{ij}$, 所以 importance score (1) 改寫如下:$$\\begin{align} \\mathcal{I}_{ij}=(\\mathcal{L}(x,y,W_0)-\\mathcal{L}(x,y,W_0|(BA)_{ij}=-w_{ij}))^2 \\end{align}$$ 如同上面一樣 first order Taylor approximation 為: $$\\begin{align} \\mathcal{\\hat I}_{ij}=\\left( {\\partial\\mathcal{L}\\over\\partial (BA)_{ij}}((BA)_{ij}+w_{ij}) \\right)^2 \\end{align}$$ 注意到 $W_0$ 是 fixed 住, 而 $A,B$ 才是 learnable parameters, 所以是對 $(BA)_{ij}$ 偏微分其中由於 SGD update 公式的關係, (6) 的偏微分那項可這麼看待:$$\\begin{align} {\\partial\\mathcal{L}\\over\\partial(BA)_{ij}}\\propto (BA)_{ij}|_t - (BA)_{ij}|_{t+1} \\end{align}$$ $t$ 為當下的 weights, $t+1$ 是要 update 的 SGD iteration, 繼續拆解如下:$$\\begin{align} {\\partial\\mathcal{L}\\over\\partial(BA)_{ij}}\\propto\\left[ B_{i:}A_{:j}- \\left(B_{i:}-\\frac{\\partial\\mathcal{L}}{\\partial B_{i:}}\\right) \\left(A_{:j}-\\frac{\\partial\\mathcal{L}}{\\partial A_{:j}}\\right) \\right] \\\\ =\\left[ \\frac{\\partial\\mathcal{L}}{\\partial B_{i:}}A_{:j} + B_{i:}\\frac{\\partial\\mathcal{L}}{\\partial A_{:j}} - \\frac{\\partial\\mathcal{L}}{\\partial B_{i:}}\\frac{\\partial\\mathcal{L}}{\\partial A_{:j}} \\right] \\end{align}$$ 將 (9) 代回 (6) 得到:$$\\begin{align} \\mathcal{\\hat I}_{ij}=\\left( (\\nabla B \\cdot A + B\\cdot\\nabla A - \\nabla B\\cdot\\nabla A)\\odot(BA+W_0) \\right)^2 \\end{align}$$ 其中 $\\odot$ 表示 element-wised 相乘, 到這裡我們發現只使用 $A,B$ 的 gradients, 因此保有了 LoRA 效率的好處. 💡 總結一下精神: 原來所有 weights 的 first-order Taylor importance scores $\\mathcal{I}_{ij}$ (式 5) 在 fine tune LoRA 時使用它的”少量”參數的 gradients 來逼近 $\\mathcal{\\hat I}_{ij}$ (式 10), 這樣計算 importance score 沒效率的情形就能被改善. Progressive LoRAPrune 在計算 forward and backward 的時候是使用 masking 的方式計算:$$\\begin{align} z=(xW_0+xBA)\\odot M \\end{align}$$ 其中 $M$ 是 binary mask, 是根據 importance score $\\mathcal{\\bar I}$ 計算得到, 而 $\\mathcal{\\bar I}$ 只是個 smoothed 過後的 $\\mathcal{\\hat I}$ (10) 而已$$\\begin{align} \\mathcal{\\bar I}|_t=\\lambda\\mathcal{\\bar I}|_{t-1}+(1-\\lambda)\\mathcal{\\hat I}|_t \\end{align}$$ 注意到由於直接乘 mask $M$, 沒有特別使用 STE 來讓 mask = 0 的地方的 gradient 流通, 因此被 mask 的 $i,j$ 會沒有 gradients, 但其實 $B_{i:}$ 或 $A_{:j}$ 還是有機會被其他位置的 gradients 更新到, 例如 $M_{ik}\\neq0$ 則 $B_{i:}$ 還是會被 update, $M_{lj}\\neq0$ 則 $A_{:j}$ 也會被 update, 綜合起來 $(BA)_{ij}$ 也被改變了. 也因此就算 $M_{ij}=0$, $w_{ij}$ 還是有敗部復活的機會. 所以 progressive LoRAPrune 流程如下 論文後面有些實驗很有意思, 例如使用 $\\frac{\\partial\\mathcal{L}}{\\partial w_{ij}}$ 來替換 (6) 中的 $\\frac{\\partial\\mathcal{L}}{\\partial (BA)_{ij}}$. 再請有興趣的讀者自行閱讀論文. References LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning [arxiv] LoRA: Low-Rank Adaptation of Large Language Models [arxiv]","tags":[{"name":"Pruning","slug":"Pruning","permalink":"https://bobondemon.github.io/tags/Pruning/"},{"name":"LoRA","slug":"LoRA","permalink":"https://bobondemon.github.io/tags/LoRA/"},{"name":"PEFT","slug":"PEFT","permalink":"https://bobondemon.github.io/tags/PEFT/"},{"name":"LoRAPrune","slug":"LoRAPrune","permalink":"https://bobondemon.github.io/tags/LoRAPrune/"}]},{"title":"Movement Pruning Adaptive Sparsity by Fine-Tuning 筆記","date":"2023-02-24T14:47:40.000Z","path":"2023/02/24/Movement-Pruning-Adaptive-Sparsity-by-Fine-Tuning-筆記/","text":"先引用這篇論文的論點 Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers [pdf] 同樣的小 model size, 從頭訓練還不如先用大的 model size 做出好效果, 再壓縮到需要的大小所以 pruning 不僅能壓小 model size, 同樣對 performance 可能也是個好策略 Introduction 使用單純的 absolutely magnitude pruning 對於在 SSL model 不好. 因為原來的 weight 是對 SSL 的 loss 計算的, 並不能保證後來的 fine tune (down stream task loss) 有一樣的重要性關聯.例如傳統上的 magnitude pruning 作法, 如這一篇 2015 NIPS 文章 [Learning both Weights and Connections for Efficient Neural Networks] (cited 5xxx) 作法很簡單:&emsp;先對 model train 到收斂, 然後 prune, 接著繼續訓練 (prune 的 weight 就 fix 為 $0$), 然侯再多 prune … iterative 下去到需要的 prune 數量但作者認為, 只靠 magnitude 大小判斷效果不好, 因為在 fine tune 過程中, 如果某一個 weight 雖然 magnitude 很大, 但 gradient update 後傾向把 magnitude 變小, 就表示它重要性應該降低才對, 這是本篇的精華思想 因此我們先定義重要性就是代表 weight 的 magnitude 會變大還是變小, 變大就是重要性大, 反之 因此作者對每一個參數都引入一個 score, 命為 $S$, 希望能代表 weight 的重要性. 而在 fine-tune 的過程, 除了對 weight $W$ update 之外, score $S$ 也會 update如果 score $S$ 正好能反映 weight 的 gradient 傾向, 即 $S$ 愈大剛好表示該對應的 weight 在 fine-tune 過程會傾向讓 magnitude 變大, 反之亦然, 那這樣的 $S$ 正好就是我們要找的. 要這麼做的話, 我們還需要回答兩個問題: 怎麼引入 score $S$? Score $S$ 正好能代表重要性? 換句話說能反映 weight 在 fine tune 過程的 magnitude 傾向嗎? 怎麼引入 score $S$? 首先, 看一下 $W$ and $S$ 的 gradients Forward: $$a=(W\\odot M)x$$ $W$是 weight matrix, 而 $M$是 mask matrix 每一個 element $\\in\\{0,1\\}$, $M$ 通常是從一個 score matrix $S$ 搭配上 masking function e.g. $\\text{Top}_v$ 而來:$$M_{ij}=\\text{Top}_v(S)_{ij}=\\left\\{ \\begin{array}{ll} 1, &amp; S_{ij}\\quad\\text{in top }v\\% \\\\ 0, &amp; \\text{o.w.} \\end{array} \\right.$$ 而 magnitude based pruning 定義 $S_{ij}=|W_{ij}|$算 $W$ 的 gradients:$$\\begin{align} \\frac{\\partial L}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial a_i}M_{ij}x_{j} \\end{align}$$ 而算 $S$ 的 gradients 時發現因為 $\\text{Top}_v$ 無法微分所以用 straight-through estimator (STE), i.e. 假裝沒有 $\\text{Top}_v$ 這個 function. 修改為可微分的 forward: 改成讓 forward 假裝沒有過 $\\text{Top}_v$ (因為 $\\text{Top}_v$ 無法微分):$$a=(W\\odot {\\color{orange}S})x$$ 所以 $S$ 的 gradients:$$\\begin{align} \\frac{\\partial L}{\\partial S_{ij}} = \\frac{\\partial L}{\\partial a_i}\\frac{\\partial a_i}{\\partial S_{ij}}=\\frac{\\partial L}{\\partial a_i}W_{ij}x_j \\end{align}$$ 所以 $S$ 仍然會被 update, 就算對應的 weight 已經在 forward 被 mask 了 這種作法稱 Straigth Through Estimator (STE)Appendix A.1 證明 training loss 會收斂 (原論文有幾個推導當下沒看懂, 後來自己補足了一些推導, 見本文最後面段落) Score $S$ 能代表重要性? 先回顧一個觀念$$\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}}&gt;0$$ 表示 loss function $\\mathcal{L}$ 與 $W_{ij}$ 方向一致, 換句話說 $$W_{ij}\\uparrow \\iff \\mathcal{L}\\uparrow \\\\ W_{ij}\\downarrow \\iff \\mathcal{L}\\downarrow \\\\$$ 如果$$\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}}&lt;0$$ 則表示方向相反我們現在觀察 $S$ 和 $W$ 在 update 時候之間的關係, 由 (1) and (2) 的關係可以寫成如下:$$\\begin{align} \\frac{\\partial L}{\\partial S_{ij}} = \\frac{\\partial L}{\\partial W_{ij}}W_{ij} / M_{ij} \\end{align}$$ 首先我們注意到如果 $\\partial L / \\partial S_{ij} &lt; 0$, 表示 $L$ 和 $S_{ij}$ 方向相反, 因為我們希望 $L\\downarrow$, 所以此時 $S_{ij}\\uparrow$.要讓 $\\partial L / \\partial S_{ij} &lt; 0$ 根據 (3) 只會有兩種情形 (我們不管 $M_{ij}$, 因為它 $\\geq0$):&emsp;Case 1: $\\partial L / \\partial W_{ij} &lt; 0$ and $W_{ij}&gt;0$. Weight 是正的, 且它的行為跟 $L$ 方向相反. 所以更新後 weight 會變得更大 (away from zero)&emsp;Case 2: $\\partial L / \\partial W_{ij} &gt; 0$ and $W_{ij}&lt;0$. Weight 是負的, 且它的行為跟 $L$ 方向相同. 所以更新後 weight 會變得更小 (close to zero)上述兩種 weight 都會離 $0$ 愈來愈遠 (magnitude 會變更大).結論就是 update 過程如果 $S_{ij}\\uparrow$ 表示 $W_{ij}$ 遠離 $0$.同樣的推理, 如果 $\\partial L / \\partial S_{ij} &gt; 0$, 表示 $S_{ij}\\downarrow$ 的情形發生在 $W_{ij}$ 更靠近 $0$ 了.所以我們得到一個結論:&emsp;因為 $S$ 升高對應到 $|W|$ 變大; $S$ 降低對應到 $|W|$ 變小. 所以合理認為 $S$ 代表的是重要性 有意思的是, 上述結論似乎跟 masking function 是否用 $\\text{Top}_v$ 無關意思是如果 masking function 用 $\\text{Bottom}_v$ (選最小的那 $v\\%$) 也會有 “$S$ 升高對應到 $W$ 變大; $S$ 降低對應到 $W$ 變小, 因此 $S$ 是重要性” 這個結論但怎麼感覺哪裡怪怪的不過其實邏輯上不衝突, 我這邊的理解是這樣的:Score $S$ 代表重要性是沒問題的, 只是這個重要性現在只針對 $\\text{Bottom}_v$ 的那些 weights 去看同時, Appendix A.1 證明 loss 能收斂也是基於 $\\text{Top}_v$ 能得到保證, 因此用 $\\text{Bottom}_v$ 搞不好收斂不起來 $S$ 的更新過程可以視為 movement (重要性) 的累積 (只要初始給 $0$ ??) Results 在 low sparsity (more than 70% of remaining weights), magnitude pruning 比 movement pruning 好在 high sparsity (less than 15% of remaining weights), 則 movement pruning 好得很明顯 總體來說在 high sparsity case, Soft movement pruning (SMvP) &gt; Movement Pruning (MvP) &gt; L0 regularization &gt; Magnitude Pruning (MaP)作者強調了一下 MvP or SMvP 比 L0 簡單又更好最後作者在 pruning 過程中加了 distillation loss, 顯示 distillation 對所有 pruning methods 都有幫助. Fig 4(a) 不意外Fig 4(b) 比較有意思, score 大的那些 weight 都不會 $0$ 靠近 (v-shape)作者實驗了 global/local NN 的 pruning, 之前是說 global 讓 NN 自己決定每個 layers 要 prune 多少比例, 所以通常比較好 (尤其在 high sparsity)但作者在自己的實驗, 發現兩者在效果上沒太大差異最後分析一下每個 layer 的 sparsity, 發現在愈後面的 layer prune 愈多 Codes HuggingFace 有實現這段 codes: 1234567891011121314151617class TopKBinarizer(autograd.Function): @staticmethod def forward(ctx, inputs: torch.tensor, threshold: float): # Get the subnetwork by sorting the inputs and using the top threshold % mask = inputs.clone() _, idx = inputs.flatten().sort(descending=True) j = int(threshold * inputs.numel()) # flat_out and mask access the same memory. flat_out = mask.flatten() flat_out[idx[j:]] = 0 flat_out[idx[:j]] = 1 return mask @staticmethod def backward(ctx, gradOutput): return gradOutput, None 注意到繼承 autograd.Function 就要 implement forward and backward 方法, 讓它可以微分我們可以看到 backward 什麼事都沒做, 這是因為 STE (Straight-Through Estimator) 的關係所以在 forward 的時候 inputs tensor 就給 score matrix $S$, 這樣可以求出對應的 mask $M$, 同時這個 TopK 又可以微分 Appendix A.1 Guarantees on the decrease of the training loss 補充推導, 先回顧一下 Forward: $$a=(W\\odot M)x$$ 針對 Backward relaxing 的 forward: $$a=(W\\odot S)x$$ 其中 $M=\\text{Top}_k(S)$, score matrix 經過選擇變成 mask matrix. 不失一般性, 我們定義 score 都為正, $S_{ij}&gt;0$.算 $W$ 的 gradients:$$\\frac{\\partial L}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial a_i}M_{ij}x_{j} \\\\ \\frac{\\partial L}{\\partial W_{kl}}=\\frac{\\partial L}{\\partial a_k}M_{kl}x_{l}$$ 算 $S$ 的 gradients, 不過由於 $\\text{Top}_k$ 無法算微分, 所以只好用 Backward relaxing 的替代方式 $$\\frac{\\partial L}{\\partial S_{ij}} = \\frac{\\partial L}{\\partial a_i}\\frac{\\partial a_i}{\\partial S_{ij}}=\\frac{\\partial L}{\\partial a_i}W_{ij}x_j \\\\ \\frac{\\partial L}{\\partial S_{kl}} = \\frac{\\partial L}{\\partial a_k}\\frac{\\partial a_k}{\\partial S_{kl}}=\\frac{\\partial L}{\\partial a_k}W_{kl}x_l$$ 要證明, movement pruning 算法造成的 $\\text{Top}_k$ 變化, 仍會使得 loss 愈來愈低.先將問題簡化為 $\\text{Top}_1$, 在 iteration $t$ 最高分的是 index $(i,j)$, i.e. $\\forall u,v,S_{uv}^{(t)}\\leq S_{ij}^{(t)}$. 然後 update 一次後, 變成 index $(k,l)$ 是最大.$$\\left\\{ \\begin{array}{ll} \\text{At } t, &amp; \\forall1\\leq u,v\\leq n,\\quad S_{uv}^{(t)}\\leq S_{ij}^{(t)} \\\\ \\text{At } t+1, &amp; \\forall1\\leq u,v\\leq n,\\quad S_{uv}^{(t+1)}\\leq S_{kl}^{(t+1)} \\end{array} \\right.$$ 所以有 $S_{kl}^{(t+1)}-S_{kl}^{(t)} \\geq S_{ij}^{(t+1)}-S_{ij}^{(t)}$.我們從定義出發:$$\\frac{\\partial L}{\\partial S_{ij}^{(t)}}=\\lim_{|\\Delta|\\rightarrow0}\\frac{L\\left(S^{(t+1)}\\right) - L\\left(S^{(t)}\\right)}{S_{ij}^{(t+1)}-S_{ij}^{(t)}},\\quad\\text{where }\\Delta=S_{ij}^{(t+1)}-S_{ij}^{(t)}$$ $$\\therefore \\quad \\frac{L\\left(S^{(t+1)}\\right)-L\\left(S^{(t)}\\right)}{S_{ij}^{(t+1)}-S_{ij}^{(t)}} \\geq \\frac{L\\left(S^{(t+1)}\\right)-L\\left(S^{(t)}\\right)}{S_{kl}^{(t+1)}-S_{kl}^{(t)}} \\\\ \\text{limit both side}\\Longrightarrow \\frac{\\partial L}{\\partial S_{ij}^{(t)}} \\geq \\frac{\\partial L}{\\partial S_{kl}^{(t)}} \\\\ \\begin{align} \\Longrightarrow \\frac{\\partial L}{\\partial a_i}W_{ij}^{(t)}x_j \\geq \\frac{\\partial L}{\\partial a_k}W_{kl}^{(t)}x_l \\qquad \\ldots \\end{align}$$ 這就是論文裡 equation (7) 的推導,因此我們觀察兩次的 losses 差異:$$L(a_i^{(t+1)},a_k^{(t+1)})-L(a_i^{(t)},a_k^{(t)}) \\\\ \\\\ \\approx \\frac{\\partial L}{\\partial a_k}(a_k^{(t+1)}-a_k^{(t)}) + \\frac{\\partial L}{\\partial a_i}(a_i^{(t+1)}-a_i^{(t)}) \\\\ \\\\ =\\frac{\\partial L}{\\partial a_k}W_{kl}^{(t+1)}x_l - \\frac{\\partial L}{\\partial a_i}W_{ij}^{(t)}x_j \\\\ \\\\ = \\frac{\\partial L}{\\partial a_k}W_{kl}^{(t+1)}x_l + (-\\frac{\\partial L}{\\partial a_k}W_{kl}^{(t)}x_l + \\frac{\\partial L}{\\partial a_k}W_{kl}^{(t)}x_l) - \\frac{\\partial L}{\\partial a_i}W_{ij}^{(t)}x_j \\\\ \\\\ = \\frac{\\partial L}{\\partial a_k}(W_{kl}^{(t+1)}x_l-W_{kl}^{(t)}x_l) + (\\frac{\\partial L}{\\partial a_k}W_{kl}^{(t)}x_l - \\frac{\\partial L}{\\partial a_i}W_{ij}^{(t)}x_j) \\\\ \\\\ = \\underbrace{\\frac{\\partial L}{\\partial a_k}x_l(-\\alpha_W\\frac{\\partial L}{\\partial a_k}x_lm(S^{(t)})_{kl})}_{\\text{term1}=0} + \\underbrace{(\\frac{\\partial L}{\\partial a_k}W_{kl}^{(t)}x_l - \\frac{\\partial L}{\\partial a_i}W_{ij}^{(t)}x_j)}_{\\text{term2}&lt;0}$$ 第二行的 $\\approx$ 使用泰勒展開式 二維的泰勒展開式$$f(t_n+\\Delta t,x_n+\\Delta x)=f(t_n,x_n)+\\left[\\begin{array}{cc}f_t(t_n,x_n) &amp; f_x(t_n,x_n)\\end{array}\\right]\\left[\\begin{array}{c}\\Delta t \\\\ \\Delta x\\end{array}\\right] + O\\left( \\left\\| \\left[\\begin{array}{c}\\Delta t \\\\ \\Delta x\\end{array}\\right] \\right\\|^2 \\right) \\\\ =f(t_n,x_n)+\\Delta t f_t(t_n,x_n) + \\Delta x f_x(t_n,x_n) + O(\\Delta t^2 + \\Delta x^2)$$ 第二到第三行的推導, 由於 $a=(W\\odot M)x$, 且因為 $(t)$ 的時候 $a_k^{(t+1)}=0$, 且 $(t+1)$ 的時候 $a_i^{(t+1)}=0$ 發生 top 1 switch 的關係然後最後一行的 term1 由下面關係可以得到:$$\\frac{\\partial L}{\\partial W_{kl}}=\\frac{\\partial L}{\\partial a_k}M_{kl}x_{l} \\\\ W_{kl}^{(t+1)} = W_{kl}^{(t)} - \\alpha_W\\frac{\\partial L}{\\partial W_{kl}}$$ 注意到 term1 為 $0$, 這是因為 $m(S^{(t)})_{kl}=0$ (index $(k,l)$ 在 iteration $t$ 不是最大的)而 term2 &lt;0, 由 (4) 得知. 因此 $$L(a_i^{(t+1)},a_k^{(t+1)})-L(a_i^{(t)},a_k^{(t)}) &lt; 0$$ Update 後 loss 會下降 References In paperswithcode: [link] Codes 請參考 paperswithcode 裡提供的連結, or [github]","tags":[{"name":"Pruning","slug":"Pruning","permalink":"https://bobondemon.github.io/tags/Pruning/"},{"name":"Straight Through Estimator (STE)","slug":"Straight-Through-Estimator-STE","permalink":"https://bobondemon.github.io/tags/Straight-Through-Estimator-STE/"},{"name":"Movement pruning","slug":"Movement-pruning","permalink":"https://bobondemon.github.io/tags/Movement-pruning/"}]},{"title":"L0 Regularization 詳細攻略","date":"2023-01-15T02:50:25.000Z","path":"2023/01/15/L0-Regularization-詳細攻略/","text":"這是一篇論文Learning Sparse Neural Networks through L0 Regularization 的詳細筆記, 同時自己實作做實驗 [My Github]主要以詳解每個部分並自己能回憶起為目的, 所以或許不是很好閱讀 Introduction NN model 參數 $\\theta$, 我們希望非$0$的個數愈少愈好, i.e. $|\\theta|_0$ 愈小愈好, 所以會加如下的 regularization term:$$\\mathcal{L}_C^0(\\theta)=\\|\\theta\\|_0=\\sum_{j=1}^{|\\theta|}\\mathbb{I}[\\theta_j\\neq0]$$ 所以 Loss 為: $$\\mathcal{L}_E(\\theta)=\\frac{1}{N}\\left( \\sum_{i=1}^N\\mathcal{L}(NN(x_i;\\theta),y_i) \\right) \\\\ \\mathcal{L}(\\theta)=\\mathcal{L}_E(\\theta)+\\mathcal{L}_C^0(\\theta)$$ 但實務上我們怎麼實現 $\\theta$ 非 $0$ 呢?一種方式為使用一個 mask random variable $Z=\\{Z_1,...,Z_{|\\theta|}\\}$ (~Bernoulli distribution, 參數 $q=\\{q_1,...,q_{|\\theta|}\\}$), 因此 Loss 改寫如下: (注意到 $\\mathcal{L}_C^0$ 可以有 closed form 並且與 $\\theta$ 無關了) $$\\begin{align} \\mathcal{L}_C^0(\\theta, q)=\\mathbb{E}_{Z\\sim\\text{Bernoulli}(q)}\\left[ \\sum_{j=1}^{|\\theta|}\\mathbb{I}[\\theta_j\\odot Z_j\\neq0] \\right] = \\mathbb{E}_{Z\\sim\\text{Bernoulli}(q)}\\left[ \\sum_{j=1}^{|\\theta|} Z_j \\right] = \\sum_j^{|\\theta|} q_j\\\\ \\mathcal{L}_E(\\theta,q)=\\mathbb{E}_{Z\\sim\\text{Bernoulli}(q)}\\left[ \\frac{1}{N}\\left( \\sum_{i=1}^N\\mathcal{L}(NN(x_i;\\theta\\odot Z_i),y_i) \\right) \\right] \\\\ \\mathcal{L}(\\theta,q)=\\mathcal{L}_E(\\theta,q)+\\lambda\\mathcal{L}_C^0(q) \\end{align}$$ 現在最大的麻煩是 entropy loss $\\mathcal{L}_E$, 原因是 Bernoulli 採樣沒辦法對 $q$ 微分, 因為 $\\nabla_q\\mathcal{L}_E(\\theta,q)$ 在計算期望值時, 採樣的機率分佈也跟 $q$ 有關 參考 Gumbel-Max Trick 開頭的介紹說明 好消息是, 可以藉由 reparameterization (Gumbel Softmax) 方法使得採樣從一個與 $q$ 無關的 r.v. 採樣 (所以可以微分了), 因此也就能在 NN 訓練使用 backpropagation.以下依序說明: (參考這篇 [L0 norm稀疏性: hard concrete门变量] 整理的順序, 但補足一些內容以及參考論文的東西)Gumbel max trick $\\Rightarrow$ Gumbel softmax trick (so called concrete distribution)$\\Rightarrow$ Binary Concrete distribution $\\Rightarrow$ Hard (Binary) Concrete distribution $\\Rightarrow$ L0 regularization最後補上對 GoogleNet 架構加上 $L0$ regularization 在 CIFAR10 上的模型壓縮實驗 文長… Gumbel Distribution $G\\sim\\text{Gumbel}(\\mu,\\beta)$, 其 CDF $F(x)$ 為$$F(x):=P(G\\leq x)=e^{-e^{-(x-\\mu)/\\beta}}$$ 當 $\\mu=0,\\beta=1$ 時為 standard Gumbel r.v., 所以 CDF 為 $\\exp{(-\\exp{(-x)}})$ [wiki] CDF 是一個 monotonely increasing function, 存在 inverse function: $$\\begin{align} F^{-1}(F(x))=x \\Rightarrow F^{-1}\\left(e^{-e^{-(x-\\mu)/\\beta}}\\right)=x \\\\ \\Longrightarrow F^{-1}(p)= \\mu-\\beta\\ln(-\\ln(p)) \\end{align}$$ CDF 的 inverse function 又稱 quantile function (Help me understand the quantile (inverse CDF) function) 所以如果 $F^{-1}(U)$ where $U\\sim\\text{Uniform}(0,1)$, 等於照機率分佈取 Gumbel random variable. Inverse transform sampling [wiki] 假設有一個 strictly monotone transfromation (所以存在 inverse) $T:[0,1]\\rightarrow\\mathbb{R}$, 使得 $T(U)=_dX$, 其中 $U\\sim\\text{Uniform}(0,1)$. 那我們就可以使用 $T$ 來做 $X$ 的採樣 令 $X$ 的 CDF 為 $F_X(x)$, 則: $$F_X(x)=Pr(X\\leq x)=Pr(T(U)\\leq x) \\\\ =Pr(U\\leq T^{-1}(x))=T^{-1}(x)$$ 則我們發現 $T$ 是 $F_X^{-1}$. 因此 $F_X^{-1}(U)$ 就可以用來採樣 $X$. Let $U\\sim\\text{Uniform}(0,1)$, then $F^{-1}(U)=\\mu-\\beta\\ln(-\\ln(U))\\sim\\text{Gumbel}(\\mu,\\beta)$.另外, 兩個 Gumbel r.v.s 的 difference 服從 Logistic distribution.If $X\\sim\\text{Gumbel}(\\mu_X,\\beta)$ and $Y\\sim\\text{Gumbel}(\\mu_Y,\\beta)$ are independent, then, $X-Y\\sim\\text{Logistic}(\\mu_X-\\mu_Y, \\beta)$Logistic random variable 其 CDF 是 sigmoid function. Categorical Distribution and Gumbel Max Trick $X\\sim\\text{Categorical}(\\alpha_1,\\alpha_2,...,\\alpha_n)$ 表示取到第 $i$ 類的機率是 $\\alpha_i$.並且有如下的 reparameterization 方式:$$X&apos;\\sim\\arg\\max\\left(G_1+\\ln\\alpha_1,...,G_n+\\ln\\alpha_n\\right)$$ 其中 $(G_1,...,G_n)$ 為 $n$ 個獨立的 Gumbel r.v.s.則 $X=_dX’$. Concrete Random Variable 簡單講, 將 Gumbel max trick 中的 $\\arg\\max$ 改成 softmax (with temperature $\\tau$)$$X\\sim\\text{Concrete}((\\alpha_1,\\alpha_2,..,\\alpha_n),\\tau)$$ 其中 $\\tau\\in(0,\\infty)$ and $\\alpha_k\\in(0, \\infty)$.則 $X$ 的取法為:&emsp;1. Sample $n$ 個獨立的 Gumbel r.v.s: $(g_1,...,g_n)\\sim(G_1,...,G_n)$, 視為 Gumbel noises&emsp;2. 將 logits, $\\ln\\alpha_k$, 加上這 $n$ 個 Gumbel noises 並做 softmax (with temperature $\\tau$) 成為 distribution:更多請參考 The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables簡而言之, Concrete distirbution 是將 discrete 的 categorical r.v. relax 成 continuous 版本, 意即取出來的 random variable 不再是 simplex 的 one-hot 形式, 而是連續的數值, 論文裡的圖示很清楚 (圖中的 $\\lambda$ 為 temperature $\\tau$)當 $\\tau\\rightarrow 0$, 則 concrete distribution 變成 categorical distribution. Binary Concrete Distribution Concrete distribution 只剩下 binary 的話, 可以做化簡剩兩個參數 $(\\alpha,\\tau)$.$$X=(X_1,X_2)\\sim\\text{Concrete}((\\alpha_1,\\alpha_2),\\tau), \\\\(X_1,X_2)\\sim\\left[\\frac{e^{(G_1+\\ln\\alpha_1)/\\tau}}{e^{(G_1+\\ln\\alpha_1)/\\tau}+e^{(G_2+\\ln\\alpha_2)/\\tau}},\\frac{e^{(G_2+\\ln\\alpha_2)/\\tau}}{e^{(G_1+\\ln\\alpha_1)/\\tau}+e^{(G_2+\\ln\\alpha_2)/\\tau}}\\right] \\\\\\Longrightarrow X_1\\sim\\frac{e^{(G_1+\\ln\\alpha_1)/\\tau}}{e^{(G_1+\\ln\\alpha_1)/\\tau}+e^{(G_2+\\ln\\alpha_2)/\\tau}} = \\frac{1}{1+e^{(G_2-G_1+\\ln\\alpha_2-\\ln\\alpha_1)/\\tau}} \\\\=\\sigma\\left(\\frac{G_1-G_2+\\ln\\alpha_1-\\ln\\alpha_2}{\\tau}\\right) = \\sigma\\left(\\frac{L+\\ln\\alpha_1-\\ln\\alpha_2}{\\tau}\\right)\\\\=\\sigma\\left(\\frac{L+\\ln(\\alpha_1/\\alpha_2)-{\\color{orange}0}}{\\tau}\\right)=\\sigma\\left(\\frac{L+\\ln(\\alpha_1/\\alpha_2)-{\\color{orange}\\ln1}}{\\tau}\\right)$$ 其中 $\\sigma(x)=1/(1+e^{-x})$ 為 sigmoid function, 且已知兩個 Gumbel r.v.s 相減, $(G_1-G_2)=L\\sim \\text{Logistic}$, 是 Logistic, 而 $L=\\ln U-\\ln(1-U)$, where $U\\sim\\text{Uniform}(0,1)$. 想成 $\\alpha_1&apos;=\\alpha_1/\\alpha_2$, 且 $\\alpha_2&apos;=1$, 則 $X=(X_1,1-X_1)\\sim\\text{Concrete}((\\alpha_1&apos;,\\alpha_2&apos;),\\tau)$, 代入 $\\alpha_2&apos;=1$ 後把下標 $1$ 去掉得到 Binary Concrete random variable:$$\\begin{align}(X,1-X)\\sim\\text{Concrete}((\\alpha,1),\\tau)\\\\\\Longrightarrow{\\color{orange}{X\\sim\\text{BinConcrete}(\\alpha,\\tau):=\\sigma\\left(\\frac{L+\\ln\\alpha}{\\tau}\\right)=\\sigma\\left(\\frac{\\ln U-\\ln(1-U)+\\ln\\alpha}{\\tau}\\right)}}\\end{align}$$圖改自 The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables可以觀察到 $\\tau$ 愈接近 $0$, 則 $X$ 的值愈有可能是 $0$ or $1$ 的 binary case.Binary Concrete 的 CDF 為 ${\\color{orange}{P(X&lt;x)=\\sigma(\\tau(\\ln x-\\ln(1-x))-\\ln\\alpha)}}$ 推導如下:已知 $L\\sim \\text{Logistic}$, 所以 $P(L&lt;\\tau(\\ln x-\\ln(1-x))-\\ln\\alpha)=\\sigma(\\tau(\\ln x-\\ln(1-x))-\\ln\\alpha)$ 官方 implementation [codes] Hard (Binary) Concrete Distribution 主要就是將 Binary concrete r.v. 拉伸平移, 並 clip 在 $(0,1)$ 區間Hard binary concrete r.v. 取法為:&emsp;1. $X\\sim\\text{BinConcrete}(\\alpha,\\tau)=\\sigma\\left((\\ln U-\\ln(1-U)+\\ln\\alpha)/\\tau\\right)$.&emsp;2. Stretch: $\\bar{X}=X(b-a)+a$, 將 $X$ 拉伸平移.&emsp;3. Hard-sigmoid to produce Gating r.v.: $Z=\\min(1,\\max(0,\\bar{X}))$.其中 $a&lt;0&lt;1&lt;b$.當 $0&lt;X&lt;-a/(b-a)$, 則 $Z=0$,當 $(1-a)/(b-a)&lt;X&lt;1$, 則 $Z=1$,否則 $(1-a)/(b-a)&lt;X&lt;-a/(b-a)$, $Z=\\bar{X}$. Stretch + hard-sigmoid functions 是為了把 Binary Concrete random variable 真的是 $0$ or $1$ 的機會再變更大請參考 “Binary Concrete Distribution” 段落裡的圖就能想像 所以 $P(Z=0)=P(X&lt;-a/(b-a))$.我們由 Binary Concrete 的 CDF 為 $P(X&lt;x)=\\sigma(\\tau(\\ln x-\\ln(1-x))-\\ln\\alpha)$ 可以得知:$${\\color{orange}{P(Z\\neq0)}}=1-P\\left(X&lt;\\frac{-a}{b-a}\\right) \\\\=1-\\sigma\\left(\\tau\\left(\\ln \\frac{-a}{b-a}-\\ln\\left(1-\\frac{-a}{b-a}\\right)\\right)-\\ln\\alpha\\right) \\\\=1-\\sigma\\left(-\\ln\\alpha+\\tau\\ln\\frac{-a}{b}\\right){\\color{orange}{=\\sigma\\left(\\ln\\alpha-\\tau\\ln\\frac{-a}{b}\\right)}}$$ 所以最後 Gating random variable $Z\\neq0$ 的機率, $P(Z\\neq0)$, 我們可以得到 closed form. $\\mathcal{L}_0$ Regularization $P(Z\\neq0)$ 其實這就是 $L_0$ regularization term 了!$$\\mathcal{L}_C^0(\\phi)=\\sum_j P(Z_j\\neq0|\\phi_j)=\\sum_j \\sigma\\left(\\ln\\alpha_j-\\tau_j\\ln\\frac{-a}{b}\\right)$$ 其中 $\\phi_j=\\{\\alpha_j,\\tau_j\\}$ 表示 Binary Concrete r.v. 的參數 實務上 $\\ln\\alpha_j$ 是 learnable 的參數, 而 $\\tau_j$ 一般直接給定 而原本的 loss 就是 weights $\\theta$ 乘上 gating variable $Z$:注意到 $\\text{BinConcrete}$ 可以藉由 reparameterization trick (變成 sample 這個 operation 跟參數無關, i.e. 利用 standard Uniform or Logistic r.v.s 取 samples) 來做 backpropagation.Total loss 就是$$\\mathcal{L}(\\theta,\\phi)=\\mathcal{L}_E(\\theta,\\phi)+\\lambda\\mathcal{L}_C^0(\\phi)$$ 論文考慮了如果加入 L2-norm 的 regularization 怎麼改動.原本的 $\\mathcal{L}_2$ regularization 只是參數的 square: $\\sum_j \\theta_j^2$, 但為了跟 $\\mathcal{L}_0$ 有個比較好的結合, 改成如下: (細節請參考論文) $$\\mathcal{L}_C^2(\\theta,\\phi)=\\sum_j \\theta_j^2 P(Z_j\\neq0|\\phi_j)$$ 所以結合後的 regularization term 如下:$$\\mathcal{L}_C(\\theta,\\phi)=\\lambda_2\\cdot 0.5\\cdot\\mathcal{L}_C^2(\\theta,\\phi)+\\lambda_0\\cdot\\mathcal{L}_C^0(\\phi) \\\\= \\sum_j \\left( \\lambda_2\\cdot0.5\\cdot\\theta_j^2 + \\lambda_0\\right)P(Z_j\\neq0|\\phi_j)$$ 因此 Total loss 就是$$\\mathcal{L}(\\theta,\\phi)=\\mathcal{L}_E(\\theta,\\phi)+\\lambda\\mathcal{L}_C(\\theta,\\phi)$$ Experimental Codes and Results Network Structure使用 GoogleNet 在 CIFAR10 上做實驗 [Github repo]具體怎麼做 L0 purning 呢? 以 convolution 舉例, 我們對 output channel 做 masking, 因此每個 channel 會對應一個 hard binary concrete r.v. $Z_i$, 由於 hard binary concrete r.v. 傾向 sample 出 exactly $0$ or $1$ (中間數值也有可能, 只是很低機率), 因此造成 output dimension 會直接下降, 所以給下一層的 layer 的 channel 數量就減少, 圖示如下:有關 hard concrete r.v. 的 module 參考 class L0Gate(nn.Module) [link]因此 Inception block 會多了一些 L0Gate layers:所以 inception layer 的 forward() 大概就是長這樣:再來就是用這些包含 L0Gate 的 inception blocks 去建立整個 GoogleNet 的 NN 了. Results Recap 一下 Loss:$$\\mathcal{L}(\\theta,\\phi)=\\mathcal{L}_E(\\theta,\\phi)+\\lambda\\mathcal{L}_C^0(\\phi)$$ 其中 $\\phi_j=\\{\\alpha_j,\\tau_j\\}$ 表示 Binary Concrete r.v. 的參數, 一般來說只有 $\\ln\\alpha_j$ 是 learnable, 而 $\\lambda$ 表示 L0 regularization 的比重我們將 L0Gate 的參數, i.e. $\\ln\\alpha_j$, 與 NN 的參數 $\\theta$ 一起從頭訓練起對比沒有 L0 regularization 的就是原始的 GoogleNet GoogleNet Validation Accuracy Test Accuracy Sparsity NO L0 90.12% 89.57% 1.0 with L0, lambda=0.25 88.66% 87.87% 0.94 with L0, lambda=0.5 86.9% 86.56% 0.78 with L0, lambda=1.0 83.2% 82.79% 0.45 其中 sparsity 的計算為所有因為 gate 為 $0$ 而造成參數無效的比例可以觀察到隨著 $\\lambda$ 愈大, 會 pruning 更多 ($\\mathcal{L}_C^0$ 的收斂值會更低), 但也造成 accuracy 的下降對比下面的圖也可以看到 $\\lambda$ 對 $\\mathcal{L}_C^0$ 的收斂值的影響 實務上乘 gate $0$ 等於事先將 weight 變成 $0$, 而因為我們使用 structure pruning, 所以可以將 convolution kernel 變小. 後來我發現比起將 $\\ln\\alpha_j$, 與 NN 的參數 $\\theta$ 一起從頭訓練起NN $\\theta$ init 使用之前 pre-train 好的 model (沒有 L0), 然後再加入 L0 regularization, 此時將 $\\ln\\alpha$ 初始成比較大的值 (接近 $1$, i.e. 讓 gate 打開), 這樣在同樣 sparsity 效果下, accuracy 會比較高 References L0 norm稀疏性: hard concrete门变量 The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables L0 Regularization Practice [My Github] In paperswithcode: [link] Pruning Filters &amp; Channels in Neural Network Distiller","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"https://bobondemon.github.io/tags/PyTorch/"},{"name":"Gumbel distribution","slug":"Gumbel-distribution","permalink":"https://bobondemon.github.io/tags/Gumbel-distribution/"},{"name":"L0 regularization","slug":"L0-regularization","permalink":"https://bobondemon.github.io/tags/L0-regularization/"},{"name":"Concrete distribution","slug":"Concrete-distribution","permalink":"https://bobondemon.github.io/tags/Concrete-distribution/"},{"name":"Hard Concrete distribution","slug":"Hard-Concrete-distribution","permalink":"https://bobondemon.github.io/tags/Hard-Concrete-distribution/"},{"name":"Pruning","slug":"Pruning","permalink":"https://bobondemon.github.io/tags/Pruning/"},{"name":"Straight Through Estimator (STE)","slug":"Straight-Through-Estimator-STE","permalink":"https://bobondemon.github.io/tags/Straight-Through-Estimator-STE/"}]},{"title":"Learning Zero Point and Scale in Quantization Parameters","date":"2022-12-04T13:14:45.000Z","path":"2022/12/04/Learning-Zero-Point-and-Scale-in-Quantization-Parameters/","text":"在上一篇 搞懂 Quantization Aware Training 中的 Fake Quantization 我們討論了 fake quantization 以及 QAT提到了 observer 負責計算 zero point and scale $(z,s)$, 一般來說只需要透過統計觀測值的 min/max 範圍就能給定, 所以也不需要參與 backward 計算 直觀上我們希望找到的 zero/scale 使得 quantization error 盡量小, 但其實如果能對任務的 loss 優化, 應該才是最佳的這就必須讓 $(z,s)$ 參與到 backward 的計算, 這種可以計算 gradient 並更新的做法稱為 learnable quantization parameters 本文主要參考這兩篇論文:&emsp;1. LSQ: Learned Step Size Quantization&emsp;2. LSQ+: Improving low-bit quantization through learnable offsets and better initialization LSQ 只討論 updating scale, 而 LSQ+ 擴展到 zero point 也能學習, 本文只推導關鍵的 gradients 不說明論文裡的實驗結果 很快定義一下 notations:&emsp;- $v$: full precision input value&emsp;- $s$: quantizer step size (scale)&emsp;- $z$: zero point (offset)&emsp;- $Q_P,Q_N$: the number of positive and negative quantization levels&emsp;&emsp;e.g.: for $b$ bits, unsigned $Q_N=0,Q_P=2^b-1$, for signed $Q_N=2^{b-1},Q_P=2^{b-1}-1$&emsp;- $\\lfloor x \\rceil$: round $x$ to nearest integer將 $v$ quantize 到 $\\bar{v}$ (1), 再將 $\\bar{v}$ dequantize 回 $\\hat{v}$ (2), 而 $v-\\hat{v}$ 就是 precision loss$$\\begin{align} \\bar{v}={clip(\\lfloor v/s \\rceil+z,-Q_N,Q_P)} \\\\ \\hat{v}=(\\bar{v}-z)\\times s\\\\ \\end{align}$$ 學習 Scale 因為在 forward 的時候是 $\\hat{v}$ 去參與 Loss $L$ 的計算 (不是 $v$), 所以計算 $s$ 的 gradient 時 Loss $L$ 必須對 $\\hat{v}$ 去微分, 因此$$\\begin{align} \\frac{\\partial L}{\\partial s}=\\frac{\\partial L}{\\partial \\hat{v}}\\cdot\\frac{\\partial \\hat{v}}{\\partial s} \\end{align}$$ 其中 ${\\partial L}/{\\partial \\hat{v}}$ 是做 backprop 時會傳進來的, 所以需要計算 ${\\partial \\hat{v}}/{\\partial s}$$$\\begin{align} \\frac{\\partial \\hat{v}}{\\partial s}=\\frac{\\partial(\\bar{v}-z)s}{\\partial s}=s\\cdot {\\color{orange}{\\frac{\\partial \\bar{v}}{\\partial s}}} +\\bar{v}-z \\\\ =s\\cdot \\left\\{ \\begin{array}{ll} &minus;vs^{-2} &amp; \\text{if }-Q_N&lt;v/s+z&lt;Q_P \\\\ 0 &amp; \\text{otherwise} \\end{array} \\right. +\\bar{v} - z \\end{align}$$ 橘色的地方 $\\color{orange}{\\partial\\bar{v}/{\\partial s}}$ 必須使用 STE (Straight Through Estimator) (參考上一篇筆記)將 $\\bar{v}$ 用這樣表達:$$\\begin{align} \\bar{v}= \\left\\{ \\begin{array}{ll} \\lfloor v/s \\rceil + z &amp; \\text{if }-Q_N&lt;v/s+z&lt;Q_P \\\\ -Q_N &amp; \\text{if }v/s+z \\leq -Q_N \\\\ Q_P &amp; \\text{if }Q_P \\leq v/s+z \\end{array} \\right. \\end{align}$$ 所以代回去 (5) 得到我們要的 scale 的 gradients:$$\\begin{align} \\frac{\\partial \\hat{v}}{\\partial s}= \\left\\{ \\begin{array}{ll} &minus;v/s+\\lfloor v/s \\rceil &amp; \\text{if }-Q_N&lt;v/s+z&lt;Q_P \\\\ -Q_N - z &amp; \\text{if }v/s+z\\leq -Q_N \\\\ Q_P - z &amp; \\text{if }v/s+z\\geq Q_P \\end{array} \\right. \\end{align}$$ 在 LSQ 這篇的作者把 gradients $\\partial\\hat{v}/\\partial s$ 畫出來, 可以看到在 quantization 的 transition 處, LSQ 能體現出 gradient 變動很大 (另外兩個方法沒有) Scale 的 Gradient 要做調整 LSQ 作者實驗認為 weights 和 scale 的 gradients 大小, 再除以各自的參數數量後, 如果在比例上一樣的話效果比較好: $R=\\left.\\left|\\frac{\\nabla_s L}{s}\\right|\\right/\\frac{\\|\\nabla_w L\\|}{\\|w\\|}\\approx 1$ 要讓更新的相對大小是接近的, 因此會把 gradients 乘上如下的 scale 值: $g=1/\\sqrt{NQ_P}$, 其中 $N$ 是那一層的 (pytorch) tensor 總數量 .numel Weight tensor $W$ 就是 W.numel, 而如果要處理 scale $s$ 的話, 假設處理的是 activations $X$, 那就是 X.numel 實作 這個 gradient scale 的技巧很好, 可以用在任何不想改變 output 大小, 而又希望改變 gradient 大小的場合使用 學習 Zero Point 推導 zero point 的 gradient (式子打不出來很怪, 只能用圖片): 對照 PyTorch 實作 Pytorch 實作: _fake_quantize_learnable_per_tensor_affine_backward 裡面註解寫著如下的敘述:The gradients for scale and zero point are calculated as below:Let $X_{fq}$ be the fake quantized version of $X$.Let $X_q$ be the quantized version of $X$ (clamped at $q_\\text{min}$ and $q_\\text{max}$).Let $\\Delta$ and $z$ be the scale and the zero point. 式子打不出來很怪, 只能用圖片: 可以發現與 gradient of scale (7) 和 gradient of zero point (12) 能對照起來 一些訓練說明 有關 initialization 可以從 post quantization 開始, 不一定要照論文的方式其中第一和最後一層都使用 8-bits (我覺得甚至用 32-bits 都可以), 這兩層用高精度能使得效果顯著提升, 已經是個標準做法了另一個標準做法是 intial 都從 full precision 開始 Reference LSQ: Learned Step Size Quantization LSQ+: Improving low-bit quantization through learnable offsets and better initialization 重训练量化·可微量化参数: 有 zero point 的微分推導 Pytorch 實作: _fake_quantize_learnable_per_tensor_affine_backward 量化训练之可微量化参数—LSQ 別人的實作: lsq-net: https://github.com/zhutmost/lsq-net/blob/master/quan/quantizer/lsq.py","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"https://bobondemon.github.io/tags/PyTorch/"},{"name":"Straight Through Estimator (STE)","slug":"Straight-Through-Estimator-STE","permalink":"https://bobondemon.github.io/tags/Straight-Through-Estimator-STE/"},{"name":"Quantization Aware Training (QAT)","slug":"Quantization-Aware-Training-QAT","permalink":"https://bobondemon.github.io/tags/Quantization-Aware-Training-QAT/"},{"name":"Fake Quantization","slug":"Fake-Quantization","permalink":"https://bobondemon.github.io/tags/Fake-Quantization/"},{"name":"LSQ","slug":"LSQ","permalink":"https://bobondemon.github.io/tags/LSQ/"},{"name":"LSQ+","slug":"LSQ","permalink":"https://bobondemon.github.io/tags/LSQ/"}]},{"title":"搞懂 Quantization Aware Training 中的 Fake Quantization","date":"2022-11-19T12:09:14.000Z","path":"2022/11/19/搞懂-Quantization-Aware-Training-中的-Fake-Quantization/","text":"看完本文會知道什麼是 fake quantization 以及跟 QAT (Quantization Aware Training) 的關聯同時了解 pytorch 的 torch.ao.quantization.fake_quantize.FakeQuantize 這個 class 做了什麼 Fake quantization 是什麼? 我們知道給定 zero ($z$) and scale ($s$) 情況下, float 數值 $r$ 和 integer 數值 $q$ 的關係如下: $$\\begin{align} r=s(q-z) \\\\ q=\\text{round_to_int}(r/s)+z \\end{align}$$ 其中 $s$ 為 scale value 也是 float, 而 $z$ 為 zero point 也是 integer, 例如 int8Fake quantization 主要概念就是用 256 個 float 點 (e.g. 用 int8) 來表示所有 float values, 因此一個 float value 就使用256點中最近的一點 float 來替換則原來的 floating training 流程都不用變, 同時也能模擬因為 quantization 造成的精度損失, 這種訓練方式稱做 Quantization Aware Training (QAT) (See Quantization 的那些事) 令一個 tensor x 如下, 數值參考 pytorch 官方範例 (link):1234import torchimport numpy as npx = torch.tensor([ 0.0552, 0.9730, 0.3973, -1.0780]).requires_grad_(True) 同時令 zero and scale 和 integer 為 int812scale, zero = 0.1, 0quant_min, quant_max = 0, 255 則我們可以使用 torch.fake_quantize_per_tensor_affine (link) 來找出哪一個256點的 float 最接近原來的 x 的 float 值1234fq_x = torch.fake_quantize_per_tensor_affine(x, scale, zero, quant_min, quant_max)print(f'fake quant of x = &#123;fq_x&#125; by funtion `fake_quantize_per_tensor_affine`')# fake quant of x = tensor([0.1000, 1.0000, 0.4000, 0.0000],# grad_fn=&lt;FakeQuantizePerTensorAffineCachemaskBackward0&gt;) by funtion `fake_quantize_per_tensor_affine` 其實我們也可以用式 (2) 先算出 quantized 的值, 然後再用 (1) 回算最靠近的 float, 這樣計算應該要跟上面使用 torch.fake_quantize_per_tensor_affine 的結果一樣:12345678# We manually check fake quantization resultsx_copy = x.clone().detach().numpy()x_int = np.clip(np.floor(x_copy/scale + 0.5) + zero, quant_min, quant_max)print(f'quantize x to int = &#123;x_int&#125;')# quantize x to int = [1.0, 10.0, 4.0, 0.0]x_back_to_float = (x_int - zero) * scaleprint(f'fake quant of x = &#123;x_back_to_float&#125; by manual calculation')# fake quant of x = [0.1, 1.0, 0.4, 0.0] by manual calculation Fake quantization 必須要能微分 既然要做 QAT, 也就是說在 back propagation 時, fake quantization 這個 function 也要能微分我們看一下 fake quantization function 長相:基本上就是一個 step function, 除了在有限的不連續點外, 其餘全部都是平的, 所以 gradient 都是 $0$.這導致沒法做 back propagation. 為了讓 gradient 流回去, 我們使用 identity mapping (假裝沒有 fake quantization) 的 gradient:那讀者可能會問, 這樣 gradient 不就跟沒有 fake quantization 一樣了嗎? 如何模擬 quantization 造成的精度損失?我們來看看加上 loss 後的情形, 就可以解答這個問題隨便假設一個 loss function 如下(可以是非常複雜的函數, 例如裡面含有NN):$$\\begin{align} loss=(x-0.1)^2 \\end{align}$$ 原來的 training flow 是上圖中的上面子圖, loss function 使用 $x$ 代入計算, 而使用 fake quantization training 的話必須代入 $\\text{fq_x}$. 這樣就能在計算 loss 的時候模擬精度損失.我們觀察一下 gradient:$$\\begin{align} \\frac{d\\text{loss}}{dx}=\\frac{d\\text{loss}}{d\\text{fq_x}}\\cdot\\frac{d\\text{fq_x}}{d\\text{x}}= 2(\\text{fq_x}-0.1)\\cdot \\{0\\quad\\text{or}\\quad1\\} \\end{align}$$ 因此精度損失反應在 $\\frac{d\\text{loss}}{d\\text{fq_x}}$ 這一項上接續上面的 codes 我們來驗算一下 gradient 是不是如同 (4) 這樣1234567# Note that x = [0.0552, 0.9730, 0.3973, -1.0780]# and fq_x = [0.1000, 1.0000, 0.4000, 0.0000]loss = torch.sum((fq_x-0.1)**2)# loss = tensor(0.9100)loss.backward()print(f'gradient of x = &#123;x.grad&#125;')# tensor([0.0000, 1.8000, 0.6000, -0.0000]) 注意到 x.grad[-1] 的值是 $0$, 這是因為 x[-1] 已經小於 quant_min 了, 所以 fake quantization 的 gradient, $\\frac{d\\text{fq_x}}{d\\text{x}}=0$, 其他情況都是 $2(\\text{fq_x}-0.1)$. 這個做法跟 so called STE (Straight-Through Estimator) 是一樣的意思 [1], 用來訓練 binary NN [6]一篇易懂的文章 “Intuitive Explanation of Straight-Through Estimators with PyTorch Implementation“ 加入 observer 要做 fake quantization 必須給定 zero and scale $(z,s)$, 而這個值又必須從 input (或說 activation) 的值域分布來統計因此我們通常會安插一個 observer 來做這件事情pytorch 提供了不同種類的統計方式來計算 $(z,s)$, 例如: MinMaxObserver and MovingAverageMinMaxObserver PerChannelMinMaxObserver and MovingAveragePerChannelMinMaxObserver HistogramObserver FixedQParamsObserver 因此一個完整個 fake quantization 包含了 observer 以及做 fake quantization 的 function, FakeQuantize 這個 pytorch class 就是這個功能: observer 只是用來給 $(z,s)$ 不需要做 back propagation但其實 scale $s$ 也可以 learnable! 參考 “Learned Step Size Quantization“ (待讀) 因此我們可以看到要 create FakeQuantize 時, 它的 init 有包含給一個 observer:1234567891011121314class FakeQuantize(FakeQuantizeBase): def __init__(self, observer=MovingAverageMinMaxObserver, quant_min=None, quant_max=None, **observer_kwargs): ... def calculate_qparams(self): # 使用 observer 來計算 zero and scale ... def forward(self, X): if self.observer_enabled[0] == 1: # 呼叫 `calculate_qparams` 計算 zeros and scale ... if self.fake_quant_enabled[0] == 1: # 使用 `torch.fake_quantize_per_channel_affine` 來做 fake quantization ... return X FakeQuantize 這個 class 是 nn.Module, 只要 forward 裡面的每個 operation 都有定義 backward (都可微分), 就自動可以做 back propagation 本文最開頭有展示 torch.fake_quantize_per_tensor_affine 可以做 backward, 是可以微分的 op 最後, 在什麼地方安插 FakeQuantize 會根據不同的 module (e.g. CNN, dethwise CNN, LSTM, GRU, … etc.) 而不同, 同時也必須考量如果有 batch normalization, concate operation, add operation 則會有一些 fusion, requantize 狀況要注意 Figure Backup fake_quant.drawio Reference Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation: STE paper 2013 Yoshua Bengio Intuitive Explanation of Straight-Through Estimators with PyTorch Implementation: STE 介紹, 包含用 Pytorch 實作 torch.ao.quantization.fake_quantize.FakeQuantize (link) torch.fake_quantize_per_tensor_affine (link) Learned Step Size Quantization: scale $s$ 也可以 learnable (待讀) 二值网络，围绕STE的那些事儿","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"https://bobondemon.github.io/tags/PyTorch/"},{"name":"Straight Through Estimator (STE)","slug":"Straight-Through-Estimator-STE","permalink":"https://bobondemon.github.io/tags/Straight-Through-Estimator-STE/"},{"name":"Quantization Aware Training (QAT)","slug":"Quantization-Aware-Training-QAT","permalink":"https://bobondemon.github.io/tags/Quantization-Aware-Training-QAT/"},{"name":"Fake Quantization","slug":"Fake-Quantization","permalink":"https://bobondemon.github.io/tags/Fake-Quantization/"},{"name":"Observer","slug":"Observer","permalink":"https://bobondemon.github.io/tags/Observer/"}]},{"title":"Weight Normalization 的筆記","date":"2022-09-26T13:37:42.000Z","path":"2022/09/26/Weight-Normalization-的筆記/","text":"使用 SGD 做優化時, 如果 ill-conditioned of Hessian matrix, i.e. $\\sigma_1/\\sigma_n$ 最大最小的 eigenvalues 之比值, 會使得收斂效率不彰(ref zig-zag). 可以想成 loss function 的曲面愈不像正圓則愈 ill-conditioned (愈扁平). 希望藉由 re-parameterization 來將 ill-conditioned 狀況降低.一般來說 NN 的 layer 可以這麼寫:$$y=\\phi(w^Tx+b)$$ 把 weight vector $w$ 重新改寫如下: $$w={g\\over\\|v\\|}v\\quad\\quad(\\star)$$ WN 就是將 $w$ 拆成用 unit vector $v/||v||$ 和 magnitude $g$ 兩個 variables 來表示 對大小 $g$ 的微分 因此 loss function $L$ 對 $g$ 微分為:$$\\begin{align} \\frac{dL}{dg}=\\nabla_wL^T\\frac{\\partial w}{\\partial g}=\\nabla_wL^T\\frac{v}{\\|v\\|} \\end{align}$$ 這裡我們寫 gradient vector 都以 column vector 來寫所以如果 loss function $L$ 是 scalar 的話, gradient 就是 transpose of Jacobian matrix (剛好是 1xn 的 row vector) 對方向向量 $v$ 的微分 Loss function $L$ 對 $v$ 微分為: 這裡要參考到 matrix cookbook equation (130) $$\\begin{align} \\nabla_vL^T = \\nabla_wL^T\\left(g\\frac{I}{\\|v\\|}-g\\frac{vv^T}{\\|v\\|^3}\\right)\\quad \\\\ = \\nabla_wL^T\\frac{g}{\\|v\\|}\\left( I-\\frac{vv^T}{\\|v\\|^2} \\right)\\quad \\end{align}$$ $$\\therefore \\quad \\nabla_vL=\\frac{g}{\\|v\\|}M_v\\nabla_wL \\quad\\text{where}\\ M_v:=I-\\frac{vv^T}{\\|v\\|^2}$$ 論文裡式 (3) 的 gradient 推導可藉由將 (1) 代進到 (2) 裡得到. $\\nabla_vL$ 的物理意義 注意到由於 $v$ 跟 $w$ 是同方向但大小不同而已. 所以$$M_v=I-\\frac{vv^T}{\\|v\\|^2}=I-\\frac{ww^T}{\\|w\\|^2}=:M_w$$ $$\\begin{align} \\therefore \\quad \\nabla_vL=\\frac{g}{\\|v\\|}M_w\\nabla_wL \\quad\\text{where}\\ M_w:=I- \\color{orange}{\\frac{ww^T}{\\|w\\|^2}} \\end{align}$$ 觀察一下 $M_w$ 裡的第二項 ((4) 的橘色部分) 乘上一個 vector $x$ 代表的意義:$$\\frac{w}{\\|w\\|}\\cdot\\frac{w^T}{\\|w\\|}\\cdot x$$ 其中 $w/\\|w\\|$ 表示 $w$ 方向的 unit vector, 而 $w^Tx/\\|w\\|$ 表示 $x$ 投影在 $w$ 方向上的長度. 所以 $$M_w\\nabla_wL=\\nabla_wL-\\frac{w}{\\|w\\|}\\cdot\\frac{w^T}{\\|w\\|}\\cdot \\nabla_wL$$ $M_w\\nabla_wL$ 就是將 $\\nabla_wL$ 扣掉在 $w$ 方向上的分量, 而 $\\nabla_vL$ 只是再多乘一個 scalar,也就是說 $\\nabla_vL\\perp w$, i.e. $w^T\\nabla_vL=0$ (只要利用 (4) 計算就可知道) SGD 會使得 $v$ 長度愈來愈大 用 SGD update $v$ 的時候公式為:$$v&apos;=v+\\Delta v$$ 且 $\\Delta v\\propto\\nabla_vL$ by steepest descent.而因為 $\\nabla_vL\\perp w$ 所以 $\\Delta v\\perp w$. (要 update 的向量與目前的 weight 垂直)由最開始的分解 $(\\star)$ 我們知道 $v$ 與 weight $w$ 同方向. 所以自然 $\\Delta v\\perp v$.這就導致了 update 後的 $v’$ 長度會比 $v$ 來得大 (三角不等式), 如下圖: 所以經過多次 SGD, $v$ 長度會愈來愈大. 與 Batch Normalization 的關聯 BN 在過一層 linear weight $v$ 後為:$$\\begin{align} v^Tf_{BN}(x)= v^T\\left(g\\cdot\\frac{x-\\mu}{\\sigma}+b\\right) \\end{align}$$ 其中 $\\mu,\\sigma$ 都是從訓練時的 mini-batch 統計的, 而 $g,b$ 是 trainable 的參數而 WN 對 weight $w$ 為 (不看 non-linear activation 那項):$$f_{WN}(x;w)= w^Tx = {g\\over\\|v\\|}v^Tx \\\\ = v^T\\left(g\\cdot\\frac{x}{\\|v\\|}\\right) = v^Tf_{BN}(x)$$ 對照 BN 可以知道設定 $\\sigma=\\|v\\|,\\mu=0,b=0$ 就變成 WN!但 WN 的好處是不依賴 mini-batch 的設定, 這在如果 batch size 較小的情況會比較有利. BN在Conv後會有Conv的Weight具有Scale Invariant特性 WN 對於 $v$ 會愈 update 愈大, 考慮 BN 是否也有這樣的狀況?一般來說, 我們會這麼串: activation(BN(convolution(x)))將 BN 放在 convolution 後 activation 之前, 這樣可以最後做完 quantizaiton 的時候, convolution 和 BN 的 weight 做融合.令 $w$ 當作 convolution 的 weights, 如果 weights 做 $\\alpha$ 倍的 scale: $w’=\\alpha w$, 則對 BN 後的結果不會有影響, 這是因為 $\\mu’=\\alpha\\mu$, and $\\sigma’=\\alpha\\sigma$ 也跟著一起 scale$$f_{BN}(\\alpha w^Tx)=f_{BN}(w^Tx)$$ 明確寫出來一個 function $f$ 對 input $w$ 是 scale invariant:$$f(\\alpha w)=f(w),\\quad \\forall \\alpha\\in\\mathbb{R}$$ 微積分我們學過 gradient vector 會跟 coutour 的 level curve 垂直把 scale invariant function 的 “等高線” contour map 畫出來, 示意圖大概這樣: 可以看到做 SGD update 的方向會跟 contour 垂直, 導致跟之前討論 WN $v$ 會愈來愈大的狀況一樣, Convolution 的 weight $w$ 也會隨著 SGD update 愈來愈大.因此我們在使用 activation(BN(convolution(x))) 這樣的 layer 的時候可能會觀察到這樣的現象.到這邊我們可能會擔心, 會不會訓練下去 $\\|w\\|_2$ 會發散?通常來說不用擔心, 因為離零點愈遠則 gradient 愈小. 這是因為 loss surface 只跟角度有關, 離零點愈遠的 loss surface 會愈稀疏、平坦. 這樣一來雖然每次 update $\\|w\\|_2$ 都會變大, 但變大的幅度愈來愈小. 這篇 blog 文章 (by inFERENCe) 也有描述, 裡面的圖也解釋得很好. 💡 另外也可以 update 完 weight 後, 再把 convolution 的 weight 直接 normalized, 因為反正是 scale invariant function, 不影響輸出結果. $v$ 和 $g$ 的初始化 可以參考 模型优化之Weight Normalization 的說明就好.論文有題到 WN 對於 initialization 比較敏感 Pytorch 的 API torch.nn.utils.weight_norm注意 weight normalization 是這種形式:$$y=\\phi(w^Tx+b)$$ (markdown渲染怪怪的, 改用圖) 注意到 conv2d 一次的”內積” 是處理 in_channel * kernel_height * kernel_width, 所以一個 $w$ 的維度也是如此.總共有 out_channel 這麼多個的 “內積”, 也就是有這麼多的 $w$.另外, 把 stride or dilation 改動不會影響 weight_g and weight_v 的 size Summary WN 直接將參數拆成大小和方向向量分別 update. 希望藉由這樣拆解能減緩 ill-conditioned 狀況, 使模型收斂速度加快. 同時 WN 也不依賴 mini-batch, 這在 batch size 如果比較小的時候不會像 BN 效果變差, 或是比較適用於 RNN.不過拆成這樣參數量也會增加, 但其實 BN 也需要額外的 memory 來存 $\\mu,\\sigma$, 這樣比就要看誰划算了.另外探討了 activation(BN(convolution(x))) 有時會觀察到 Convolution 的 weight $w$ 也會隨著 SGD update 愈來愈大.這個現象跟本文 WN 裡面討論到方向向量 $v$ 的大小也會愈 update 愈大道理是很像的. 不過目前遇到的實務上, 比較少使用 WN, 大部分還是用 BN, LN (Layer Normalization).有效性我自己還要再多觀察 最後透過看這篇論文, 仔細推導裡面的數學和理解其物理意義, 這對我來說還是很有幫助的. Reference Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks 详解深度学习中的Normalization，BN/LN/WN Weight Normalization 相比batch Normalization 有什么优点呢？ torch.nn.utils.weight_norm Exponentially Growing Learning Rate? Implications of Scale Invariance induced by Batch Normalization by inFERENCe","tags":[{"name":"Weight Normalization","slug":"Weight-Normalization","permalink":"https://bobondemon.github.io/tags/Weight-Normalization/"},{"name":"Batch Normalization","slug":"Batch-Normalization","permalink":"https://bobondemon.github.io/tags/Batch-Normalization/"},{"name":"Scale Invariant Function","slug":"Scale-Invariant-Function","permalink":"https://bobondemon.github.io/tags/Scale-Invariant-Function/"}]},{"title":"Why Stochastic Weight Averaging? averaging results V.S. averaging weights","date":"2022-07-20T14:56:34.000Z","path":"2022/07/20/Why-Stochastic-Weight-Averaging-averaging-results-V-S-averaging-weights/","text":"由以前這篇文章知道, 對多顆不同 models 的結果取平均通常會得到更好的結果.但如果對 models 的參數先取平均呢? 一樣會好嗎?Stochastic Weight Averaging (SWA) 的這篇文章 “Averaging Weights Leads to Wider Optima and Better Generalization“ 嘗試說明這是有效的.而實務上, PyTorch 和 PyTorch Lightning 也已經直接導入了 SWA 的 API. 甚至在語音辨識業界裡, 有取代 Kaldi 勢頭的 WeNet 裡面也有類似的機制. 本文直接截圖自己的 slides 內容, 而 Pdf 檔案可參考 這裡 投影片內容 直接上圖: References Why-Aggregation-Work Averaging Weights Leads to Wider Optima and Better Generalization PyTorch 1.6 now includes Stochastic Weight Averaging Stochastic Weight Averaging in PyTorch Lightning WeNet","tags":[{"name":"Stochastic Weight Averaging","slug":"Stochastic-Weight-Averaging","permalink":"https://bobondemon.github.io/tags/Stochastic-Weight-Averaging/"},{"name":"SWA","slug":"SWA","permalink":"https://bobondemon.github.io/tags/SWA/"}]},{"title":"SGD 泛化能力的筆記","date":"2022-05-28T14:15:58.000Z","path":"2022/05/28/SGD-Ggeneralization-Notes/","text":"Sharp V.S. Flat Local Minimum 的泛化能力先簡單介紹這篇文章:On large-batch training for deep learning: Generalization gap and sharp minima考慮下圖兩個 minimum, 對於 training loss 來說其 losses 一樣. 從圖可以容易理解到, 如果找到太 sharp 的點, 由於 test and train 的 mismatch, 會導致測試的時候 data 一點偏移就會對 model output 影響很大.論文用實驗的方式, 去評量一個 local minimum 的 sharpness 程度, 簡單說利用 random perturb 到附近其他點, 然後看看該點 loss 變化的程度如何, 變化愈大, 代表該 local minimum 可能愈 sharp.然後找兩個 local minimums, 一個估出來比較 sharp 另一個比較 flat. 接著對這兩點連成的線, 線上的參數值對應的 loss 劃出圖來, 長相如下: 這也是目前一個普遍的認知: flat 的 local minimum 泛化能力較好.所以可以想像, step size (learning rate) 如果愈大, 愈有可能跳出 sharp minimum.而 batch size 愈小, 表示 gradient 因為 mini-batch 造成的 noise 愈大, 相當於愈有可能”亂跑”跑出 sharp minimum.但這篇文章僅止於實驗性質上的驗證. Step size and batch size 對於泛化能力, 或是說對於找到比較 flat optimum 的機率會不會比較高? 兩者有什麼關聯呢?DeepMind 的近期 (2021) 兩篇文章給出了很漂亮的理論分析. Full-Batch Gradient (Steepest) Descent 再來介紹這篇: Implicit Gradient Regularization, DeepMind 出品.想探討為什麼 NN 的泛化能力這麼好? 結論就是跟 Gradient Descent 本身算法特性有關.一般我們對 cost (loss) function 做 gradient (steepest) descent 公式如下: $$\\begin{align} \\omega_{n+1}=\\omega_n-h\\nabla C(\\omega_n) \\end{align}$$ 其中 $h$ 為 step size (learning rate), $\\omega\\in\\mathbb{R}^d$ 表示 parameters.當 $h\\rightarrow 0$, $n$ 變成連續的時間 $t$, 則可視為一個 Ordinary Differential Equation (ODE) system, 整理如下:$$\\begin{align} \\text{Cost Function}: C(\\omega) \\\\ \\text{ODE}: \\dot{\\omega}=f(\\omega)=-\\nabla C(\\omega) \\end{align}$$給定 initial point $\\omega_0$, 上面的 ODE 求解就是一條連續的 trajectory. 💡 我們在 Numerical Methods for Ordinary Differential Equations 有介紹各種數值方法, 可以知道 gradient descent 就是 Euler method, 而這樣的 error 是 $O(h^2)$. 用式 (1) gradient descent ($h$ 固定) 求解, 會使得 trajectory 跟連續的 ODE (3) 的不同. 注意到這裡沒有使用 mini-batch, 用的是 full-batch, 所以不是 Stochastic gradient descent (SGD). 如果我們能對 gradient descent 的 trajectory 用另一個 ODE system 的 trajectory 代表的話 (怎麼找等等再說), 分析修改過後的 ODE 和原來的 ODE systems 說不定能看到什麼關聯. 這正是這篇論文的重要發現.先來看看修改過後的 ODE 長什麼樣:$$\\begin{align} \\text{Cost Function}: \\tilde{C}_{gd}(\\omega)=C(\\omega)+\\frac{h}{4}\\|\\nabla C(\\omega)\\|^2 \\\\ \\text{ODE}: \\dot{\\omega}=\\tilde{f}(\\omega)=-\\nabla\\tilde{C}_{gd}(\\omega) \\end{align}$$ 注意到最佳解與原來的 ODE system 一樣: $C(\\omega)$ 和 $\\tilde{C}_{gd}(\\omega)$ 最佳解相同. (很容易可以看出來因為 minimal points 其 gradient 必定為 $0$)將三條 trajectories 用圖來表示的話如下:&emsp;- Gradient descent 的 trajectory 式 (1): 綠色箭號線&emsp;- ODE 的 trajectory 式 (3): 黑色線&emsp;- 修改後的 ODE 的 trajectory 式 (5): 黃色線, 可以用來代表 gradient descent 的 trajectory(參考自 inFERENCe blog 文章: Notes on the Origin of Implicit Regularization in SGD)為什麼可以用修改後的 ODE 代表 gradient descent 的 trajectory 呢?因為兩者差異夠小, 為 $O(h^3)$, 比 gradient descent 和原本 ODE 之間的 error $O(h^2)$ 更小.(綠色箭號線比起黑色線更接近黃色線) 再來我們回答這個問題: 怎麼找到 (4) (5) 這樣的 ODE 可以用來代表 gradient descent 的 trajectory 呢?💡 需利用 backward error analysis, 這裡略過, 請參考 [ref1] [ref2] 其中 ref2 裡的二階 Taylor expansion 補充推導:$$\\left.\\frac{d^2}{dt^2}\\tilde{y}(t)\\right|_{t=t_n}=\\left.\\frac{d}{dt}\\left[ f(\\tilde{y}(t))+hf_1(\\tilde{y}(t)) \\right]\\right|_{t=t_n} \\\\ =\\left.\\left[ f&apos;(\\tilde{y}(t))\\frac{d\\tilde{y}(t)}{dt}+hf_1&apos;(\\tilde{y}(t))\\frac{d\\tilde{y}(t)}{dt} \\right]\\right|_{t=t_n} \\\\ =\\left.\\left[ f&apos;(\\tilde{y}(t))\\tilde{f}(\\tilde{y}(t))+hf_1&apos;(\\tilde{y}(t))\\tilde{f}(\\tilde{y}(t)) \\right]\\right|_{t=t_n} \\\\ =\\left.\\left[ \\left( f&apos;(\\tilde{y}(t))+hf_1&apos;(\\tilde{y}(t)) \\right)\\tilde{f}(\\tilde{y}(t)) \\right]\\right|_{t=t_n} \\\\ =(f&apos;(\\tilde{y}_n)+hf_1&apos;(\\tilde{y}_n))\\tilde{f}(\\tilde{y}_n)$$ 觀察 (4) 的 $\\tilde{C}_{gd}(\\omega)$, 可以發現相當於在原來的 cost function $C(\\omega)$ 加上一個正則項. 而該項正比於 gradient norm 的平方.白話就是如果 gradient 愈大, penalty 愈大, 所以優化的時候會傾向於找 gradient 小的區域. 相當於找比較 flat 的區域. 這樣有什麼好處呢? 如同一開始說的, 能提高泛化能力!另外正則項也正比於 step size $h$, 所以如果 step size 愈大, 表示對 sharp 區域的 penalty 愈大, 因此更加傾向找 flat 區域. 這也符合我們之前提到愈有可能跳出 sharp minimum 的觀點. 另外作者的 presentation 開頭也用以下例子說明這個現象: 大的 learning rate 傾向找比較 flat 的 minimum, 也就是泛化能力較好. 所以對應到上圖顯示的 Test 情況下最好的 learning rate 比 training 的要大.總結來說提供了一個看法, 說明為什麼 NN 的表現這麼好, 特別是泛化能力. 很意外的是, 其實跟我們用的 gradient descent 天生的特性有關. Mini-Batch Stochastic Gradient Descent 上一段都還沒考慮 mini-batch 的情況. 因為一旦變成 mini-batch 相當於 gradient 被加上了 random noise 變的更難分析. 因此 DeepMind 他們發了一篇後續文章: On the Origin of Implicit Regularization in Stochastic Gradient Descent, 將 mini-batch 考量進去, 相當於分析 SGD 算法.由於 mini-batches 在一個 epoch 可能的順序不一樣, 所以一條 trajectory 對應到一個順序.(參考自 inFERENCe blog 文章: Notes on the Origin of Implicit Regularization in SGD) 我們變成要考量的是 “mean” trajectory. 類似地, mean trajectory 一樣可以用一個修改後的 ODE system 來代表它:$$\\begin{align} \\text{Mean Trajectory}: \\mathbb{E}(\\omega_m)=\\omega(mh)+O(m^3h^3)\\\\ \\text{Cost Function}:\\tilde{C}_{sgd}(\\omega)= \\tilde{C}_{gd}(\\omega) + \\underbrace{\\frac{h}{4m}\\sum_{i=0}^{m-1}\\|\\nabla \\hat{C}_i(\\omega)-\\nabla C(\\omega)\\|^2}_\\text{additional regularizer} \\\\ \\text{ODE}: \\dot{\\omega}=-\\nabla\\tilde{C}_{sgd}(\\omega) \\end{align}$$其中 $m$ 表示整個 training data 可以分成 $m$ 個 mini-batches. $\\nabla \\hat{C}_i(\\omega)$ 表示 i-th mini-batch 的 gradient.可以看到多了一項正則項: mini-batches 的 gradients 減掉 full-batch gradient 的 variance.我們就先當 $\\omega$ 已經是 local minimum 好了 ($\\nabla C(\\omega)=0$). 所以該正則項簡化成 mini-batches gradients 的 variance.相當於告訴我們, 如果 mini-batches 的那些 gradients 差異都很大的話, penalty 會比較大, 比較不會是 SGD 會找到的解.這樣的特性對於泛化能力有什麼關聯? inFERENCe 文章給了一個很清楚的說明:x-軸是 parameter $\\omega$, y-軸是 loss $C(\\omega)$.Variance of mini-batches’ gradients 左圖比右圖小, 因而造成右圖的 penalty 比較大, 所以 (8) 會傾向選擇左圖. 明顯的, 對於 test data 來說左圖的解會比右圖 robust, 因為 test data 可以看成上面不同 batches 的表現.可以從 (7) 看出來, 由於 additional regularizer 的關係, SGD 最佳解會跟原來 full-batch 的最佳解不同了. 除非所有 mini-batches 的 gradients 也都是 $0$.另外 (7) 在論文中也推導成另一個形式 (對比(7)為 additional regularizer 改寫了):$$\\mathbb{E}(\\tilde{C}_{sgd}(\\omega))=\\tilde{C}_{gd}(\\omega)+\\frac{N-B}{N-1}\\frac{\\color{orange}{h}}{4\\color{orange}{B}}\\Gamma(\\omega) \\\\ \\Gamma(\\omega)=\\frac{1}{N}\\sum_{i=1}^N \\|\\nabla C_i(\\omega)-\\nabla C(\\omega)\\|^2$$ 可以看出 learning rate and batch size 的關係, $h/B$ 如果維持一定比例, 則正則項的影響力大約相同. 作者 presentation 說, 經驗上 batch size double, learning rate 也要 double. [YouTube time]對應到 $h/B$ 比例不變, 所以 performance 應該也維持一樣 (在 $B$ 不大的情況下). 論文做了實驗結果如下: 結論 雖然存在一些假設才會使 SGD 的估計正確 ⚠️ 論文推導的假設: batch shuffle 的方式取 data, 也就是一個 epoch 會依序跑完 shuffle 後的所有 batches learning rate is finite (就是有 lower bound) 只分析 SGD, 其他更多變形例如 Adam, Adagrad, RMSProp, 等的行為不知道 $m^3h^3$ 必須要夠小, SGD 的 “mean” trajectory 才會符合 (7), (8) 的 ODE 結果. 一般 dataset 都很大 ($m$ 很大), 所以要把 $h$ 都設定很小, 感覺也有點難符合 (?). 影片: [here] 但總結來說, 在 full-batch 設定下, 實務上使用 steepest descent 從連續變成離散的路徑, 本身就提供了泛化能力的好處. 加上 mini-batch 的設定, 使得泛化能力更好了. 沒想到已經習以為常的 SGD 方法, 背後竟然藏了這樣的觀點, 太厲害了! References Implicit Gradient Regularization On the Origin of Implicit Regularization in Stochastic Gradient Descent inFERENCe: Notes on the Origin of Implicit Regularization in SGD Numerical Methods for Ordinary Differential Equations On large-batch training for deep learning: Generalization gap and sharp minima Paper presentation by author: On the Origin of Implicit Regularization in Stochastic Gradient Descent","tags":[{"name":"Ordinary Differential Equations","slug":"Ordinary-Differential-Equations","permalink":"https://bobondemon.github.io/tags/Ordinary-Differential-Equations/"},{"name":"ODE","slug":"ODE","permalink":"https://bobondemon.github.io/tags/ODE/"},{"name":"Gradient Descent","slug":"Gradient-Descent","permalink":"https://bobondemon.github.io/tags/Gradient-Descent/"},{"name":"Stochastic Gradient Descent","slug":"Stochastic-Gradient-Descent","permalink":"https://bobondemon.github.io/tags/Stochastic-Gradient-Descent/"},{"name":"SGD","slug":"SGD","permalink":"https://bobondemon.github.io/tags/SGD/"}]},{"title":"Numerical Methods for Ordinary Differential Equations","date":"2022-05-15T15:26:51.000Z","path":"2022/05/15/Numerical-Methods-for-Ordinary-Differential-Equations/","text":"如果對於 Differential Equation 完全沒概念, 建議先看以下兩分鐘的影片&emsp;- Solving Differential Equations vs. Solving Algebraic Equations主要筆記了 Prof. Jeffrey Chasnov 在 Coursera 的兩門課 針對 numerical solution 解 ODE 的內容:&emsp;1. Differential Equations for Engineers&emsp;2. Numerical Methods for Engineers本文介紹:&emsp;1️⃣ Introduction to ODE: linear? ordinary? n-th order?&emsp;2️⃣ Euler Method: 雖然簡單, 但 error 很大&emsp;3️⃣ Modified Euler Method: error $O(\\Delta t^3)$, 比 Euler method 小了一個 order&emsp;4️⃣ Runge Kutta Methods: Modified Euler 方法是 Second-order RK 的一個特例&emsp;5️⃣ Higher-order Runge-Kutta Methods: $n$-th order RK 的 error 為 $O(\\Delta t^{n+1})$&emsp;6️⃣ Higher-order ODEs and Systems: 以上都只介紹 first-order ODE 逼近法, 那 higher-order ODE 怎解? 👏 那兩門課的講義教授很佛心得都有附上:Lecture notes: Differential Equations for EngineersLecture notes: Numerical Methods for Engineers 1️⃣ Introduction to differential equations | Lecture 1 | Differential Equations for Engineers [YouTube]$$\\begin{align} L\\frac{d^2q}{dt^2}+R\\frac{dq}{dt}+\\frac{1}{C}q=\\varepsilon_0\\cos wt \\\\ ml\\frac{d^2\\theta}{dt^2}+cl\\frac{d\\theta}{dt}+mg\\sin\\theta=F_0\\cos wt \\\\ \\frac{\\partial u}{\\partial t}=D\\left(\\frac{\\partial^2u}{\\partial x^2}+\\frac{\\partial^2u}{\\partial y^2}+\\frac{\\partial^2u}{\\partial z^2}\\right) \\end{align}$$ 其中 $q=q(t)$, $\\theta=\\theta(t)$, $u=u(x,y,z,t)$全部都是 $2^{\\text{nd}}$-order, 因為 independent variable $x,y,z,t$ 的微分最高 order 等於 2.(1) and (2) 是 ordinary differential equation (ODE)(3) 是 partial differential equation (PDE)(1) and (3) 是 linear, (2) 是 nonlinear, 因為有 $\\sin\\theta$.Linear 指的是要微分的那個 function (i.e. $q,\\theta,u$) 不管有沒有要微分, 都不能過一個 nonlinear function, 例如 $d(q^2)/dt$ 這樣就不行 (平方是 nonlinear function), 或沒有要微分, 直接 $q^2$ 也不行.Linear N-order ODE/PDE 有解析解, 所以 “Differential Equations for Engineers” 這門課主要講這部分.不過如果是 non-linear $n^\\text{th}$-order ODE 雖沒有解析解, 但可利用以下介紹的 numerical methods 求近似解.The general linear third-order ode, where $y=y(x)$:$$a_3(x)y&apos;&apos;&apos;+a_2(x)y&apos;&apos;+a_1(x)y&apos;+a_0(x)y=b(x)$$ where the $a$ and $b$ coefficients can be any function of $x$. 2️⃣ Euler method | Lecture 2 | Differential Equations for Engineers | Lecture 48 | Numerical Methods for Engineers [YouTube] [YouTube]討論一個 first-order ode (不只 linear, 也包含 non-linear, 所以一般沒有 analytical solution) 的逼近求解方法.這裡說包含 nonlinear 是因為 $f(x,y)$ 有可能使的 function $y=y(x)$ 會變成 nonlinear$$\\frac{dy}{dx}=f(x,y) \\\\ y(x_0)=y_0$$ 用 numerical method 去逼近. Euler method 是 first order method. 3️⃣ Modiﬁed Euler Method | Lecture 49 | Numerical Methods for Engineers [YouTube]原來的 Euler method 要計算 $x_{n+1}$ 的時候用下式逼近$$x_{n+1}=x_n+\\Delta t\\underbrace{f(t_n,x_n)}_\\text{slope}$$ 但可以看到 slope 其實一直在變化. 如果說 slope 改成用 $f(t_n,x_n)$ and $f(t_n+\\Delta t,x_{n+1})$ 的平均呢?這就是 Modified Euler Method 主要想法. 💡 Modified Euler Method 屬於 $2^\\text{nd}$-order Runge-Kutta Methods (RK2) 的一種.下段介紹 RK 方法時會推導可以知道確實 error order 會比較小注意到 Runge-Kutta 說的 order 指 error 的 order, 跟 ODE 的 order 代表 independent variable 的最高次數微分意思不一樣. 但我們不知道 $x_{n+1}$ 要怎麼算 $f(t_n+\\Delta t,x_{n+1})$ 呢? 所以就先算一個 $x_{n+1}$ 的 prediction. $$\\begin{align} x_{n+1}^p=x_n+\\Delta tf(t_n,x_n) \\\\ x_{n+1}=x_n+\\frac{\\Delta t}{2}\\left[f(t_n,x_n)+f(t_n+\\Delta t,x_{n+1}^p)\\right] \\end{align}$$ 可以簡化一下變成下面幾個 stages:$$\\begin{align} K_1=\\Delta t f(t_n,x_n) \\\\ K_2=\\Delta tf(t_n+\\Delta t,x_n+K_1) \\\\ x_{n+1}=x_n+\\frac{1}{2}(K_1+K_2) \\end{align}$$ 4️⃣ Runge Kutta Methods | Lecture 50 | Numerical Methods for Engineers [YouTube]對一個 first order ODE, 且已知 $x(t_n)=x_n$ initial value 來說$$\\dot{x}=f(t,x)$$ 經過時間 $\\Delta t$ 後的 $x(t_n+\\Delta t)$ 要怎麼估計比較準? 我們先看 Taylor expansion:$$\\begin{align} x(t_n+\\Delta t)=x(t_n)+\\Delta tf(t_n,x_n)+\\frac{(\\Delta t)^2}{2}\\left.\\frac{d}{dt}f(t,x(t)) \\right|_{t=t_n} + O(\\Delta t^3) \\end{align}$$ 二次微分項我們用 chain rule 繼續展開:$$\\left.\\frac{d}{dt}f(t,x(t))\\right|_{t=t_n}=\\left.\\frac{\\partial}{\\partial t}f(t,x)\\right|_{t=t_n} + \\left.\\frac{\\partial}{\\partial x}f(t,x(t))\\frac{dx}{dt}\\right|_{t=t_n} \\\\ = \\frac{\\partial}{\\partial t}f(t_n,x_n) + \\left.\\frac{\\partial}{\\partial x}f(t,x(t))f(t,x(t))\\right|_{t=t_n} \\\\ = \\frac{\\partial}{\\partial t}f(t_n,x_n) + \\frac{\\partial}{\\partial x}f(t_n,x_n)f(t_n,x_n) \\\\ \\triangleq f_t(t_n,x_n)+f_x(t_n,x_n)f(t_n,x_n)$$ 代回去 (9) 得到 $x(t_n+\\Delta t)$ 的泰勒展開式: $$\\begin{align} x(t_n+\\Delta t)=x_n+\\Delta tf(t_n,x_n) + \\frac{(\\Delta t)^2}{2}(f_t(t_n,x_n)+f_x(t_n,x_n)f(t_n,x_n)) + O(\\Delta t^3) \\end{align}$$ Second order Runge-Kutta methods 的步驟如下:$$\\begin{align} K_1=\\Delta tf(t_n,x_n) \\\\ K_2=\\Delta tf(t_n+\\alpha\\Delta t,x_n+\\beta K_1) \\\\ x_{n+1}=x_n+aK_1+bK_2 \\end{align}$$ 全部合成一個式子:$$\\begin{align} x_{n+1}=x_n+a\\Delta tf(t_n,x_n)+b\\Delta t \\underbrace{f(t_n+\\alpha\\Delta t,x_n+\\beta \\Delta tf(t_n,x_n))}_\\text{using Taylor expansion} \\\\ =x_n+a\\Delta tf(t_n,x_n)+b\\Delta t \\left[ f(t_n,x_n) + \\alpha\\Delta t f_t(t_n,x_n)+\\beta\\Delta tf(t_n,x_n)f_x(t_n,x_n) + O(\\Delta t^2) \\right] \\\\ = x_n+(a+b)\\Delta tf(t_n,x_n)+(\\Delta t)^2 \\left[ \\alpha b f_t(t_n,x_n) + \\beta bf(t_n,x_n)f_x(t_n,x_n) \\right]+O(\\Delta t^3) \\end{align}$$ 💡 補充一下 (14) 的 Taylor expansion:$$f(t_n+\\Delta t,x_n+\\Delta x)=f(t_n,x_n)+\\left[\\begin{array}{cc}f_t(t_n,x_n) &amp; f_x(t_n,x_n)\\end{array}\\right]\\left[\\begin{array}{c}\\Delta t \\\\ \\Delta x\\end{array}\\right] + O\\left( \\left\\| \\left[\\begin{array}{c}\\Delta t \\\\ \\Delta x\\end{array}\\right] \\right\\|^2 \\right) \\\\ =f(t_n,x_n)+\\Delta t f_t(t_n,x_n) + \\Delta x f_x(t_n,x_n) + O(\\Delta t^2 + \\Delta x^2)$$ 不管 $O(\\Delta t^3)$ 項的情況下, 令 $x(t_n+\\Delta t)=x_{n+1}$, i.e. (10)=(16) , 得到:$$\\begin{align} a+b=1 \\\\ \\alpha b=\\frac{1}{2}\\\\ \\beta b=\\frac{1}{2} \\end{align}$$ 所以結論就是使用 Second order Runge-Kutta methods 的步驟 (11)-(13), 所產生的 error order 為 $O(\\Delta t^3)$. Check (6)-(8) 的 Modified Euler method 步驟再跟 Second order Runge-Kutta methods 的步驟 (11)-(13) 對比.很容易可以發現這是 $a=b=1/2,\\alpha=\\beta=1$ 的情況, 同時也因為滿足 (17)-(19) 的條件, 所以 Modified Euler method 是 Second order Runge-Kutta methods 的一種情況.另一種 case 是叫 midpoint method: $a=0, b=1, \\alpha=\\beta=1/2$. 5️⃣ Higher-order Runge-Kutta Methods | Lecture 52 | Numerical Methods for Engineers [YouTube]Runge-Kutta Methods 的 order 跟精確度有關, 例如 $n^{th}$-order 表示 error term 只有 $O(\\Delta t^{n+1})$ 大小. 💡 給定一個可容忍的 error tolerance $\\varepsilon$, 怎麼決定 step size $\\Delta t$ 多大, 在 RK4/5 是 ok 的, 這樣的決定方法稱為 Adaptive Runge-Kutta methods. 這邊就不筆記了. 大概了解一下愈高 order 所需要的 stages 愈多$4^\\text{th}$-order 需要 $4$ stages, 但 $5^\\text{th}$-order 變成要 $6$ stages. 所以 $4^\\text{th}$-order 很不錯, 稱 RK4. 6️⃣ Higher-order ODEs and Systems | Lecture 53 | Numerical Methods for Engineers [YouTube]ODE 的 order 指的是 independent variable 的最高微分次數.例如 $F=ma$ 就是 $2^\\text{nd}$-order ODE:$$F=m\\frac{d^2x}{dt^2}$$ 建議直接看 Lecture 53 Higher-order odes and systems.概念就是 $n^\\text{th}$-order ODE 可以拆成 $n$ 個 $1^\\text{st}$-order ODEs, 然後當成一個 dimension $n$ 的 vector 來看, 套用 Runge Kutta Methods (RK2, RD4 都可以) 來解. [Second-order ODE 範例]:Write down the second-order Runge-Kutta modified Euler method (predictor-corrector method) for the following system of two first-order odes:$$\\dot{x}=f(t,x,y) \\\\ \\dot{y}=g(t,x,y)$$ [Ans:]我們令 $Z=[x,y]^T$: (符號有點濫用，不過看得懂就可以)$$\\nabla_tZ=[\\dot{x}, \\dot{y}]^T \\\\ \\nabla_tZ(t_n,Z_n)=[f(t_n,x_n,y_n),g(t_n,x_n,y_n)]^T$$ 對 $Z$ 做 modified Euler method:$$Z_{n+1}^p=Z_n+\\Delta t\\nabla_tZ(t_n,Z_n) \\\\ Z_{n+1}=Z_n+\\frac{\\Delta t}{2}\\left(\\nabla_tZ(t_n,Z_n)+\\nabla_tZ(t_n+\\Delta t,Z_{n+1}^p)\\right)$$ 整理成 Runge-Kutta steps:$$A_1=\\Delta t\\nabla_tZ(t_n,Z_n) \\\\ A_2=\\Delta t\\nabla_tZ(t_n+\\Delta t,Z_n+A_1) \\\\ Z_{n+1}=Z_n+\\frac{1}{2}(A_1+A_2)$$ 拆開每個維度來看:$$A_1=\\Delta t[f(t_n,x_n,y_n),g(t_n,x_n,y_n)]^T = [K_1,L_1]^T\\\\ A_2=\\Delta t \\left[\\begin{array}{cc} f(t_n+\\Delta t,x_n+K_1,y_n+L_1) \\\\ g(t_n+\\Delta t,x_n+K_1,y_n+L_1) \\end{array}\\right] =\\left[\\begin{array}{c} K_2\\\\L_2 \\end{array}\\right] \\\\ Z_{n+1}=\\left[\\begin{array}{cc} x_{n+1}\\\\y_{n+1} \\end{array}\\right] =\\left[\\begin{array}{cc} x_n+\\frac{1}{2}(K_1+K_2) \\\\ y_n+\\frac{1}{2}(L_1+L_2) \\end{array}\\right]$$ 結論就是可以拆成兩個 parallel 的 update 步驟, 最後的公式: 🤔 一開始不確定兩個 parallel 的 update 步驟是不是正確的, 因為會互相參照. 但如果當成一個 random vector $Z$, 就如上面推導, 拆開看各個維度就沒錯了.","tags":[{"name":"Ordinary Differential Equations","slug":"Ordinary-Differential-Equations","permalink":"https://bobondemon.github.io/tags/Ordinary-Differential-Equations/"},{"name":"ODE","slug":"ODE","permalink":"https://bobondemon.github.io/tags/ODE/"},{"name":"Euler Method","slug":"Euler-Method","permalink":"https://bobondemon.github.io/tags/Euler-Method/"},{"name":"Modified Euler Method","slug":"Modified-Euler-Method","permalink":"https://bobondemon.github.io/tags/Modified-Euler-Method/"},{"name":"Runge Kutta Methods","slug":"Runge-Kutta-Methods","permalink":"https://bobondemon.github.io/tags/Runge-Kutta-Methods/"}]},{"title":"忘記物理也要搞懂的 Hamiltonian Monte Carlo (HMC) 筆記","date":"2022-05-07T10:09:02.000Z","path":"2022/05/07/Hamiltonian-Monte-Carlo/","text":"先說我物理什麼的都還給老師了, 只能用自己理解的方式, 筆記下 Hamiltonian dynamic. 💡 如果連我都能懂, 相信大家都能理解 HMC 了 但還是建議先看 MCMC by Gibbs and Metropolis-Hasting Sampling, 因為這篇要說的 Hamiltonian Monte Carlo (HMC) 是 Metropolis-Hastings (MH) 方法的一種, 只是 proposal distribution 從 random walk 改成使用 Hamiltonian dynamics 來做, 因而變的非常有效率 (accept rate 很高), 且對於高維度資料採樣也很有效. 首先粗體字如 $\\mathbf{x}, \\mathbf{v}, \\mathbf{p}$ 都是 column vector, 而非粗體字表 scalar, e.g. $m,t$ Hamiltonian dynamic 一物體在位置 $\\mathbf{x}$ (這裡可想成是高度) 的重力位能 (potential energy) 為 $$\\begin{align} U(\\mathbf{x})=m\\mathbf{g}^T\\mathbf{x} \\end{align}$$ 其中 $m$ 表質量, $\\mathbf{g}$ 表重力加速度 (是個向量, 所以有方向性).同時該物體, 其本身也存在動能 (kinetic energy) 且可表示為: $$\\begin{align} K(\\mathbf{p})=\\frac{\\mathbf{p}^T\\mathbf{p}}{2m}\\left(=\\frac{1}{2}m\\mathbf{v}^2\\right) \\\\ \\mathbf{p}=m\\frac{d\\mathbf{x}}{dt}\\left(=m\\mathbf{v}\\right) \\end{align}$$ $\\mathbf{v}$ 表速度 (是個向量, 所以有方向性), $\\mathbf{p}$ 我們稱為動量 (momentum).整個封閉系統 (沒有外界的其他能量介入) 的能量為: $$\\begin{align} H(\\mathbf{x},\\mathbf{p})=U(\\mathbf{x})+K(\\mathbf{p}) \\\\ =m\\mathbf{g}^T\\mathbf{x}+\\frac{\\mathbf{p}^T\\mathbf{p}}{2m} \\end{align}$$ 根據能量守恆 (energy conservation), 不管時間 $t$ 是什麼, 整個系統的能量 $H(\\mathbf{x},\\mathbf{p})$ 都維持相同.此時如果我們知道該物體的初始狀態 $(\\mathbf{x}_0,\\mathbf{p}_0)$ 的話, 事實上可以知道任何時間 $t$ 下的位置和動量 $(\\mathbf{x},\\mathbf{p})$而這樣的關係可以由下面的 Hamiltonian equations 描述出來: $$\\begin{align} \\frac{dx_i}{dt}=\\frac{\\partial H}{\\partial p_i} \\\\ \\frac{dp_i}{dt}=-\\frac{\\partial H}{\\partial x_i} \\end{align}$$ 其中 $i\\in\\{1,..,d\\}$, $d$ 表空間的維度. 只要使用 $\\mathbf{p}=m\\mathbf{v}$, 速度 $\\mathbf{v}$ 是 $\\mathbf{x}$ 對時間的微分, 以及速度對時間的微分等於負加速度 $-\\mathbf{g}$ (座標系統定義為往上的座標是正的, 而重力加速度是向下的, 所以值為負)就可以從 (5) 推導出 (6) 和 (7).$$\\frac{\\partial H}{\\partial p_i}=\\frac{\\partial K(\\mathbf{p})}{\\partial p_i}=\\frac{\\partial}{\\partial p_i}\\frac{\\sum_j p_j^2}{2m}=\\frac{p_i}{m}=\\frac{m v_i}{m}=v_i=\\frac{dx_i}{dt} \\\\ \\frac{\\partial H}{\\partial x_i}=\\frac{\\partial U(\\mathbf{x})}{\\partial x_i}=\\frac{\\partial m\\sum_j g_jx_j}{\\partial x_i}=mg_i=m\\frac{d(-v_i)}{dt}=-\\frac{dmv_i}{dt}=-\\frac{dp_i}{dt}$$ 所以如果已知時間 $t$ 的位置 $x_i(t)$, 想預估 $t+\\varepsilon$ 的位置 $x_i(t+\\varepsilon)$ 的話, 可以透過 (6) 的方式更新: $$x_i(t+\\varepsilon)\\approx x_i(t)+\\varepsilon\\cdot\\frac{dx_i(t)}{dt}=x_i(t)+\\varepsilon\\cdot\\frac{\\partial K(\\mathbf{p}(t))}{\\partial p_i} \\\\ =x_i(t)+\\varepsilon\\cdot\\frac{\\partial\\left(\\mathbf{p}(t)^T\\mathbf{p}(t)/2m\\right)}{\\partial p_i} = x_i(t)+\\varepsilon\\cdot\\frac{p_i(t)}{m}$$ 只要 $\\varepsilon$ 夠小的話, 就會夠接近.同理 $p_i(t+\\varepsilon)$ 也能用 (7) 估計出來. 總結為以下 update 方法 (令 $m=1$), 而這個方法稱為 Euler’s method: $$\\begin{align} x_i(t+\\varepsilon) = x_i(t)+\\varepsilon\\cdot p_i(t) \\\\ p_i(t+\\varepsilon) = p_i(t)-\\varepsilon\\cdot\\frac{\\partial U(\\mathbf{x}(t))}{\\partial x_i} \\end{align}$$ 但致命的缺點是一旦時間長了, 估計就愈來愈不準了. 因此實作上會採用 Leapfrog method: pdf 介紹.我們先看看兩種方法的精確度差異 (取自 DeepBayes 2019 Summer School Day 5, MCMC slides): Leapfrog method 描述如下: $$\\begin{align} p_i(t+\\varepsilon/2)=p_i(t)-(\\varepsilon/2)\\cdot\\frac{\\partial U(\\mathbf{x}(t))}{\\partial x_i} \\\\ x_i(t+\\varepsilon)=x_i(t)+\\varepsilon \\cdot p_i(t+\\varepsilon/2) \\\\ p_i(t+\\varepsilon)=p_i(t+\\varepsilon/2)-(\\varepsilon/2)\\cdot\\frac{\\partial U(\\mathbf{x}(t+\\varepsilon))}{\\partial x_i} \\end{align}$$ 主要的想法是, 在 update $x_i(t+\\varepsilon)$ 時 (式 (8)), 原來使用 $p_i(t)$ 來更新, 改成使用”更準的” $p_i(t+\\varepsilon/2)$ 來更新, 如同式 (11). 然後 $p_i(t+\\varepsilon)$ 分成兩次的 $\\varepsilon/2$ steps 來更新. 抱歉沒有嚴謹的數學來證明 error 的 order. 注意到, 我們只需要 $\\nabla_\\mathbf{x}U(\\mathbf{x})$ 就能更新 $(\\mathbf{x},\\mathbf{p})$!也就是說只要有 potential energy 的 gradient 就可以模擬 Hamiltonian dynamic!這點很重要, 因為變成 sampling 方法後等同於這句話: 只要有 score function 就能採樣! 而 score function 怎麼估計, Score Matching 是個好方法.Sample codes from (Markov Chain Monte-Carlo Solution.ipynb): 12345678def _leapfrog(self, x, v): self.energy = [] for _ in range(self.n_steps): v -= 0.5 * self.eps * self.dist.grad_log_density(x) x = x + self.eps * v v -= 0.5 * self.eps * self.dist.grad_log_density(x) self.energy.append(self._energy(x, v)) return x, v 彈簧例子 參考自 MCMC: Hamiltonian Monte Carlo (a.k.a. Hybrid Monte Carlo)不知道怎麼來的也沒關係, 反正此時的系統能量為: $$U(x)=\\frac{x^2}{2} \\\\ K(p)=\\frac{p^2}{2} \\\\ H(x,p)=U(x)+K(p)$$ 位置 $x$ 的參考原點定義在彈簧中心, 也就是剛好 potential energy 為 $0$ 的時候. 使用 Leapfrog method 來模擬 Hamiltonian equations 更新狀態 $(\\mathbf{x},\\mathbf{p})$: 可以看到 Hamiltonian dynamic 的過程, 能量上 (左下角的圖) 只是 potential 和 kinetic energy 的互相交換 (黃色和青色互相消長), 其總能量是不變的.哎不對~總能量 $H$ 的那條全黃色的 bar 沒有固定不動啊, 看起來還是會小幅度上上下下的.是的, 縱使用了 Leapfrog method, 還是會漂移, 這是因為我們對時間離散化造成的. 想要 error 更小, 就必需切更細碎的時間去逼近.另外需要特別說明, 右下角的 “Phase Space” 圖, 畫出了 $(x,p)$ 狀態值的移動路徑. 由於能量守恆, 這路徑代表了相同的 Hamiltonian energy. 為什麼要特別說明這點, 在後面講 sampling 的時候會再次提到. 能量怎麼看成機率分佈? 或反過來? 給定能量 $E(x)$, 總是可以用下式變成機率分佈 (Gibbs distribution): $$p(x)=\\frac{1}{Z}e^{-E(x)}$$ $Z$ 就是一個 normalization constant 變成機率分佈用的. 所以能量愈大表示的機率密度就愈小.我們來將 Hamiltonian energy 變機率分佈看看: $$p(\\mathbf{x},\\mathbf{p})=\\frac{1}{Z}e^{-H(\\mathbf{x},\\mathbf{p})} \\\\ \\propto e^{-(U(\\mathbf{x})+K(\\mathbf{p}))} =e^{-U(\\mathbf{x})}e^{-K(\\mathbf{p})} =p(\\mathbf{x})p(\\mathbf{p})$$ 這裡可以發現 $\\mathbf{x}$ 與 $\\mathbf{p}$ 互相獨立.原本從物理那邊我們是先定義了能量, 再利用 Gibbs distribution 變成了機率分佈.現在我們反過來操作: 先定義我們要的機率分佈, 然後反推能量長什麼樣. 🤔 題外話, 寫到這我就在想, 反過來從先定義機率分布再推導能量是不是仍然能滿足 energy conservation?i.e. 能量隨時間不變.$$\\begin{align} \\frac{dH(x,p)}{dt}=0 \\end{align}$$但其實不用擔心, 因為只要用 Hamiltonian equations (6), (7) 去更新狀態, 就會滿足 energy conservation.我們考慮 1-D case 即可, 能量為 $H(x(t),p(t))$, 滿足 (6) and (7) 並觀察$$\\frac{dH(x(t),p(t))}{dt}=\\frac{dH}{dx}\\frac{dx}{dt}+\\frac{dH}{dp}\\frac{dp}{dt} \\\\ \\text{by }(6)=\\frac{dH}{dx}\\frac{dH}{dp}+\\frac{dH}{dp}\\frac{dp}{dt} \\\\ \\text{by }(7)=\\frac{dH}{dx}\\frac{dH}{dp}-\\frac{dH}{dp}\\frac{dH}{dx}=0$$ 因為互相獨立, 我們可以先定義 $\\mathbf{p}$ 是 normal distribution $\\mathcal{N}(\\mathbf{p}|0,I)$ $$p(\\mathbf{p})=\\frac{1}{Z_\\mathbf{p}}e^{-\\frac{\\mathbf{p}^T\\mathbf{p}}{2}}$$ 可以看得出來其 (kinetic) energy 為: $$\\begin{align} K(\\mathbf{p})=\\frac{\\mathbf{p}^T\\mathbf{p}}{2} \\end{align}$$ 然後不要忘記我們的目的是要從一個目標分佈 $p^*(\\mathbf{x})$ 取樣, 因此 $\\mathbf{x}$ 的機率分佈就直接定義成目標分佈. 而其 (potential) energy 為: $$\\begin{align} p^*(\\mathbf{x})=\\frac{1}{Z_\\mathbf{x}}e^{-U(\\mathbf{x})} \\\\ \\Longrightarrow U(\\mathbf{x})=-\\log p^*(\\mathbf{x}) + \\text{const.} \\end{align}$$ 還記得我們之前說過這句話嗎? “我們只需要 $\\nabla_\\mathbf{x}U(\\mathbf{x})$ 就能更新 $(\\mathbf{x},\\mathbf{p})$!”因此 (16) 的 $\\text{const.}$ 就不重要了, 所以 potential energy 這麼定義就可以了: $$\\begin{align} U(\\mathbf{x})=-\\log p^*(\\mathbf{x}) \\end{align}$$ MHC 採樣過程 好了, 到目前為止我們藉由設定好的 distribution $p^*(\\mathbf{x}),\\mathcal{N}(\\mathbf{p}|0,I)$, 可以找到對應的 energy functions $U(\\mathbf{x}),K(\\mathbf{p})$.那就可以套用 Hamiltonian equations (6), (7) 來模擬能量不變的情形下, $(\\mathbf{x},\\mathbf{p})$ 隨時間的變化.給定一個初始狀態 $(\\mathbf{x}_0,\\mathbf{p}_0)$ 可以得到: $$(\\mathbf{x}_0,\\mathbf{p}_0)\\xrightarrow[]{(6)(7)}(\\mathbf{x}_1,\\mathbf{p}_1)\\xrightarrow[]{(6)(7)}(\\mathbf{x}_2,\\mathbf{p}_2)\\xrightarrow[]{(6)(7)}...$$ 其中 $H(\\mathbf{x}_0,\\mathbf{p}_0)=H(\\mathbf{x}_1,\\mathbf{p}_1)=...$實作上由於每一次 (6), (7) 的更新都採用很小的 $\\varepsilon$ (Leapfrog sample codes 裡的 self.eps), 這樣才能確保夠準確.但我們也希望能夠走遠一些 (這對 multi-modual 的 distribution 很有幫助, 如下圖所示), 所以會跑個 $T$ 步 updates (Leapfrog sample codes 裡的 self.n_steps) 但就算如此, 由於 energy conservation (13) 的關係, $\\mathbf{x}$ 只會在 phase space 上具有相同能量的 contour 上採樣.(Phase space 定義為 $(\\mathbf{x},\\mathbf{p})$ 的空間)為了能夠採樣出其它的點, 我們需要換到其他能量的 contour, 因此改變 $\\mathbf{p}$, 即對它重新採樣即可.(follow 之前定義好的分佈 $\\mathcal{N}(\\mathbf{p}|0,I)$).但是別忘了, Hamiltonian MC 所提出的採樣是 Metorpolis-Hastings 的 proposal distribution. 所以也需要有 accept/reject, 但也得益於 energy conservation 所以會有非常高的 accept rate. 因而採樣效率很好.總結一下 HMC 方法 [ref 3]: 而 Tuning HMC 有幾個要點 [ref 3]: 一般來說要控制 rejection rate 在 $[1/4,3/4]$ 之間會比較好. 還有多跑幾個 threads 來確認收斂狀況. References 马尔可夫链蒙特卡洛算法 (二) HMC MCMC: Hamiltonian Monte Carlo (a.k.a. Hybrid Monte Carlo) DeepBayes 2019 Summer School Day 5, MCMC slides MHC sample codes from (https://github.com/bayesgroup/deepbayes-2019/blob/master/seminars/day5/Markov Chain Monte-Carlo Solution.ipynb) Leapfrog method: https://www2.atmos.umd.edu/~ekalnay/syllabi/AOSC614/NWP-CH03-2-2.pdf","tags":[{"name":"MCMC","slug":"MCMC","permalink":"https://bobondemon.github.io/tags/MCMC/"},{"name":"HMC","slug":"HMC","permalink":"https://bobondemon.github.io/tags/HMC/"},{"name":"Metropolis Hastings","slug":"Metropolis-Hastings","permalink":"https://bobondemon.github.io/tags/Metropolis-Hastings/"},{"name":"Hamiltonian Dynamic","slug":"Hamiltonian-Dynamic","permalink":"https://bobondemon.github.io/tags/Hamiltonian-Dynamic/"},{"name":"Hamiltonian Monte Carlo","slug":"Hamiltonian-Monte-Carlo","permalink":"https://bobondemon.github.io/tags/Hamiltonian-Monte-Carlo/"}]},{"title":"Score Matching 系列 (五) SM 加上 Langevin Dynamics 變成生成模型","date":"2022-03-26T09:31:38.000Z","path":"2022/03/26/Generative-Modeling-by-Estimating-Gradients-of-the-Data-Distribution/","text":"主要內容為這篇文章 “Generative Modeling by Estimating Gradients of the Data Distribution“ 背景知識 Score-based generative modeling 的兩個核心概念: Score matching (SM): 使用 score matching loss 讓 NN 直接學 score function: $\\nabla_x\\log p_{data}(x)$, 其中 $p_{data}(x)$ 為 data p.d.f. 因此我們有一個 NN: $s_\\theta(x)\\approx \\nabla_x\\log p_{data}(x)$ Score matching 在做什麼, 請參考系列文章: Score Matching 系列 (一) Non-normalized 模型估計 Score Matching 系列 (二) Denoising Score Matching (DSM) 改善效率並可 Scalable Score Matching 系列 (三) Sliced Score Matching (SSM) 同時保持效率和效果 Score Matching 系列 (四) SM 的 Toy Example in PyTorch Langevin dynamics: Langevin dynamics 可以使用 score function, i.e. $\\nabla_x\\log p_{data}(x)$, 來取 sample, 取出來的 sample 具有 $p_{data}(x)$ 的分佈 而我們已經用一個 NN $s_\\theta(x)$ 來逼近 score function 了, 因此可以用 $s_\\theta(x)$ 來取 sample, 步驟如下: 給定一個固定的 step size $\\epsilon&gt;0$, initial value $z=\\tilde{x}_0\\sim\\pi(x)$, 其中 $\\pi(x)$ 是固定的 prior distribution, and $z_t\\sim\\mathcal{N}(0,I)$ $$\\tilde{x}_t = \\tilde{x}_{t-1}+\\frac{\\epsilon}{2}\\nabla_x\\log p_{data}(\\tilde{x}_{t-1})+\\sqrt{\\epsilon}z_t \\\\ \\approx \\tilde{x}_{t-1}+\\frac{\\epsilon}{2}s_\\theta(\\tilde{x}_{t-1})+\\sqrt{\\epsilon}z_t$$ 當 $\\epsilon\\rightarrow 0$, $T\\rightarrow\\infty$ 則 $\\tilde{x}_T$ 等同於從 $p_{data}(x)$ 取樣! 藉由 Score Matching + Langevin Dynamics (SMLD), 我們發現如果成功學到 score function, 則可以從 random noise $z$ 產生符合 data distribution 的 sample.而這正是 generative model 在做的事情, 此種方法論文稱為 SMLD但是直接訓練出來的 SMLD 在真實資料上會有兩個問題導致做不好, 接著論文裡說明是什麼原因以及解決方法 兩個主要問題 The manifold hypothesis: Score matching (SM) 要做得好是基於資料分布是布滿整個 space 的 (意思是 data pdf 的 rank 沒有降低), 然而真實資料都是在低維度的 manifold. 這會讓 SM 做不好 (even using Sliced SM). 例如3維空間中, 資料分佈的 manifold 只有 rank 2 (平面), 或是只剩下 rank 1 (線) Low data density regions: 在 density 密度低的地方, 由於 training data 也很少, 導致這些地方 SM 根本估不準. (見下圖) 另一方面, 這個問題也會影響 Langevin dynamics 的採樣: 舉例來說, 如果資料分布是由兩個 disjoint 的 mode density 構成 (e.g. GMM with 2 mixtures), 則一開始的 initial 在哪個資料分布就完全決定了會收斂在哪邊. 這種現象稱為 multi-modal 的 mixing problem. 同時不好的 score 估計也會導致 Langevin dynamics 無法有效跑到對的方向, 因而收斂很慢 解決辦法 要解決第一個問題 (manifold hypothesis) 可以透過加入 Gaussian noise 緩解, 因為會使得分布充滿整個空間, 不再只存在低維度的 mainfold.上圖左是使用了 SSM 但沒加噪, 可以看到訓練不起來. 上圖右則加了非常小的噪聲 (人眼無法分辨), loss 就能穩定下降!既然加了一點 noise 就可以 train 起來了, 那就用此學出來的 “接近真實 clean 的 noisy 分布” 去用 Langevin dynamics 採樣看看結果發現還是沒法採樣出有效的影像:上圖 (a), (b) and (c) 的每一個 row 左到右表示 Langevin dynamics 的演化過程. 發現採樣不出有效 samples. 不同 rows 表示不同的 random noise $z$ 的結果.我們學到了接近 clean 的 score function 分佈了, 為何無法用 Langevin dynamics 採樣出來呢?原因是上面說的第二個問題: multi-modal 的 mixing problem. 而要解決, 可以將加入的 Gaussian noise level 加大 (更廣), 這樣可以讓低密度的地方變少, 因此 scores 就都準了.但也只能學到 noisy 的分布 (跟 DSM 缺點一樣)!因此, 論文的作法就是學一個 Noise Conditional Score Networks (NCSN): 也就是原來的不同 noise level 都會訓練出對應的 score networks, 現在直接用一個 nework 吃 noise level 當 condition 來訓練就好. 藉由這樣的做法, 可以將 noise 從很大漸漸變小, 從而還原到真實的資料分布也因為我們有不同 noise 程度的 data 分布 (NCSN approximates 的分佈), 原來的 Langevin dynamics 需要改成 annealed 版本, 這樣也能解決 multi-modal problem. 因為最開始的 noisy 分布是 noise 最大的情況, 此時不同 modes 之間區別不大, 且 low density 區域也很少 Annealed Langevin dynamics 算法如下:$\\{\\sigma_i\\}_{i=1}^L$ 是一個正的等比數列, 表示每不同的 noise level 程度, 我們讓 $\\sigma_1$表示最大噪聲等級, $\\sigma_L$ 為最小噪聲等級且接近 $0$, 表示滿足: $$\\frac{\\sigma_1}{\\sigma_2}=\\dots=\\frac{\\sigma_{L-1}}{\\sigma_L}&gt;1$$ 核心概念就是用 Langevin dynamics 從 $q_{\\sigma_{i-1}}(x)$ (較大噪聲的估計分佈) 採樣, 然後該 sample 當成初始 sample 再從 $q_{\\sigma_i}(x)$ (較小噪聲的估計分佈) 中繼續用 Langevin dynamics 採樣.藉由從大噪聲一路採樣到小噪聲的分佈, 我們就能接近原始情況下的採樣step size $\\alpha_i$ 會漸漸變小, 如演算法的第3列這個 step size $\\alpha_i$ 的選擇是為了讓 Langevin dynamics 的 “signal-to-noise” ratio:$$\\frac{\\alpha_i s_\\theta(x,\\sigma_i)} {2\\sqrt{\\alpha_i}z}$$ 與加的 noise level $\\sigma_i$ 無關 (作者在經驗上發現 $\\|s_\\theta(x,\\sigma)\\|_2\\propto 1/\\sigma$, 代入上面的 SNR ratio 會發現與 $\\sigma_i$ 無關) 結果 藉由 NCSN 和 annealed Langevin dynamics 方法, 論文可以很好的產生影像For image Inpainting, sampling 算法如下沒有被遮擋的地方就是原本的 image + noise $\\tilde{z}$, 遮擋的地方是從 annealed Langevin dynamics 產生的結果如下: 討論 使用 NCSN 的 network 架構學習 score function. 一但有了 score function, 就可以使用 annealed Langevin dynamics 採樣. 因此生成模型就完成了 💡 SMLD = NCSN + annealed Langevin dynamics 但這樣的做法還有一些問題, 雖然 NCSN 使用不同尺度的 noise 訓練, 但 noise 的尺度怎麼選? Langevin dynamics 的 step size 參數 $\\epsilon$, 以及次數 $T$ 怎麼定?這些都必須仔細調整, 才會有比較好的效果. 也因此到這篇文章為止 (2019) 只能產生較小的圖 (32x32 以下)所以在接下來的一篇文章 “Improved Techniques for Training Score-Based Generative Models”, 2020, Yang Song 從理論上探討了這些建議的設定, 結果能讓 SMLD 的生成模型穩定產生 64x64, 256x256 的結果.更進一步, Yang Song 在 ICLR 2021 的論文 “Score-Based Generative Modeling through Stochastic Differential Equations” 將 SMLD 與 Diffusion Probabilistic Modeling (DPM) 的生成模型透過 SDE 的 framework 統一起來.在這架構下, SMLD 與 DPM 其實是一體兩面, 不同的解讀而已! 太讓人讚嘆了! 在該篇結果已經能做到 1024x1024 的高解析度圖片, 這讓人非常期待 score-based generative modeling 接下來的發展! 精彩精彩!","tags":[{"name":"Score Matching","slug":"Score-Matching","permalink":"https://bobondemon.github.io/tags/Score-Matching/"},{"name":"Langevin Dynamics","slug":"Langevin-Dynamics","permalink":"https://bobondemon.github.io/tags/Langevin-Dynamics/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://bobondemon.github.io/tags/Generative-Model/"}]},{"title":"Score Matching 系列 (四) SM 的 Toy Example in PyTorch","date":"2022-03-26T06:30:44.000Z","path":"2022/03/26/score-matching-toy-example-in-pytorch/","text":"看了一些 score matching 的論文後, 就在網路上找了一下有沒有範例, 然後找到了這個 repo: [toy_gradlogp]非常清晰易懂, 重點實作了: Denoising Score Matching (DSM) Deep Energy Estimator Networks (DEEN, 我怎麼覺得跟 DSM 等價?!) Sliced Score Matching (SSM) Sliced Score Matching with Variance Reduction (SSM-VR) Langevin Dynamics (可以只根據 score function 取 sample) 雖然主要都是 PyTorch, 但 data pipeline 仍然使用 tensorflow因此我就改寫了一下, 變成純 PyTorch, 並且也改成我習慣的資料夾結構和 config 使用 Hydra 改寫後的在這: [Score Matching Practicing in PyTorch] 以下接著說明兩個重點: Langevin Dynamics 簡介 怎麼把 gradient 也當成 loss? Langevin Dynamics 簡介 Langevin dynamics 可以使用 score function, i.e. $\\nabla_x\\log p_{data}(x)$, 來取 sample, 取出來的 sample 具有 $p_{data}(x)$ 的分佈而我們知道可以用 SM 來訓練一個 NN $s_\\theta(x)$ 逼近 score function, 因此就可以用 $s_\\theta(x)$ 來取 sample, 步驟如下:給定一個固定的 step size $\\epsilon&gt;0$, initial value $z=\\tilde{x}_0\\sim\\pi(x)$, 其中 $\\pi(x)$ 是固定的 prior distribution, and $z_t\\sim\\mathcal{N}(0,I)$ $$\\tilde{x}_t = \\tilde{x}_{t-1}+\\frac{\\epsilon}{2}\\nabla_x\\log p_{data}(\\tilde{x}_{t-1})+\\sqrt{\\epsilon}z_t \\\\ \\approx \\tilde{x}_{t-1}+\\frac{\\epsilon}{2}s_\\theta(\\tilde{x}_{t-1})+\\sqrt{\\epsilon}z_t$$ 當 $\\epsilon\\rightarrow 0$, and $T\\rightarrow\\infty$, 則 $\\tilde{x}_T$ 等同於從 $p_{data}(x)$ 取樣! 只要我們能把 score function 用 NN 學得很好, 就可以利用 Langevin dynamics 採樣了. 疑?! 這樣不就完成了一個 generative modeling 了嗎?沒錯, 這就是最初的 score-based generative modeling 想發開頭. 但這最 naive 的方法事實上會面臨到一些困難.下一篇會介紹 Yang Song 使用這種方法時, 他是如何解決這些困難, 並成功鍊成生成模型. 怎麼把 gradient 也當成 loss? 其實直接參考這篇文章 retain_graph和create_graph参数 就可以了, 說得很清楚.重點是利用 torch.autograd.grad 並且將其 create_graph 設定為 True 這邊簡單說明一下.一般我們要得到 gradient 是對 loss 取 loss.backward(), pytorch 會在其 computational graph 裡做 backprop, 然後 tensor 如果 requires_grad 為 True 的話, 該 tensor 就會保留住 gradient.但現在問題是, 這些 gradients 本身也是 loss 的一部分, 這該怎麼辦?只要使用 torch.autograd.grad 並且將其 create_graph 設定為 True, pytorch 就會針對這些求 gradients 的 operations 生出其對應的 computational graph 並加進原來的圖裡. 也可以利用此方法求更高階的導數 在 SM 裡會用到是因為其 loss 包含 score function $\\nabla_x\\log NN(x;\\theta)$, 而 score function 正是我們 energy-based model (是一個 NN) 的微分一般我們定義 energy-based model $E(x)$ 為:$$E(x)=-\\log q(x)$$其中 $q(x)$ is non-normalized distribution, i.e. $p(x)=q(x)/Z$ where $Z$ is partition function 不過如果我們的 NN 直接就是估計 score function (也就是不透過 energy-based model), 就不需要這麼做, 後面 Yang Song 的很多工作就直接這樣了.","tags":[{"name":"Denoising Score Matching","slug":"Denoising-Score-Matching","permalink":"https://bobondemon.github.io/tags/Denoising-Score-Matching/"},{"name":"Score Matching","slug":"Score-Matching","permalink":"https://bobondemon.github.io/tags/Score-Matching/"},{"name":"Langevin Dynamics","slug":"Langevin-Dynamics","permalink":"https://bobondemon.github.io/tags/Langevin-Dynamics/"},{"name":"Sliced Score Matching","slug":"Sliced-Score-Matching","permalink":"https://bobondemon.github.io/tags/Sliced-Score-Matching/"}]},{"title":"Score Matching 系列 (三) Sliced Score Matching (SSM) 同時保持效率和效果","date":"2022-03-06T02:06:04.000Z","path":"2022/03/06/Sliced-Score-Matching-A-Scalable-Approach-to-Density-and-Score-Estimation/","text":"這是一篇論文筆記: “Sliced-Score-Matching-A-Scalable-Approach-to-Density-and-Score-Estimation”建議看本文前請先參前兩篇: Score Matching 系列 (一) 和 Score Matching 系列 (二) 雖然 DSM (文章在系列二) 比起 SM 可以非常有效率的訓練, 但最多只能還原到 noisy 的分布, 且加噪的強度不易調整.本篇 SSM or SSM-VR 則不會有此缺點, 且效果可以接近原來的 SM. 背景回顧真實資料的 pdf $p_d(x)$ 和其 score function 定義如下:$$s_d(x) \\triangleq \\nabla_x \\log p_d(x)$$ Model 的 non-normalized density $\\tilde{p}_m(x;\\theta)$ 和 pdf $p(x;\\theta)$ 以及 score function 定義如下:$$p_m(x;\\theta)=\\frac{\\tilde{p}_m(x;\\theta)}{Z_\\theta}, \\\\ s_m(x;\\theta) \\triangleq \\nabla_x\\log p_m(x;\\theta) = \\nabla_x\\log \\tilde{p}_m(x;\\theta)$$ 最原始的 loss function (Fisher divergence), 或在我們前面的文章稱 Explicit Score Matching (ESM):$$\\begin{align} L(\\theta) \\triangleq \\frac{1}{2}\\mathbb{E}_{p_d}\\left[ \\| s_m(x;\\theta) - s_d(x) \\|_2^2 \\right] \\end{align}$$ 其中式 (1) 等價於下式的 Implicit Score Matching (ISM) 的目標函式:$$\\begin{align} J(\\theta)=\\mathbb{E}_{p_d}\\left[ tr(\\nabla_x s_m(x;\\theta))+\\frac{1}{2}\\|s_m(x;\\theta)\\|_2^2 \\right] \\end{align}$$ 雖然 ISM 可以計算, 但需要用到二次微分, 當 network 參數量大的時候, Hessian matrix 效率很低. 同時 $x$ 維度高的時候效率也是很低 (無法很好 scalable)為此, 上一篇 DSM 利用加入 noise 的方式避掉這兩個問題, 但有兩個缺點 最多只能學到加噪聲的分布 noise 的 level, i.e. $\\sigma^2$, 很難調 SSM(-VR) 能改善這兩個缺點 Sliced Score Matching (SSM) 本篇 sliced score matching 則利用另一種想法, 不在高維度的 score function 上比較, 而是將 score function randomly 投影在低維度上再比較, 因此目標函式從 (1) 變成下式:$$\\begin{align} L(\\theta;p_v)\\triangleq \\frac{1}{2}\\mathbb{E}_{p_v}\\mathbb{E}_{p_d}\\left[ \\left( v^T s_m(x;\\theta) - v^T s_d(x) \\right)^2 \\right] \\end{align}$$ 其中 $v$ 是 random direction, $v \\sim p_v$, $x\\sim p_d$ are independent, 同時要求$$\\mathbb{E}_{p_v}[vv^T] \\succ 0, \\mathbb{E}_{p_v}[\\|v\\|_2^2]&lt;\\infty$$ 如同 ESM 推導成等價的 ISM (式 (1) 到 (2) 去掉 $s_d$), (3) 也可以將 $s_d$ 去掉:$$\\begin{align} J(\\theta;p_v) \\triangleq \\mathbb{E}_{p_v}\\mathbb{E}_{p_d} \\left[ v^T\\nabla_x s_m(x;\\theta)v + \\frac{1}{2}(v^Ts_m(x;\\theta))^2 \\right] \\end{align}$$ 基本上對每個 sample 出來的 $x_i$, 我們都可以 sample 出 $M$ 個投影向量, 所以 empirical expecation 寫法如下:$$\\begin{align} \\hat{J}(\\theta)\\triangleq \\frac{1}{N}\\frac{1}{M}\\sum_{i=1}^N\\sum_{j=1}^M v_{ij}^T\\nabla_x s_m(x_i;\\theta)v_{ij} + \\frac{1}{2}(v_{ij}^T s_m(x_i;\\theta))^2 \\end{align}$$ 同時如果 $p_v$ 是 multivariate standard normal or Rademacher distribution, 則可以簡化為:$$\\begin{align} \\hat{J}_{vr}(\\theta)\\triangleq \\frac{1}{N}\\frac{1}{M}\\sum_{i=1}^N\\sum_{j=1}^M v_{ij}^T\\nabla_x s_m(x_i;\\theta)v_{ij} + \\frac{1}{2}\\|s_m(x_i;\\theta)\\|_2^2 \\end{align}$$稱為 Sliced Score Matching with Variance Reduction (SSM-VR) 文章說 SSM-VR 比 SSM 表現更好, 同時投影向量的數量, $M$, 選擇 1 個就很好了 看到這可能還是有疑問, 看起來還是得算 Hessian matrix, $\\nabla_x s_m(x;\\theta)$, 阿? 不是說要可以加速有效率?其實是這樣的, 先算 $v^T s_m(x;\\theta)$ 對 $x$ 的微分, 由於是 scalar 的 backprob 就快很多, 因此得到 $v^T\\nabla_x s_m(x;\\theta)$, 然後再跟 $v$ 內積就結束了因此算法如下 Codes 可以參考 https://github.com/Ending2015a/toy_gradlogp/blob/master/toy_gradlogp/energy.py#L152 實驗 論文裡一段實驗結果如下: SM loss 指的是式 (1) 的 loss, SM 算法則是式 (2) Implicit Score Matching (ISM)DSM 指的是 Denosing Score Matching. 先忽略 CP 和 Approx BP (因為我沒看 XD) 從 Figure 1 可以看出 SSM(-VR) 的 performance 可以達到跟 SM 接近, 也比 DSM 好上一截.而 Figure 2 可以看出 SSM(-VR) 的 scalibility (DSM 也很有效率), 這是原來的 SM 達不到的 (因為要算 Hessian)Table 1 也可以看出 DSM 對於 noise 的強度 ($\\sigma$) 較敏感. 總之, SSM(-VR) 可以跟 DSM 一樣 scalable 和 efficient, 且 performance 比 DSM 好又接近原來的 SM.另外提一下這篇的作者, Yang Song, 對於 score matching 以及後來的 diffusion probabilistic model 都有很重要的著作和進展, 值得讀他的論文 👏","tags":[{"name":"Score Matching","slug":"Score-Matching","permalink":"https://bobondemon.github.io/tags/Score-Matching/"},{"name":"Sliced Score Matching","slug":"Sliced-Score-Matching","permalink":"https://bobondemon.github.io/tags/Sliced-Score-Matching/"}]},{"title":"Score Matching 系列 (二) Denoising Score Matching (DSM) 改善效率並可 Scalable","date":"2022-03-06T01:30:22.000Z","path":"2022/03/06/A-Connection-Between-Score-Matching-and-Denoising-Autoencoders/","text":"這是一篇論文筆記: “A Connection Between Score Matching and Denoising Autoencoders”建議看本文前請先參前一篇: Score Matching 系列 (一) Non-normalized 模型估計 前言基於 Score Matching, 提出 Denoising Score Matching (DSM) 的目標函式, 好處是在 energy-based model 下: 不用將 score function 的 gradients 也納入 loss 去計算 (避免二次微分做 backprop 提高效率) 當 input $x$ 的維度很大也沒問題 (可以 scalable) 但缺點是: 最多只能學到加 noise 後的分布 加 noise 的 level 不好調整 這兩個缺點在下一篇 Sliced Score Matching (SSM) 可以得到改善這篇論文最後也點出了 denoising autoencoder 跟 score matching 的關係 (實際上就是 DSM loss) 以下正文開始 正文 $q(x)$ 表示 data (真實資料) 的 pdf, $p(x;\\theta)$ 表示 model 參數為 $\\theta$ 的 pdfExplicit Score Matching (ESM) 為:$$\\begin{align} J_{ESM,q}(\\theta) = \\mathbb{E}_{q(x)}\\left[ \\frac{1}{2}\\left\\| \\psi(x;\\theta)-\\frac{\\partial \\log q(x)}{\\partial x} \\right\\|^2 \\right] \\end{align}$$實際上我們不知道 $q(x)$, 因此式 (1) 的 $J_{ESM}$ 我們無法用. 不過, 最開始的 Score Matching 論文已經證明 $J_{ESM}$ 等價於如下的 $J_{ISM}$, 而 $J_{ISM}$ 是我們能實作的Implicit Score Matching (ISM) 為:$$\\begin{align} J_{ISM,q}(\\theta)=\\mathbb{E}_{q(x)}\\left[ \\frac{1}{2}\\|\\psi(x;\\theta)\\|^2+ tr(\\nabla_x\\psi(x;\\theta)) \\right] \\end{align}$$如果我們對 $q(x)$ 做 pertrub, i.e. 每一個 data point $x$ 都加上一個 pdf 為 $q_\\sigma(\\tilde{x}|x)$ 的 random noise其中 $\\sigma$ 為該 noise pdf 的參數, 如果以 isotropic Gaussian pdf 為例子, $\\sigma^2$ 就是 variance則 noisy 的 data pdf $q_\\sigma(\\tilde{x})$ 就變成:$q_\\sigma(\\tilde{x})=\\int q_\\sigma(\\tilde{x}|x) q(x) dx$ 💡 兩個 independent random variables $x,y$ 相加 $z=x+y$, 則 $z$ 的 pdf 為 $x,y$ 的 pdfs 的 convolution 然後 $\\theta$ 要學的目標 pdf 變成 $q_\\sigma(\\tilde{x})$ 後, 其 ESM 為:$$\\begin{align} J_{ESM,q_\\sigma}(\\theta) = \\mathbb{E}_{q_\\sigma(\\tilde{x})}\\left[ \\frac{1}{2}\\left\\| \\psi(\\tilde{x};\\theta)-\\frac{\\partial \\log q_\\sigma(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right] \\end{align}$$ 只是將原來的 ESM data pdf 換成 noisy 版本而已. 因此, $J_{ESM,q_\\sigma}$ 也會等價 $J_{ISM,q_\\sigma}$ (只要適當的 noise pdf 讓 $q_\\sigma(\\tilde{x})$ 滿足原來 ESM 等於 ISM 的條件)其中:$$\\begin{align} J_{ISM,q_\\sigma}(\\theta)=\\mathbb{E}_{q_\\sigma(\\tilde{x})}\\left[ \\frac{1}{2}\\|\\psi(\\tilde{x};\\theta)\\|^2+ tr(\\nabla_x\\psi(\\tilde{x};\\theta)) \\right] \\end{align}$$ ⚠️ 乍看之下 (4) 好像跟原來的 ISM (式 (2)) 沒什麼不同, 其實不同的地方在 “expectation on what pdf” ISM 的缺點是必須要計算二次微分, 並且要當成 loss 的一部分, 這會導致計算過慢 (因為微分的 operations 也會加進 graph 裡, 可以想像 NN 本來的 graph 就很大了, 還要加一二次微分的 ops 進去) 有關如何將一二次微分加入 loss 中可參考 [retain_graph和create_graph参数] 其實我們可以直接用一個 NN 來表示 score function $\\psi(x;\\theta)$, 這樣原來的二次微分就是該 NN 的一次微分, 雖然這樣做比較有效率, 但 Deep Energy Estimator Networks 指出會不 robust. 不過如果使用 Sliced Score Matching (SSM), 該文作者說一樣可以有效的用 NN 直接 predict score function. 這篇論文提出的 Denoising Score Matching (DSM) 的目標函式 $J_{DSM,q_\\sigma}(\\theta)$ 可以避開上述將微分項也加入 loss 計算導致不有效率的缺點. 該目標函式為:$$\\begin{align} J_{DSM,q_\\sigma}(\\theta)=\\mathbb{E}_{q_\\sigma(x,\\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x};\\theta) - \\frac{\\partial\\log q_\\sigma(\\tilde{x}|x)}{\\partial\\tilde{x}} \\right\\|^2 \\right] \\end{align}$$注意到若 noise pdf 為 isotropic Gaussian nosie $\\mathcal{N}(x,\\sigma^2I)$ 則: $$\\frac{\\partial\\log q_\\sigma(\\tilde{x}|x)}{\\partial\\tilde{x}} = \\frac{1}{\\sigma^2}(x-\\tilde{x})$$ 因此式 (5) 變得很容易計算也有效率 💡 還有一個直觀的解釋, 我們給定一個 pair $(x,\\tilde{x})$, 希望 $\\tilde{x}$ 的 gradient 跟 noise pdf 的 gradient 流向一樣, 相當於希望將 nosiey 的 $\\tilde{x}$ 還原成 clean 的 $x$ 文章並證明了 $J_{DSM_{q_\\sigma}}$ 等價於 $J_{ESM_{q_\\sigma}}$, i.e. (5)=(3) 我們擷取論文 Appendix 的證明: 最後文章說明了一個有趣的發現最簡單的 Denoise Autoencoder NN (一層 linear hidden layer, tied weights) 其 reconstruction mse 的目標函式等價於使用 $J_{DSM_{q_\\sigma}}$讓我們將 Score Matching 跟 Denoise Autoencoder 有了連結","tags":[{"name":"Denoising Score Matching","slug":"Denoising-Score-Matching","permalink":"https://bobondemon.github.io/tags/Denoising-Score-Matching/"},{"name":"Score Matching","slug":"Score-Matching","permalink":"https://bobondemon.github.io/tags/Score-Matching/"}]},{"title":"Score Matching 系列 (一) Non-normalized 模型估計","date":"2022-01-07T16:01:27.000Z","path":"2022/01/08/Estimation-of-Non-Normalized-Statistical-Models-by-Score-Matching/","text":"這是一篇論文筆記: “Estimation of Non-Normalized Statistical Models by Score Matching”, 其實推薦直接讀論文, 數學式很清楚, 表達也明確, 只是想順著自己的說法做一下筆記 動機介紹在 Machine Learning 中, 我們常常希望用參數 $\\theta$ 估出來的 pdf $p(.;\\theta)$ 能跟真實 data (training data) 的 pdf $p_x(.)$ 愈像愈好.由於是 pdf $p(.;\\theta)$, 必須滿足機率形式, i.e. 積分所有 outcomes 等於 1, 因此引入一個 normalization term $Z(\\theta)$$$p(\\xi;\\theta)=\\frac{1}{Z(\\theta)}q(\\xi;\\theta)$$其中 $\\xi\\in\\mathbb{R}^n$ 為一個 data point假設我們有 $T$ 個 observations $\\{x_1,...,x_T\\}$, 套用 empirical expecation 並對 likelihood estimation 找出最佳 $\\theta$ (MLE):$$\\theta_{mle}=\\arg\\max_\\theta \\sum_{t=1}^T \\log p(x_t;\\theta)$$計算 gradient, 會發現由於存在 $Z(\\theta)$ 變得很難計算, 導致 gradient-based optimization 也很困難. 山不轉路轉, 如果我們能換個想法:不要求找到的 $\\theta$ 對每個 data points $x_t$ 使其 $p(x_t;\\theta)$ 跟真實機率分佈的結果很接近, i.e. $p(x_t;\\theta)\\approx p_x(x_t)$改成希望使其$$\\begin{align} \\nabla_x\\log p_x(x_t) \\approx \\nabla_x\\log p(x_t;\\theta) {\\color{orange}{=\\nabla_x\\log q(x_t;\\theta)}} \\end{align}$$意思是希望每個 data points 他們的 (log) gradient 都跟真實分布的 (log) gradient 接近可以想像兩個 functions 的變化一致的話, 它們的長相應該會很接近 (只差在scale不同)以下圖舉例來說 (該圖引用自 Yang Song: Generative Modeling by Estimating Gradients of the Data Distribution), vector field 箭號表示那些 (log) gradients, 而 contours 表示一個 mixture of two Gaussians我們要求 vector field 一致藉由這樣找 $\\theta$ 的方法, 我們其實找到的是 non-normalized distribution $q(.;\\theta)$. 直觀的 Objective Function (Explicit Score Matching) 我們使用 MSE loss 來計算式 (1), 利用此 loss 找最佳 $\\theta$ 的方法稱為 Score Matching定義 score function, 它其實就是 gradient of the log-density with respect to the data vector: Data pdf 的 score function: $\\psi_x(\\xi)=\\nabla_\\xi\\log p_x(\\xi) \\in \\mathbb{R}^n$ Model pdf 的 score function: $$\\psi(\\xi;\\theta)= \\left( \\begin{array}{c} \\frac{\\partial\\log p(\\xi;\\theta)}{\\partial\\xi_1} \\\\ \\vdots\\\\ \\frac{\\partial\\log p(\\xi;\\theta)}{\\partial\\xi_n} \\end{array} \\right) =\\left( \\begin{array}{c} \\psi_1(\\xi;\\theta) \\\\ \\vdots \\\\ \\psi_n(\\xi;\\theta) \\end{array} \\right) = \\nabla_\\xi\\log p(\\xi;\\theta) =\\color{orange}{\\nabla_\\xi\\log q(\\xi;\\theta)}$$ 因此 objective function 就是這兩個 score functions 的 MSE (又稱為 Fisher Divergence: Interpretation and Generalization of Score Matching):$$\\begin{align} J(\\theta)=\\frac{1}{2}\\int_{\\xi\\in\\mathbb{R}^n} p_x(\\xi) \\| \\psi(\\xi;\\theta)-\\psi_x(\\xi) \\|^2 d\\xi \\end{align}$$而 score matching estimator 的參數就是$$\\hat{\\theta}=\\arg\\min_\\theta J(\\theta)$$要算期望值需要知道真實資料的 pdf $p_x(.)$, 雖然我們無法得到, 但可以根據 training data 去計算 empirical 期望值.例如我們有 $T$ 個 observations $\\{x_1,...,x_T\\}$, 則 empirical expectation 為$$\\tilde{J}(\\theta)=\\frac{1}{T}\\sum_{t=1}^T \\| \\psi(x_t;\\theta) - \\psi(x_t) \\|^2$$不過 (2) 最麻煩的是我們不知道真實資料的 score function $\\psi_x(.)$, 論文的定理說明了在某些簡單的條件下, 可以避免掉計算這項. 條件為: 除了 pdf 可微, 1st/2nd moment 存在, 一個比較特殊的條件為 (但也容易滿足): $$p_x(\\xi)\\psi(\\xi;\\theta)\\xrightarrow[\\|\\xi\\|\\rightarrow \\infty]{}0$$ 我們在下面會看出來 Practical Objective Function (Implicit Score Matching) 擷自論文. 將 Appendix 的證明描述如下:將 (2) 展開變成$$J(\\theta)=\\int p_x(\\xi)\\left[ \\frac{1}{2}\\|\\psi_x(\\xi)\\|^2 + \\frac{1}{2}\\|\\psi(\\xi;\\theta)\\|^2 - \\psi_x(\\xi)^T\\psi(\\xi;\\theta) \\right] d\\xi \\\\ =\\int p_x(\\xi)\\left[ \\frac{1}{2}\\|\\psi(\\xi;\\theta)\\|^2 {\\color{orange} {- \\psi_x(\\xi)^T\\psi(\\xi;\\theta)}} \\right]d\\xi + const$$將向量的內積展開, 改寫橘色部分積分如下:$$-\\sum_{i=1}^n \\int p_x(\\xi)\\psi_{x,i}(\\xi)\\psi_i(\\xi;\\theta)d\\xi \\\\ \\text{by def. of score function }= -\\sum_{i=1}^n \\int p_x(\\xi) \\frac{\\partial \\log p_x(\\xi)}{\\partial \\xi_i} \\psi_i(\\xi;\\theta) d\\xi \\\\ = -\\sum_{i=1}^n \\int \\frac{p_x(\\xi)}{p_x(\\xi)} \\frac{\\partial p_x(\\xi)}{\\partial \\xi_i} \\psi_i(\\xi;\\theta) d\\xi = \\sum_{i=1}^n {\\color{green}{ -\\int \\frac{\\partial p_x(\\xi)}{\\partial \\xi_i} \\psi_i(\\xi;\\theta) d\\xi}}$$ 使用分部積分 (為簡化notation, 只看第一維的變數 $\\xi_1$)改寫綠色部分變成只要假設$$p_x(\\xi)\\psi(\\xi;\\theta)\\xrightarrow[\\|\\xi\\|\\rightarrow \\infty]{}0$$則最後只剩下$$-\\int\\frac{\\partial p_x(\\xi)}{\\partial\\xi_i}\\psi_i(\\xi;\\theta)d\\xi = \\int\\frac{\\partial\\psi_i(\\xi;\\theta)}{\\partial\\xi_i}p_x(\\xi)d\\xi$$將此結果一路代回去 (需要一點耐心而以), 就可以得到式 (3) 了, 重複一下式 (3) 如下:$$\\begin{align} J(\\theta)=\\int_{\\xi\\in\\mathbb{R}^n}p_x(\\xi)\\sum_{i=1}^n\\left[ \\partial_i\\psi_i(\\xi;\\theta)+\\frac{1}{2}\\psi_i(\\xi;\\theta)^2 \\right]d\\xi + const \\end{align}$$Optimize 這個目標函式就容易得多了, 只要使用 empirical expecation 就可以:$$\\begin{align} \\tilde{J}(\\theta)=\\frac{1}{T}\\sum_{t=1}^T\\sum_{i=1}^n\\left[ \\partial_i\\psi_i(x_t;\\theta)+\\frac{1}{2}\\psi_i(x_t;\\theta)^2 \\right] + const \\end{align}$$ 最佳解的存在唯一性 證明:對照式 (2) 的目標函數:$$\\begin{align} J(\\theta)=\\frac{1}{2}\\int_{\\xi\\in\\mathbb{R}^n} p_x(\\xi) \\| \\psi(\\xi;\\theta)-\\psi_x(\\xi) \\|^2 d\\xi \\end{align}$$只要我們的 parameter space 夠大能夠包含真實的 pdf, 當目標函式達到最小, i.e. $=0$, 則解就是真實的 pdf 討論 總結最後的 objective function:$$J(\\theta)=\\int_{\\xi\\in\\mathbb{R}^n}p_x(\\xi)\\sum_{i=1}^n\\left[ \\partial_i\\psi_i(\\xi;\\theta)+\\frac{1}{2}\\psi_i(\\xi;\\theta)^2 \\right]d\\xi$$簡化改寫一下$$\\begin{align} J(\\theta)=\\mathbb{E}_{p_x(\\xi)}\\left[ tr(\\nabla_\\xi\\psi(\\xi;\\theta))+\\frac{1}{2}\\|\\psi(\\xi;\\theta)\\|_2^2 \\right] \\end{align}$$其中 $\\psi(\\xi;\\theta):\\mathbb{R}^n\\rightarrow \\mathbb{R}^n$, 實務上我們就用一個 NN 參數為 $\\theta$ 表示, 因此原本需要二階導數, 變成只需要一階導數在 loss 中. 雖然這樣做比較有效率, 但 Deep Energy Estimator Networks 指出會不 robust. 後續在這篇文章 “A Connection Between Score Matching and Denoising Autoencoders”, 作者提出 Denoising Score Matching (DSM) 目標函式, 可以不需要將 gradient 也納入 loss 中.其想法就是將每個 data $x$ 根據 Gaussian pdf 做些擾動, 所以一個副作用是永遠學不到最準的 data pdf (除非沒有擾動), 但可藉由加入的擾動愈小, 讓估計愈真實 Score matching 的作法是 2005 年, 而作者 Aapo Hyv¨arinen 其實也是 Noise Contrastive Estimation (NCE) 的作者. (參考之前的 NCE 筆記)NCE 也是在做一樣的事情: 想辦法避開 $Z(\\theta)$ 來估計真實資料的 pdf. 但發表在 2010 年. 這一問題, probabilistic model flexibility 與 tractability 的 trade-off, 在 Machine Learning 是被探討很久的問題, 這篇 Deep Unsupervised Learning using Nonequilibrium Thermodynamics, (Diffusion Probabilistic Model 開始的重要文章) 的 introduction 描述一些主要方法, 可以盡量滿足 flexibility 情況下, 又能 tractable. Score matching 可以應用在 Langevin dynamics, 透過只使用 score function $\\nabla_x\\log p(x)$ 就可以用 MCMC 取 samples, 而這一步驟在 Diffusion Probabilistic Model 中扮演著重要的角色同時 NCE 在 Self-supervised Learning 同樣也是關鍵, 衍生了 infoNCE, CPC, … 這一派的 SSL 方法 👏真神人也👏","tags":[{"name":"Score Matching","slug":"Score-Matching","permalink":"https://bobondemon.github.io/tags/Score-Matching/"},{"name":"Score Function","slug":"Score-Function","permalink":"https://bobondemon.github.io/tags/Score-Function/"},{"name":"Fisher Divergence","slug":"Fisher-Divergence","permalink":"https://bobondemon.github.io/tags/Fisher-Divergence/"}]},{"title":"Score Function and Fisher Information Matrix","date":"2022-01-07T15:17:40.000Z","path":"2022/01/07/Score-Function-and-Fisher-Information-Matrix/","text":"Bayesian statistics 視 dataset $\\mathcal{D}=\\{x_1,...,x_n\\}$ 為固定, 而 model parameter $\\theta$ 為 random variables, 透過假設 prior $p(\\theta)$, 利用 Bayes rule 可得 posterior $p(\\theta|\\mathcal{D})$. 而估計的參數就是 posterior 的 mode, i.e. $\\theta_{map}$ (Maximum A Posterior, MAP) 關於 Fisher information matrix 在 Bayesian 觀點的用途, 其中一個為幫助定義一個特殊的 prior distribution (Jeffreys prior), 使得如果 parameter $\\theta$ 重新定義成 $\\phi$, 例如 $\\phi=f(\\theta)$, 則 MAP 解不會改變. 關於這部分還請參考 Machine Learning: a Probabilistic Perspective by Kevin Patrick Murphy. Chapters 5 的 Figure 5.2 圖很清楚 反之 Frequentist statistics 視 dataset $\\mathcal{D}=\\{X_1,...,X_n\\}$ 為 random variables, 透過真實 $\\theta^*$ 採樣出 $K$ 組 datasets, 每一組 dataset $\\mathcal{D}^k$ 都可以求得一個 $\\theta_{mle}^k$ (MLE 表示 Maximum Likelihood Estimator), 則 $\\theta_{mle}^k$ 跟 $\\theta^*$ 的關係可藉由 Fisher information matrix 看出來 本文探討 score function 和 Fisher information matrix 的定義, 重點會放在怎麼直觀理解. 然後會說明 Fisher information matrix 在 Frequentist statistics 角度代表什麼意義. 先定義一波 Score Function [Score Function Definition]:&emsp;針對某一點 data point $x$, 在 $\\hat{\\theta}$ 點的 log-likelihood 的 gradient 定義為 score function:&emsp;$$\\begin{align} s(x,\\hat{\\theta}) \\triangleq \\nabla_\\theta \\log p(x|\\hat\\theta) \\end{align}$$ 注意到如果 $x$ 是 random variables, $s(x,\\hat{\\theta})$ 也會是 random variable在 Frequentist statistics 觀點下, data $x$ 是 random variables, 所以可以對 true parameter $\\theta^*$ 的 data distribution $p(x|\\theta^*)$ 計算期望值:$$\\begin{align} \\mathbb{E}_{p(x|\\theta^*)}[s(x,\\hat{\\theta})] = \\int p(x|\\theta^*) \\nabla_\\theta \\log p(x|\\hat\\theta) dx \\end{align}$$ 我們舉個離散的例子來說明式 (2): score_function_example.drawio 可以看到 score function 相當於在描述 parameter 變化對於 log-likelihood 的變化程度.而該變化程度在真實 $\\theta^*$ 那點的期望值為 $0$, i.e. 在 $\\hat\\theta=\\theta^*$ 這點計算 score function 的期望值:$$\\mathbb{E}_{p(x|\\theta^*)}[s(x,\\theta^*)] = \\int p(x|\\theta^*) \\nabla_\\theta \\log p(x|\\theta^*) dx \\\\ = \\int p(x|\\theta^*)\\frac{\\nabla_\\theta p(x|\\theta^*)}{p(x|\\theta^*)} dx \\\\ = \\int \\nabla_\\theta p(x|\\theta^*) dx \\\\ = \\nabla_\\theta \\int p(x|\\theta^*) dx = \\nabla_\\theta 1 = 0$$ 💡 注意到計算期望值都是基於真實資料分佈, i.e. $p(x|\\theta^*)$. 為何強調這一點, 是因為我們手頭上的 training data 一般來說都是假設從真實分佈取樣出來的, 也就是說只要用 training data 計算期望值, 隱含的假設就是用 $p(x|\\theta^*)$ 來計算. 因此我們有$$\\begin{align} \\mathbb{E}_{p(x|\\theta^*)}[s(x,\\theta^*)] = 0 \\end{align}$$ 💡 其實對任何其他點 $\\hat{\\theta}$ 式 (3) 也成立, i.e. $\\mathbb{E}_{p(x|\\hat{\\theta})}[s(x,\\hat{\\theta})]=0$. 注意到期望值必須基於 $p(x|\\hat\\theta)$ 而非真實資料分佈了. Fisher Information Matrix 💡 $\\mathbb{E}_{p(x|\\theta^*)}[\\cdot]$ 我們簡寫為 $\\mathbb{E}_{p^*}[\\cdot]$ [Fisher Information Matrix Definition]:&emsp;在 $\\hat{\\theta}$ 點的 Fisher information matrix 定義為:&emsp;$$I(\\theta^* ; \\hat\\theta) =\\mathbb{E}_{p^*}\\left[ s(x,\\hat\\theta)s(x,\\hat\\theta)^T \\right] \\\\ = \\mathbb{E}_{p^*}\\left[\\nabla_\\theta\\log p(x|\\hat\\theta)\\nabla_\\theta\\log p(x|\\hat\\theta)^T\\right]$$&emsp;其中 $\\theta^*$ 為真實資料的參數其實就是 score function 的 second moment. Fisher information matrix 在 $\\hat\\theta=\\theta^*$ 此點上為:$$\\begin{align} I(\\theta^* ; \\theta^*) =\\mathbb{E}_{p^*}\\left[ (s(x,\\theta^*)-0)(s(x,\\theta^*)-0)^T \\right] \\\\ = \\mathbb{E}_{p^*}\\left[ (s(x,\\theta^*)- \\mathbb{E}_{p^*}[s(x,\\theta^*)] )(s(x,\\theta^*)- \\mathbb{E}_{p^*}[s(x,\\theta^*)] )^T \\right] \\\\ = Cov_{p^*}\\left( s(x,\\theta^*),s(x,\\theta^*) \\right) \\end{align}$$由於我們已經知道 score function 在 $\\hat\\theta=\\theta^*$ 的期望值是 $0$ , 因此 Fisher information matrix 變成 Covariance matrix of score function 💡 同樣對任何其他點 $\\hat{\\theta}$ 式 (6) 也成立, i.e. $I(\\hat\\theta ; \\hat\\theta)=Cov_{\\hat p}(s(x,\\hat\\theta),s(x,\\hat\\theta))$. 同樣注意到期望值必須基於 $p(x|\\hat\\theta)$ 而非真實資料分佈了. 示意圖為:score_function_example_on_true_parameters.drawio 另外假如我們思考 score function (gradient) 計算的是一次微分, 可以想成是斜率. 那如果考慮二次微分 (Hseeian matrix), 則可想成是 curvature, 因此示意圖為:Hessian_example_on_true_parameters.drawiocurvature_example.pptx紅色的那三條 curves 就是 3 個 data points 的 Hessian matrices. 因此 Hessian matrix (at $\\theta^*$) 的期望值直觀上可以想成 log-likelihood 的 graph 在 $\\theta^*$ 的彎曲程度 (curvature).以上這個觀點其實跟 Fisher information matrix 有關聯的, 描述如下:“Fisher information matrix = score function 的 covariance matrix” 等於 “負的 Hessian matrix 之期望值”. 注意到這性質成立在 $\\theta^\\star$. (或更精確地說, 任何 $\\hat\\theta$ 都滿足 $I(\\hat\\theta,\\hat\\theta)=-\\mathbb{E}_{\\hat p}[H(x|\\hat\\theta)]$, 只是期望值基於 $p(x|\\hat\\theta)$ 而不是真實資料分佈, 抱歉第三次囉嗦這一點, 不再強調了 XD) 證明可參考 Wiki 的 Fisher information, 或參考 Agustinus Kristiadi’s Blog: Fisher Information Matrix. 這裡就不重複.本篇目的為了解其物理意義. 因此我們有如下的等式:$$\\begin{align} I(\\theta^* ; \\theta^*) = \\mathbb{E}_{p^*}\\left[ \\nabla_\\theta\\log p(x|\\theta^*) \\nabla_\\theta\\log p(x|\\theta^*)^T\\right] = - \\mathbb{E}_{p^*}\\left[ \\nabla_\\theta^2\\log p(x|\\theta^*) \\right] \\end{align}$$ KL-divergence (or relative entropy) 與 MLE 與 $I(\\theta^* ; \\theta^*)$ 關聯 KL-divergence 為, 通常 $p(x)$ 表示 ground truth distribution:$$KL(p(x);q(x))=\\int p(x)\\log\\frac{p(x)}{q(x)}dx$$則我們知道 MLE (maximum log likelihood estimation) 等同於求解最小化 KL-divergence [ref]$$\\arg\\min_{\\theta} KL(p(x|\\theta^*);p(x|\\theta)) \\\\ =\\arg\\min_{\\theta} \\int p(x|\\theta^*)\\log\\frac{p(x|\\theta^*)}{p(x|\\theta)}dx \\\\ =\\arg\\min_{\\theta} \\int p(x|\\theta^*)\\log\\frac{1}{p(x|\\theta)}dx \\\\ =\\arg\\min_{\\theta} -\\mathbb{E}_{p^*}[\\log p(x|\\theta)] =: \\arg\\min\\text{NLL}\\\\ =\\arg\\max_{\\theta}\\mathbb{E}_{p^*}[\\log p(x|\\theta)] =: \\theta_{mle}$$其中 NLL 表示 Negative Log-Likelihood由於我們已經知道 $I(\\theta^* ; \\theta^*)$ 描述了 NLL 的 curvature, 由剛剛的推導知道 NLL (就是 MLE) 跟 KL 等價, 所以 $I(\\theta^* ; \\theta^*)$ 也描述了 KL-divergence 的 curvature, 具體推導如下:$$\\text{curvature of KL at }\\theta^* = \\nabla_\\theta^2 KL(p(x|\\theta^*); p(x|\\theta^*)) \\\\ =\\left( \\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j} KL(p(x|\\theta^*); p(x|\\theta)) \\right)_{\\theta=\\theta^*} \\\\ = -\\int p(x|\\theta^*)\\left( \\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j} \\log p(x|\\theta) \\right)_{\\theta=\\theta^*} dx \\\\ =-\\mathbb{E}_{p^*}[\\nabla_\\theta^2\\log p(x|\\theta^*)] \\\\ \\text{by (7) } = I(\\theta^*; \\theta^*)$$ $I(\\theta^* ; \\theta^*)$ 在 Frequentist statistics 的解釋 了解了 Fisher information matrix 的物理意義後, 還能給我們什麼洞見?回到文章開頭說的: Frequentist statistics 視 dataset $\\mathcal{D}=\\{X_1,...,X_n\\}$ 為 random variables, 透過真實 $\\theta^*$ 採樣出 $K$ 組 datasets, 每一組 dataset $\\mathcal{D}^k$ (共 $n$ 筆 data) 都可以求得一個 $\\theta_{mle}^k$ (MLE 表示 Maximum Likelihood Estimator) 所以可以視 $\\theta_{mle}$ 為一個 random variables, 其 distribution 稱為 sampling distribution.則 $\\theta_{mle}$ 跟 $\\theta^*$ 有如下關係 (sampling distribution 為 Normal distribution) :$$\\begin{align} \\sqrt{n}(\\theta_{mle}-\\theta^*) \\xrightarrow[]{d} \\mathcal{N}\\left( 0,I^{-1}(\\theta^*;\\theta^*) \\right), \\qquad \\text{as }n\\rightarrow\\infty \\end{align}$$其中 $\\xrightarrow[]{d}$ 表示 converge in distribution. 又或者這麼寫$$\\begin{align} (\\theta_{mle}-\\theta)\\approx^d \\mathcal{N}\\left( 0, \\frac{1}{nI(\\theta^*;\\theta^*)} \\right) \\end{align}$$直觀解釋為如果 $I(\\theta^*; \\theta^*)$ 愈大 (or $n$ 愈大) , MLE 愈有高的機率接近 $\\theta^*$因此 Fisher information matrix $I(\\theta^*; \\theta^*)$ 量測了 MLE 的準確度 Summary 本文討論了 score function and Fisher information matrix 的定義和直觀解釋, 同時也說明了 MLE 的估計 $\\theta_{mle}$ 其距離真實參數 $\\theta^*$ 可被 Fisher information matrix $I(\\theta^* ; \\theta^*)$ 描述出來 (雖然實務上我們無法算 $I(\\theta^* ; \\theta^*)$ 因為不知道 true parameters $\\theta^*$) 關於 Fisher information 知乎這篇文章很好: 费雪信息 (Fisher information) 的直观意义是什么？同時 Fisher information 也能描述參數 $\\theta$ 和 random variable $x$ 之間的信息量 (這部分本文沒描述), 可參考 Maximum Likelihood Estimation (MLE) and the Fisher Information 和 A tutorial on Fisher information 利用 binomial distribution 的例子來說明 另外與 Natural gradient 的關聯可參考: Natural Gradient by Yuan-Hong Liao (Andrew). 寫得也非常棒! 在基於第 $t$ 次的 $\\theta^t$ 時, Natural gradient 要找得 optimize 方向 $d^*$ 為如下:$$d^*=\\mathop{\\arg\\min}_{KL(p(x|\\theta^t)\\|p(x|\\theta^t+d))=c} \\mathcal{L}(\\theta^t+d)$$ 也就是說希望 distribution 變化也不要太大. 結論會發現 $d^*$ 的近似解:$$d^*\\propto-I(\\theta^t;\\theta^t)^{-1}\\nabla_\\theta\\mathcal{L}(\\theta)|_{\\theta=\\theta^t}$$ 正好跟 optimization 的 Newton’s method 方法一樣. 最後提一下, 本文定義的 score function 是基於 $\\theta$ 的 gradient, 但同時有另一種方法稱 Score Matching [ref 9], 其定義的 score function 是基於 data point $x$ 的 gradient:$$\\nabla_x\\log p(x;\\theta)$$ 因此看這 score function 就不是在 parameter space 上觀察, 而是在 input space 上. Reference Machine Learning: a Probabilistic Perspective by Kevin Patrick Murphy. Chapters 5 and 6 Agustinus Kristiadi’s Blog: **Fisher Information Matrix Wiki Fisher information A tutorial on Fisher information [pdf] Maximizing likelihood is equivalent to minimizing KL-Divergence 费雪信息 (Fisher information) 的直观意义是什么？ Maximum Likelihood Estimation (MLE) and the Fisher Information by Xichu Zhang Yang Song: Generative Modeling by Estimating Gradients of the Data Distribution Estimation of Non-Normalized Statistical Models by Score Matching Natural Gradient by Yuan-Hong Liao (Andrew)","tags":[{"name":"Score Function","slug":"Score-Function","permalink":"https://bobondemon.github.io/tags/Score-Function/"},{"name":"Fisher Information Matrix","slug":"Fisher-Information-Matrix","permalink":"https://bobondemon.github.io/tags/Fisher-Information-Matrix/"}]},{"title":"Stochastic Processes Week 8 Lévy processes","date":"2021-12-12T15:17:32.000Z","path":"2021/12/12/Stochastic-Processes-Week-8-Levy-processes/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes (本文) Week 8.1: Definition of a Lévy process. Stochastic continuity and càdlàg paths$N_t,W_t,L_t$ 分別表示 Poisson process, Brownian motion, and Levy process$$\\begin{array}{|c | c |c |} \\hline N_t &amp; W_t &amp; L_t \\\\ \\hline N_0=0\\text{ a.s.} &amp; W_0=0\\text{ a.s.} &amp; L_0=0\\text{ a.s.} \\\\ \\hline \\text{indep. increments} &amp; \\text{indep. increments} &amp; \\text{indep. increments} \\\\ \\hline \\text{stationary increments} &amp; \\text{stationary increments} &amp; \\text{stationary increments} \\\\ \\hline N_t-N_s\\sim Pois(\\lambda(t-s)) &amp; W_t-W_s\\sim\\mathcal{N}(0,t-s) &amp; {L_t-L_s\\sim\\mathcal{P}(t-s),\\mathcal{P}\\text{ is }\\color{orange}{idd}} \\\\ \\hline \\end{array}$$&emsp;- Independent increments:&emsp;&emsp;$\\forall t_0&lt;t_1&lt;\\ldots&lt;t_n$, we have the following random variables jointly independent (所以一定也是 pairwise indep.)&emsp;&emsp;$X_{t_n}-X_{t_{n-1}},\\ldots,X_1-X_0$&emsp;- Stationary increments:&emsp;&emsp;$\\forall t,s\\geq0$, and $h&gt;0$, we have&emsp;&emsp;$X_{t+h}-X_{s+h} =^d X_t-X_s$&emsp;&emsp;注意到是 in distribution sense (最 weak 的那個 sense)&emsp;&emsp;我們知道 “almost surely”, “in probability”, or “in mean square” senses 都會導致 in distribution sense.&emsp;- $iid$ stands for Infinitely divisible distribution: will be discussed in next section 注意到對所有的 Levy process $L_t$, 其 covariance function 有如下關係:$$K(t,s)=Cov(L_t,L_s)\\\\ \\text{assume } t&gt;s \\text{ } =Cov(L_t-L_s+L_s,L_s) = Cov(L_t-L_s,L_s) + Var(L_s) = Var(L_s)\\\\ \\therefore K(t,s)=Var(L_{\\min(t,s)})$$ [Cadlag Function Def]:&emsp;中文稱 “右連左極函數”&emsp;We say $f$ is a Cadlag function, if 左右 limit 都存在且定義 $f$ 為右 limit 的值&emsp;$$\\exists\\lim_{s\\rightarrow t,s&lt;t} f(s) =: f(t^-) \\\\ \\exists\\lim_{s\\rightarrow t,s&gt;t} f(s) =: f(t^+) = f(t)$$ Wiki 的介紹很清楚 記得在 Brownian motion 時有講到 Kolmogorov continuity theorem: 一定存在一個 continuous modification of a Brownian motion 類似.所以看到 Brownian motion 可以只考慮其 almost surely continuous trajectories 的版本 [Proposition, Cadlag Trajectories of Levy Process]:&emsp;在 Levy process 也有類似的情況. 給定一個 Levy process $L_t$&emsp;存在另一個 process $L_t’$ such that 所有 trajectories 都是 almost surely Cadlag function. Week 8.2: Examples of Lévy processes. Calculation of the characteristic function in particular cases[Infinitely Divisible Distributions Def]:&emsp;$\\xi$ is a infinitely divisible distribution if $\\forall n\\geq2$, $\\exists Y_1,\\ldots,Y_n$ - i.i.d. such that&emsp;$\\xi=^d Y_1+\\dots+Y_n$&emsp;Note that it is equals in distribution sense. It is equivalent to saying:&emsp;$$\\Phi_\\xi(u)=\\left(\\Phi_{Y_1}(u)\\right)^n, \\qquad \\forall n \\\\ \\Longleftrightarrow \\left(\\Phi_\\xi(u)\\right)^{1/n} \\text{ is a characteristic function}, \\qquad\\forall n$$&emsp;where $\\Phi_\\xi(u)$ is the characteristic function of random variable $\\xi$ [Levy Process can be Characterized by an I.D.D.]:&emsp;1. $\\forall$ Levy process $L_t$ at $\\forall$ time $t^*$, $L_{t^*}$ has an infinitely divisible distribution&emsp;2. $\\forall$ infinitely divisible distribution, $\\exists$ a Levy process $L_t$ such that $L_1$ has this distribution[Proof]:&emsp;只證明第一條. $L_t$ 可以改寫如下: $\\forall n$ 都滿足&emsp;$L_t=\\sum_{k=1}^n\\left(L_{t\\cdot\\frac{k}{n}} - L_{t\\cdot\\frac{k-1}{n}}\\right)$&emsp;由於 stationary increments 特性&emsp;$\\left(L_{t\\cdot\\frac{k}{n}} - L_{t\\cdot\\frac{k-1}{n}} \\right) =^d L_{\\frac{t}{n}}$&emsp;再由於 independent increments 特性, 變成 $n$ 個 i.i.d. random variables 相加. Q.E.D. [Normal Distribution is an i.d.d.]:&emsp;所以可以發現 $L_t$ is a Levy process ($W_t$ is a Brownian motion)&emsp;$L_t=\\mu t+\\sigma W_t$; 且 $L_1\\sim\\mathcal{N}(\\mu,\\sigma^2)$ [Cauchy Distribution is an i.d.d.]:&emsp;Cauchy distribution 的 p.d.f. 為:&emsp;$p(x)=\\frac{1}{\\pi\\gamma\\left[1+\\left(\\frac{x-x_0}{\\gamma}\\right)^2\\right]}$&emsp;其中 $x_0$ 稱為 location parameter, $\\gamma$ 稱為 scale parameter&emsp;觀察特徵函數可以發現是 i.d.d. [Gamma Distribution is an i.d.d.] Distributions that are infinitely divisible distributions: [Stable Distribution Def]:&emsp;We say that $\\xi$ is stable distribution if $\\forall n\\geq2$, $\\exists$ $a_n&gt;0,b_n\\in\\mathbb{R}$&emsp;$\\xi,\\xi_1,\\ldots,\\xi_n$ - are i.i.d. such that&emsp;$\\xi_1+\\ldots+\\xi_n =^d a_n\\xi+b_n$ 是 stable distribution 一定是 infinitely divisible, 很容易看出來, 只要改寫成$\\left(\\frac{\\xi_1}{a_n}-\\frac{b_n}{n\\cdot a_n}\\right) + \\ldots + \\left(\\frac{\\xi_n}{a_n}-\\frac{b_n}{n\\cdot a_n}\\right) =^d \\xi$反之不成立所以上述的 i.d.d. 中, 只有 Normal and Cauch distributions 是 stable Week 8.3: Relation to the infinitely divisible distributions[Properties of Infinitely Divisible Distributions]:&emsp;Let $\\xi$ is a i.d.d. then&emsp;1. $\\Phi_\\xi(u)=0$ doesn’t have any $\\mathbb{R}$ solution&emsp;&emsp;i.e. all solutions are imaginary numbers. 特徵方程式的 roots 沒有實數解&emsp;2. Support set of $\\xi$, $Supp(\\xi)$, is unbounded Random variable $\\xi$ 為一個 measurable function $\\Omega\\rightarrow\\mathbb{R}$. 而 support of a measurable function $\\mu$ 定義為:$Supp(\\mu):=\\left\\{x:\\forall\\text{ open set }G\\text{ containing }x\\text{ s.t. }\\mu(G)&gt;0\\right\\}$ 參考 wiki 的定義): 白話是任何 $x$ 的 open neighbourhood 的 measure 都 $&gt;0$, 則 $x$ 屬於 support set 因此 random variable 的 support set 就用 measurable function 的方式去想 [Uniform Distribution is NOT i.d.d.]:&emsp;Let $\\xi$ is uniform distribution $\\mathcal{U}(a,b)$, then the characteristic function is&emsp;$\\Phi_\\xi(u)=\\int_a^b e^{iux}\\frac{1}{b-a}dx=\\frac{1}{b-a}\\cdot\\frac{e^{iua}}{iu}\\cdot\\left(e^{iu(b-a)}-1\\right)$&emsp;We have $\\Phi_\\xi(u)=0$ if and only if&emsp;$e^{iu(b-a)}=1 \\Longleftrightarrow u=\\frac{2\\pi k}{b-a},\\qquad k\\in\\mathbb{Z}/\\{0\\}$&emsp;所以存在 $\\mathbb{R}$ 的 root, 因此不是 infinitely divisible distributions [Bernoulli Distribution is NOT i.d.d.]:&emsp;Let $\\xi$ is Bernoulli random variable&emsp;$$\\xi=\\left\\{ \\begin{array}{rl} 1, &amp; p\\\\ 0, &amp; 1-p \\end{array} \\right.$$&emsp;then $Supp(\\xi)=\\{0,1\\}$, so is not bounded and hence NOT infinitely divisible distributions 我們常看到的 distribution 大部分都是 infinitely divisible distribution, 如Gaussian, Cauchy, Gamma, Poisson, Compound Poisson但是 uniform and Bernoulli distributions 不是 i.d.d. 最後試題計算一下 binomial distribution $Bin(n,p)$ 的特徵函數:$$\\Phi_X(u)=\\mathbb{E}\\left[e^{iuX}\\right]=\\sum_{k=0}^n e^{iuk}\\mathcal{P}(X=k) \\\\ = \\sum_{k=0}^n e^{iuk}\\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k} \\\\ = \\sum_{k=0}^n\\frac{n!}{k!(n-k)!}(e^{iu}p)^k(1-p)^{n-k} = (e^{iu}p+(1-p))^n$$ Week 8.4: Characteristic exponent[Characteristic Exponent Def with Proposition]:&emsp;$\\forall$ Levy process $L_t$, $\\exists \\Psi:\\mathbb{R}\\rightarrow\\mathbb{C}$ call characteristic exponent, such that&emsp;$\\Phi_{L_t}(u)=\\mathbb{E}[e^{iuL_t}]=e^{t\\cdot\\Psi(u)}$ [Example 1]:&emsp;$L_t=\\mu\\cdot t$, 其 characteristic function is $\\Phi_{L_t}(u)=\\mathbb{E}[e^{iu\\mu t}]$, 所以 $\\Psi(u)=iu\\mu$ [Brownian Motion Example 2]:&emsp;$L_t=\\sigma W_t$, where $W_t$ is a Brownian motion. 所以 $L_t\\sim\\mathcal{N}(0,\\sigma^2t)$.&emsp;我們知道 $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ 的特徵方程式為:&emsp;$\\Phi_X(u)=e^{i\\mu u-\\frac{1}{2}\\sigma^2u^2}$&emsp;所以&emsp;$\\Phi_{L_t}(u)=e^{-\\frac{1}{2}\\sigma^2t u^2}\\Longrightarrow \\Psi(u)=-\\frac{1}{2}\\sigma^2u^2$ [Compound Poisson processes Example 3]:&emsp;Let $X_t$ is a Compound Poisson processes. We have it’s characteristic function of increments as:&emsp;$\\Phi_{X_t-X_s}(u)=e^{\\lambda(t-s)(\\Phi_{\\xi_1}(u)-1)}$&emsp;Therefore, characteristic function of $L_t=X_t$:&emsp;$\\Phi_{L_t}(u)=e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)}$&emsp;and hence&emsp;$$\\Psi(u)=\\lambda(\\Phi_{\\xi_1}(u)-1) \\\\ =\\lambda\\left( \\int e^{iux}\\mathcal{P}_{\\xi_1}(x)dx - \\int\\mathcal{P}_{\\xi_1}(x)dx \\right) \\\\ = \\int (e^{iux}-1)\\lambda \\mathcal{P}_{\\xi_1}(x)dx =\\int (e^{iux}-1)\\lambda F_{\\xi_1}(dx)$$ [Linear Brownian Motion Plus Compound Poisson processes Example 4]:&emsp;Let $L_t$ is&emsp;$L_t=\\mu t+\\sigma W_t + C.P.P.$&emsp;then we have&emsp;$\\Psi(u)=iu\\mu-\\frac{1}{2}\\sigma^2u^2 + \\int (e^{iux}-1)\\lambda F_\\xi(dx)$ 這個形式很重要, for such Levy process, 其特徵方程式可以 explicitly 寫出來 [Corollaries]:&emsp;1. The distribution of $L_t$ is determined by $L_1$&emsp;&emsp;$\\Phi_{L_1}(u)=e^{\\Psi(u)}$&emsp;&emsp;所以可以由 $L_1$ 決定出 characteristic exponent $\\Psi(u)$&emsp;&emsp;而知道 $\\Psi(u)$ 我們就知道任何時間點的 characteristic function, 也就決定了 distribution&emsp;2. Expectation and variance of $L_t$&emsp;&emsp;$$\\mathbb{E}L_t=t\\cdot\\mathbb{E}L_1 \\\\ Var(L_t)=t\\cdot Var(L_1) \\\\ K(t,s) =Var(L_{\\min(t,s)}) = \\min(t,s)\\cdot Var(L_1)$$ 由此 $K(t,s)$ 可知, Levy process is NOT stationary (in any sense). 因為 WSS 必須滿足 $K(t,s)=K(t+h,s+h)$, 然而 $\\min(t,s)\\neq \\min(t+h,s+h)$ 注意到, Levy process is NOT stationary but is stationary increment Week 8.5: Properties of a Lévy process, which directly follow from the existence of characteristic exponent[Levy Measure Def]:&emsp;Levy measure of a Levy process $X_t$ is defined as&emsp;$$\\nu(B)=\\mathbb{E}\\left[ \\# t\\in[0,1]: \\triangle X_t \\in B \\right], \\qquad \\forall B\\subset\\mathbb{R} \\setminus\\{0\\}$$&emsp;where $\\triangle X_t=X_t-X_{t^-}$ (size of jump), also reference to definition of Cadlag function $B$ 定義了我們要關注的那些 jump 值, 而 measure 就是: 有多少 (以期望值來看) random variables 他們的 jump 值屬於 $B$, 且 random variables 只限制在 $t\\in[0,1]$又或者這麼說, 對於 $X_t,\\forall t\\in[0,1]$ 這些 random variables 來說, measure 就是發生 jump 次數的期望值 (constrained on jump 的值是我們要的, i.e. in $B$)此段課程後面會提供一個 Levy measure 的充要條件, 實在很神奇 這定義是否符合 measure 之定義? 檢查如下特性&emsp;1. Non-negativity: $\\nu(B)\\geq 0$&emsp;2. Null empty set: $\\nu(\\phi) = 0$&emsp;3. Countable additivity:&emsp;&emsp;For all countable collections ${B_k}$ of pairwise disjoint sets&emsp;&emsp;$\\nu\\left(\\bigcup_{k=1}^\\infty B_k \\right) = \\bigcup_{k=1}^\\infty \\nu(B_k)$可發現 $\\nu$ 符合 measure 定義, 所以是個 measure Brownain motion is continuous a.e. 所以 $\\nu(B)=0$Poisson process 如圖所示 考慮 Compound Poisson process, let $N_t$ is Poisson process, and $\\xi_1,\\xi_2,…$ are i.i.d. then C.P.P. $X_t$ is defined as$X_t=\\sum_{k=1}^{N_t} \\xi_k$其 Levy measure 如下:$$\\nu(B)=\\lambda\\cdot\\mathcal{P}(\\xi_1\\in B) \\\\ = \\int_B \\lambda \\mathcal{P}_\\xi(x)dx := \\int_B s(x)dx$$where $s(x)$ is called Levy density$s(x):=\\lambda \\mathcal{P}_\\xi(x)$and we have $\\nu(dx)=s(x)dx$ 可以這麼想, 在原來的 Poisson process, 每一次的 jump 都固定是 $1$, 但在 C.P.P. jump 的值是根據 $\\xi_k$ 來決定的. 所以能不能算進一次 jump 要看 jump 的值是不是在 $B$ 中 [Sufficient and Necessary Condition of Levy Measure]:&emsp;If $\\nu$ is a Levy measure for some Levy process $L_t$, if and only if satisfying&emsp;$$\\int_{|x|&lt;1} x^2\\nu(dx) =\\int_{|x|&lt;1} x^2s(x)dx&lt;\\infty \\\\ \\int_{|x|\\geq1} \\nu(dx)=\\int_{|x|\\geq1} s(x)dx&lt;\\infty$$ Week 8.6: Lévy-Khintchine representation and Lévy-Khintchine triplet-1 💡 課程老師說這是整個課程最重要的定理 [Levy-Khintchine Theorem]:&emsp;The characteristic exponent $\\Psi(u)$ of a Levy process $L_t$ can be represented as follows:&emsp;$$\\color{orange}{ \\Psi(u)=iu\\mu-\\frac{1}{2}u^2\\sigma^2 + \\int_{\\mathbb{R}}\\left( e^{iux}-1-iux\\cdot\\mathbf{1}\\{|x|&lt;1\\} \\right)\\nu(dx) }$$&emsp;where $\\mu\\in\\mathbb{R},\\sigma\\geq0$, and $\\nu$ is a Levy measure. $(\\mu,\\sigma,\\nu)$ is called Levy triplet&emsp;which completely determined the distribution of Levy process at any time moment $t$ 可以證明 $\\forall L_t$ - Levy process, 可以表示如下$X_t = \\underbrace{\\mu t + \\sigma W_t}_\\text{continuous part} + \\underbrace{\\mathcal{J}_t}_{\\text{jump part}}$ 所以 Levy process 的 continuous part 可以視為 Brownian motion而 jump part $\\mathcal{J}_t$:$$\\mathcal{J}_t \\approx \\overbrace{ \\left( \\sum_{0&lt;s&lt;t,|\\triangle X_s|&gt;1} \\triangle X_s \\right) }^{\\text{C.P.P.}} + \\left( \\lim_{\\varepsilon\\rightarrow0} \\overbrace{ \\sum_{0&lt;s&lt;t,\\varepsilon&lt;|\\triangle X_s|\\leq1} \\triangle X_s }^{\\text{C.P.P.}} \\right)$$ 我們可以將 Levy process 的 trajectory 畫出來 trajectory_of_Levy_process.pptx 接下來下一節課程會介紹一個 sufficient condition 使我們可以把 jump process 的 limit 那段拿掉, 使得 jump process 完全可以由一個 C.P.P. 替代 Week 8.7: Lévy-Khintchine representation and Lévy-Khintchine triplet-2重複一次 Levy process 的 characteristic exponent:$$\\Psi(u)=iu\\mu-\\frac{1}{2}u^2\\sigma^2 + \\int_{\\mathbb{R}}\\left( e^{iux}-1- \\color{orange}{ iux\\cdot\\mathbf{1}\\{|x|&lt;1\\} } \\right)\\nu(dx)$$有些情況下可以簡化上式, 尤其橘色部分. 首先回顧一下什麼是 bounded variation [Bounded Variation of a Stochastic Processes]:&emsp;對於一個 process $X_t$, 考慮一個 partition $\\bigtriangleup:0=t_0&lt;t_1&lt;...&lt;t_n=t$&emsp;如果滿足以下條件, 則稱 $X_t$ is bounded variation:&emsp;$\\lim_{|\\bigtriangleup|\\rightarrow0} \\sum_{k=1}^n | X_{t_k}-X_{t_{k-1}} | &lt; \\infty$&emsp;$|\\bigtriangleup|\\rightarrow0$ 表示:&emsp;$\\max_{k=0,...,n}\\{|t_k-t_{k-1}|\\}\\rightarrow0, \\text{ for }n\\rightarrow0$ 注意到 Brownian motion $W_t$ 不是 bounded variation [When Levy Process is Bounded Variation]:&emsp;一個 Levy process $L_t$ 是 bounded variation 若且為若&emsp;$$\\begin{align} \\sigma=0 \\\\ \\int_{|x|&lt;1} x\\nu(dx) &lt; \\infty \\end{align}$$&emsp;條件 $(2)$ 會有如下結果&emsp;$\\int_\\mathbb{R}iux\\cdot\\mathbf{1}\\{|x|&lt;1\\}\\nu(dx) = iu\\int_{|x|&lt;1}x\\nu(dx)&lt;\\infty$&emsp;因此 characteristic exponent 變成:&emsp;$$\\Psi(u)=iu\\mu-\\frac{1}{2}u^2\\sigma^2 + \\int_{\\mathbb{R}}\\left( e^{iux}-1- iux\\cdot\\mathbf{1}\\{|x|&lt;1\\} \\right)\\nu(dx) \\\\ =iu\\mu + \\int_{\\mathbb{R}}(e^{iux}-1)\\nu(dx)-iu\\int_{|x|&lt;1}x\\nu(dx) \\\\ = iu\\left(\\mu-\\int_{|x|&lt;1}x\\nu(dx)\\right) + \\int_{\\mathbb{R}}(e^{iux}-1)\\nu(dx)$$&emsp;所以 characteristic function: $\\Phi(u)=\\exp\\{t\\cdot\\Psi(u)\\}$ 重寫一遍, 這很重要$$\\color{orange}{ \\Psi(u) = iu \\underbrace{\\left(\\mu-\\int_{|x|&lt;1}x\\nu(dx)\\right)}_{=\\tilde{\\mu}} + \\int_{\\mathbb{R}}(e^{iux}-1)\\nu(dx) }$$or$$\\color{orange}{ \\Psi(u) = iu \\underbrace{\\left(\\mu-\\int_{|x|&lt;1}x\\cdot s(x)dx\\right)}_{\\tilde{\\mu}} + \\int_{\\mathbb{R}}(e^{iux}-1)\\cdot s(x)dx }$$where $s(x)$ is Levy density [When Levy Process is C.P.P.]&emsp;一個 Levy process $L_t$ 是 C.P.P. 若且為若&emsp;$$\\begin{align} \\sigma=0, \\\\ \\int_{\\mathbb{R}}\\nu(dx)=\\nu(\\mathbb{R})&lt;\\infty \\end{align}$$&emsp;可以發現這樣的 $L_t$ 也是 bounded variation, 所以也可以簡化 [When Levy Process is Subordinator]:&emsp;一個 Levy process $L_t$ 稱為 subordinator 如果滿足&emsp;$$\\begin{align} X_t\\geq0\\text{ a.s. }\\Longleftrightarrow X_t\\geq X_s\\text{ a.s. }, \\forall t\\geq s \\end{align}$$&emsp;上式會等價是因為 Levy process 的 stationary increment. i.e.&emsp;$X_t-X_s =^d X_{t-s}, \\qquad \\forall t\\geq s$&emsp;而 Levy process $L_t$ 為 subordinator 若且為若&emsp;$$\\begin{align} \\sigma=0 \\\\ \\nu(\\mathbb{R}^-)=0 \\\\ \\int_0^1 x\\nu(dx)&lt;\\infty \\end{align}$$ 式 (7) 的 Levy measure 表示 jump 的值一定都是正的 (為負的次數其 measure 為 0) 一樣可以發現這樣的 $L_t$ 也是 bounded variation, 所以也可以簡化最後一個問題是因為 $\\sigma&gt;0$ 表示有 Brownian motion, 所以 variation 不會 bounded Week 8.8: Lévy-Khintchine representation and Lévy-Khintchine triplet-3RecapLevy process 常用來描述 jumps, recap Levy process $L_t$其 characteristic exponent 為$$\\Psi(u)=iu\\mu-\\frac{1}{2}u^2\\sigma^2 + \\int_{\\mathbb{R}}\\left( e^{iux}-1- iux\\cdot\\mathbf{1}\\{|x|&lt;1\\} \\right)\\nu(dx)$$然後 characteristic function 為$\\Phi_{L_t}(u)=\\exp\\{t\\cdot \\Psi(u)\\}$而 $\\nu(x)$ 是 Levy measure$\\nu(B)=\\int_B s(x)dx$稱 $s(x)$ 為 Levy density, 滿足$$\\begin{align} \\int_{|x|&lt;1} x^2\\nu(dx) =\\int_{|x|&lt;1} x^2s(x)dx&lt;\\infty \\\\ \\int_{|x|\\geq1} \\nu(dx)=\\int_{|x|\\geq1} s(x)dx&lt;\\infty \\end{align}$$ Recap 結束由 Levy measure and density 式子 (9) and (10) 知道, characteristic exponent 的積分項存在, 分析如下$$\\int_{\\mathbb{R}}\\left( e^{iux}-1- iux\\cdot\\mathbf{1}\\{|x|&lt;1\\} \\right)\\nu(dx) \\\\ = \\int_{|x|&lt;1}\\left(e^{iux}-1-iux\\right)\\nu(dx) + \\int_{|x|\\geq1}(e^{iux}-1)\\nu(dx) \\\\ \\leq \\int_{|x|&lt;1}\\left(O(x^2)\\cdot u^2\\right)\\nu(dx) + \\int_{|x|\\geq1}2\\nu(dx)&lt;\\infty$$我們可以分析 $r$ 最小可以多小, 使得下式仍滿足$\\int_{|x|&lt;1}x^r\\nu(dx)&lt;\\infty$我們知道如果 $r$ 滿足的話, 則 $\\forall s, $\\alpha\\in(0,2]$, such that&emsp;$$\\left\\{ S_{at}\\right\\}_{t\\geq0} =^d \\left\\{ a^{1/\\alpha}S_t+b(t)\\right\\}_{t\\geq0}$$ Brownian motion is a stable process, 因為 $\\alpha=2$, $b(t)=0$ 滿足上式$W_{at}\\sim\\mathcal{N}(0,at)\\sim\\sqrt{2}\\cdot\\mathcal{N}(0,t)\\sim\\sqrt{2}\\cdot W_t$ For stable processes, $BG(\\nu)=\\alpha$ 如果 $\\alpha\\approx0$, 則 trajectories 會很接近 C.P.P., 而如果 $\\alpha\\approx2$, 則 trajectories 會很接近 Brownian motion, 其他則介於中間的兩者混合 Week 8.9: Modelling of jump-type dynamics. Lévy-based models這節主要講如何從 data 去 estimate 出對應的 Levy process$X_t$ is a Levy process with bounded variation, 所以特徵方程式為:$$\\Phi_{X_\\Delta}(u) = \\exp\\left\\{ \\Delta\\cdot\\left( iu\\tilde\\mu+\\int_\\mathbb{R}(e^{iux}-1)s(x)dx \\right) \\right\\}$$對其一次和二次微分: 注意到二次微分跑出了一個 Fourier transform 項, 所以改寫一下變成$$\\begin{align} \\mathcal{F}[x^2s(x)](u)=\\frac{-1}{\\Delta}\\cdot\\left( \\frac{\\Phi_{X_\\Delta}&apos;&apos;(u)}{\\Phi_{X_\\Delta}(u)} - \\left(\\frac{\\Phi_{X_\\Delta}&apos;(u)}{\\Phi_{X_\\Delta}(u)}\\right)^2 \\right) \\end{align}$$ 所以如果我們有如下的 data:$X_\\Delta,X_{2\\Delta},...,X_{n\\Delta}$ 我們可以估計出 $\\Phi$ 如下:$$\\hat{\\Phi}_{X_\\Delta}(u)=\\frac{1}{n}\\sum_{k=1}^n \\exp\\left\\{iu\\left(X_{k\\Delta}-X_{(k-1)\\Delta}\\right) \\right\\}$$ 我不是很懂為何可以這麼估計, 發問題到論壇了. 因此對其微分也就得到估計的 $\\hat\\Phi’,\\hat\\Phi’’$, 然後可以得到式 (11) 的結果, 再對其求 inverse Fourier transform 可以得到 Levy density $s(x)$ 老實說沒有例子很難想像怎麼做…. 🙁 Week 8.10: Time-changed stochastic processes. Monroe theorem有始有終吧, 最後一課了!!說明一些 Levy-based model課程說單純的 Levy process 對於真實情況仍過於簡化, 因此要有一些變形, 其中一個例子是 stochastic time change model [Stochastic Time Change Model]:&emsp;$X_t$ is a Levy process, 其中 $t=T(s)$, $T(s)$ 是一個 subordinator Levy process, 所以變成 $X_{T(s)}$&emsp;同時如果 $X_t\\perp T(s)$ 則 $X_{T(s)}$ 也會是 Levy process 一個例子如下圖, $t$ 有時很密集, 有時很鬆散 $T(s)$ 是 cumulative amount of transactions in time interval $[0,s]$ 第二個例子真的聽不懂… 課程結束! 最後剩 Quiz and Final Exam! 👏","tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://bobondemon.github.io/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"https://bobondemon.github.io/tags/Stochastic-Processes/"},{"name":"Lévy Processes","slug":"Levy-Processes","permalink":"https://bobondemon.github.io/tags/Levy-Processes/"},{"name":"Characteristic Exponent","slug":"Characteristic-Exponent","permalink":"https://bobondemon.github.io/tags/Characteristic-Exponent/"}]},{"title":"Stochastic Processes Week 7 Stochastic integration & Itô formula","date":"2021-12-12T13:17:41.000Z","path":"2021/12/12/Stochastic-Processes-Week-7-Stochastic-integration-Ito-formula/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula (本文) Week 8: Lévy processes 四種 types 的 stochastic integration, where $X_t,H_t$ are stochastic processes, $f(t)\\in L^2([a,b])$ is a deterministic function, $W_t$ is a Brownian motion, and $a,b\\in\\mathbb{R}$$$\\begin{align} \\int_a^b X_t dt \\\\ \\int_a^b f(t)dW_t \\\\ \\int_a^b X_t dW_t \\\\ \\int_a^b X_t dH_t \\end{align}$$ 在 stochastic integral 跟以往的 integral 很不一樣的一點是, 我們發現 integral 的結果仍然是一個 random variable閱讀以下內容時可以看出這一點 Week 7.1: Different types of stochastic integrals. Integrals of the type $∫ X_t dt$Given a Stochastic process $X_t:\\Omega\\times\\mathbb{R}_+\\rightarrow\\mathbb{R}$如果我們固定一個 outcome $\\omega$, 則 $X_t(\\omega):\\mathbb{R}_+\\rightarrow\\mathbb{R}$, 因此我們就可以考慮 Riemann integral:$\\int_a^b X_t(\\omega)dt=\\color{orange}{\\lim_{|\\Delta|\\rightarrow0}}\\sum_{k=1}^n X_{t_{k-1}}(\\omega)\\cdot(t_k-t_{k-1})$For partition $\\Delta:a=t_0\\leq t_1\\leq …\\leq t_n=b$, $|\\Delta|=\\max\\{t_k-t_{k-1}\\},k=1,...,n$若考慮 stochastic process, 上述的 limit 必須以 mean square sense 來考慮, 因此正式定義如下: [Stochastic Integrals of Simplest Type $∫ X_t dt$]:&emsp;Given a Stochastic process $X_t:\\Omega\\times\\mathbb{R}_+\\rightarrow\\mathbb{R}$&emsp;Given partition $\\Delta:a=t_0\\leq t_1\\leq ...\\leq t_n=b$, $|\\Delta|=\\max\\{t_k-t_{k-1}\\},k=1,...,n$&emsp;If the following expectation converges to some value $A$ in mean square sense:&emsp;$$\\mathbb{E}\\left[ \\left( A - \\sum_{k=1}^n X_{t_{k-1}}(t_k-t_{k-1}) \\right)^2 \\right] \\xrightarrow[|\\Delta|\\longrightarrow0]{} 0$$&emsp;Then we denote(define) the converged value $A$ as: $\\int_a^b X_t dt$ [Existence of the Simplest Type of Stochastic Integral]:&emsp;$X_t:\\mathbb{E}[X_t^2]&lt;\\infty$, if&emsp;1. $m(t)$ continuous&emsp;2. $K(t,s)$ continuous&emsp;Then the stochastic integral exists, i.e. $\\int_a^b X_tdt&lt;\\infty$ 有上述定理好處如下We assume integral is taken in an bounded interval $[a,b]$&emsp;1. Expectation of stochastic integral:&emsp;&emsp;$\\mathbb{E}\\left[ \\int X_tdt \\right] = \\int \\mathbb{E}[X_t]dt$&emsp;&emsp;課程老師說 apply Frobenius theorem 可得此結果. 看不懂 Frobenius theorem&emsp;2. Expectation of squared stochastic integral:&emsp;&emsp;$$\\mathbb{E}\\left[ \\left(\\int X_t dt\\right)^2 \\right] = \\mathbb{E}\\left[ \\int\\int X_t X_s dtds \\right] \\\\ = \\int\\int\\mathbb{E}[X_tX_s]dtds$$&emsp;3. Variance of stochastic integral:&emsp;&emsp;$$Var\\left[ \\int X_t dt \\right] = \\mathbb{E}\\left[ \\left(\\int X_t dt\\right)^2 \\right] - \\left(\\mathbb{E}\\left[ \\int X_tdt \\right]\\right)^2 \\\\ = \\int\\int \\mathbb{E}[X_tX_s]dtds - \\mathbb{E}\\left[ \\int X_tdt \\right]\\mathbb{E}\\left[ \\int X_sds \\right]\\\\ = \\int\\int \\mathbb{E}[X_tX_s]dtds - \\int\\mathbb{E}[X_t]dt\\cdot\\int\\mathbb{E}[X_s]ds \\\\ = \\int_a^b\\int_a^b K(t,s) dtds \\\\ \\because\\text{symmetric}= 2\\int_a^b \\int_a^s K(t,s) dtds$$ 注意到 $W_t\\sim\\mathcal{N}(0,t)$, 所以 $Var(W_t)=\\mathbb{E}W_t^2=t$ Week 7.2-3: Integrals of the type $∫ f(t) dW_t$第二種 type 的 integral 我們稱為 Wiener integral$\\int_a^b f(t)dW_t$where $f(t)\\in L^2([a,b])$ is a deterministic function, $W_t$ is a Brownian motion, and $a,b\\in\\mathbb{R}$ [Inner Product in Function Space]:&emsp;Define inner product between real valued functions $f,g$ in $[a,b]$&emsp;$&lt;f,g&gt;=\\int_a^b f(x)g(x)dx$&emsp;Having properties:&emsp;1. $&lt;f,g&gt;=&lt;g,f&gt;$&emsp;2. $&lt;a_1f_1+a_2f_2,g&gt;=a_1&lt;f_1,g&gt;+a_2&lt;f_2,g&gt;$&emsp;3. $&lt;f,f&gt;\\geq0$, and $=0\\Longleftrightarrow f=0$ [Informal Def of $L^2$ Space]:&emsp;定義 norm 為 $\\|f\\|_2 = \\sqrt{&lt;f,f&gt;}$, 則所有 $\\|f\\|_2\\leq\\infty$ 所形成的空間&emsp;(搭配 $\\|\\cdot\\|_2$ 則為 Banach space, 搭配 $&lt;\\cdot,\\cdot&gt;$ 則為 Helbert sapce) 我們稱為 $L^2$ space 比較嚴謹的定義參考數學課本 因此我們可以說在 $L^2$ space 上的收斂, 視為該 normed (Banach) space 上的收斂, i.e.$$f_n\\xrightarrow[]{L^2}f\\Longleftrightarrow \\|f_n-f\\|_2\\xrightarrow[n\\rightarrow\\infty]{}0 \\\\ \\text{, or } \\Longleftrightarrow \\int_a^b \\left( f_n(x)-f(x) \\right)^2 dx \\xrightarrow[n\\rightarrow\\infty]{}0$$ 我們先定義 Wiener integral of a step function [Wiener Integral of a Step Function]:&emsp;Let $a=t_0\\leq t_1\\leq …\\leq t_n=b$, $\\alpha_i\\in\\mathbb{R}$, and step function $f(x)$ defined as:&emsp;$f(x)=\\sum_{i=1}^n \\alpha_i\\cdot\\mathbf{1}\\{t_{i-1}\\leq x&lt;t_i\\}$&emsp;Then Wiener integral of $f(x)$ is:&emsp;$\\int_a^b f(x)dW_t = \\sum_{i=1}^n \\alpha_i\\cdot(W_{t_i}-W_{t_{i-1}})$ [Example]:&emsp;我們可以發現 integral 的結果都是 normal distribution [Distribution of Wiener Integral of a Step Function]:&emsp;Let $f(x)$ is a step function, and Wiener integral as:&emsp;$I(f):=\\int_a^b f(t)dW_t$&emsp;then&emsp;$I(f)\\sim\\mathcal{N}\\left(0,\\int_a^bf^2(x)dx\\right)$ [Proof]:&emsp;根據 Wiener integral 的定義, 我們可以知道&emsp;$\\int_a^b f(x)dW_t = \\sum_{i=1}^n \\alpha_i\\cdot(W_{t_i}-W_{t_{i-1}})$&emsp;而因為 $W_t$ is Brownian motion, 具有 independent increment 性質&emsp;因此等於是互相獨立的 normal distribution 線性組合的結果, 所以也是 normal. 因此我們只需計算 mean and variance [Wiener Integral of a Deterministic $L^2$ Function]:&emsp;Let $f(x)\\in L^2([a,b])$, and if we can find a sequence of step functions $(f_n)_n$&emsp;such that converges to $f$ in $L^2$ space, i.e.:&emsp;$f_n\\xrightarrow[]{L^2}f\\Longleftrightarrow \\int_a^b\\left( f_n(t)-f(t) \\right)^2 dt \\xrightarrow[n\\rightarrow\\infty]{}0$&emsp;then we define $I(f)$ as the limit of $I(f_n)$ as $n\\rightarrow\\infty$, i.e.:&emsp;$I(f):=\\lim_{n\\rightarrow\\infty} I(f_n)$&emsp;Where the limit should be understood by mean square sense:&emsp;$$\\mathbb{E}\\left[ (I(f_n)-I(f))^2 \\right] \\xrightarrow[n\\rightarrow\\infty]{}0$$ 觀察以上的定義, 有 3 個重要的問題需要回答: Why $I(f)$ doesn’t depend on $(f_n)_n$? Properties of $I(f)$? How to counstruct $(f_n)_n$? Will be answered in Week 7.4-5 [Theorem Explian Why $I(f)$ doesn’t depend on $(f_n)_n$]:&emsp;$(f_n)_n,(\\tilde{f}_n)_n$ sequences of step functions in $L^2$ space and both converges to $f$, i.e.&emsp;$f_n\\xrightarrow[]{L^2}f, \\tilde{f}_n\\xrightarrow[]{L^2}f$&emsp;Then&emsp;$\\lim_{n\\rightarrow\\infty} I(f_n) = \\lim_{n\\rightarrow\\infty} I(\\tilde{f_n})$&emsp;The limit is understood as mean square sense, that is:&emsp;$$\\mathbb{E}\\left[ (I(f_n)-I(\\tilde{f}_n))^2 \\right] \\xrightarrow[n\\rightarrow\\infty]{}0$$ Note that the Wiener integral of a step function $f$ is defined as:$I(f):=\\int_a^b f(t)dW_t$and $W_t$ is Brownian motion [Proof]:&emsp;$I(f_n)-I(\\tilde{f}_n)=I(f_n-\\tilde{f}_n)$ also is a step function&emsp;所以這個 step function 的 Wiener integral 會 follow normal distribution:&emsp;$I(f_n-\\tilde{f}_n)\\sim\\mathcal{N}\\left(0,\\int_a^b (f_n(x)-\\tilde{f}_n(x))^2 dx\\right)$&emsp;所以 Variance:&emsp;$$Var(I(f_n-\\tilde{f}_n)) = \\mathbb{E}\\left[ (I(f_n-\\tilde{f}_n))^2 \\right] \\\\ = \\int_a^b (f_n(x)-\\tilde{f}_n(x))^2 dx \\xrightarrow[n\\rightarrow\\infty]{} 0$$&emsp;最後會 approach to $0$ 是因為 both converges to $f$.&emsp;所以馬上就發現 limit converges in mean square sense:&emsp;$$\\because \\mathbb{E}\\left[ (I(f_n)-I(\\tilde{f}_n))^2 \\right] = \\mathbb{E}\\left[ (I(f_n-\\tilde{f}_n))^2 \\right] \\\\ = Var(I(f_n-\\tilde{f}_n)) \\xrightarrow[n\\rightarrow\\infty]{}0 \\\\ \\therefore\\lim_{n\\rightarrow\\infty} I(f_n) = \\lim_{n\\rightarrow\\infty} I(\\tilde{f_n})$$ [The Wiener Integral of Function in $L^2$ Space]:&emsp;$\\forall f\\in L^2([a,b])$, The Wiener integral:&emsp;$I(f)\\sim\\mathcal{N}\\left(0,\\int_a^b f^2(x)dx\\right)$[Proof]:&emsp;For any $(f_n)_n$ converges to $f$ in $L^2$, we have&emsp;$$I(f)=\\lim_{n\\rightarrow\\infty} I(f_n)\\\\ I(f_n)\\sim\\mathcal{N}\\left( 0, \\int_a^b f^2_n(x)dx \\right)$$&emsp;Normal distribution 的 limit 仍是 normal, 其結果為 mean and variance 的 limit, 所以&emsp;$$I(f)\\sim\\mathcal{N}\\left( 0, \\lim_{n\\rightarrow\\infty}\\int_a^b f^2_n(x)dx \\right) = \\mathcal{N}\\left( 0, \\int_a^b f^2(x)dx \\right)$$&emsp;Q.E.D. Week 7.4-5: Integrals of the type $∫ X_t dW_t$要定義這樣的 integral, $X_t$ 必須定義在 filtered probability space 上.[Filteration Def]: from Wiki [Filteration Def]:&emsp;Filtration is a sequence of $\\sigma$-algebras $\\mathcal{F}_t$ defined on the same probability space $(\\Omega,\\mathcal{F},\\mathcal{P})$&emsp;such that, $\\mathcal{F}_t$ is a sub-$\\sigma$-algebra of $\\mathcal{F}_s$ if $t\\leq s$, i.e.:&emsp;$\\mathcal{F}_t \\subseteq \\mathcal{F}_s, \\forall t\\leq s$ [$L^2$ Space of Stochastic Processes]:&emsp;給定一個 filtered probability space $(\\Omega,\\mathcal{F}, \\{\\mathcal{F}_t\\} ,\\mathcal{P})$&emsp;我們可以定義 $X_t$ 是從如下的 stochastic process 的 space 來的, ($ad$ 表示 adapted)&emsp;$L^2_{ad}([a,b],\\Omega)$&emsp;這個 space 有如下兩個 properties:&emsp;1. 由於我們知道 random variable $X_t$ 其實是一個 measurable function&emsp;&emsp;從 measurable space $(\\Omega,\\mathcal{F}_t)$ mapping 到另一個 measurable space $(\\mathbb{R},B(\\mathbb{R}))$, where $B(\\mathbb{R})$ 表示 Borel set of $\\mathbb{R}$&emsp;&emsp;由 measurable function 的定義知道 pre-image is still in $\\sigma$-algebra&emsp;&emsp;i.e., $\\{\\omega:X_t(\\omega)\\in B\\}\\in\\mathcal{F}_t$, $\\forall B\\in B(\\mathbb{R})$&emsp;&emsp;我們稱 $X_t$ is $\\mathcal{F}_t$ measurable 或稱 $X_t$ is $\\mathcal{F}_t$-adapted&emsp;&emsp;我們有 $X_t$ is $\\mathcal{F}_t$-adapted, $\\forall t$&emsp;2. 滿足如下特性&emsp;&emsp;$\\int_a^b \\mathbb{E}X_t^2 dt &lt; \\infty$ 若要積分$\\int_a^b X_tdW_t$Brownian motion $W_t$ 也必須在相同的 filtered space 上. [$\\mathcal{F}_t$-Brownian Motion Def]:&emsp;我們稱 $W_t$ is $\\mathcal{F}_t$-Brownian motion, if&emsp;1. $W_t$ is $\\mathcal{F}_t$-adapted&emsp;2. $(W_t-W_s)\\perp\\mathcal{F}_s,\\forall t&gt;s$ [Wiener Integral of a $L^2$-adapted Stochastic Process $X_t$]:&emsp;$X_t\\in L_{ad}^2$, $W_t$ is a $\\mathcal{F}_t$-Brownian motion, consider defining&emsp;$\\int_a^b X_t dW_t$&emsp;(Stage 1): Let $\\xi_i,\\forall i=0,…,n-1$ are random variables. If $X_t$ is a step processes, i.e.&emsp;&emsp;$X_t=\\sum_{i=1}^n\\xi_{i-1}\\cdot\\mathbf{1}\\{t_{i-1}\\leq t&lt;t_i\\}$&emsp;&emsp;then we define Wiener integral of this type is&emsp;&emsp;$I(X_t)=\\sum_{i=1}^n \\xi_{i-1}(W_{t_i}-W_{t_{i-1}})$&emsp;(Stage 2): For any $X_t\\in L_{ad}^2$, we can find a sequence of step processes $(X_t^n)_n$ which converges in $L_{ad}^2$ sense, i.e.:&emsp;&emsp;$\\int_a^b\\mathbb{E}\\left(X_t^n-X_t\\right)^2dt\\xrightarrow[n\\rightarrow\\infty]{}0$&emsp;&emsp;Such $X_t^n$ can be difined as:&emsp;&emsp;$X_t^n:=\\sum_{i=1}^nX_{t_{i-1}}\\cdot\\mathbf{1}\\{t_{i-1}\\leq t&lt;t_i\\}$&emsp;&emsp;and will be proved later that this construction is converges in $L_{ad}^2$ sense.&emsp;&emsp;then we can define the integral as follows:&emsp;&emsp;$I(X_t):=\\lim_{n\\rightarrow\\infty}I(X_t^n)$&emsp;&emsp;and the limit is understood in mean square sense, i.e.:&emsp;&emsp;$$\\mathbb{E}\\left[ (I(X_t^n)-I(X_t))^2 \\right]\\xrightarrow[n\\rightarrow0]{}0$$ 同樣的, 這個 integration doesn’t depend on 怎麼選 sequence of step processes $(X_t^n)_n$ 💡 所以找出一個 sequence of step processes converges (in $L_{ad}^2$ sense) to $X_t$ 後, 由於每一個 step process 的 Wiener integral 都可以求得, 所以該 Wiener integral 的 limit (in m.sq. sense) 我們就可以定義為 $X_t$ 的 Wiener integral 以下定理說明怎麼找到這個 step process $(X_t^n)_n$ such that converge to $X_t$ in $L_{ad}^2$ sense [Convergence of Step Processes]:&emsp;Let stochastic process $X_t$ be such that $m(t)$ and $K(t,s)$ are continuous, then define&emsp;$X_t^n:=\\sum_{i=1}^nX_{t_{i-1}}\\cdot\\mathbf{1}\\{t_{i-1}\\leq t&lt;t_i\\}$&emsp;then we have&emsp;$X_t^n\\xrightarrow[]{L^2_{ad}} X_t \\Longleftrightarrow \\int_a^b\\mathbb{E}\\left(X_t^n-X_t\\right)^2dt\\xrightarrow[n\\rightarrow\\infty]{}0$[Proof]:&emsp;首先證明 $X_t$ is continuous in mean square sense, i.e.&emsp;$\\mathbb{E}(X_t-X_s)^2\\xrightarrow[s\\rightarrow t]{}0$&emsp;展開來並全部用 $m(t),K(t,s)$ 替換:&emsp;$$\\mathbb{E}(X_t-X_s)^2 = \\mathbb{E}X_t^2 - 2\\mathbb{E}X_tX_s + \\mathbb{E}X_s^2 \\\\ = (K(t,t)+m^2(t))-2(K(t,s)+m(t)m(s))+(K(s,s)+m^2(s)) \\\\ \\xrightarrow[s\\rightarrow t]{}0 \\Longrightarrow X_t^n\\xrightarrow[n\\rightarrow\\infty]{m.sq.}X_t \\text{ (for a given }t) \\\\ \\text{i.e. }\\lim_{n\\rightarrow\\infty}\\mathbb{E}\\left(X_t^n-X_t\\right)^2=0$$&emsp;其中 $X_t^n$ converges to $X_t$ in m.sq. sense 可以參考 link&emsp;所以&emsp;$$\\lim_{n\\rightarrow\\infty}\\int_a^b\\mathbb{E}\\left(X_t^n-X_t\\right)^2dt \\\\ (\\text{by Lebesgue&apos;s dominated convergence theorem})\\\\ = \\int_a^b \\lim_{n\\rightarrow\\infty}\\mathbb{E}\\left(X_t^n-X_t\\right)^2dt = 0\\ldots(\\star)$$&emsp;參考 Lebesgue’s dominated convergence theorem&emsp;If $\\exists M(t)$ such that&emsp;$$|f(n,t)|\\leq M(t),\\forall n,t \\\\ \\int M(t)dt&lt;\\infty$$&emsp;then&emsp;$\\int\\lim_{n\\rightarrow\\infty} f(n,t)dt = \\lim_{n\\rightarrow\\infty}\\int f(n,t)dt$&emsp;由於有以下關係&emsp;$$\\mathbb{E}(X_t^n-X_t)^2 \\leq 2\\mathbb{E}(X_t^n)^2 + 2\\mathbb{E}(X_t)^2 \\\\ \\leq 4\\max_{t\\in[a,b]}\\mathbb{E}X_t^2=4\\max_{t\\in[a,b]}(K(t,t)+m^2(t))$$&emsp;由於 $m,K$ continuous 所以該 maximum 存在是 finite, 因此我們找到了 bounded function $M(t)$&emsp;所以 limit and integral 可互換.&emsp;結果 $(\\star)$ 成立, Q.E.D. [Quadratic Variation of a Brownian Motion]:&emsp;Assume $W_t$ is a Brownian motion. The interval from $0$ to $t$ is divided into $n$ parts by points $0=t_0,t_1,…,t_n=t$. Prove&emsp;$t=\\lim_{n\\rightarrow\\infty}\\sum_{i=1}^n (W_{t_i}-W_{t_{i-1}})^2$[Sol]: [Example]: Quiz 第七題有計算這個 stochastic integral 產生的 process 之 mean and variance 這段課程的結語, 使用定義的方式來計算出 Wiener integral of process in $L_{ad}^2$ 很多時候仍然很困難但接下來要介紹的 Ito formula 可以幫助我們計算大部分的情況 Week 7.6: Integrals of the type $∫ X_t dY_t$, where $Y_t$ is an Itô process[Ito Process Def]:&emsp;Let $\\mathcal{F}_t$ is a filtration, $b_t,\\sigma_t$ are processes $\\mathcal{F}_t$-adapted, $W_t$ is $\\mathcal{F}_t$ Brownian motion&emsp;and $H_0$ is a random variable which is a measurable function w.r.t. $\\mathcal{F}_0$&emsp;$H_t$ is a Ito process:&emsp;$H_t=H_0+\\int_0^t b_sds + \\int_0^t\\sigma_sdW_s$&emsp;or equivalently&emsp;$dH_t=b_tdt+\\sigma_tdW_t$ $H_t$ 的變化 (也還是一個 random process) 等於 $b_t$ 乘上時間的變化 $dt$, 再加上 $\\sigma_t$ 乘上 Brownian motion 的變化 $dW_t$ [Stochastic Integral with Ito Process]:&emsp;Let $\\mathcal{F}_t$ be some filtration, $X_t$ is $\\mathcal{F}_t$-adapted and $H_t$ is a Ito process which is also $\\mathcal{F}_t$-adapted.&emsp;If $X_t$ fulfilled&emsp;$\\int_a^b|X_sb_s|+X_s^2\\sigma_s^2ds&lt;\\infty$&emsp;then the integral is defined as&emsp;$\\int_a^b X_tdH_t:=\\int_a^bb_sX_xds + \\int_a^b\\sigma_sX_sdW_s$ r.h.s. 的第一項和第二項 integrals 之前都討論過 [Ito Lemma or Ito Formula]:&emsp;Let $H_t$ is a Ito process, i.e.&emsp;$dH_t=b_tdt+\\sigma_tdW_t$&emsp;where $b_t,\\sigma_t$ are processes $\\mathcal{F}_t$-adapted, $W_t$ is $\\mathcal{F}_t$ Brownian motion.&emsp;If $f(t,x)$ is twice continuously differentiable, then $f(t,H_t)$ be a Ito process with the following equation:&emsp;$$\\color{orange}{ f(t,H_t)=f(0,H_0)+\\int_0^tf_1&apos;(s,H_s)ds + \\int_0^tf_2&apos;(s,H_s)dH_s + \\frac{1}{2}\\int_0^tf_{22}&apos;&apos;(s,H_s)\\sigma_s^2ds }$$&emsp;where&emsp;$$f_1&apos;(t,x)=\\frac{\\partial}{\\partial t}f(t,x) \\\\ f_2&apos;(t,x)=\\frac{\\partial}{\\partial x}f(t,x) \\\\ f_{22}&apos;&apos;(t,x)=\\frac{\\partial^2}{\\partial t\\partial t}f(t,x)$$&emsp;or equivalently,&emsp;$$\\color{orange}{ df(t,H_t)=f_1&apos;(t,H_t)dt + f_2&apos;(t,H_t)dH_t + \\frac{1}{2}f_{22}&apos;&apos;(t,H_t)\\sigma_t^2dt }$$ Week 7.7: Itô’s formula若 $H_t$ 為 Ito-process, i.e.: $b_t,\\sigma_t$ are processes $\\mathcal{F}_t$-adapted, $W_t$ is $\\mathcal{F}_t$ Brownian motion.$dH_t=b_tdt+\\sigma_tdW_t$ If $f(t,x)$ is twice continuously differentiable, then $f(t,H_t)$ be a Ito process with the following equation:$$f(t,H_t)\\\\ =f(0,H_0)+\\int_0^tf_1&apos;(s,H_s)ds + \\int_0^tf_2&apos;(s,H_s)dH_s + \\frac{1}{2}\\int_0^tf_{22}&apos;&apos;(s,H_s)\\sigma_s^2ds$$ Ito formula 可以用來幫助計算以下形式的積分, 假設 $g(t,x)$ is twice continuously differentiable:$\\int_0^t g(s,W_s)dW_s$ 對照 Ito formula 我們在 Week 7.5 有利用 Wiener process Integral 的定義來計算過 $\\int W_sdW_s$, 此時 $g(s,W_s)=W_s$, 但如果 $g(.,.)$ 複雜一點就會很難計算. 不過我們可以借用 Ito formula 來幫忙 我們令 $f$-antiderivative of $g$ w.r.t. 2nd-argument, i.e., $f_2’=g$, 注意到 $f(t,x)$ 如果加上 $h(t)$ 也滿足 $f_2’=g$, 且不影響以下推導, 可自行帶入多了一個 $h(t)$ 會發現不影響結果由於 Brownian motion 也是 Ito process, 所以此時 $H_t=W_t$ and $\\sigma_s=1$, 重新改寫 Ito formula 變成:$f(t,W_t)=f(0,0)+\\int_0^tf_1&apos;(s,W_s)ds + \\int_0^t g(s,W_s)dW_s + \\frac{1}{2}\\int_0^t g_2&apos;(s,W_s)ds$ 移項一下:$$\\color{orange}{ \\int_0^t g(s,W_s)dW_s=f(t,W_t)-f(0,0) - \\int_0^t\\left( f_1&apos;(s,W_s)+\\frac{1}{2}g_2&apos;(s,W_s) \\right)ds }\\ldots(\\square)$$ [Example]:&emsp;計算 $\\int_0^t W_sdW_s$[Sol]:&emsp;$g(t,x)=x$, 所以 $f(t,x)=\\frac{1}{2}x^2$. 利用 $(\\square)$ 則&emsp;$\\int_0^t W_sdW_s = \\frac{1}{2}W_t^2-\\int_0^t\\frac{1}{2}ds=\\frac{1}{2}W_t^2-\\frac{1}{2}t$ [Example]:&emsp;計算 $\\int_0^t \\frac{W_s}{1+W_s^2}dW_s$[Sol]: Week 7.8: Calculation of stochastic integrals using the Itô formula. Black-Scholes model重複一次 Ito formula (in derivative form)$df(t,H_t)=f_1&apos;(t,H_t)dt + f_2&apos;(t,H_t)dH_t + \\frac{1}{2}f_{22}&apos;&apos;(t,H_t)\\sigma_t^2dt$其中 $H_t$ 為 Ito-process$dH_t=b_tdt+\\sigma_tdW_t$ [Black-Scholes model Def]:&emsp;$X_t$ 為以下 Stochastic Differential Equation (SDE) 的解:&emsp;$dX_t = X_t\\mu dt+X_t\\sigma dW_t$&emsp;where $\\mu,\\sigma\\in\\mathbb{R},\\sigma&gt;0$, $W_t$ is $\\mathcal{F}_t$ Brownian motion. 對比 Ito-process 的定義會發現 $X_t$ 是 Ito-process &emsp;上式等同於&emsp;$X_t=X_0+\\mu\\int_0^t X_sds+\\sigma\\int_0^tX_sdW_s$&emsp;證明其解為:&emsp;$X_t=X_0\\cdot\\exp\\left\\{(\\mu-\\frac{\\sigma^2}{2})t+\\sigma W_t\\right\\}$[Proof]:&emsp;Let $f(t,x)=\\ln x$, $H_t=X_t$ 並代入 Ito formula: 課程教授說這常用在 modeling stock prices Week 7.9: Vasicek model. Application of the Itô formula to stochastic modelling重複一次 Ito formula (in derivative)$df(t,H_t)=f_1&apos;(t,H_t)dt + f_2&apos;(t,H_t)dH_t + \\frac{1}{2}f_{22}&apos;&apos;(t,H_t)\\sigma_t^2dt$其中 $H_t$ 為 Ito-process$dH_t=b_tdt+\\sigma_tdW_t$ [Vasicek model Def]:&emsp;$X_t$ 為以下 Stochastic Differential Equation (SDE) 的解:&emsp;$dX_t=(a-bX_t)dt+cdW_t,\\qquad b,c&gt;0$ 對比 Ito-process 的定義會發現 $X_t$ 是 Ito-process &emsp;證明其解為:&emsp;$X_t=e^{-bt}X_0 + \\frac{a}{b}(1-e^{-bt})+c\\int_0^te^{b(s-t)}dW_s$[Proof]:&emsp;使用 Ito formula, 並定義 $H_t=X_t$, and $f(t,x)=xe^{bt}$&emsp;得到&emsp;$$d\\left(X_te^{bt}\\right)=bX_te^{bt}dt+e^{bt}\\left((a-bX_t)dt+cdW_t\\right) \\\\ = ae^{bt}dt+ce^{bt}dW_t$$&emsp;所以&emsp;$X_t=e^{-bt}X_0 + \\frac{a}{b}(1-e^{-bt})+c\\int_0^te^{b(s-t)}dW_s$ SDE 改寫成$dX_t=b\\left(\\frac{a}{b}-X_t\\right)dt+cdW_t,\\qquad b,c&gt;0$ 如果 $X_t&gt;a/b$, $dX_t&lt;0$, 也就是說 $X_t$ 的斜率為負, $X_t$ 會變小. 反之斜率為正, 值變大. 所以觀察 trajectory 為下圖: $b$ 稱為 speed of reversion Week 7.10: Ornstein-Uhlenbeck process. Application of the Itô formula to stochastic modelling.繼續重複 Ito formula (in derivative)$df(t,H_t)=f_1&apos;(t,H_t)dt + f_2&apos;(t,H_t)dH_t + \\frac{1}{2}f_{22}&apos;&apos;(t,H_t)\\sigma_t^2dt$其中 $H_t$ 為 Ito-process$dH_t=b_tdt+\\sigma_tdW_t$ [Ornstein-Uhlenbeck process Def]:&emsp;$V_t$ 為以下 Stochastic Differential Equation (SDE) 的解:&emsp;$mdV_t=dW_t-\\lambda\\cdot V_tdt$&emsp;where $m$ is mass (質量), $V_t$ 表示速度, $W_t$ is Brownian motion. $\\lambda$ is called friction coefficient&emsp;證明其解為:&emsp;$$V_t=e^{-\\frac{\\lambda}{m}t} \\left( V_0+\\frac{1}{m}\\int_0^t e^{\\frac{\\lambda}{m}s}dW_s \\right)$$[Proof]:&emsp;使用 Ito formula, 並定義 $H_t=X_t$, and&emsp;$f(t,x)=xe^{\\frac{\\lambda}{m}t}$&emsp;則解為&emsp;$$V_t=e^{-\\frac{\\lambda}{m}t} \\left( V_0+\\frac{1}{m}\\int_0^t e^{\\frac{\\lambda}{m}s}dW_s \\right)$$ 如果$V_0\\sim\\mathcal{N}\\left(0,\\frac{1}{2\\lambda m}\\right) \\perp W_t$then $V_t$ is a Gaussian process with $m(t)=0$$K(t,s)=\\frac{m}{2\\lambda}e^{-\\frac{\\lambda}{m}|t-s|}$且看的出只與 $t-s$ 有關, 因此 auto-covariance 存在 $\\gamma(t-s)=K(t,s)$因此是 WSS, 加上是 Gaussian process, 所以也是 strictly stationary","tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://bobondemon.github.io/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"https://bobondemon.github.io/tags/Stochastic-Processes/"},{"name":"Stochastic Integration","slug":"Stochastic-Integration","permalink":"https://bobondemon.github.io/tags/Stochastic-Integration/"},{"name":"Itô formula","slug":"Ito-formula","permalink":"https://bobondemon.github.io/tags/Ito-formula/"}]},{"title":"Stochastic Processes Week 6 Ergodicity, differentiability, continuity","date":"2021-12-12T12:16:36.000Z","path":"2021/12/12/Stochastic-Processes-Week-6-Ergodicity-differentiability-continuity/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity (本文) Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes Self Study: Convergence of random variables主要參考 wikipedia 的資料筆記 Convergence of random variables[Converge in distribution Def]:&emsp;記做 $X_n \\xrightarrow[]{d}X$ Converge in distribution 是最 weak 的, 也就是滿足 converge in distribution 不一定會滿足 converge in probability 或滿足 almost surely 或滿足 converge in mean[機率論] 兩隨機變數相等表示兩者有相同分布但反之不然, 這篇文章最後給了一個例子:考慮均勻分布 $X$ 為隨機變數服從均勻分布 $\\mathcal{U}[-1,1]$ 現在取另一隨機變數 $Y:=-X$ 則 $Y$ 亦為在 $[-1,1]$ 上均勻分布，亦即 $X$ 與 $Y$ 具有同分布。然而$\\mathcal{P}(X=Y)=0$ Converge in distribution 的其他等價定義 [Converge in probability Def]:&emsp;記做 $X_n \\xrightarrow[]{p}X$ [Properties]: 自己的想法: $X_n$ 與 $X$ 的 sample spaces 可以不同, 只要 mapping 到 $\\mathbb{R}$ 之後相減就好, 所以考量的是 joint distributionFor a random process $X_t$ converges to a constant in probability sense$X_t\\xrightarrow[t\\rightarrow\\infty]{p} c \\text{ (is const.)}$則表示一定會 (必要條件)$$\\mathbb{E}X_t\\xrightarrow[t\\rightarrow\\infty]{}c \\\\ Var(X_t)\\xrightarrow[t\\rightarrow\\infty]{}0$$ [Almost sure convergence Def]:&emsp;記做 $X_n \\xrightarrow[]{a.s.} X$ 可參考: 謝宗翰的隨筆 [機率論] Almost Sure Convergence 自己的想法: 需要 sample space 一樣, 且都是對個別 outcome 去比較的. 把所有這些符合的 outcomes 蒐集起來的集合, 其 probability measure 為 $1$.這是一個很強的條件, 幾乎針對每一個 outcome 都要符合. [Convergence in mean Def]:&emsp;記做 $X_n \\xrightarrow[]{L^r} X$. Convergence in the $r$-th mean, for $r\\geq1$, implies convergence in probability (by Markov’s inequality). Furthermore, if $r&gt;s\\geq1$, convergence in $r$-th mean implies convergence in $s$-th mean. Hence, convergence in mean square implies convergence in mean.It is also worth noticing that if $$X_n\\xrightarrow[]{L^r}X$$ then $$\\lim_{n\\rightarrow\\infty}\\mathbb{E}[|X_n|^r]=\\mathbb{E}[|X|^r]$$ [Relation btw Stochastic Convergences]: 各種 convergence 的 proofs:https://en.wikipedia.org/wiki/Proofs_of_convergence_of_random_variables#propA2 補上課程針對以上四個 convergences 的定義 Week 6.1: Notion of ergodicity. ExamplesMotivated by LLN (Law of Large Number)[LLN Thm]:&emsp;$\\xi_1,\\xi_2,…$ - i.i.d. and $\\mathbb{E}\\xi_1&lt;\\infty$, then&emsp;$\\frac{1}{N}\\sum_{n=1}^N \\xi_n \\xrightarrow[N\\rightarrow\\infty]{p} \\mathbb{E}\\xi_1$&emsp;上式 $\\xrightarrow[]{p}$ 表示 convergence in probability&emsp;但也滿足 $\\xrightarrow[]{a.s.}$ almost sure convergence (Strong LLN) Ergodicity 嘗試將 LLN 概念延伸到 stochastic process [Ergodicity Def]:&emsp;$X_t$ is a stochastic process, where $t=1,2,3,…$&emsp;$X_t$ is said ergodic if $\\exists c$ constant such that&emsp;$$M_T:=\\frac{1}{T}\\sum_{t=1}^T X_t \\xrightarrow[T\\rightarrow\\infty]{p} c$$&emsp;where $c$ is some constant. $T$ is called horizon.&emsp;And we consider convergence in probability sense. 所以要證明 ergodic 可以 $\\xrightarrow[]{p}$, or $\\xrightarrow[]{a.s.}$, or $\\xrightarrow[]{m.sq.}$, or $\\xrightarrow[]{d}c$ [Example 1]:&emsp;$X_t=\\xi\\sim\\mathcal{N}(0,1)$, trajectory 為 constant for all $t$&emsp;$m(t)=0,K(t,s)=Var\\xi=1$&emsp;所以是 weak stationary, 我們考慮 ergodicity&emsp;$\\frac{1}{T}\\sum_{t=1}^T X_t=\\xi\\neq c$&emsp;不存在一個 constant for $T\\rightarrow \\infty$, 所以 non-ergodic [Example 2]:&emsp;$X_t$ stochastic process defined as:&emsp;$X_t=\\varepsilon_t+a\\cos\\frac{\\pi t}{6}$&emsp;where $a\\neq0$ and $\\varepsilon_1, \\varepsilon_2, …$ i.i.d. $\\mathcal{N}(0,1)$ 其 trajectory 為下圖曲線, 並對該曲線每個位置都有 std normal noise$m(t)=a\\cos\\frac{\\pi t}{6}\\neq const$, 所以不是 stationary. 考慮 ergodicity:$\\frac{1}{T}\\sum_{t=1}^T X_t\\sim\\mathcal{N}\\left(\\frac{a}{T}\\sum_{t=1}^T \\cos\\frac{\\pi t}{6},\\frac{1}{T}\\right)$互相獨立之 normal distributions 相加仍為 normal, mean and variance 都為相加variance 收斂到 $0$, 觀察 mean:由於 trajectory 是以 12 為一個週期, 所以最多只會有 12 個非 0 的值, 而每一個都小於等於 1結論是 mean 也收斂到 $0$ (因為不管哪一個 outcome, 其 trajectory 最後都到 $0$, 所以收斂的 random variable 為 contant $0$)所以$\\frac{1}{T}\\sum_{t=1}^T X_t\\sim\\mathcal{N}\\left(0,0\\right) \\text{ for }T\\rightarrow\\infty$所以 $=0$ a.s. 因此是 ergodicExample 1 是 (weak) stationary but non-ergodicExample 2 是 non-stationary but erogodic因此 stationary 跟 ergodic 是不同概念 Week 6.2: Ergodicity of wide-sense stationary processes[Proposition]:For $X_t$ is a discrete time stochastic process. Define&emsp;$$M_T=\\frac{1}{T}\\sum_{t=1}^TX_t\\\\ C(T)=Cov(X_T,M_T)$$&emsp;If $\\exists\\alpha$ such that the covariance function is bounded by $\\alpha$, i.e.&emsp;$|K(s,t)|\\leq\\alpha;\\forall s,t$&emsp;Then&emsp;$$Var(M_T)\\xrightarrow[T\\rightarrow\\infty]{}0 \\Longleftrightarrow C(T)\\xrightarrow[T\\rightarrow\\infty]{}0$$ [Stolz-Cesaro Thm]:&emsp; For $a_n,b_n\\in\\mathbb{R}$, $b_n$ is strictly increasing and unbounded, we have:&emsp;$\\lim_{n\\rightarrow\\infty}\\frac{a_n-a_{n-1}}{b_n-b_{n-1}}=q\\Longrightarrow \\frac{a_n}{b_n}\\xrightarrow[n\\rightarrow\\infty]{}q$ See wiki for more detials and proof 當 stationary 時, 有兩個 sufficient conditions 滿足則保證 ergodic[Sufficient Conditions of Ergodicity when WSS]:&emsp;$X_t$ is weakly stationary and $\\gamma(\\cdot)$ is its auto-covariance function with $|\\gamma(\\cdot)|&lt;\\infty$.&emsp;1. Sufficient condition: If&emsp;&emsp;$\\frac{1}{T}\\sum_{r=0}^{T-1}\\gamma(r) \\xrightarrow[T\\rightarrow\\infty]{}0$&emsp;&emsp;Then $X_t$ is ergodic&emsp;2. Sufficient condition: If&emsp;&emsp;$\\gamma(r) \\xrightarrow[r\\rightarrow\\infty]{}0$&emsp;&emsp;Then $X_t$ is ergodic[Proof 1]:&emsp;考慮 $C(T)$&emsp;$$C(T)=Cov\\left(X_T,\\frac{1}{T}\\sum_{t=1}^TX_t\\right) =\\frac{1}{T}\\sum_{t=1}^T Cov(X_T,X_t) \\\\ =\\frac{1}{T}\\sum_{t=1}^T \\gamma(T-t) =\\frac{1}{T}\\sum_{r=0}^{T-1}\\gamma(r) \\longrightarrow0\\text{ (by assumption)}$$&emsp;由 proposition 知道 $Var(M_T)\\rightarrow0$, as $T\\rightarrow\\infty$&emsp;又因為已知 $X_t$ is weakly stationary, 所以 $\\exists c$ constant, s.t.&emsp;$\\mathbb{E}X_t=c\\Rightarrow\\mathbb{E}M_t=c$&emsp;則 variance converge to $0$ as $T\\rightarrow\\infty$ 表示&emsp;$VarM_T=\\mathbb{E}\\left[(M_T-c)^2\\right]\\xrightarrow[T\\rightarrow\\infty]{}0$&emsp;則&emsp;$M_T \\xrightarrow[]{L^2} c \\Longrightarrow M_T \\xrightarrow[]{p} c$&emsp;根據定義 $X_t$ is ergodic. Q.E.D. [Proof 2]:&emsp;我們利用 proof 1 的結果和 Stolz-Cesaro thm, 定義&emsp;$$a_n:=\\sum_{r=0}^{n-1}\\gamma(r)\\\\ b_n:=n$$&emsp;則&emsp;$$\\frac{a_n-a_{n-1}}{b_n-b_{n-1}}=\\frac{\\gamma(n-1)}{1}\\xrightarrow[n\\rightarrow\\infty]{}0=q \\\\ \\Longrightarrow \\frac{a_n}{b_n} = \\frac{1}{n}\\sum_{r=0}^{n-1}\\gamma(r)\\xrightarrow[n\\rightarrow\\infty]{}0=q \\\\ \\Longrightarrow X_t \\text{ ergodic}$$ [Example 1]:&emsp;$N_t$ is Poisson process with $\\lambda$. 給定 $p&gt;0$, 並定義&emsp;$X_t:=N_{t+p}-N_t$&emsp;則我們可得知 $X_t$ is ergodic You can use the independent increment property of Poisson Process to get this formula.$\\gamma(t-s) = Cov(N_{t+p}-N_t,N_{s+p}-N_s)$If $|s-t|&gt;p$, then $N_{t+p}-N_t$ is independent of $N_{s+p}-N_s$, so $\\gamma(t-s)=0$If $t&lt;s&lt;t+p$, then $N_t$ is independent of $N_{s+p}-N_s$, so$$Cov(N_{t+p}-N_t,N_{s+p}-N_s) = Cov(N_{t+p},N_{s+p}-N_s) \\\\ = Cov(N_{t+p},N_{s+p}) - Cov(N_{t+p},N_s) \\\\ = Cov(N_{t+p},N_{s+p} - N_{t+p} + N_{t+p}) - Cov(N_{t+p} - N_s + N_s,N_s) \\\\ = Var(N_{t+p}) - Var(N_s) = \\lambda(t+p-s)$$For $s&lt;t&lt;s+p$. you can deal with it similarly. [Example 2]:&emsp;$A,B$ 都是 r.v.s 有如下圖的 expectation and uncorrelated relation&emsp;$\\omega$ 是任意給定的 fixed value&emsp;則我們發現 $X_t$ is weakly stationary and ergodic Week 6.3: Definition of a stochastic derivative[Stochastic Derivative Def]:&emsp;We say that a random process $X_t$ is differentiable at $t=t_0$&emsp;if the following limit converges in m.sq. sense to some random variable $\\eta$&emsp;$\\frac{X_{t+h}-X_t}{h} \\xrightarrow[h\\rightarrow0]{L^2}\\eta:=X_{t_0}&apos;$&emsp;equivalently we can write as follows&emsp;$\\mathbb{E}\\left[\\left(\\frac{X_{t+h}-X_t}{h}-\\eta\\right)^2\\right] \\xrightarrow[h\\rightarrow0]{}0$ [Proposition, Differentiability of Stochastic Process]:&emsp;Let $X_t$ is a stochastic process and $\\mathbb{E}X_t^2&lt;\\infty$.&emsp;Then $X_t$ is differentiable at $t=t_0$ if and only if&emsp;$$\\exists\\frac{dm(t)}{dt}\\text{, at }t=t_0 \\\\ \\exists\\frac{\\partial}{\\partial t\\partial s}K(t,s)\\text{, at }(t_0,t_0)$$ [Brownian Motion is NOT differentiable at any time $t$]: [Differentiability of Independent Increments]:&emsp;$X_t$ is independent increments and $X_0=0$ a.s. 則我們知道&emsp;$K(t,s)=Var(X_{\\min(t,s)})$&emsp;因此大部分的 stochastic process with independent increments 都不是可微的.&emsp;我們寫一下 covariance function 的推導:&emsp;$$K(t,s)=Cov(X_t,X_s) \\\\ \\text{(for t&gt;s) }=Cov(X_t-X_s,X_s-X_0)+Cov(X_s,X_s) \\\\ = 0+Var(X_s)\\\\ \\text{(consider t&gt;s and t&lt;s) }= Var(X_{\\min(t,s)})$$ [Differentiability of Weakly Stationary]:&emsp;$X_t$ is weakly stationary. 所以 $m(t)$ is constant $\\Rightarrow$ differentiable at all $t$.&emsp;我們知道 $K(t,s)=\\gamma(t-s)$, 計算 partial derivatives:&emsp;$$\\left.\\frac{\\partial^2K(t,s)}{\\partial t\\partial s} \\right\\vert_{(t_0,t_0)} = \\frac{\\partial^2 \\gamma(t-s) }{\\partial t\\partial s} \\\\ = \\left.\\frac{\\partial}{\\partial t} \\left(-\\gamma&apos;(t-s)\\right)\\right|_{(t_0,t_0)} = \\left.-\\gamma&apos;&apos;(t-s)\\right|_{(t_0,t_0)} \\\\ = -\\gamma&apos;&apos;(0)$$&emsp;所以 $X_t$ is differentiable (at any time $t$) if and only if $-\\gamma’’(0)$ 存在 [Example 1]:&emsp;承上 weakly stationary differentiable. 如果 $\\gamma(r)=e^{-\\alpha|r|}$&emsp;則 $X_t$ is not differentiable, 因為 $-\\gamma’’(0)$ 不存在 ($\\gamma’(0)$ 就不存在了). $\\gamma(t)$ 如下: [Example 2]:&emsp;承上 weakly stationary differentiable. 如果 $\\gamma(r)=\\cos(\\alpha r)$&emsp;則 $X_t$ is differentiable Week 6.4: Continuity in the mean-squared sense[Continuity in the probability sense Def]:&emsp;$X_t\\xrightarrow[t\\rightarrow t_0]{p}X_{t_0} \\Longleftrightarrow \\mathcal{P}(|X_t-X_{t_0}|&gt;\\varepsilon)\\xrightarrow[t\\rightarrow t_0]{}0, \\qquad \\forall\\varepsilon&gt;0$ Converges in mean-squared sense 確保了 converges in probability, 且 in m.sq. sense 比較容易確認, 因此以下著重在 in m.sq. sense [Continuity in the mean-squared sense Def]:&emsp;$X_t\\xrightarrow[t\\rightarrow t_0]{L^2}X_{t_0} \\Longleftrightarrow \\mathbb{E}(X_t-X_{t_0})^2\\xrightarrow[t\\rightarrow t_0]{}0$ [Proposition, Continuity of Stochastic Process]:&emsp;$X_t$ stochastic process and $\\mathbb{E}X_t=0$&emsp;1. If $K(t,s)$ is continuous at $(t_0, t_0)$, then $X_t$ is continuous in the m.sq. sense at $t=t_0$&emsp;2. If $X_t$ is continuous in the m.sq. sense at $t=t_0,s_0$, then $K(t,s)$ is continuous at $(t_0,s_0)$ [Proof 1]:&emsp;proved by definition of m.sq. sense&emsp;$$\\mathbb{E}(X_t-X_{t_0})^2 = \\mathbb{E}X_t^2-2\\mathbb{E}X_tX_{t_0} + \\mathbb{E}X_{t_0}^2 \\\\ =K(t,t)-2K(t,t_0)+K(t_0,t_0)\\xrightarrow[t\\rightarrow t_0]{}0$$[Proof 2]:&emsp;$$K(t,s)-K(t_0,s_0) \\\\ = (K(t,s)-K(t_0,s)) + (K(t_0,s) - K(t_0,s_0))$$&emsp;使用 $\\mathbb{E}X_t=0$ and 科西不等式, 考慮第一項 $K(t,s)-K(t_0,s)$&emsp;$$|K(t,s)-K(t_0,s)|=|\\mathbb{E}[(X_t-X_{t_0})X_s]| \\\\ \\leq \\sqrt{\\mathbb{E}(X_t-X_{t_0})^2}\\cdot\\sqrt{\\mathbb{E}X_s^2}\\\\ \\text{by assumption }\\xrightarrow[t\\rightarrow t_0]{}0$$&emsp;第二項 $K(t_0,s) - K(t_0,s_0)$ 也一樣. Q.E.D. Differentiability 要關注 $m(t)$ and $K(t,s)$ 的微分性, 而 continuity 只關注 $K(t,s)$ 的連續性另外對於 simplest type of stochastic integral $\\int X_tdx$ 來說, 關注的是 $m(t)$ and $K(t,s)$ 的連續性 [Corollary]:&emsp;Covariance function $K(t,s)$ is continuous at $(t_0,s_0),\\forall t_0,s_0$ if and only if&emsp;$K(t,s)$ is continuous at diagonal, i.e. $(t_0,t_0),\\forall t_0$ [Proof]:&emsp;只需證明 $\\Longleftarrow$&emsp;$K(t,s)$ is continuous at $(t_0,t_0)$, $\\forall t_0$. 由 proposition 1 知道 $X_t$ is continuous in m.sq. sense $\\forall t$&emsp;因此 $X_t$ is continuous in the m.sq. sense at $t=t_0,s_0$ for all points&emsp;由 proposition 2 知道 $K(t,s)$ is continuous at $(t_0,s_0),\\forall t_0,s_0$&emsp;Q.E.D.","tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://bobondemon.github.io/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"https://bobondemon.github.io/tags/Stochastic-Processes/"},{"name":"Stochastic Convergences","slug":"Stochastic-Convergences","permalink":"https://bobondemon.github.io/tags/Stochastic-Convergences/"},{"name":"Ergodicity","slug":"Ergodicity","permalink":"https://bobondemon.github.io/tags/Ergodicity/"},{"name":"Continuity","slug":"Continuity","permalink":"https://bobondemon.github.io/tags/Continuity/"}]},{"title":"Stochastic Processes Week 5 Stationarity and Linear filters","date":"2021-12-12T11:11:51.000Z","path":"2021/12/12/Stochastic-Processes-Week-5-Stationarity-and-Linear-filters/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters (本文) Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes Week 5.1-2: Two types of stationarity[Strictly Stationary Def]:&emsp;$X_t$ is (strictly) stationary if $\\forall(t_1,...,t_n)\\in\\mathbb{R}_+^n,\\forall h&gt;0$&emsp;$(X_{t_1+h},...,X_{t_n+h})=^d (X_{t_1},...,X_{t_n})$ Finite dimenstional distributions are invariant in shift in time [Weakly Stationary Def]:&emsp;$X_t$ is (weakly) stationary, if $\\forall t,s\\in \\mathbb{R}_+,\\forall h&gt;0$&emsp;$$m(t)=\\mathbb{E}X_t=const \\\\ K(t,s)=Cov(X_t,X_s)=K(t+h,s+h)$$&emsp;存在 function $\\gamma:\\mathbb{R}\\rightarrow\\mathbb{R}$ (auto-covariance function) such that&emsp;$K(t,s)=\\gamma(t-s)$ Shift in time 不變的只有 mean 和 covarianceWeak stationarity 也稱 second order stationarity 或 wide sense stationarity (WSS) [Properties of Auto-covariance $\\gamma(\\cdot)$]:&emsp;1. $\\gamma(0)\\geq0$&emsp;&emsp;$\\gamma(0)=Cov(X_t,X_t)=Var(X_t)\\geq0,\\forall t\\geq0$&emsp;2. $|\\gamma(t)|\\leq\\gamma(0)$&emsp;&emsp;$$|\\gamma(t)|=|Cov(X_t,X_0)|\\leq\\sqrt{Var(X_t)}\\sqrt{Var(X_0)} \\\\ = \\sqrt{Cov(X_t,X_t)}\\sqrt{Cov(X_0,X_0)} \\\\ = \\sqrt{\\gamma(t-t)}\\sqrt{\\gamma(0-0)}=\\gamma(0)$$&emsp;3. $\\gamma$ is even&emsp;&emsp;$\\gamma(t)=Cov(X_t,X_0)=Cov(X_0,X_t)=\\gamma(-t)$ [Statements btw Strictly and Weakly Stationarity]:&emsp;1. We assume $\\mathbb{E}X_t^2&lt;\\infty$, then $X_t$ is strictly stationary $\\Longrightarrow$ $X_t$ is weakly stationary&emsp;2. $X_t$ is Gaussian process, then $X_t$ is strictly stationary $\\Longleftrightarrow$ $X_t$ is weakly stationary [Stationary of White Noise Process? Is Weak]:&emsp;$X_t,t=\\pm1,\\pm2,…$ is called white noise process, $WN(0,\\sigma^2)$, if&emsp;$$\\mathbb{E}X_t=0, Var(X_t)=\\sigma^2 \\\\ Cov(X_t,X_s)=0,\\forall t\\neq s$$&emsp;i.e.&emsp;$m(t)=0,\\gamma(0)=\\sigma^2,\\gamma(t)=0,\\forall t&gt;0$&emsp;or&emsp;$$\\mathbb{E}X_t=0\\\\ K(t,s)=\\sigma^2\\mathbf{1}\\{t=s\\}=\\gamma(t-s) \\\\ \\therefore\\gamma(x)=\\sigma^2\\mathbf{1}\\{x=0\\}$$&emsp;以上都是相同意思 $WN(0,\\sigma^2)$ 可以看出是 weakly stationary, 通常情況下不一定是 strictly stationary, 因為根據定義只看 mean and covariance. 但有些情況會是 strict, 如: $X_1,X_2,…$ are i.i.d. noise $X_t$ is a Gaussian process (因為此時 strictly if and only if weakly) [Stationary of Random Walk Process? Not Weak/Strict]:&emsp;$S_n=S_{n-1}+\\xi_n$ where $\\xi_1,\\xi_2,...$ are i.i.d. with&emsp;$$\\xi=\\left\\{ \\begin{array}{rl} 1, &amp; p \\\\ -1, &amp; 1-p \\end{array} \\right.$$&emsp;Also assume $S_0=0$&emsp;知道 $S_n=\\xi_1+…+\\xi_n$, 計算一下 expectation&emsp;$\\mathbb{E}S_n=n\\mathbb{E}\\xi_1=n(2p-1)$&emsp;所以如果 $p\\neq\\frac{1}{2}$, 則 $\\mathbb{E}S_n$ depends on $n$&emsp;所以 $S_n$ 不是 weakly stationary (因此也一定不是 strictly)&emsp;所以我們 focus 在 $p=\\frac{1}{2}$, 接著計算 covariance, for $n&gt;m$&emsp;$$K(n,m)=Cov(S_m+\\xi_{m+1}+...+\\xi_n, S_m) \\\\ = Cov(S_m,S_m)+Cov(\\xi_{m+1}+...+\\xi_n,S_m) \\\\ =m\\cdot Var(\\xi_1)+0=\\min\\{n,m\\}Var(\\xi_1)$$&emsp;所以 $K(n,m)$ 無法只用 $n-m$ 來表示, 因此 Random walk 不是 strictly/weakly stationary [Stationary of Brownian Motion? Not Weak/Strict]:&emsp;$\\mathbb{E}B_t=0$, and $Var(B_t)=t$. 且知道 $B_t-B_s\\sim\\mathcal{N}(0,t-s)$&emsp;$K(t,t)=Cov(B_t,B_t)=Var(B_t)=t$, $\\because B_t\\sim\\mathcal{N}(0,t)$&emsp;所以不存在 $\\gamma(t-s)=K(t,s)$, 因為我們看 $\\gamma(0)$ 就知道不固定, $\\gamma$ 不符合 function 定義&emsp;或可以從 $K(t+h,s+h)=?K(t,s)$ 觀察, 已知 Brownian motion $K(t,s)=\\min\\{t,s\\}$&emsp;則 $K(t+h,s+h)\\neq K(t,s)$&emsp;結論 Brownian motion is not weakly stationary (因此一定也不是 strictly) [Stationary of Moving Average Process? Is Weak]:&emsp;$X_t\\sim WN(0,\\sigma^2)$. Given $a_1,…,a_q\\in\\mathbb{R};a_0=1$. Moving average $Y_t$ defined as:&emsp;$Y_t=X_t+a_1X_{t-1}+...+a_qX_{t-q}$&emsp;用 $MA(q)$ 表示&emsp;$\\mathbb{E}Y_t=\\mathbb{E}[X_t]+a_1\\mathbb{E}[X_{t-1}]+...+a_q\\mathbb{E}[X_{t-q}]=0$&emsp;$$K(t,s)=Cov\\left(\\sum_{j=0}^qa_jX_{t-j},\\sum_{k=0}^qa_kX_{s-k}\\right) \\\\ = \\sum_{j=0}^q \\sum_{k=0}^q a_ja_kCov(X_{t-j},X_{s-k}) \\\\ = \\sum_{j=0}^q \\sum_{k=0}^q a_ja_k \\sigma^2\\mathbf{1}\\{t-s=j-k\\}$$&emsp;因此可以看出 $K(t,s)$ 只跟 $t-s$ 有關&emsp;MA(1):&emsp;所以是 Weakly stationary 這個 moving average 是 FIR (linear), input $X_t$ 是 white noise, 所以為 weakly stationary. 由 weakly stationary 經過 linear filter 其 output 仍為 weakly stationary 可得到結論. [Stationary of Autoregressive Model? Weak in Constraint]:&emsp;$\\xi_t\\sim WN(0,\\sigma^2)$, we say that $Y_t$ is autoregressive model, $AR(p)$, if:&emsp;$Y_t=b_1Y_{t-1}+...+b_pY_{t-p}+\\xi_t$&emsp;Also we assume $Cov(\\xi_t,Y_s)=0,\\forall t&gt;s$&emsp;考慮 $AR(1)$:&emsp;$$Y_t=bY_{t-1}+\\xi_t \\\\ =b(bY_{t-2}+\\xi_{t-1})+\\xi_t \\\\ = ... = \\sum_{j=0}^\\infty b^j\\xi_{t-j}$$&emsp;明顯可以看出 $\\mathbb{E}Y_t=0$, 接著計算 covariance:&emsp;$$K(t,s)=Cov(Y_t,Y_s)=\\sum_{j,k=0}^\\infty b^{j+k}\\cdot Cov(\\xi_{t-j},\\xi_{s-k}) \\\\ = \\sum_{j,k=0}^\\infty b^{j+k} \\cdot \\sigma^2\\cdot\\mathbf{1}\\{t-s=j-k\\}$$&emsp;看起來 covariance 只跟 $t-s$ 有關, 但我們計算一下 $\\gamma(0)=K(t,t)$:&emsp;$$t-s=0\\Rightarrow K(t,t)=\\sum_{k=0}^\\infty b^{2k}\\cdot \\sigma^2 \\\\ \\therefore K(t,t)&lt;\\infty\\Leftrightarrow |b|&lt;1$$&emsp;只有在 $|b|&lt;1$ 的情況 variance 才收斂, 因此 weakly stationary 才成立&emsp;考慮 $AR(p)$, 課程說 $Y_t$ 一樣可以寫出 $\\xi_t$ 的和, 找出所有根並在 norm&lt;1 才收斂 這就是 IIR 的 poles 要在單位圓內 &emsp;總之, autoregressive model 有條件地成為 weakly stationary Week 5.3-4: Spectral density of a wide-sense stationary process[Bochner–Khinchin Theorem]:&emsp;$\\Phi:\\mathbb{R}\\rightarrow\\mathbb{C}$, and $\\Phi(u)$ is a characteristic function, i.e.&emsp;$\\exists\\xi$ random variable such that $\\Phi(u)=\\mathbb{E}[e^{iu\\xi}]$&emsp;$\\Longleftrightarrow$&emsp;1. $\\Phi$ is continuous&emsp;2. $\\Phi$ is positive semi-definite&emsp;&emsp;$$\\forall(z_1,...,z_n)\\in\\mathbb{C}^n,\\forall(u_1,...,u_n)\\in\\mathbb{R}^n \\\\ \\sum_{j,k=1}^n z_j\\overline{z_k}\\Phi(u_j-u_k)\\geq0$$&emsp;3. $\\Phi(0)=1$ [Bochner–Khinchin Theorem Alternative 1]:&emsp;如果只滿足 1. and 2. 則可以得到 $\\exists\\mu$ is some measure such that&emsp;$\\Phi(u)=\\int e^{iux}\\mu(dx)$&emsp;$\\mu$ 可以不必是 probability measure (但我們知道一定會有個 measure 滿足上式) [Bochner–Khinchin Theorem Alternative 2]:&emsp;如果滿足 1. and 2. and 如下條件&emsp;$\\int|\\Phi(u)|du&lt;\\infty$&emsp;則 $\\Phi:\\mathbb{R}\\rightarrow\\mathbb{C}$, and $\\Phi(u)$ is a characteristic function&emsp;i.e. 某個 r.v. $\\xi$ 的 p.d.f. 為 $s(x)$ 的 Fourier transform&emsp;相比 alternative 1, 此時 measure $\\mu$ 有 density $s(x)$&emsp;$\\Phi(u)=\\int e^{iux}s(x)dx$ 💡 注意到課程的 Fourier transform 跟我們一般在訊號處理的有點不同, 課程的定義為:$\\mathcal{F}[g](u)=\\int e^{iux}g(x)dx$且課程的 spectral density 在訊號處理我們稱 “power” spectral density [Spectral Density of Weakly Stationary Def]:&emsp;Let $X_t$ is weakly stationary 且 $\\gamma:K(t,s)=\\gamma(t-s)$&emsp;If $\\gamma$ is continuous (本身已經是半正定) and $\\int|\\gamma(u)|du&lt;\\infty$&emsp;使用 Bochner–Khinchin Theorem Alternative 2 則 $\\exists g(x)$ a density such that&emsp;$$\\color{orange}{ \\gamma(u)=\\int_\\mathbb{R} e^{iux}g(x)dx }\\\\ = \\mathcal{F}[g](u) \\text{ (i.e. Fourier transform of }g)$$&emsp;也等於&emsp;$$\\color{orange}{ g(x)=\\frac{1}{2\\pi}\\int_\\mathbb{R} e^{-iux}\\gamma(u)du }\\\\ = \\frac{1}{2\\pi}\\mathcal{F}[\\gamma](-x)$$&emsp;此時 $g(x)$ 稱為 $X_t$ 的 spectral density&emsp;在 discrete case 為&emsp;$$\\color{orange}{ g(x)=\\frac{1}{2\\pi}\\sum_{h=-\\infty}^\\infty e^{-ihx}\\gamma(h) }$$ 在 WSS 條件下, auto-covariance $\\gamma$ 就等同於特徵方程式了(注意到特徵方程式為 pdf 的 Fourier transform) [Examples]:&emsp;White noise, $WN(0,\\sigma^2)$, $\\gamma(u)=\\sigma^2\\cdot\\mathbf{1}\\{u=0\\}$, 所以&emsp;$g(x)=\\frac{\\sigma^2}{2\\pi}$&emsp;Moving average, $MA(1)$&emsp;$$\\gamma(u)=\\left\\{ \\begin{array}{rl} 0, &amp; |u|&gt;1 \\\\ a\\sigma^2, &amp; |u|=1 \\\\ (1+a^2)\\sigma^2, &amp; u=0 \\end{array} \\right. \\\\ g(x)=\\frac{\\sigma^2}{2\\pi}\\left(1+a^2+a\\cdot2\\cos(x)\\right)$$ [Sufficient and Necessary Condition for a Function is a Spectral Density]:&emsp;A real-valued function $g(x)$ defined on $(-\\pi,\\pi]$&emsp;is a spectral density of a stochastic process $X_t$&emsp;if and only if&emsp;1. $g(x)\\geq0$&emsp;2. $g(x)$ is even&emsp;3. $\\int_{-\\pi}^\\pi g(x)dx&lt;\\infty$ 用 “power” spectral density 來記憶, 因為是 power 所以 1. 2. 合理一定要 Week 5.5: Stochastic integration of the simplest type[Stochastic Integrals Def]:&emsp;Given a Stochastic process $X_t$&emsp;Given partition $\\Delta:a=t_0\\leq t_1\\leq ...\\leq t_n=b$, $|\\Delta|=\\max\\{t_k-t_{k-1}\\},k=1,...,n$&emsp;If the following expectation converges to some value $A$ in mean square sense:&emsp;$$\\mathbb{E}\\left[ \\left( A - \\sum_{k=1}^n X_{t_{k-1}}(t_k-t_{k-1}) \\right)^2 \\right] \\xrightarrow[|\\Delta|\\longrightarrow0]{} 0$$&emsp;Then we denote(define) the converged value $A$ as:&emsp;$\\int_a^b X_t dt$ 💡 參考課程 Week 7.1: Different types of stochastic integrals. Integrals of the type $∫ X_t dt$ [Existence of Stochastic Integral]:&emsp;$X_t:\\mathbb{E}[X_t^2]&lt;\\infty$, if&emsp;1. $m(t)$ continuous&emsp;2. $K(t,s)$ continuous&emsp;Then the stochastic integral exists&emsp;$\\int_a^b X_tdt&lt;\\infty$ 要證明 $K(t,s)$ continuous, 其實只需要 check diagonal 項就可以 💡 只對 covariance function $K(t,s)$ 有這樣的特性, 我們再 Week 6.4 會證明 [Lemma]:&emsp;1. Stochastic process $X_t$. If $K(t,s)$ is continuous at $(t_0, t_0)$&emsp;&emsp;then $X_t$ is continuous at $t_0$ (in mean square sense), that is&emsp;&emsp;$\\mathbb{E}[(X_t-X_{t_0})^2]\\rightarrow0,\\text{ as }t\\rightarrow t_0$&emsp;2. On the other side, if $X_t$ is continuous at $t_0$ and $s_0$&emsp;&emsp;then $K(t,s)$ is continuous at $(t_0,s_0)$, that is $K(t_0,s_0)$ is continuous [Covariance Function is Continuous when Continuous on Diagonal]:&emsp;Covariance function $K(t,s)$ is continuous $\\forall(t_0,s_0)$ $\\Longleftrightarrow$&emsp;$K(t,s)$ is continuous $\\forall(t_0,t_0)$[Proof]:&emsp;$(\\Longrightarrow)$: of course&emsp;$(\\Longleftarrow)$:&emsp;$K(t,s)$ is continuous $\\forall(t_0,t_0)$, 由 Lemma 1 知道 $X_t$ is countinuous $\\forall t_0$&emsp;因此 $X_t$ is continuous $\\forall t_0,s_0$, 由 Lemma 2 知道 $K(t_0,s_0)$ is countinuous $\\forall t_0,s_0$ [Properties of Stochastic Integral]: assume integral exists&emsp;We assume integral is taken in an bounded interval $[a,b]$&emsp;1. Expectation of stochastic integral:&emsp;&emsp;$\\mathbb{E}\\left[ \\int X_tdt \\right] = \\int \\mathbb{E}[X_t]dt$&emsp;&emsp;課程老師說 apply Frobenius theorem 可得此結果. 看不懂 Frobenius theorem&emsp;2. Expectation of squared stochastic integral:&emsp;&emsp;$$\\mathbb{E}\\left[ \\left(\\int X_t dt\\right)^2 \\right] = \\mathbb{E}\\left[ \\int\\int X_t X_s dtds \\right] \\\\ = \\int\\int\\mathbb{E}[X_tX_s]dtds$$&emsp;3. Variance of stochastic integral:&emsp;&emsp;$$Var\\left[ \\int X_t dt \\right] = \\mathbb{E}\\left[ \\left(\\int X_t dt\\right)^2 \\right] - \\left(\\mathbb{E}\\left[ \\int X_tdt \\right]\\right)^2 \\\\ = \\int_a^b\\int_a^b K(t,s) dtds \\\\ \\because\\text{symmetric}= 2\\int_a^b \\int_a^s K(t,s) dtds$$ 課程 pop up quiz Week 5.6-8: Moving-average filtersFilter 就是把某個 stochastic process $X_t$ transform 到另一個 $Y_t$Filter 有 linearity and time-invariant 兩個重要 properties課程舉兩個 filter 的例子 (第一個為 FIR, 第二個為 simplest stochastic integral)$$\\begin{align} Y_t=a_0X_t+a_1X_{t-1}+...+a_nX_{t-n} \\\\ Y_t=\\int_\\mathbb{R} e^{-\\beta(t-s)}X_sds \\end{align}$$ 都具有上述 properties, 也很容易證然後上圖右下角寫出 filtering 的 continuous and discrete 的數學式, 顯然這是 convolution$$Y_t=\\int_\\mathbb{R}\\rho(s)X_{t-s}ds \\\\ Y_t=\\sum_{h=-\\infty}^\\infty \\rho(h)X_{t-h}$$ [Spectral Density and Weakly Stationary by Linear Filter]:&emsp;$X_t$ is weakly stationary with $\\mathbb{E}X_t=0$ and some spectral density $g(x)$.&emsp;$Y_t$ is linear filtered by $\\rho(x)$ from $X_t$:&emsp;$Y_t=\\int_\\mathbb{R} \\rho(s)X_{t-s}ds$&emsp;Then,&emsp;1. $Y_t$ is weakly stationary&emsp;2. Spectral density of $Y_t$:&emsp;&emsp;$$\\color{orange}{ g_Y(u)=g_X(u)\\cdot|\\mathcal{F}[\\rho](u)|^2 }$$&emsp;here the Fourier transform of $\\rho$ is:&emsp;$\\mathcal{F}[\\rho](u)=\\int_\\mathbb{R} e^{-iux}\\rho(x)dx$[Proof 1. weakly stationary]:&emsp;觀察 expectation and covariance functions 是否符合 weakly stationary&emsp;$\\mathbb{E}[Y_t]=\\int_\\mathbb{R}\\rho(s)\\mathbb{E}[X_{t-s}]ds=0$&emsp;因為 $\\mathbb{E}[Y_t]=0$, 所以 $K(t,s)=Cov(Y_t,Y_s)=\\mathbb{E}[Y_tY_s]$, then&emsp;$$K_Y(t_1,t_2)=\\mathbb{E}\\left[ \\int_\\mathbb{R}\\rho(s_1)X_{t_1-s_1}ds_1 \\cdot \\int_\\mathbb{R}\\rho(s_2)X_{t_2-s_2}ds_2 \\right] \\\\ =\\int_\\mathbb{R}\\int_\\mathbb{R} \\rho(s_1)\\rho(s_2)\\mathbb{E}[X_{t_1-s_1}X_{t_2-s_2}] ds_1ds_2 \\\\ =\\int_\\mathbb{R}\\int_\\mathbb{R} \\rho(s_1)\\rho(s_2)\\gamma( \\color{orange}{t_2-t_1} -(s_2-s_1)) ds_1ds_2 \\\\$$&emsp;所以只 depends on $t_2-t_1$, 所以 auto-covariance of $Y_t$ 存在如下:&emsp;$$\\gamma_Y(x)=\\int_\\mathbb{R}\\int_\\mathbb{R} \\rho(s_1)\\rho(s_2)\\gamma( x -(s_2-s_1)) ds_1ds_2$$&emsp;Q.E.D.[Proof 2. Spectral density of $Y_t$]:&emsp;由 proof 1 知道 $\\gamma_Y(x)$ 改寫如下:&emsp;$$\\gamma_Y(x)=\\int_\\mathbb{R}\\rho(s_1) \\color{orange}{\\int_\\mathbb{R}\\rho(s_2)\\gamma((x+s_1)-s_2)ds_2} ds_1 \\\\ =\\int_\\mathbb{R}\\rho(s_1) \\color{orange}{[\\gamma_X*\\rho](x+s_1)} ds_1 \\ldots(\\star)\\\\$$&emsp;定義 $\\rho^o(x)=\\rho(-x)$, 在把 $s_1$ 改寫成 $-s_1$, 則 $(\\star)$ 變成:&emsp;$$\\gamma_Y(x)=(\\star)=\\int_\\mathbb{R}\\rho^o(s_1) [\\gamma_X*\\rho](x-s_1) ds_1 \\\\ =[\\gamma_X*\\rho*\\rho^o](x)\\ldots(\\square)$$&emsp;注意到 spectral density 跟 auto-covariance 的關係為 Fourier transform 的關係:&emsp;$g_Y(u)=\\frac{1}{2\\pi}\\mathcal{F}[\\gamma_Y](-u)$&emsp;所以對 $(\\square)$ 兩邊取 Fourier transform 我們得到:&emsp;$$g_Y(u)= \\frac{1}{2\\pi}\\mathcal{F}[\\gamma_Y](-u)=\\frac{1}{2\\pi}\\mathcal{F}[\\gamma_X](-u)\\cdot\\mathcal{F}[\\rho](-u)\\cdot\\mathcal{F}[\\rho^o](-u) \\\\ =g_X(u)\\cdot|\\mathcal{F}[\\rho](u)|^2$$&emsp;Q.E.D. Let $X_t$ is weakly stationary, and it’s spectral density is $g_X(u)$. 我們想用 moving average $MA(2)$ 來預測當下這個時間點的 $X_t$:$Y_n=a_1X_{n-1}+a_2X_{n-2}$如何決定 $a_1,a_2$ 來使得預測最準 (in least square error sense):$\\mathbb{E}[(X_n-Y_n)^2]\\rightarrow \\min$我們知道 $Z_n=X_n-Y_n=X_n-a_1X_{n-1}-a_2X_{n-2}$ 也是 weakly stationary, 因為過一個 linear filter $\\rho(x)$:$\\rho(x)=\\mathbf{1}\\{x=0\\}-a_1\\mathbf{1}\\{x=1\\}-a_2\\mathbf{1}\\{x=2\\}$其 Fourier transform 為: $\\mathcal{F}[\\rho](u)=1-a_1e^{iu}-a_2e^{2iu}$則 spectral density of $Z$ 為: $g_Z(u)=g_X(u)\\cdot|\\mathcal{F}[\\rho](u)|^2$所以問題等同於計算$\\arg\\min_{a_1,a_2} VarZ_n$計算 $Var Z_n$ 如下$$Var(Z_n)=K_Z(n,n)=\\gamma_Z(0)\\\\ =\\int e^{i\\cdot u\\cdot 0}g_Z(u)du \\\\ =\\int g_X(u)\\cdot|1-a_1e^{iu}-a_2e^{2iu}|^2 du \\\\ =\\int g_X(u)\\cdot(1-a_1e^{iu}-a_2e^{2iu})\\cdot(1-a_1e^{-iu}-a_2e^{-2iu}) du \\\\ =...=\\sum_{i,j=1}^2 B_{ij}a_ia_j + \\sum_{i=1}^2 C_ia_i + D$$所以最小化一個二次式即可求得最佳解 $a_1,a_2$ [Pop up Quiz]: 有點難算 XD 容易算錯","tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://bobondemon.github.io/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"https://bobondemon.github.io/tags/Stochastic-Processes/"},{"name":"Spectral Density","slug":"Spectral-Density","permalink":"https://bobondemon.github.io/tags/Spectral-Density/"},{"name":"Wide-Sense Stationary","slug":"Wide-Sense-Stationary","permalink":"https://bobondemon.github.io/tags/Wide-Sense-Stationary/"},{"name":"Stochastic Integration","slug":"Stochastic-Integration","permalink":"https://bobondemon.github.io/tags/Stochastic-Integration/"}]},{"title":"Stochastic Processes Week 4 Gaussian Processes","date":"2021-12-12T09:55:59.000Z","path":"2021/12/12/Stochastic-Processes-Week-4-Gaussian-Processes/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes (本文) Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes Week 4.1: Random vector. Definition and main propertiesNormal distribution of one r.v.$$X\\sim\\mathcal{N}(\\mu,\\sigma^2), \\text{ for }\\sigma&gt;0,\\mu\\in\\mathbb{R} \\\\ p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$The characteristic function of normal distribution is:$\\Phi(u)=e^{iu\\mu-\\frac{1}{2}u^2\\sigma^2}$我們可以說一個 random variable $X$ almost surely constant $=\\mu$ 表示如下:$\\mathcal{P}\\{X=\\mu\\}=1$如果此時也定義 $\\sigma=0$, 我們可將其也視為 normal distribution $\\mathcal{N}(\\mu,0)$ 等於把 constant 也加入 normal distribution 的定義 [Gaussian Vector Def]:&emsp;A random vector $\\vec{X}=(X_1,...,X_n)$ is Gaussian if and only if&emsp;$$(\\star):\\sum_{k=1}^n \\lambda_k X_k \\sim \\mathcal{N}\\\\ \\forall(\\lambda_1,...,\\lambda_n)\\in\\mathbb{R}$$ Week 4.2: Gaussian vector. Definition and main propertiesDefinition of multivariate characteristic function, ref from wiki我們說明一下為何 $C$ (covariance matrix) is positive semi-definite, 檢查 $\\vec{u}^TC\\vec{u},\\forall \\vec{u}$$$\\vec{u}^TC\\vec{u} = \\sum_{k,j=1}^n u_k c_{kj} u_j = \\sum_{k,j=1}^n u_k Cov(X_j,X_k) u_j \\\\ = Cov\\left(\\sum_{j=1}^n u_jX_j,\\sum_{k=1}^n u_kX_k\\right) = Var\\left(\\sum_{j=1}^n u_jX_j\\right) \\geq 0$$ [Gaussian Vector Thm]:&emsp;$\\vec{X}$ is Gaussian vector if and only if any of the following conditions holds:&emsp;1. Characteristic function of Gaussian vector:&emsp;$$(\\square): \\Phi_{\\vec{X}}(\\vec{u})=\\mathbb{E}\\left[ e^{i\\cdot \\vec{u}^T\\vec{X}} \\right] = \\exp\\left\\{ i\\cdot \\vec{u}^T\\vec{\\mu} - \\frac{1}{2}\\vec{u}^TC\\vec{u} \\right\\} \\\\ \\text{where }\\vec{\\mu}\\in\\mathbb{R}^n; C\\in\\mathbb{R}^{n\\times n}\\text{-positive semi-definite}$$&emsp;&emsp;同時 $\\vec{\\mu}$ and $C$ 為&emsp;$$\\vec{\\mu}=(\\mathbb{E}X_1,...,\\mathbb{E}X_n) \\\\ C=\\left(c_{jk}\\right)_{j,k=1}^n; c_{jk}=Cov(X_j,X_k)$$&emsp;2. Can be represented by Standard Gaussian vector $\\vec{X^o}$&emsp;$$(\\blacksquare):\\vec{X}=A\\cdot \\vec{X^o}+\\vec{\\mu} \\\\ \\text{where }A\\in\\mathbb{R}^{n\\times n},\\vec{X^o}\\text{ is standard Gaussin vector}$$&emsp;&emsp;同時 $A=C^{1/2}$, i.e. $AA=C$ 要寫出 $A$ 只要將 $C$ 對角化, 然後對中間的 diagonal matrix (eigenvalues 組成的) 取 sqrt 即可 [Proof]:&emsp;我們先證明 Definition $(\\star)\\Rightarrow 1.(\\square)$&emsp;Let $\\xi=\\vec{u}^T\\vec{X}=\\sum_{k=1}^n u_kX_k$ 根據定義 $\\xi\\sim\\mathcal{N}$&emsp;我們將 $\\vec{X}$ 的特徵方程式寫出來:&emsp;$\\Phi_{\\vec{X}}(\\vec{u})=\\mathbb{E} [e^{i\\vec{u}^T\\vec{X}}]=\\mathbb{E}[e^{i\\xi\\cdot1}]=\\Phi_\\xi(1)$&emsp;因為 $\\xi\\sim\\mathcal{N}$, 所以特徵方程式是知道的:&emsp;$\\Phi_\\xi(1)=\\exp\\left(i\\mu_\\xi-\\frac{1}{2}\\sigma_\\xi^2\\right)$&emsp;所以我們只需計算出 $\\mu_\\xi,\\sigma_\\xi^2$ 在代回去即可&emsp;$\\mu_\\xi=\\mathbb{E}\\left[\\sum_{k=1}^n u_kX_k\\right] = \\sum_{k=1}^n u_k \\mu_k = \\vec{u}^T \\vec{\\mu}$&emsp;$$\\sigma_\\xi^2 = cov\\left( \\sum_{k=1}^n u_k X_k, \\sum_{j=1}^n u_j X_j \\right) \\\\ = \\sum_{k,j=1}^n u_k\\cdot cov(X_k,X_j) \\cdot u_j = \\vec{u}^TC\\vec{u}$$&emsp;代回去即可發現 $=(\\square)$&emsp;Q.E.D.&emsp;我們再證明 $1.(\\square)\\Rightarrow$ Definition $(\\star)$&emsp;因為特徵函數跟 P.D.F. 是一對一對應的, 所以知道 $\\vec{X}$ 的特徵函數為 $(\\square)$&emsp;很顯然這是 Gaussian 的特徵函數, 所以 $\\vec{X}$ 是 Gaussian vector&emsp;證明 $2. (\\blacksquare) \\Rightarrow 1. (\\square)$&emsp;$\\vec{X}^o$ is standard Gaussian vector, 我們將它的特徵函數寫下來:&emsp;$\\Phi_{\\vec{X}^o}(\\vec{u}) = \\exp\\{-\\frac{1}{2}\\vec{u}^T\\vec{u}\\}$&emsp;計算 $\\vec{X}$ 的特徵函數:&emsp;$$\\text{from }(\\blacksquare) \\text{ we know that } \\Phi_{\\vec{X}}(\\vec{u})=\\mathbb{E}\\left[ e^{i\\vec{u}^T(A\\vec{X}^o+\\vec{\\mu})} \\right] \\\\ = e^{i\\vec{u}^T\\vec{\\mu}} \\cdot \\Phi_{\\vec{X}^o}(A^T\\vec{u}) \\\\ = e^{i\\vec{u}^T\\vec{\\mu}} \\cdot e^{-\\frac{1}{2}\\vec{u}^TAA^T\\vec{u}} \\\\ = e^{i\\vec{u}^T\\vec{\\mu}} \\cdot e^{-\\frac{1}{2}\\vec{u}^TC\\vec{u}} = (\\square)$$&emsp;Q.E.D.&emsp;證明 $1. (\\square) \\Rightarrow 2. (\\blacksquare)$&emsp;我們定義 $A:=C^{1/2}$, 則代入 $(\\square)$ 再整理一下可以得到這是一個 $\\vec{X}=A\\cdot \\vec{X^o}+\\vec{\\mu}$ 的特徵函數&emsp;Q.E.D. Week 4.3: Connection between independence of normal random variables and absence of correlation[Thm]:&emsp;Let $X_1,X_2\\sim\\mathcal{N}(0,1)$ and $corr(X_1,X_2)=0$, 則&emsp;$X_1 \\perp X_2 \\Longleftrightarrow (X_1,X_2) \\text{ is Gaussian vector}$[Proof]:&emsp;證明 $(\\Rightarrow)$&emsp;$\\lambda_1 X_1+\\lambda_2 X_2 \\sim \\mathcal{N} \\Longrightarrow (X_1,X_2) \\text{ is Gaussian vector}$&emsp;可以從特徵方程式對於獨立 r.v.s 的線性組合, 以及 Gaussian 的特徵方程式看出來&emsp;證明 $(\\Leftarrow)$&emsp;因為 uncorrelated&emsp;$$\\therefore C=\\left[ \\begin{array}{c} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right] \\Rightarrow A=\\left[ \\begin{array}{c} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right]=C^{1/2}$$&emsp;定理 $2. (\\blacksquare)$ 告訴我們 $(X_1,X_2)$ 可以改寫如下:&emsp;$$\\left[ \\begin{array}{c} X_1 \\\\ X_2 \\end{array} \\right] =^d \\left[ \\begin{array}{c} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right] \\left[ \\begin{array}{c} \\xi_1 \\\\ \\xi_2 \\end{array} \\right] + \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right]$$&emsp;$=^d$ 表示相同分布, $\\xi_1,\\xi_2\\sim\\mathcal{N}(0,1)$&emsp;乘開來就發現 $X_1 =^d \\xi_1$ and $X_2 =^d \\xi_2$, 因此 $X_1 \\perp X_2$&emsp;Q.E.D. [Exercise1]:&emsp;Given $X_1\\sim\\mathcal{N}(0,1)$, $\\mathcal{P}\\{\\xi=1\\}=0.5; \\mathcal{P}\\{\\xi=-1\\}=0.5$,&emsp;and $\\xi\\perp X_1$&emsp;考慮 $X_2 = |X_1|\\cdot \\xi$, 證明 $X_2 \\sim \\mathcal{N}(0,1)$[Proof]:&emsp;$$\\mathcal{P}\\{X_2\\leq x\\},\\forall x&gt;0\\\\ = \\mathcal{P}\\{|X_1|\\leq x | \\xi=1\\}\\mathcal{P}\\{\\xi=1\\} + \\mathcal{P}\\{|X_1|\\geq -x | \\xi=-1\\}\\mathcal{P}\\{\\xi=-1\\} \\\\ \\text{Note }(X_1\\perp\\xi\\text){ }= \\mathcal{P}\\{|X_1|\\leq x\\}/2 + \\mathcal{P}\\{|X_1|\\geq -x\\}/2 \\\\ \\text{Note }(\\mathcal{P}\\{|X_1|\\geq-x\\}=1) = \\frac{1}{2}\\left( 1+\\mathcal{P}\\{|X_1|\\leq x\\} \\right)\\\\ \\text{by std normal} = \\mathcal{P}\\{X_1\\leq x\\}$$&emsp;注意到 Standard normal $\\mathcal{P}\\{X_1\\geq x\\}=\\mathcal{P}\\{X_1\\leq -x\\}$, 所以&emsp;$$\\frac{1}{2}\\left( 1+\\mathcal{P}\\{|X_1|\\leq x\\}\\right) = \\frac{1}{2}\\left( 1 +\\mathcal{P}\\{X_1\\leq x\\}-\\mathcal{P}\\{X_1\\leq -x\\}\\right) \\\\ = \\frac{1}{2}\\left( {\\color{orange}1} +\\mathcal{P}\\{X_1\\leq x\\}-\\mathcal{P}\\{X_1\\geq x\\}\\right) \\\\ = \\frac{1}{2}\\left( \\color{orange}{\\mathcal{P}\\{X_1\\leq x\\}+\\mathcal{P}\\{X_1\\geq x\\}} +\\mathcal{P}\\{X_1\\leq x\\}-\\mathcal{P}\\{X_1\\geq x\\}\\right) \\\\ = \\mathcal{P}\\{X_1\\leq x\\}$$&emsp;Q.E.D. [Exercise2]:&emsp;Given $X_1\\sim\\mathcal{N}(0,1)$, $\\mathcal{P}\\{\\xi=1\\}=0.5; \\mathcal{P}\\{\\xi=-1\\}=0.5$, and $\\xi\\perp X_1$&emsp;考慮 $X_2 = |X_1|\\cdot \\xi$, 證明 $cov(X_1,X_2)=0$[Proof]:&emsp;$$cov(X_1,X_2)=\\mathbb{E}[X_1X_2]-\\mathbb{E}[X_1]\\mathbb{E}[X_2] \\\\ =\\mathbb{E}[X_1\\cdot|X_1|\\cdot\\xi]-0\\cdot \\mathbb{E}[X_2] \\\\ =\\mathbb{E}[X_1\\cdot|X_1|] \\mathbb{E}[\\xi] = \\mathbb{E}[X_1\\cdot|X_1|]\\cdot 0=0$$&emsp;Q.E.D. [Exercise3]:&emsp;Given $X_1\\sim\\mathcal{N}(0,1)$, $\\mathcal{P}\\{\\xi=1\\}=0.5; \\mathcal{P}\\{\\xi=-1\\}=0.5$, and $\\xi\\perp X_1$&emsp;考慮 $X_2 = |X_1|\\cdot \\xi$, 證明 $X_1$ and $X_2$ are dependent[Proof]:&emsp;由 Exercise 1 &amp; 2 我們知道 $X_1,X_2\\sim\\mathcal{N}(0,1)$ and $corr(X_1,X_2)=0$&emsp;則如果 $X_1\\perp X_2$, 等價於說明 $(X_1,X_2)$ 是 Gaussian vector.&emsp;利用反證法, 如果是 Gaussian vector 則滿足 linear combination 仍是 normal distribution:&emsp;$Z=X_1-X_2=X_1-|X_1|\\xi \\sim \\mathcal{N}$&emsp;考慮&emsp;$$(a)\\ldots \\mathcal{P}\\{Z&gt;0\\} \\geq \\mathcal{P}\\{X_1&gt;0,\\xi=-1\\} \\\\ = \\mathcal{P}\\{X_1&gt;0\\}\\mathcal{P}\\{\\xi=-1\\}=1/4 \\\\ (b)\\ldots \\mathcal{P}\\{Z=0\\} \\geq \\mathcal{P}\\{X_1&gt;0,\\xi=1\\} \\\\ =\\mathcal{P}\\{X_1&gt;0\\}\\mathcal{P}\\{\\xi=1\\}=1/4$$&emsp;條件 $(a),(b)$ 不可能同時滿足, 因為如果 $\\sigma^2&gt;0$ 則 $\\mathcal{P}\\{Z=0\\}=0$ 矛盾 $(b)$&emsp;否則就是 $\\sigma^2=0$, i.e. constant, 明顯矛盾&emsp;Q.E.D. Exercise 1, 2, &amp; 3 告訴我們, 可以有兩個 normal r.v.s 是 uncorrelated 但不是 independet Week 4.4-5: Definition of a Gaussian process. Covariance function[Gaussian Processes Def]:&emsp;A Gaussian process $X_t$ is a stochastic process if&emsp;$\\forall t_1, ..., t_n: (X_{t_1},...,X_{t_n})$ is a Gaussian vector Denotes two terms $m(t)$ and $K(t,s)$:$m(t)=\\mathbb{E}X_t$, mathmetical expectation, $\\mathbb{R}_+\\rightarrow\\mathbb{R}$$K(t,s)=Cov(X_t,X_s)$, covariance function, $\\mathbb{R}_+\\times\\mathbb{R}_+\\rightarrow \\mathbb{R}$ $K(t,t)=Var[X_t]$$K(t,s)=K(s,t)$ [Covariance function $K(t,s)$ is positive semi-definite]:&emsp;For a Gaussian process $X_t$, $K(t,s)$ is positive semi-definite[Proof]:&emsp;考慮 $\\forall(t_1,...,t_n)\\in\\mathbb{R}_+^n$ and $\\forall(u_1,…,u_n)\\in\\mathbb{R}^n$, 則&emsp;$$\\sum_{k,j=1}^n u_k u_j K(t_k,t_j) = Cov\\left(\\sum_{k=1}^n u_k X_{t_k}, \\sum_{j=1}^n u_j X_{t_j}\\right) \\\\ = Var\\left(\\sum_{k=1}^n u_k X_{t_k}\\right) \\geq 0$$&emsp;Q.E.D. [Gaussian Processes Thm]:&emsp;若 $m:\\mathbb{R}_+\\rightarrow\\mathbb{R}$ 和 $K:\\mathbb{R}_+\\times\\mathbb{R}_+\\rightarrow\\mathbb{R}$ 為任意 functions 滿足&emsp;$K$ is symmetric positive semi-definite, 則&emsp;$\\exists$ Gaussian process $X_t:$&emsp;$$\\begin{align} \\mathbb{E}X_t=m(t) \\\\ Cov(X_t,X_s)=K(t,s) \\end{align}$$ 討論一下 sufficient statistics&emsp;- Gaussian random variable: $\\mu\\in\\mathbb{R}$, and $\\sigma\\in\\mathbb{R}_+$&emsp;- Gaussian vector: $\\vec{\\mu}\\in\\mathbb{R}^n$, and $C\\in\\mathbb{R}^{n\\times n}$ and is symmetric positive semi-definite&emsp;- Gaussian process: $m:\\mathbb{R}_+\\rightarrow\\mathbb{R}$, and $K:\\mathbb{R}_+\\times\\mathbb{R}_+\\rightarrow\\mathbb{R}$ and is symmetric positive semi-definite [Example1]: $K(t,s)=|t-s|$, 證明不是 positive semi-definite[Proof]:&emsp;如果 $K$ 是半正定, 則存在 Gaussian process $X_t$ such that&emsp;$Cov(X_t,X_s)=|t-s|$&emsp;則可發現 $Var(X_t)=0$, 因此 $X_t=f(t)$ 是 deterministic function, i.e. constat&emsp;因此&emsp;$Cov(X_t,X_s)=\\mathbb{E}[X_tX_s]-\\mathbb{E}[X_t]\\mathbb{E}[X_s]=0$&emsp;明顯不是半正定, Q.E.D. [Example2]: $K(t,s)=\\min(t,s)$, 證明是半正定[Proof]:&emsp;能否證明&emsp;$(\\star)\\ldots\\sum_{j,k=1}^n u_j u_k \\min(t_j,t_k) \\geq 0$&emsp;我們導入 $f_t(x)$ 幫忙推導&emsp;$f_t(x)=\\mathbf{1}\\{x\\in[0,t]\\}, \\text{ where }\\mathbf{1}\\{\\cdot\\} \\text{ is indicator function} \\\\$&emsp;則&emsp;$\\min(t,s)=\\int_0^\\infty f_t(x)f_s(x) dx$&emsp;代入 $(\\star)$ 得到&emsp;$$\\sum_{j,k=1}^n u_j u_k \\int_0^\\infty f_{t_j}(x)f_{t_k}(x)dx \\\\ = \\int_0^\\infty \\left(\\sum_{j=1}^n u_j f_{t_j}(x) \\right) \\left(\\sum_{k=1}^n u_k f_{t_k}(x) \\right) dx \\\\ =\\int_0^\\infty \\left(\\sum_{j=1}^n u_j f_{t_j}(x) \\right)^2 dx \\geq 0$$&emsp;Q.E.D. Week 4.6: Two definitions of a Brownian motionBrownian Motion 又稱 Wiener process[Brownian Motion Def1]:&emsp;We say that $B_t$ is Brownian motion if and only if&emsp;$B_t$ is a Gaussin process with $m(t)=0$ and $K(t,s)=\\min(t,s)$ [Brownian Motion Def2]:&emsp;We say that $B_t$ is Brownian motion if and only if&emsp;$(0)$ $B_0=0$ almost surely (a.s.)&emsp;$(1)$ $B_t$ is independent increaments, i.e.&emsp;&emsp;$\\forall t_0&lt;t_1&lt;...&lt;t_n$, we have $B_{t_1}-B_{t_0}, ..., B_{t_n}-B_{t_{n-1}}$ are independent&emsp;$(2)$ $B_t-B_s\\sim\\mathcal{N}(0,t-s)$, $\\forall t&gt;s\\geq0$ [證明 Def1 $\\Rightarrow$ Def2]:&emsp;先證明 $(0)$, 我們知道 $\\mathbb{E}B_0 = m(0)=0$, $Var(B_0)=K(B_0, B_0)=0$&emsp;所以 $B_0=0$ a.s.&emsp;再來證明 $(1)$: $B_b-B_a \\perp B_d-B_c, \\forall 0\\leq a\\leq b\\leq c\\leq d$&emsp;我們先證明它們是 uncorrelated&emsp;$$Cov(B_b-B_a, B_d-B_c) \\\\ =Cov(B_b,B_d) - Cov(Ba,B_d) - Cov(B_b,B_c) + Cov(B_a,B_c) \\\\ = b-a-b+a = 0$$&emsp;所以 $B_b-B_a$ uncorrelated with $B_d-B_c$&emsp;之前我們學過, 如果 $(B_b-B_a, B_d-B_c)$ 是 Gaussian vector 的話&emsp;uncorrelated 等同於 independent&emsp;所以我們用 Gaussian vector 的定義來證明:&emsp;$$\\lambda_1(B_b-B_a)+\\lambda_2(B_d-B_c) \\\\ = \\lambda_1B_b-\\lambda_1B_a+\\lambda_2B_d-\\lambda_2B_c \\\\ \\text{by Def1 }\\sim \\mathcal{N}$$&emsp;證得 $(B_b-B_a, B_d-B_c)$ 是 Gaussian vector 且已證過 uncorrelated, 所以 independent&emsp;最後證明 $(2)$:&emsp;首先 $B_t-B_s\\sim\\mathcal{N}$, 因為 by Def1 $B_t$ is Gaussian process, 所以線性組合也是 Gaussian&emsp;$\\mathbb{E}[B_t-B_s]=\\mathbb{E}[B_t]-\\mathbb{E}[B_s]=m(t)-m(s)=0$&emsp;$$Var[B_t-B_s]=Cov(B_t-B_s,B_t-B_s) \\\\ = Cov(B_t,B_t)-2Cov(B_t,B_s)+Cov(B_s,B_s) \\\\ = t-s$$&emsp;Q.E.D. [證明 Def2 $\\Rightarrow$ Def1]:&emsp;我們先證明是 Gaussian process, $\\forall t_1&lt;t_2&lt;...&lt;t_n$ 考慮&emsp;$$\\sum_{k=1}^n \\lambda_k B_{t_k} = \\lambda_n(B_{t_n}-B_{t_{n-1}}) + (\\lambda_n+\\lambda_{n-1})B_{t_{n-1}} + \\sum_{k=1}^{n-2} \\lambda_k B_{t_k} \\\\ = \\sum_{k=1}^n d_k(B_{t_n}-B_{t_{n-1}}) \\\\ \\text{by sum of independent Guassian is also Gaussian} \\sim \\mathcal{N}$$&emsp;所以 $(B_{t_1},...,B_{t_n})$ 是 Gaussian vector implies that $B_t$ 是 Gaussian process&emsp;是 Gaussian process 後, 我們找一下 $m(t),K(t,s)$&emsp;首先注意到我們有 $B_t=B_t-B_0\\sim\\mathcal{N}(0,t)$&emsp;所以 $m(t)=\\mathbb{E}B_t=0$&emsp;接著計算 $K(t,s)$, for $t&gt;s$&emsp;$$K(t,s)=Cov(B_t,B_s) \\\\ = Cov(B_t-B_s+B_s,B_s) \\\\ = Cov(B_t-B_s, B_s) + Cov(B_s,B_s) \\\\ = Cov(B_t-B_s, B_s-B_0) + s \\\\ \\text{by indep. increment }= 0 + s = s$$&emsp;如果 $t&lt;s$ 則 $K(t,s)=t$, 因此&emsp;$K(t,s)=\\min(t,s)$&emsp;Q.E.D. 由 Brownian motion 的 Def2 我們可以推出一些東西:For $t&gt;s\\geq0$, $B_t\\sim\\mathcal{N}(0,t)$, $B_s\\sim\\mathcal{N}(0,s)$, $B_{t-s}\\sim\\mathcal{N}(0,t-s)$由 independent increament 知道 $B_{t-s}\\perp B_s$, 又因為獨立 normal r.v.s 相加也仍是 normal, 且其 mean and variance 為直接相加所以$(B_{t-s}+B_s)\\sim\\mathcal{N}(0,t-s+s)=\\mathcal{N}(0,t)=^d B_t \\\\$$B_t(x)=B_{t-s}(x)+B_s(x)$ Week 4.7: Modification of a process. Kolmogorov continuity theorem[Stochastically Equivalent Def]:&emsp;$X_t,Y_t$ are called stochastically equivalent if&emsp;$\\mathcal{P}\\{X_t=Y_t\\}=1,\\forall t\\geq0$ [Example]: stochastically equivalent 但行為卻不一樣的 r.v.s&emsp;$X_t=0,\\forall t\\in[0,1]$. And $Y_t=\\mathbf{1}\\{\\tau=t\\},\\tau\\sim Unif(0,1)$&emsp;在看 $Y_t$ 這個 random variable 的時候, $t$ 是給定的 (fixed), 而它的值只有可能是 $0$ or $1$&emsp;則很顯然 $X_t$ 和 $Y_t$ 是 stochastically equivalent:&emsp;$\\mathcal{P}\\{X_t=Y_t\\}=\\mathcal{P}\\{Y_t=0\\}=\\mathcal{P}\\{t\\neq\\tau\\}=1$&emsp;我們劃出 $X_t$ 和 $Y_t$ 的 trajectory:&emsp;Stochastic process $X_t:\\Omega\\times\\mathbb{R}_+\\rightarrow\\mathbb{R}$&emsp;Trajectory of an given outcome $\\omega$, $X_t(\\omega):\\mathbb{R}_+\\rightarrow\\mathbb{R}$&emsp;上圖顯示了給定某一個 $t$, $X_t$ 和 $Y_t$ 的一條 trajectory&emsp;對於所有的 $t$, $X_t$ 都長一個樣, 而 $Y_t$ 都長不一樣, 且都不連續 所以 $X_t$ 和 $Y_t$ 是 stochastically equivalent 但是 $X_t$ trajectory 是 continuous 而 $Y_t$ 不是有沒有可能把一個 trajectory 不連續的 stochastic process 找道其對應 stochastically equivalent 的 process 呢 ? [Kolmogorov continuity Theorem]:&emsp;Let $X_t$ be a stochastic process, if $\\exists C,\\alpha,\\beta&gt;0$ such that&emsp;$\\mathbb{E}\\left[ |X_t-X_s|^\\alpha \\right] \\leq C\\cdot |t-s|^{1+\\beta},\\forall t,s\\in[a,b]$&emsp;then $\\exists Y_t$ is stochastically equivalent to $X_t$ and $Y_t$ has continuous trajectories&emsp;We can say that the process $X_t$ has a continuous modification. [Brownian motion has continuous modification]:&emsp;Let $B_t$ be a Brownian motion&emsp;We know that $B_t-B_s\\sim\\mathcal{N}(0,t-s)$, which also means&emsp;$B_t-B_s=^d\\sqrt{t-s}\\cdot\\xi,\\xi\\sim\\mathcal{N}(0,1)$&emsp;Then (using the 4-th moment of normal distribution)&emsp;$\\mathbb{E}\\left[ |B_t-B_s|^4 \\right] = (t-s)^2\\mathbb{E}\\xi^4= (t-s)^2\\cdot3$&emsp;We can see that it fullfilled Kolmogorov continuity Theorem&emsp;Conclude that Brownian motion has continuous modification. 一般我們稱 Brownian motion 都預設稱這個 continuous modification Week 4.8: Main properties of Brownian motion[Quadratic Variation Def]: wiki&emsp;$X_{t}$ is a stochastic process. It’s quadratic variation is the process, written as $[X_t]$, defined as&emsp;$[X_t]=\\lim_{|\\Delta|\\rightarrow0}\\sum_{k=1}^n (X_{t_k}-X_{t_{k-1}})^2$&emsp;where $\\Delta:0=t_0&lt;t_1&lt;...&lt;t_n=t$, is a partition, and $|\\Delta|=\\max_{i=1,...,n}\\{t_i-t_{i-1}\\}$&emsp;This limit, if it exists, is defined using convergence in probability (See Week 6 的開頭). 課程沒提供證明, 說在很多 stochastic 的書都有提供了 [Quadratic Variation of Brownian Motion]:&emsp;給一個 Brownian motion $B_t$, 考慮一個 partition $\\bigtriangleup:0=t_0&lt;t_1&lt;...&lt;t_n=t$&emsp;我們稱該 partition 的 quadratic variation 為以下 limit:&emsp;$\\lim_{|\\bigtriangleup|\\rightarrow0} \\sum_{k=1}^n\\left( B_{t_k}-B_{t_{k-1}} \\right)^2 = t$&emsp;$|\\bigtriangleup|\\rightarrow0$ 表示:$\\max_{k=0,...,n}\\{|t_k-t_{k-1}|\\}\\rightarrow0, \\text{ for }n\\rightarrow0$&emsp;從上面可以看出 Brownian motion 的 quadratic variation 極限值存在且等於 $t$[Proof]:&emsp;使用 convergence in mean square sense 證明.&emsp;因為我們知道 m.sq. sense 成立則 converges in probability&emsp;(quadratic variation 的 limit 定義) 也成立. 更多參考 Week 6 開頭. 如果我們令:$S_n=\\sum_{k=1}^n\\left( B_{t_k}-B_{t_{k-1}} \\right)^2$則 quadratic variation 告訴我們$\\mathbb{E}(S_n-t)^2\\rightarrow0, \\text{ for }n\\rightarrow\\infty$ [Variation of Brownian Motion]:&emsp;給一個 Brownian motion $B_t$, 考慮一個 partition $\\bigtriangleup:0=t_0&lt;t_1&lt;...&lt;t_n=t$&emsp;我們稱該 partition 的 variation 為以下 limit:&emsp;$\\lim_{|\\bigtriangleup|\\rightarrow0} \\sum_{k=1}^n | B_{t_k}-B_{t_{k-1}} | = \\infty$&emsp;從上面可以看出 Brownian motion 的 variation 極限值不存在 課程說 variation 不存在的證明會用到 quadratic variation 存在的定理, 但沒給證明 總結來說, Brownian motion 是 quadratic variation, 但不是 bounded variation [Brownian Motion Continuity and Differentiability]:&emsp;Brownian motion is everywhere continuous but nonwhere differentiable.&emsp;我們說 stochastic process 是 continuous at time $t$ 意思為:&emsp;$$B_{t+h}\\xrightarrow[h\\rightarrow0]{p}B_t \\\\ \\Longleftrightarrow\\mathcal{P}\\{|B_{t+h}-B_t|&gt;\\varepsilon\\}=0, \\text{ for }h\\rightarrow0, \\forall\\varepsilon&gt;0$$&emsp;而 differentiable 會在 week6 提到. 接下來的定理可以用來了解 Brownian motion $B_t$ 跑向 $\\infty$ 的速度有多快.$$\\begin{align} \\begin{array}{rl} \\lim_{t\\rightarrow\\infty}\\frac{B_t}{t}=0, &amp;a.s. \\\\ \\lim\\sup_{t\\rightarrow\\infty}\\frac{B_t}{\\sqrt{t}}=\\infty, &amp;a.s. \\end{array} \\end{align}$$ 課程說很容易證明… 但對我來說不是 (Law of the iterated logarithm) 直觀理解為: Brownian motion 跑得比 $t$ 還慢, 但是比 $\\sqrt{t}$ 還快但準確來說 Brownian motion 的速度應該是什麼? Law of the Iterated Logarithm 告訴了我們答案 [Law of the Iterated Logarithm]:&emsp;Brownian motion $B_t$, we have&emsp;$\\lim\\sup_{t\\rightarrow\\infty} \\frac{B_t}{\\sqrt{2t\\log(\\log t)}}=1$ 無法太深入理解這些定理的用處和意義, 同意課程要給更多範例說明 最後一個問題可以由 $\\lim_{h\\rightarrow0} B_{t+h}-B_t \\sim \\lim_{h\\rightarrow0}\\mathcal{N}(0,h)$ 看出, 為 constant $0$ a.s. END OF CLASS","tags":[{"name":"Gaussian Process","slug":"Gaussian-Process","permalink":"https://bobondemon.github.io/tags/Gaussian-Process/"},{"name":"Coursera","slug":"Coursera","permalink":"https://bobondemon.github.io/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"https://bobondemon.github.io/tags/Stochastic-Processes/"},{"name":"Gaussian Vector","slug":"Gaussian-Vector","permalink":"https://bobondemon.github.io/tags/Gaussian-Vector/"},{"name":"Brownian Motion","slug":"Brownian-Motion","permalink":"https://bobondemon.github.io/tags/Brownian-Motion/"}]},{"title":"Stochastic Processes Week 3 Markov Chains","date":"2021-12-12T09:15:30.000Z","path":"2021/12/12/Stochastic-Processes-Week-3-Markov-Chains/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes Week3: Markov Chains (本文) Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes Week 3.1: Definition of a Markov chain. Some examples[Markov Chain Def]:&emsp;令 State space $\\mathcal{S}$ which is countable. 則 $S_n$, $n=0,1,2,…$ 稱為 Markov chain 需滿足 Markov property 如下:&emsp;$\\mathcal{P}\\{S_n=j | S_{n-1}=i_{n-1},...,S_0=i_0\\} = \\mathcal{P}\\{S_n=j | S_{n-1}=i_{n-1}\\}$&emsp;其中 $i_0,...,i_{n-1},j\\in\\mathcal{S}$, and $\\mathcal{P}\\{S_{n-1}=i_{n-1},...,S_0=i_0\\}\\neq0$ 如果滿足 Markov property, 則我們發現:$$\\mathcal{P}\\{S_n=i_n,S_{n-1}=i_{n-1},...,S_0=i_0\\} \\\\ = \\mathcal{P}\\{ S_n=i_n | S_{n-1}=i_{n-1},...,S_0=i_0 \\}\\cdot \\mathcal{P}\\{ S_{n-1}=i_{n-1},...,S_0=i_0 \\} \\\\ = \\mathcal{P}\\{ S_n=i_n | S_{n-1}=i_{n-1} \\}\\cdot \\mathcal{P}\\{ S_{n-1}=i_{n-1},...,S_0=i_0 \\} \\\\ =...\\\\ = \\mathcal{P}\\{ S_n=i_n | S_{n-1}=i_{n-1} \\} \\mathcal{P}\\{ S_{n-1}=i_{n-1} | S_{n-2}=i_{n-2} \\} ... \\mathcal{P}\\{ S_1=i_1 | S_0=i_0 \\} \\mathcal{P}\\{S_0=i_0\\}$$ 所以是 pairwise independent Any renewal process is also a Markov chain [Random Walk Example]:&emsp;考慮&emsp;$$\\mathcal{P}\\{ S_n=j | S_{n-1}=i_{n-1} \\} = \\left\\{ \\begin{array}{rl} p, &amp; j=i_{n-1}+1 \\\\ 1-p, &amp; j=i_{n-1}-1 \\\\ 0, &amp; \\text{otherwise} \\end{array} \\right.$$&emsp;發現只會跟前一個 state 有關 注意到 random walk 不是 renewal process, 因為 $\\xi$ 可以產生負的值. 而在 renewal process, $\\xi$ 為 inter-arrival time $\\geq0$ [Taxis in the airport Example]:&emsp;$1$ taxi at $n$ moment, $n=1,2,3,…$&emsp;如果當下沒有乘客, taxi 就離開. 如果有乘客, taxi 當場載走一名並馬上就離開.&emsp;$X_k$: # people waiting for taxi at time $k$&emsp;$Y_k$: # people arriving at time $k$&emsp;現在說明 $X_k$ 是 Markov chain&emsp;$$X_k=Y_k+(X_{k-1}-1)_+= \\left\\{ \\begin{array}{rl} Y_k, &amp; \\text{if }X_{k-1}=0 \\\\ Y_k+X_{k-1}-1, &amp; \\text{if }X_{k-1}\\geq0 \\\\ \\end{array} \\right.$$&emsp;所以 $X_k$ 只跟 $X_{k-1}$ 有關 [Example]:&emsp;考慮以下 $X_n$, for some fixed $m\\in\\mathbb{N}$&emsp;$\\mathcal{P}\\{ X_n | X_{n-1},...,X_0 \\} = \\mathcal{P}\\{ X_n | X_{n-1},...,X_{n-m} \\}$&emsp;當下的 state 跟前 $m$ 個 states 有關, 而原本 Markov property 是只能與前 $1$ 個 state 有關.&emsp;我們定義 $S_n$ (a vector of $X_n$) 如下:&emsp;$S_n=(X_n,...,X_{n-m+1}),\\text{ where } n=m-1,m,...$&emsp;則我們發現:&emsp;$$\\mathcal{P}\\{ X_n | X_{n-1},...,X_0 \\} = \\frac{\\mathcal{P}\\{ X_n,X_{n-1},...,X_0\\}}{\\mathcal{P}\\{ X_{n-1},...,X_0\\}} \\\\ = \\frac { \\mathcal{P}\\{ (X_n,...,X_{n-m+1}), (X_{n-1},...,X_{n-m}),...,(X_{m-1},...,X_0) \\} } { \\mathcal{P}\\{ (X_{n-1},...,X_{n-m}),...,(X_{m-1},...,X_0) } \\\\ = \\frac {\\mathcal{P}\\{S_n,S_{n-1},...,S_{m-1}\\}} {\\mathcal{P}\\{S_{n-1},...,S_{m-1}\\}} = \\mathcal{P}\\{S_n | S_{n-1},...,S_{m-1}\\} \\ldots(\\star) \\\\ \\mathcal{P}\\{ X_n | X_{n-1},...,X_{n-m} \\} = \\frac{\\mathcal{P}\\{ X_n,X_{n-1},...,X_{n-m}\\}}{\\mathcal{P}\\{ X_{n-1},...,X_{n-m}\\}} \\\\ = \\frac { \\mathcal{P}\\{ (X_n,...,X_{n-m+1}), (X_{n-1},...,X_{n-m}) \\} } { \\mathcal{P}\\{ (X_{n-1},...,X_{n-m}) } \\\\ = \\frac {\\mathcal{P}\\{S_n,S_{n-1}\\}} {\\mathcal{P}\\{S_{n-1}\\}} = \\mathcal{P}\\{S_n | S_{n-1}\\}\\ldots(\\square)$$&emsp;因為 $X_n$ 是 Markov chain, 所以 $(\\star)=(\\square)$, 得知 $S_n$ 為 Markov chain Week 3.2: Matrix representation of a Markov chain. Transition matrix. Chapman-Kolmogorov equationMatrix representation 如課程上圖的說明, 這裡很容易看上圖即可. [Thm]:&emsp;Define $p_{ij}^{(m)} = \\mathcal{P}\\{X_{n+m}=j | X_n=i\\}$, 即 step $n$ 時在 state $i$, 且走了 $m$ steps, i.e. step $n+m$, 時在 state $j$ 的機率.&emsp;並定義 $P^{(m)}=\\left( p_{ij}^{(m)} \\right)$ 表示 m-step 的 transition matrix. 則我們有:&emsp;$P^{(m)}= P^m=\\underbrace{PP\\cdots P}_{m\\text{-times}}$ 定義了 stationary distribution $\\vec\\pi^*=\\vec\\pi^*P$ Week 3.3-5: Graphic representation. Classification of states定義了 $i\\rightarrow j$ 表示有一條 walk (多條 arcs 連結) from $i$ to $j$. 稱 $j$ is accesible from $i$定義了 $i\\leftrightarrow j$ 表示有一條雙向 walk 連結 $i$ and $j$. 稱 $i$ and $j$ communicate [Equivalence Relation Def]:&emsp;給定 set $A$, 若有一個 binary operation $\\sim$ 滿足以下關係則稱 $\\sim$ 為 equivalence relation:&emsp;$$\\begin{array}{rl} a\\sim a, a\\in A &amp; \\text{reflectivity} \\\\ a\\sim b\\Rightarrow b\\sim a, \\forall a,b\\in A &amp; \\text{symmetry} \\\\ a\\sim b, b\\sim c \\Rightarrow a\\sim c &amp; \\text{transtivity} \\end{array}$$ 給定 set $A$ 和它的一個 equivalence relation $\\sim$, 則我們可以定義出 equivalence class The equivalence class of an element $a$ is often denoted $[a]$ or $[a]_\\sim$ and is defined as the set ${ x\\in A: a\\sim x}$ of elements that are related to $a$ by $\\sim$. 我們對 Markov chain 的圖可以定義一個 equivalence relation (by communicate $\\leftrightarrow$) $B_1,B_2,…$ 為 states 的 equivalence classes by definition below$$\\forall j\\in B_i, \\forall k \\in \\mathcal{S} \\left\\{ \\begin{array}{rl} k\\in B_i, &amp;k\\leftrightarrow j \\\\ k \\notin B_i, &amp; k \\nleftrightarrow j \\end{array} \\right.$$簡單講就是拿兩個 states $j$ and $k$ 出來, 如果他們 $\\leftrightarrow$ 則屬於同一個 set. 這樣會使得 state space $\\mathcal{S}$ 形成 equivalence classes 課程裡的 state $4$ 只有 out going arcs, 所以 $4 \\nleftrightarrow 4$, 因此不應該有一個 equivalence class $\\{4\\}$ 才對啊 ? 我猜應該要多補上這條: 自己跟自己一定屬於同一個 equivalence class. [Recurrent and Transient Def]:&emsp;$i$ is recurrent if $\\forall j:i\\rightarrow j \\Rightarrow j\\rightarrow i$&emsp;$i$ is transient if $\\exist j: i\\rightarrow j, j\\nrightarrow i$ 我們說一個 state $i$ 是 recurrent 必須滿足對”所有”其他 states 都滿足 “如果 $i$ 到的了該 state, 那該 state 也到的了 $i$”而 transient 只需滿足 “存在” 一個 state 使得 $i$ 到的了但回不來 [Thm, Nodes are ALL Recurrent or ALL Transient in An Equivalence Class]: 因此 equivalence classes:$$\\begin{array}{rl} \\{1\\}, &amp; \\text{transient} \\\\ \\{2,3\\}, &amp; \\text{recurrent} \\\\ \\{4\\}, &amp; \\text{transient} \\\\ \\{5,6\\}, &amp; \\text{transient} \\end{array}$$ 其實我們可以把 equivalence class 想成是一個 state 就可以. 然後如果有出去的 arc 就是 transient. 反之沒有出去的 arcs 就會是 recurrent. [Period of a State $i$ Def]:&emsp;給定一個 Markov chain (discrete time and countable state space $\\mathcal{S}$), 定義 peroid of a state $i$:&emsp;$d(i):=GCD\\{n:p_{ii}^{(n)}\\neq0\\}$&emsp;其中 $GCD$ 為 greates common divisor, 若 $d(i)=1$ 我們稱 $i$ 為 aperiodic 假設 $n$ 為走回自己要花的步數, 所有那些 $n$ 的最大公因數就是 period [範例]: 同樣用課程的圖&emsp;$d(1)=1$&emsp;$d(2)=d(3)=2$&emsp;$d(4)=1$, 走不回自己 period 定義為 $1$&emsp;$d(5)=GCD{2,3,…}=1$, 其中 $2$ 是因為 $5\\rightarrow6\\rightarrow5$, $3$ 是因為 $5\\rightarrow6\\rightarrow6\\rightarrow5$&emsp;$d(6)=1$ $a\\vdots b$ 表示 $a$ 能被 $b$ 整除 [Thm]: All elements in 1 equivalence class have the same period[Proof]:&emsp;選定 states $i$ and $j$ 在同一個 equivalence class, 並令 $i\\rightarrow j$ 走 $n$ 步, $j\\rightarrow i$ 走 $m$ 步.&emsp;假定 for some $k$ 滿足:&emsp;$p_{jj}^{(k)}\\neq 0 \\Longrightarrow k\\vdots d(j) \\ldots (\\star)$&emsp;表示 $j\\rightarrow j$ 可以走 $k$ 步, 因此 $k$ 一定會被 $d(j)$ 整除.&emsp;如圖:&emsp;我們有如下的關係:&emsp;$$\\left. \\begin{array}{r} p_{ii}^{(n+m)}\\neq0 \\Longrightarrow (n+m) \\vdots d(i) \\\\ p_{jj}^{(k)}\\neq0 \\Longrightarrow p_{ii}^{(n+m+k)}\\neq0 \\Longrightarrow (n+m+k) \\vdots d(i) \\end{array} \\right\\} \\Longrightarrow k\\vdots d(i)$$&emsp;因為 $j$ 可以走 $k$ 步回到自己, 所以 $i$ 可以走 $n+m+k$ 步也回到自己. 得到 $k$ 可以被 $d(i)$ 整除的結論.&emsp;因此對所有可能的 $k$ (那些滿足 $(\\star)$ 的 $k$) 我們都可以得到被 $d(i)$ 整除的結論.&emsp;表示 $d(i)$ 是這些所有可能的 $k$ 的 “公因數”&emsp;根據定義, $d(j)$ 是這些所有可能的 $k$ 的 “最大公因數”&emsp;因此 $d(j)\\geq d(i)$&emsp;同樣的流程我們 $i$ and $j$ 的腳色互換, 也可以得到 $d(i)\\geq d(j)$&emsp;得到結論 $d(i)=d(j)$&emsp;Q.E.D.&emsp;課程的說明我覺得有點問題, 以下為課程的解釋: 注意到 $d(i)$ 是”某一個” $k$ 的因數, 但是 $d(j)$ 是所有可能的 $k$ 的最大公因數. 因此 $d(i)$ 一定可以被 $d(j)$ 整除 (??) :$$d(i) \\vdots d(j)$$同樣的流程我們 $i$ and $j$ 的腳色互換, 也可以得到 $d(j)$ 會被 $d(i)$ 整除得到結論 $d(i)=d(j)$ Week 3.6-7: Ergodic chains. Ergodic theorem[Ergodic Markov Chain using Graphic Representation]:&emsp;我們說一個 Markov chain is ergodic, 需滿足以下條件&emsp;1. 只有一個 class of equivalence&emsp;2. 所有 nodes are recurrent&emsp;3. $d(i)=1,\\forall i\\in\\mathcal{S}$. 所有 nodes are aperiodic [Example]:&emsp;如果只有實線的 arcs, 則滿足 1. 只有一個 equivalence class, 2. 所有 nodes are recurrent. 但是 $d(i)=6$.&emsp;但如果包含虛線的 arc 則 $d(i)=GCD(\\{5,6\\})=1$&emsp;算出 equivalence class 裡的一個 node, 就等於算出了全部 nodes [Thm of Connection between Graphic and Matrix Representation in Ergodic Markov Chain]:&emsp;Markov chain is ergodic (defined in graphic representation)&emsp;$\\Longleftrightarrow \\exists m\\in \\mathbb{N}:p_{ij}^{(m)}\\neq0, \\forall i,j \\in \\mathcal{S} \\ldots (\\star)$ 我們舉個例子 $(\\star)$ 條件不成立. 我們發現$$\\left\\{m=5,11,17,...:p_{ij}^{(m)}\\neq0\\right\\} \\\\ \\left\\{m=1,7,13,...:p_{ji}^{(m)}\\neq0\\right\\}$$ 不存在一個 $m$ 能同時滿足所有 nodes 利用以上定理我們可以寫出 matrix representation for ergodic Markov chain[Ergodic Markov Chain using Matrix Representation]:&emsp;我們稱一個 Markov chain 是 ergodic, 如果滿足:&emsp;$p_{ij}^{(m)}\\neq0, \\forall i,j \\in \\mathcal{S}, \\forall m\\geq(M-1)^2+1$&emsp;其中 $M$ 是 states 數量, i.e. $|\\mathcal{S}|$ 課程試題:將 graph 圖畫出來, 可以發現滿足 graphic representation of ergodic Markov chain 的三個條件 用 matrix 的定義目前還不知道怎麼驗證 $\\forall m\\geq26$ 都滿足 [Ergodic Theorem]:&emsp;令 $X_t$ 為一個 ergodic Markov chain, (i.e. 1-class equivalence, recurrent, and aperiodic.), 則極限值存在:&emsp;$\\exists \\lim_{n\\rightarrow\\infty}p_{ij}^{(n)} = \\pi_j^* &gt; 0$&emsp;注意到已經跟出發點的 state $i$ 無關了, 且 strictly positive. 另外 $\\vec\\pi$ 的定義如下:&emsp;$\\vec{\\pi}^* = (\\vec{\\pi}_1^*,...,\\vec{\\pi}_M^*), \\text{ with } \\sum_{j=1}^M \\pi_j^* = 1$ $i$ transition $n$ 步到 $j$ 的機率, 當 $n\\rightarrow\\infty$ 時會收斂, 並且 $&gt;0$ [Corollary]:&emsp;令 $X_t$ 為一個 ergodic Markov chain, 則 Ergodic theorem 成立, 則我們有:&emsp;1. $\\vec{\\pi}^*$ is stationary distribution, i.e. $\\vec{\\pi}^*=\\vec{\\pi}^*P$&emsp;2. $\\lim_{n\\rightarrow\\infty}\\mathcal{P}\\{X_n=j\\}=\\pi_j^*$[Proof]: [Example]: Applications of the Markov ChainsApplications of the Markov Chains.pdf","tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://bobondemon.github.io/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"https://bobondemon.github.io/tags/Stochastic-Processes/"},{"name":"Markov Chains","slug":"Markov-Chains","permalink":"https://bobondemon.github.io/tags/Markov-Chains/"},{"name":"Ergodic Markov Chains","slug":"Ergodic-Markov-Chains","permalink":"https://bobondemon.github.io/tags/Ergodic-Markov-Chains/"}]},{"title":"Stochastic Processes Week 2 Poisson Processes","date":"2021-12-12T00:42:36.000Z","path":"2021/12/12/Stochastic-Processes-Week-2-Poisson-Processes/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes (本文) Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes Week 2.1-4: Definition of a Poisson process as a special example of renewal process. Exact forms of the distributions of the renewal process and the counting processJust a recap令我們有如上面所述的 renewal process[Poisson Processes Def1]:&emsp;A Poisson process 是一個 renewal process, 且 $\\xi_i\\sim p(x)=\\lambda e^{-\\lambda x}\\cdot \\mathbf{1}(x&gt;0)$ which is exponential density function&emsp;此時 $\\mathbf{1}(\\cdot)$ 為 indicator function, 而 $\\lambda$ 稱為 intensity parameter, 或 rate of Poisson process 簡單說就是 exponential random variables 的 renewal process此時的 renewal process 的 $S_n$ (arrival time process) and $N_t$ (counting process) 有 closed form [Thm] Arrival time process $S_n$ 的 distribution and density functions:&emsp;$$F_{S_n}(x)=\\left\\{ \\begin{array} {rl} 1-e^{-\\lambda x} \\sum_{k=0}^{n-1}\\frac{(\\lambda x)^k}{k!}, &amp; x&gt;0 \\\\ 0, &amp; x&lt;0 \\end{array} \\right. \\\\ \\mathcal{P}_{S_n}(x)=\\lambda\\frac{(\\lambda x)^{n-1}}{(n-1)!}e^{-\\lambda x}\\cdot \\mathbf{1}(x&gt;0)$$ [Proof]:&emsp;我們證明 p.d.f., 使用數學歸納法&emsp;考慮 $n=1$ 時 $\\mathcal{P}_{S_1}=?$&emsp;$S_1=\\xi_1\\sim \\lambda e^{-\\lambda x}\\cdot \\mathbf{1}(x&gt;0)$&emsp;假設 $n$ 成立, 考慮 $n+1$&emsp;$$\\mathcal{P}_{S_{n+1}}(x) = \\int_{y=0}^x \\mathcal{P}_{S_n}(x-y) \\cdot \\mathcal{P}_{\\xi_{n+1}}(y) dy \\\\ = \\int_{y=0}^x \\frac{\\lambda^n(x-y)^{n-1}}{(n-1)!}e^{-\\lambda (x-y)} \\lambda e^{-\\lambda y} dy \\\\ = \\int_{y=0}^x \\frac{\\lambda^{n+1}(x-y)^{n-1}}{(n-1)!}e^{-\\lambda x} dy \\\\ = \\frac{\\lambda^{n+1}}{(n-1)!}e^{-\\lambda x} \\int_{y=0}^x (x-y)^{n-1}dy \\\\ = \\frac{\\lambda^{n+1}}{(n-1)!} e^{-\\lambda x} \\cdot \\frac{x^n}{n} = \\lambda\\frac{(\\lambda x)^n}{n!} e^{-\\lambda x}$$ [Thm] Counting process $N_t$ 是 Poisson distribution, $\\mathcal{P}{N_t=n}\\sim Pois(\\lambda t)$:&emsp;$${\\color{orange} { \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\lambda t)=e^{-\\lambda t}\\frac{(\\lambda t)^n}{n!} } }$$[Proof]:&emsp;我們有 $\\mathcal{P}\\{N_t=n\\}=\\mathcal{P}\\{ S_n\\leq t\\} - \\mathcal{P}\\{ S_{n+1}\\leq t\\}\\ldots(\\star)$&emsp;L.H.S. 意思是, 到時間 $t$ 為止正好發生 $n$ 次事件的機率&emsp;R.H.S. 以下兩種都可以解釋:&emsp;1. 到時間 $t$ 為止已發生至少 $n$ 次事件的機率, 扣掉, 到時間 $t$ 為止已發生至少 $n+1$ 次事件的機率&emsp;&emsp;—&gt; 很顯然只會剩下到時間 $t$ 為止正好發生 $n$ 次事件的機率&emsp;2. 第 $n$ 個事件發生的時間比 $t$ 小的機率, 扣掉, 第 $n+1$ 個事件發生的時間比 $t$ 小的機率.&emsp;&emsp;$\\{N_t=n\\}=\\{S_n\\leq t\\} \\cap \\{S_{n+1}&gt;t\\}$ 用 $A \\cap B$ 表示, 由於&emsp;$$\\left. \\begin{array}{r} A\\cap B=A\\backslash B^c \\\\ B^c\\subset A \\end{array} \\right\\} \\Rightarrow \\mathcal{P}(A\\cap B)=\\mathcal{P}(A)-\\mathcal{P}(B^c)$$&emsp;因此&emsp;$$(\\star) = \\left( 1-e^{-\\lambda t} \\sum_{k=0}^{n-1}\\frac{(\\lambda t)^k}{k!} \\right) - \\left( 1-e^{-\\lambda t} \\sum_{k=0}^{n}\\frac{(\\lambda t)^k}{k!} \\right) \\\\ = e^{-\\lambda t}\\frac{(\\lambda t)^n}{n!}$$ Week 2.5: Memoryless property[Def] Memoryless Property:&emsp;A random variable $X$ possesses the memoryless property, $iff$&emsp;$\\mathcal{P}\\{X&gt;u+v\\} = \\mathcal{P}\\{X&gt;u\\} \\mathcal{P}\\{X&gt;v\\}$ If $\\mathcal{P}\\{x&gt;v\\}&gt;0$, then$\\mathcal{P}\\{ X&gt;u+v \\vert X&gt;v \\} = \\mathcal{P}\\{ X&gt;u \\}\\ldots(\\square)$這是因為$$\\mathcal{P}\\{X&gt;u+v\\} = \\mathcal{P}\\{X&gt;u+v , X&gt;v\\}=\\mathcal{P}\\{X&gt;u+v \\vert X&gt;v\\}\\mathcal{P}\\{X&gt;v\\} \\\\ \\text{mem.less prop.}\\Rightarrow \\mathcal{P}\\{X&gt;u\\} \\mathcal{P}\\{X&gt;v\\} = \\mathcal{P}\\{X&gt;u+v \\vert X&gt;v\\}\\mathcal{P}\\{X&gt;v\\} \\\\ \\Rightarrow \\mathcal{P}\\{X&gt;u\\} = \\mathcal{P}\\{X&gt;u+v \\vert X&gt;v\\}$$ 從這就可以看出為何稱 memoryless. 這是因為 已經等了 $v$ 的時間, 要再多等 $u$ 的時間, 跟一開始就等 $u$ 的時間的機率是一樣的 再來有一個定理可以看出 Poisson process 適不適合用來 model 一個問題 [Thm] Memoryless is exactly exponential distribution:&emsp;Let $X$ be a random variable with density $p(x)$, 則&emsp;$X\\text{ memoryless } \\Longleftrightarrow p(x)=\\lambda e^{-\\lambda x}\\text{, }X\\sim\\text{ exponential p.d.f.}$ 範例: 公車每 $20 \\pm 2$ 分鐘來一班, 也就是說 $X$ 是公車到的時間的 r.v. 值在 $20 \\pm 2$. 這個問題是否可以用 Poisson process 來 model ? (檢察 r.v. $X$ 是否具有 memoryless property)令 $v=19$, $u=10$, 則考慮 $(\\square)$L.H.S. 為 $\\mathcal{P}\\{X&gt;29|X&gt;19\\}=0$R.H.S. 為 $\\mathcal{P}\\{ X&gt;10 \\}=1$兩者不相等, 所以不具有 memoryless property, 因此此 random variable 不能用在 renewal process 讓它成為 Poisson process [Quiz]: Week 2.6-7: Other definitions of Poisson processes之前定義 Poisson process 是 renewal process 的一個特例 (當 $\\xi$ 是 exponential distribution)現在給出另一種定義, 這種定義跟 Poisson process 與 Levy precess 的關聯有很大的關係. (之後才會知道) [Poisson Processes Def2] from Levy precess:&emsp;Poisson process is an integer-valued process, $N_t$, with $N_0=0$ a.s. (almost surely), such that&emsp;1. $N_t$ has independent increments&emsp;&emsp;$\\forall t_0&lt;t_1&lt;...&lt;t_n$, we have $N_{t_1}-N_{t_0}, ..., N_{t_n}-N_{t_{n-1}}$ are independent&emsp;2. $N_t$ has stationary increments&emsp;&emsp;$N_t-N_s$ 與 $N_{t-s}$ 具有相同的 distribution&emsp;3. $N_t-N_s\\sim Pois(\\lambda(t-s))$&emsp;&emsp;$${\\color{orange} { \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\lambda t)=e^{-\\lambda t}\\frac{(\\lambda t)^n}{n!} } }$$ 特性 3 可以推導出特性 2$X \\sim Pois(\\mu)$ has $\\mathbb{E}[X]=Var[X]=\\mu$ 我們再來推導一個定理, 該定理讓我們有第三種 Poisson process 的定義. 且跟 queueing theory 有關聯.當在一個極短的時間段之內, Poisson process 能分成三種情況:&emsp;1. 沒有事件發生&emsp;2. 事件發生 $1$ 次&emsp;3. 事件發生 $\\geq 2$ 次 定義 $f(h)=o(g(h))$ 為$\\lim_{h\\rightarrow0}\\frac{f(h)}{g(h)}=0$ 直觀理解為 $f(h)$ 比 $g(h)$ 小的更快. [Thm] Poisson process 在極小時間段的行為:&emsp;$$\\left\\{ \\begin{array}{rl} \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1 -\\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h), &amp; h\\rightarrow0 \\end{array} \\right.$$[Proof]:&emsp;我們先證明 $=0$ 的情況&emsp;根據之前的 定義2 我們知道 $\\mathcal{P}\\{N_{t+h}-N_t=0\\}=e^{-\\lambda h}$, 則&emsp;$$\\lim_{h\\rightarrow0}\\frac{1-\\mathcal{P}\\{N_{t+h}-N_t=0\\}}{h} = \\lim_{h\\rightarrow0}\\frac{1-e^{-\\lambda h}}{h}\\\\ \\text{by L&apos;Hospital&apos;s rule} = \\lim_{h\\rightarrow0}\\frac{\\lambda e^{-\\lambda h}}{1} = \\lambda$$&emsp;i.e., as $h\\rightarrow0$,&emsp;$$\\frac{1-\\mathcal{P}\\{N_{t+h}-N_t=0\\}}{h} = \\lambda + o(1) \\\\ \\Rightarrow \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1-\\lambda h + ho(1) \\\\ = 1-\\lambda h + o(h)$$&emsp;$o(h)$ 的正負號不重要, Q.E.D.&emsp;而 $\\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h), h\\rightarrow0$ 這一條可以藉由計算&emsp;$$\\lim_{h\\rightarrow0}\\frac{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}{h} = \\lim_{h\\rightarrow0}\\frac{e^{-\\lambda h}\\lambda h}{h}\\\\ \\text{by L&apos;Hospital&apos;s rule} = \\lim_{h\\rightarrow0}\\frac{-\\lambda e^{-\\lambda h}\\lambda h + \\lambda e^{-\\lambda h}}{1} = \\lambda$$&emsp;i.e., as $h\\rightarrow0$,&emsp;$$\\frac{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}{h} = \\lambda + o(1) \\\\ \\Rightarrow \\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h)$$&emsp;Q.E.D. &emsp;最後, $\\mathcal{P}\\{N_{t+h}-N_t\\geq2\\}=1-\\mathcal{P}\\{N_{t+h}-N_t=0\\} - \\mathcal{P}\\{N_{t+h}-N_t=1\\}$ 可以計算得到&emsp;注意到 $-o(h)=o(h)$, $2o(h)=o(h)$&emsp;Q.E.D. [Poisson Processes Def3] from queueing process:&emsp;Poisson process is an integer-valued process, $N_t$, with $N_0=0$ a.s. (almost surely), such that&emsp;1. $N_t$ has independent increments&emsp;&emsp; $\\forall t_0&lt;t_1&lt;...&lt;t_n$, we have $N_{t_1}-N_{t_0}, ..., N_{t_n}-N_{t_{n-1}}$ are independent&emsp;2. $N_t$ has stationary increments&emsp;&emsp;$N_t-N_s$ 與 $N_{t-s}$ 具有相同的 distribution&emsp;3. 滿足:&emsp;&emsp;$\\lim_{h\\rightarrow0} \\frac{\\mathcal{P}\\{N_{t+h} - N_t\\geq2\\}}{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}=0$ 定義 2 和定義 3 的差別只在第 3 點的條件. 然而兩種定義等價.我們其實已經證明了 (定義 2 $\\Rightarrow$ 定義 3), 但另一個方向還沒有 Week 2.8-9: Non-homogeneous Poisson processes也可以參考 https://www.randomservices.org/random/poisson/Nonhomogeneous.html, 但對我來說有點難懂 [Non-homogeneous Poisson Processes Def1]:&emsp;Let $\\Lambda(t)$ be a differentiable increasing function, and $\\Lambda(0)=0$,&emsp;$N_t$ is a N.H.P.P. (Non-Homogeneous Poisson Processes), if&emsp;1. $N_0=0$&emsp;2. $N_t$ has independent increments&emsp;3. $N_t-N_s \\sim Pois(\\Lambda(t)-\\Lambda(s))$&emsp;&emsp;$${\\color{orange} { \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\Lambda(t))=e^{-\\Lambda(t)}\\frac{\\Lambda(t)^n}{n!} } }$$&emsp;定義 $\\lambda(t) = \\Lambda’(t)$, 稱 intensity function 在 P.P. 的定義 2, 我們知道 $N_t-N_s\\sim Pois(\\lambda(t-s))$, 所以當 $\\Lambda(t)=\\lambda t$ 的話, non-homogeneous P.P. 等於 P.P. 我們可以發現 non-homogeneous 去掉 stationary increment 特性, 也就是 Poisson distribution 的 rate $\\lambda$ 在每個時間段落不一定會都一樣, depends on $\\Lambda(t)$ [Properties of N.H.P.P.]:&emsp;1. $\\mathbb{E}[N_t]=\\Lambda(t)$&emsp;&emsp;我們算一下 $\\mathbb{E}[N_t]$ for N.H.P.P.:&emsp;&emsp;$N_t = N_t - N_0 \\sim Pois(\\Lambda(t)-\\Lambda(0))=Pois(\\Lambda(t))$&emsp;&emsp;$\\therefore \\mathbb{E}[N_t]=\\Lambda(t)$&emsp;2. 如果 $\\lambda(t)=\\text{const} \\Rightarrow \\Lambda(t) = \\text{const}\\cdot t$, 回退到原來的 (homogeneous) P.P.&emsp;3. 因為 $\\Lambda(t)$ is differentialble and increasing, 所以 $\\Lambda^{-1}(t)$ 存在&emsp;&emsp;讓我們假設 $\\text{Image}(\\Lambda(t))=\\mathbb{R}^+$, 所以 $\\Lambda^{-1}(t)$ 對於 $t\\in\\mathbb{R}^+$ 都是 well defined&emsp;&emsp;對於一個 N.H.P.P. 的 $N_t$ 來說, 考慮 $N_{\\Lambda^{-1}(t)}$ 這些 r.v.s 的話, 會發現變成了 homogeneous P.P. 了! 第三點提供了一個 N.H.P.P. 與 P.P. 的對應方法但具體來說, 如果一個 N.H.P.P. 剛好是 P.P. 的話, iff 他也是個 renewal process, 下一節證明 Week 2.10-12: Relation between renewal theory and non-homogeneous Poisson processes我們試著從一個 $N_t$ 是 N.H.P.P. 去建構 renewal process 看看, 我們有如下的 N.H.P.P.Renewal process 的 arrival time $S_n$ 可以這麼構建$S_n=\\arg\\min_t\\{N_t=n\\}$ $\\{N_t=n\\}$ 表示發生 $n$ 次事件的時間的集合, 所以很顯然的取最小那個就是剛剛好第 $n$ 個事件發生的時間, i.e. $=S_n$有 $S_n$ 就可以得到 interarrival time $\\xi_n=S_n-S_{n-1}$ 這樣子的 arrival times $S_n$ and interarrival times $\\xi_n$ 是一個 renewal process 嗎?首先我們知道若要成為一個 renewal process, $\\xi_1, \\xi_2, …$ 必須是 i.i.d. 才行Note that:$${ \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\Lambda(t))=e^{-\\Lambda(t)}\\frac{\\Lambda(t)^n}{n!} }$$ 先證明以下兩個等式: $\\mathcal{P}_{\\xi_1}(x)=\\lambda(x)e^{-\\Lambda(x)}$:[Proof]:&emsp;$$\\mathcal{P}\\{\\xi_1\\leq x\\}=\\mathcal{P}\\{S_1\\leq x\\}=\\mathcal{P}\\{N_x\\geq 1\\} = 1 - \\mathcal{P}\\{N_x=0\\} \\\\ = 1- Pois(\\Lambda(x)) = 1-e^{-\\Lambda(x)}$$&emsp;用到 $\\{S_n\\leq t\\}=\\{N_t\\geq n\\}$, 然後左右等式微分得到:&emsp;$\\mathcal{P}_{\\xi_1}(x)=\\lambda(x)e^{-\\Lambda(x)}$&emsp;其中 $\\lambda=\\Lambda’$, Q.E.D. $\\mathcal{P}_{(\\xi_2|\\xi_1)}(t|s)=\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)}$:[Proof]:&emsp;先計算一下兩個 r.v.s $\\xi_1,\\xi_2$ 的 joint C.D.F. $\\mathcal{F}_{\\xi_1,\\xi_2}(s,t)$ &emsp;$$\\mathcal{F}_{(\\xi_1,\\xi_2)}(\\xi_1=s,\\xi_2=t)=\\mathcal{P}\\{\\xi_1\\leq s, \\xi_2\\leq t\\} \\\\ =\\int_{y=0}^s \\mathcal{P}\\{ \\xi_1\\leq s, \\xi_2\\leq t \\vert \\xi_1=y \\} \\mathcal{P}_{\\xi_1}(y)dy$$&emsp;$\\because y\\leq s$&emsp;$$=\\int_{y=0}^s \\mathcal{P}\\{ \\xi_2\\leq t \\vert \\xi_1=y \\} \\mathcal{P}_{\\xi_1}(y) dy \\\\ = \\int_{y=0}^s \\mathcal{P}\\{ N_{t+y} - N_y \\geq 1 \\vert \\xi_1 = y \\} \\mathcal{P}_{\\xi_1}(y) dy \\ldots(\\star)$$&emsp;由 N.H.P.P. 的 independent increments 特性知道 $(N_{t+y} - N_y),(N_y-N_0)$ 這兩個 r.v.s 為 independent. 因此&emsp;$$\\mathcal{P}\\{N_{t+y} - N_y \\geq 1 | \\xi_1=y\\} = \\mathcal{P}\\{ N_{t+y} - N_y \\geq 1 | N_y-N_0=1 \\} \\\\ = \\mathcal{P}\\{N_{t+y} - N_y \\geq 1 \\}$$&emsp;接續 $(\\star)$&emsp;$$(\\star) = \\int_{y=0}^s \\mathcal{P}\\{ N_{t+y}-N_y \\geq 1 \\} \\cdot \\mathcal{P}_{\\xi_1}(y) dy \\\\ = \\int_{y=0}^s (1 - \\mathcal{P}\\{ N_{t+y}-N_y = 0 \\}) \\cdot \\mathcal{P}_{\\xi_1}(y) dy \\\\ = \\int_{y=0}^s (1 - e^{-\\Lambda(t+y)+\\Lambda(y)}) \\cdot \\lambda(y)e^{-\\Lambda(y)} dy$$&emsp;所以兩個 r.v.s $\\xi_1,\\xi_2$ 的 joint C.D.F.&emsp;$\\mathcal{F}_{(\\xi_1,\\xi_2)}(s,t) = \\int_{y=0}^s (1 - e^{-\\Lambda(t+y)+\\Lambda(y)}) \\cdot \\lambda(y)e^{-\\Lambda(y)} dy$&emsp;微分可以計算 P.D.F.&emsp;$$\\mathcal{P}_{(\\xi_1,\\xi_2)}(s,t) = \\frac{\\partial}{\\partial t} \\left( \\frac{\\partial}{\\partial s} \\mathcal{F}_{(\\xi_1,\\xi_2)}(s,t) \\right) \\\\ \\text{replace y by s} = \\frac{\\partial}{\\partial t} \\left( (1 - e^{-\\Lambda(t+s)+\\Lambda(s)}) \\cdot \\lambda(s)e^{-\\Lambda(s)} \\right) \\\\ = \\lambda(t+s) e^{-\\Lambda(t+s)+\\Lambda(s)}\\cdot \\lambda(s) e^{-\\Lambda(s)} \\\\ = \\lambda(t+s) e^{-\\Lambda(t+s)+\\Lambda(s)} \\cdot \\mathcal{P}_{\\xi_1}(s)$$&emsp;所以&emsp;$$\\mathcal{P}_{(\\xi_2|\\xi_1)}(t|s) = \\frac{\\mathcal{P}_{(\\xi_1,\\xi_2)}(s,t)}{\\mathcal{P}_{\\xi_1}(s)} \\\\ =\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)}$$&emsp;Q.E.D. 回到考慮什麼情況下的 N.H.P.P. 的 $\\xi_1, \\xi_2, …$ 是 i.i.d. 這個問題我們先假設 $\\xi_1, \\xi_2, …$ 是 i.i.d., i.e. 假設 N.H.P.P. 是 renewal process, 則$$\\mathcal{P}_{\\xi_2 | \\xi_1}(t|s)=\\mathcal{P}_{\\xi_2}(t) \\\\ \\because\\text{i.i.d.}=\\mathcal{P}_{\\xi_1}(t), \\forall t,s&gt;0$$帶入我們花很多力氣推導的上述兩個結果得到:$\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)} = \\lambda(t)e^{-\\Lambda(t)}$然後對兩邊都做積分 $\\int_0^T\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)} dt = \\int_0^T \\lambda(t)e^{-\\Lambda(t)} dt \\ldots (\\square)$ $(\\square)$ R.H.S.:$$\\int_0^T \\lambda(t)e^{-\\Lambda(t)} dt = \\int_0^T e^{-\\Lambda(t)}d\\Lambda(t) \\left(= \\int_0^Te^{-y}dy\\right) \\\\ = -e^{-\\Lambda(T)}+e^{-\\Lambda(0)} = 1 -e^{-\\Lambda(T)}$$ $(\\square)$ L.H.S. 同理$$\\int_0^T\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)} dt = e^{\\Lambda(s)}\\int_0^T e^{-\\Lambda(t+s)} d\\Lambda(t+s) \\\\ \\left(\\text{this as: }e^{\\Lambda(s)}\\int e^{-y}dy\\right) \\\\ = e^{\\Lambda(s)} \\left[\\left. -e^{\\Lambda(t+s)}\\right|_{t=0}^T \\right] = e^{\\Lambda(s)}\\left[-e^{-\\Lambda(T+s)}+e^{-\\Lambda(s)}\\right] = -e^{-\\Lambda(T+s)+\\Lambda(s)}+e^0$$ 所以 L.H.S = R.H.S.$$\\Rightarrow e^0-e^{-\\Lambda(T+s)+\\Lambda(s)} = 1 - e^{-\\Lambda(T)} \\\\ \\Rightarrow \\Lambda(T+s) - \\Lambda(s) = \\Lambda(T), \\forall T,s&gt;0$$ 因為 $\\Lambda$ is increasing function, 上式知道是 linear function, 所以會得到 $\\Lambda(t)=\\text{const}\\cdot t$而這正好表明了 N.H.P.P. 變成了 homogeneous P.P. 了 結論是:N.H.P.P. is a renewal process $\\Longleftrightarrow$ $\\Lambda (t)=\\lambda t$ (i.e. 此時的 N.H.P.P. 也變成 homogenous 了) $(\\Longleftarrow)$ 證明很容易, 因為 homogeneous P.P. 是 renewal process 換句話說 N.H.P.P. is a renewal process $\\Longleftrightarrow$ it is homogenous P.P. Week 2.13: Elements of the queueing theory. M/G/k systems-1我們先前證明過 [Thm] Poisson process 在極小時間段:&emsp;$$\\left\\{ \\begin{array}{rl} \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1 -\\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h), &amp; h\\rightarrow0 \\end{array} \\right.$$ 對於 N.H.P.P. 也有類似的結果 [Thm] Non-homogeneous Poisson process 在極小時間段:&emsp;$$\\left\\{ \\begin{array}{rl} \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1 -{\\color{orange}{\\lambda(t)}} h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t=1\\} = {\\color{orange}\\lambda(t)} h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h), &amp; h\\rightarrow0 \\end{array} \\right.$$ 所以類似的對於 N.H.P.P. 我們也有另一種定義 (類似 [Poisson Processes Def3]) [Non-homogeneous Poisson Processes Def2]:&emsp;Let $\\Lambda(t)$ be a differentiable increasing function, and $\\Lambda(0)=0$,&emsp;$N_t$ is a N.H.P.P. (Non-Homogeneous Poisson Processes), if&emsp;1. $N_0=0$&emsp;2. $N_t$ has independent increments&emsp;3. 滿足:&emsp;&emsp;$\\lim_{h\\rightarrow0} \\frac{\\mathcal{P}\\{N_{t+h} - N_t\\geq2\\}}{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}=0$ (Non-homogeneous) Poisson processes 很適合用來 modeling queueing processes. 我們用 $M,D,G$ 來表示 distribution 種類: $M$: exponential distribution. 所以如果用來描述 arrival processes (memoryless) 就會變成 Poisson processes $D$: deterministic (constant distribution, 即不受 time $t$ 的影響? 還是連 value 都是 constant?) $G$: general, 表示可以是任何 distribution Queueing processes 包含了三個字母, e.g. $M/G/k$, 分別表示 Arrival process, Service time, 和 Number of servers Arrival process: $\\in\\{M,D,G\\}$ Service time: $\\in\\{M,D,G\\}$. 同時為 I.I.D. Number of servers: $\\in\\{1, 2, ..., \\infty\\}$ 我們以學生上機房用電腦來舉例, 若用 queueing process 為 $M/G/\\infty$ 來描述的話Arrival processes (by Poisson process) 描述了學生來的 $N(t)$Service time 表示學生會用多久電腦, 用 $G_Y(t)$ 這個 distribution 描述而電腦 (server) 的數量為 $\\infty$, 表示學生一到就馬上有一台電腦可以用, 無需等待 (更複雜的 queueing process 會考慮等待時間)所以 $N(t)$ 是一個 Poisson process, 考慮一個 fixed time $\\tau&gt;0$, 我們會有兩個 processes $N_1(t)$ and $N_2(t)$.$N_1(t)$ 表示有多少 還在處理 的事件 at time $\\tau$ (即在時間 $\\tau$ 還有多少學生在用電腦)$N_2(t)$ 表示有多少 已處理完 的事件 at time $\\tau$ (即在時間 $\\tau$ 已有多少學生用完電腦) $t$ and $\\tau$ 的關係在討論區有人這麼回答 考慮 $N_1(t+h) - N_1(t)$, 這個值表示在這一段時間內共來了多少事件並且還在處理 (即這段時間來了多少學生, 並且這些學生都還在用電腦), 所以:$$\\mathcal{P}\\{ N_1(t+h)-N_1(t)=1 \\} \\\\ = \\mathcal{P}\\{ N(t+h)-N(t)=1 \\} \\cdot \\mathcal{P}\\{ Y&gt;\\tau-t \\} + o(h) \\ldots(\\blacktriangle)$$ $Y$ 表示 service time 的 random variable, 其 distribution 為 $G_Y(t)$ “這段 $h$ 時間來了多少學生, 並且這些學生都還在用電腦” 可以近似於:“這段 $h$ 時間來了 $1$ 個學生, 並且這 $1$ 個學生還在用電腦” + $o(h)$注意到 $2$ 個學生以上的情形我們用 $o(h)$ 表示即可, 這是因為 Poisson process 的一個性質:$\\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h),h\\rightarrow0$ 所以用上開頭的 Theorem,$$(\\blacktriangle) = (\\lambda h + o(h)) \\cdot (1-G_Y(\\tau-t)) + o(h) \\\\ = \\lambda h(1-G_Y(\\tau-t)) + o(h)$$ 重述一遍, 我們有:$\\mathcal{P}\\{ N_1(t+h)-N_1(t)=1 \\} = \\lambda(1-G_Y(\\tau-t))h + o(\\delta)$令 $\\lambda(1-G_Y(\\tau-t))$ 等於某個 function $\\lambda_1(t)$, 則上式與 N.H.P.P. 的性質結果相同.同樣可以對 $\\mathcal{P}\\{ N_1(t+h)-N_1(t)=0 \\}$ 和 $\\mathcal{P}\\{ N_1(t+h)-N_1(t)\\geq2 \\}$ 推導出相同結果. 所以根據 [Non-homogeneous Poisson Processes Def2] 我們得到 $N_1$ 是 N.H.P.P. 的結論 $N_1$ 是 N.H.P.P. 且其 intensity function $\\lambda_1(t)=\\lambda(1-G_Y(\\tau-t))$ 我們對於 $N_1$ 的推論同樣也可以用在 $N_2$ 上, 結果也是一樣: $N_2$ 是 N.H.P.P. 且其 intensity function $\\lambda_2(t)=\\lambda \\cdot G_Y(\\tau-t)$ 下一節課會證明 $N_1$ and $N_2$ 為互相獨立的 r.v.s Week 2.14: Elements of the queueing theory. M/G/k systems-2欲證 $\\mathcal{P}\\{N_1(t)=n_1, N_2(t)=n_2\\} = \\mathcal{P}\\{N_1(t)=n_1\\}\\mathcal{P}\\{ N_2(t)=n_2\\}$$$\\mathcal{P}\\{N_1(t)=n_1, N_2(t)=n_2\\}\\\\ = \\mathcal{P}\\{ N_1(t)=n_1, N_2(t)=n_2 | N(t)=n_1+n_2 \\} \\cdot \\mathcal{P}\\{N(t)=n_1+n_2\\} \\\\ = \\mathcal{C}_{n_1}^{n_1+n_2}(1-G(\\tau-t))^{n_1}(G(\\tau-t))^{n_2} \\cdot e^{-\\lambda t}\\frac{(\\lambda t)^{n_1+n_2}}{(n_1+n_2)!} \\ldots(=)$$$(=)$ 為 Binomial term, 把”成功”的機率當成 “事件在時間 $\\tau$ 還在served的機率”, 這個機率由 service time 的 distribution probability 可以知道:$\\mathcal{P}\\{Y&gt;(\\tau-t)\\}=1-\\mathcal{P}\\{Y\\leq(\\tau-t)\\}=1-G(\\tau-t)$ 所以失敗的話就是 $G(\\tau-t)$共有 $n$ 個事件, 共有 $n_1$ 個事件 $\\in N_1(t)$, and $n_2$ 個事件 $\\in N_2(t)$, 所以 $\\mathcal{C}_{n_1}^{n_1+n_2}$ $$(=)= \\frac {(\\lambda t(1-G(\\tau-t)))^{n_1}}{n_1!} e^{-\\lambda t(1-G(\\tau-t))} \\cdot \\frac {(\\lambda tG(\\tau-t))^{n_2}}{n_2!} e^{-\\lambda tG(\\tau-t)} \\\\ =\\mathcal{P}\\{N_1(t)=n_1\\}\\mathcal{P}\\{N_2(t)=n_2\\}$$ 我不知道的是為何 $\\frac {(\\lambda t(1-G(\\tau-t)))^{n_1}}{n_1!} e^{-\\lambda t(1-G(\\tau-t))} = \\mathcal{P}\\{N_1(t)=n_1\\}$因為只知道 $N_1$ 是 N.H.P.P. 且其 intensity function $\\lambda_1(t)=\\lambda(1-G_Y(\\tau-t))$所以必須求得 $\\Lambda_1(t)$ 才能代入$\\mathcal{P}\\{N_t=n\\} \\sim Pois(\\Lambda(t))=e^{-\\Lambda(t)}\\frac{\\Lambda(t)^n}{n!}$所以看起來 $\\Lambda_1(t)=\\lambda t(1-G(\\tau-t))?$ 不懂… Q.E.D. Week 2.15-17: Compound Poisson processes可參考一個淺顯易懂的定義: https://gtribello.github.io/mathNET/COMPOUND_POISSON_PROCESS.html [Compound Poisson Processes (C.P.P.) Def]:&emsp;$X_t=\\sum_{k=1}^{N_t} \\xi_k$&emsp;其中&emsp;- $N_t$ 是 Poisson process with intensity $\\lambda$&emsp;- $\\xi_1,\\xi_2,…$ are i.i.d.&emsp;- $\\xi_1,\\xi_2,…$ and $N_t$ are independent $X_t$ 的 distribution 沒有 cloded form, 但若是某些特定的 $\\xi$ distribution 可以算出來. 注意到若 $\\xi_k=1$, 則 $X_t=N_t$, 可藉此想像一下 C.P.P. 的物理意義 如果 $\\xi$ (C.P.P.) 是 non-negative integer values, 我們使用 PGF 幫助計算[Probability Generating Function (PGF) Def]:&emsp;Let $\\xi$ 是一個 integer 的 random variable, with $\\geq 0$ values. 則 PGF 定義為:&emsp;$\\varphi_\\xi(u)=\\mathbb{E}[u^\\xi], \\text{ where }|u|&lt;1$ 根據定義我們可以得到 (expectation 寫出來), 如果 $\\xi_1 \\perp \\xi_2$, 則 $\\varphi_{\\xi_1+\\xi_2}(u)=\\varphi_{\\xi_1}(u)\\varphi_{\\xi_2}(u)$ 如果 $\\xi$ (C.P.P.) 是 non-negative (real) values, 我們使用 MGF 幫助計算[Moment Generating Function (MGF) Def]:&emsp;跟 Laplace transform 密切相關.&emsp;$\\mathcal{L}_\\xi(u)=\\mathbb{E}[e^{-u\\xi}], \\text{ where } \\xi\\geq0, u&gt;0$ 其實就是 $\\mathcal{L}_f(s)=\\int_{x=0}^\\infty e^{-sx}f(x)dx$, 將 $f$ 以 $\\xi$ 的 P.D.F. 代入 根據定義我們可以得到, 如果 $\\xi_1 \\perp \\xi_2$, 則 $\\mathcal{L}_{\\xi_1+\\xi_2}(u)=\\mathcal{L}_{\\xi_1}(u)\\mathcal{L}_{\\xi_2}(u)$. 注意到 ${\\xi_1+\\xi_2}$ 其 P.D.F. 是 $\\xi_1,\\xi_2$ 的 P.D.F.s 的 convolution. 之前我們也證過 Laplace transform 這個性質 對於 $\\xi$ (C.P.P.) 是 general case 的情況來說, 我們需借助 characteristic function 幫忙[Characteristic Function Def]:&emsp;For random variable $\\xi$, 定義 characteristic function $\\Phi:\\mathbb{R}\\rightarrow \\mathbb{C}$ 為&emsp;$\\Phi_\\xi(u) = \\mathbb{E}\\left[ e^{iu\\xi} \\right]$ 同樣根據定義我們可以得到, 如果 $\\xi_1 \\perp \\xi_2$, 則 $\\Phi_{\\xi_1+\\xi_2}(u)=\\Phi_{\\xi_1}(u)\\Phi_{\\xi_2}(u)$ [Characteristic Function of Increment of C.P.P.]:&emsp;For $t&gt;s\\geq0$, and $X_t$ is a C.P.P., we have&emsp;$\\Phi_{X_t-X_s}(u)=e^{\\lambda(t-s)(\\Phi_{\\xi_1}(u)-1)}$[Proof]:&emsp;$$\\Phi_{X_t-X_s}(u)=\\mathbb{E}\\left[ e^{iu(X_t-X_s)} \\right] \\\\ =\\sum_{k=0}^\\infty {\\color{orange} {\\mathbb{E}\\left[ \\left. e^{iu(X_t-X_s)} \\right| N_t-N_s=k\\right]} } \\cdot {\\color{green} {\\mathcal{P}\\{N_t-N_s=k\\}} }$$&emsp;注意到已知 $N_t-N_s=k$ 的情況下, $X_t-X_s$ 根據 C.P.P. 的定義就是 $\\xi_1+…+\\xi_k$, 再加上我們知道 $\\xi\\perp N_t$, 所以橘色部分的 condition 就可以拔掉. 同時已知綠色部分為 Poisson Processes.&emsp;因此:&emsp;$$= \\sum_{k=0}^\\infty \\mathbb{E}\\left[e^{iu(X_t-X_s)}\\right] \\cdot Pois(\\lambda(t-s)) \\\\ = \\sum_{k=0}^\\infty (\\Phi_{\\xi_1}(u))^k \\cdot e^{-\\lambda(t-s)}\\frac{(\\lambda(t-s))^k}{k!} \\\\ =e^{-\\lambda(t-s)}\\sum_{k=0}^\\infty\\frac{(\\Phi_{\\xi_1}(u)\\lambda(t-s))^k}{k!} \\\\ = e^{-\\lambda(t-s)}e^{\\Phi_{\\xi_1}(u)\\lambda(t-s)} =e^{\\lambda(t-s)(\\Phi_{\\xi_1}(u)-1)}$$&emsp;Q.E.D. 此定理描述了 increment of C.P.P. 的 characteristic function, 其具有 closed form solution所以 characteristic function of $X_t$ is$\\Phi_{X_t}(u)=e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)}$ 課程老師說這個 Theorem 很重要, 可以根據它推導出很多 Corollaries [Expectation and Variance of C.P.P.]:&emsp;對於此節定義的 C.P.P. 我們有&emsp;$$\\mathbb{E}[X_t]=\\lambda t\\mathbb{E}[\\xi_1] \\\\ Var[X_t]=\\lambda t\\mathbb{E}[\\xi_1^2]$$ 原來的 Poisson distribution $Pois(\\lambda t)$ 的 mean and variance 為 $\\lambda t$, 所以 C.P.P. 等於多乘上 $\\xi_1$ 的 moments [Proof]:&emsp;課程證 expectation, 而 variance 可用同樣流程證明&emsp;$\\mathbb{E}[\\xi^r]&lt;\\infty\\Rightarrow\\Phi_\\xi(u)$ is r-times 可微 at 0&emsp;因為 derivative and expectation 都是線性的, 所以我們有&emsp;$\\Phi^{(1)}_\\xi(u)=\\frac{d}{du}\\mathbb{E}[e^{iu\\xi}]=\\mathbb{E}[(i\\xi)e^{iu\\xi}]$&emsp;$\\Phi^{(2)}_\\xi(u)=\\frac{d}{du}\\frac{d}{du}\\mathbb{E}[e^{iu\\xi}]=\\mathbb{E}[(i\\xi)^2 e^{iu\\xi}]$&emsp;$…$&emsp;$\\Phi^{(r)}_\\xi(u)=\\mathbb{E}[(i\\xi)^r e^{iu\\xi}]$&emsp;$\\therefore \\Phi_\\xi^{(r)}(0)=i^r\\cdot\\mathbb{E}[\\xi^r]$&emsp;由前面的 Theorem 知道 $\\Phi_{X_t}(u)=e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)}$, 所以&emsp;$$\\Phi_{X_t}^{(1)}(u)=\\frac{d}{du}\\Phi_{X_t}(u) =\\frac{d}{du}e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)} \\\\ =\\lambda t \\Phi_{\\xi_1}^{(1)}(u)e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)} = \\lambda t \\Phi_{\\xi_1}^{(1)}(u)\\Phi_{X_t}(u)$$&emsp;計算 $\\mathbb{E}[X_t]$:&emsp;$$\\mathbb{E}[X_t]=\\frac{\\Phi_{X_t}^{(1)}(0)}{i} = \\frac{\\lambda t \\Phi_{ \\xi_1}^{(1)}(0) \\overbrace{\\Phi_{X_t}(0)}^{=1} } {i}$$&emsp;而根據 characteristic function 的特性&emsp;$\\frac{\\Phi_{\\xi_1}^{(1)}(0)}{i} = \\mathbb{E}[\\xi_1]$&emsp;所以 $\\mathbb{E}[X_t]=\\lambda t\\mathbb{E}[\\xi_1]$&emsp;Q.E.D. Applications of the Poisson Processes and Related ModelsApplications of the Poisson Processes and Related Models.pdf","tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://bobondemon.github.io/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"https://bobondemon.github.io/tags/Stochastic-Processes/"},{"name":"Poisson Process","slug":"Poisson-Process","permalink":"https://bobondemon.github.io/tags/Poisson-Process/"}]},{"title":"Stochastic Processes Week 1 Introduction & Renewal processes","date":"2021-12-11T12:56:53.000Z","path":"2021/12/11/Stochastic-Processes-Week-1-Introduction-Renewal-processes/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 Week 1: Introduction &amp; Renewal processes (本文) Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes Week 1.2: Difference between various fields of stochastics數學上的 stochastics 跟三個主要學科有關: Probability theory Mathematical statistics stochastic processes 用一個池子裡面的魚來當例子, 假設我們要分析某個時間點池子裡有多少魚, i.e. $N$ 條魚Probability theory 就是找出這個 $N$ 的機率分布, 然後可以分析其 mean, variance …而 mathematical statistics 是藉由一些統計實驗, 去估計出 $N$ 舉例來說, 抓 $M$ 條魚做記號後放回池子. 然後一次實驗為抓 $n$ 條魚, 若發現有 $m$ 條做過記號, 則這個機率我們可以算出來:$\\mathcal{P}\\{\\#\\text{marked}=m\\}=(C_M^m\\cdot C_{N-M}^{n-m})/C_N^n$ 我們重複這個實驗 $q$ 次, 得到的做過記號的魚的次數為 ${m_1,m_2,…,m_q}$, 因此可以算出 log-likelihood:$L(N)=\\sum_{k=1}^q \\log\\mathcal{P}\\{\\#\\text{marked}=m_k\\}$ 所以求解 $\\arg\\max_N L(N)$對於 stochastic processes 我們也可以問同樣的問題: $N$ 是多少? 不過此時會多考慮 $N$ 隨著時間變化 Week 1.3: Probability space建議先閱讀[測度論] Sigma Algebra 與 Measurable function 簡介: https://ch-hsieh.blogspot.com/2010/04/measurable-function.html[機率論] 淺談機率公理 與 基本性質: https://ch-hsieh.blogspot.com/2013/12/blog-post_7.html 課程使用兩個隨機實驗 (如同上面的參考連結): 從閉區間 $[0,1]$ 之中 任選一個數字 做 $n$ 次的丟銅板實驗 我們引用參考連結的定義$\\mathcal{F}$ 是一個 collection of subsets of $\\Omega$, 也就是說每一個 element 都是一個 $\\Omega$ 的 subset. 除此之外, 還必須是 $\\sigma$-algebra. $\\sigma$-algebra 和 topology 定義可參考 https://www.themathcitadel.com/topologies-and-sigma-algebras/ 所以一個 “事件” $A$ 是一個 subset of $\\Omega$ (i.e. $A\\subseteq\\Omega$), 也是一個 element of $\\mathcal{F}$, (i.e. $A\\in\\mathcal{F}$). 💡 注意 $A$ 是 subset of $\\Omega$, 還不夠. $A$ 還必須從是 $\\sigma$-algebra 的 $\\mathcal{F}$ 裡面挑. 最後 $\\mathcal{P}:\\mathcal{F} \\rightarrow [0,1]$ 此函數必須滿足以下公理對比 measure 的定義, 相當於多出了一條 $P(\\Omega)=1$, 所以 probability measure 是一個特殊的 measure再對比 measure space 的定義, 差別只在於 probability space 用的 measure 為 probability measure Week 1.4: Definition of a stochastic function. Types of stochastic functions.首先 random variables 其實是一個 measurable function $\\xi:\\Omega\\rightarrow\\mathbb{R}$, 要搞清楚什麼是 measurable function 之前, 我們先要了解 Borel $\\sigma$-algebra, Measurable Space 與 Measurable sets 一樣, 主要參考 [測度論] Sigma Algebra 與 Measurable function 簡介 $\\sigma(\\mathcal{E})$ 表示包含 $\\mathcal{E}$ 的最小 $\\sigma$-algebra, 且一定存在 (證明請參考該文章)令 $X:=\\mathbb{R}$, 則我們可以取所有 open sets 產生 Borel $\\sigma$-algebra, 記做 $\\mathcal{B}(\\mathbb{R})$ $\\mathcal{B}(\\mathbb{R})$ 可以想成實數軸上包含所有 open sets 的最小 $\\sigma$-algebra, 由 $\\sigma$-algebra 定義知也包含 closed sets $[a,b]$, ${a}$, $(a,b]$, $[a,b)$, 以及上述這些的 countable union (complement) 可稱 $\\sigma$-algebra 中的元素為 measurable set接著定義可測函數:對於 $f$ 來說, 其 domain ($X$) and image ($Y$) spaces 都配備了對應的 $\\sigma$-algebra $\\mathcal{A},\\mathcal{B}$回到 probability space, $(\\Omega, \\mathcal{F}, \\mathcal{P})$, 我們知道 $\\mathcal{F}$ 是定義在 sample space $\\Omega$ 的 $\\sigma$-algebra, i.e. $(\\Omega,\\mathcal{F})$ 是 measurable space.而 random variable $Z$ 其實是定義為由 $\\Omega$ 映射到 $\\mathbb{R}$ 的 measurable function. $\\mathbb{R}$ 配備 $\\mathcal{B}(\\mathbb{R})$.舉一個 Khan Academy 淺顯的例子, $X,Y$ 為兩個 r.v.s 分別把 outcomes 對應到 $\\mathbb{R}$ $Y$ 的 outcomes 是$\\{(a_1,...,a_7):a_i\\in \\{\\text{head,tail} \\} \\}$共 $2^7$ 種可能Mapping 到 $\\mathbb{R}$ 後我們就可以算對應的機率, 例如$\\mathcal{P}(Y\\leq30)$ or $\\mathcal{P}(Y\\text{ is even})$而需要 r.v. 是 measurable function 的原因可以從這看出來, 因為我們要知道 pre-image:$\\{\\omega:Y(\\omega)\\leq30\\}$也就是滿足這條件的 outcomes 集合, 必須要 $\\in\\mathcal{F}$. 所以它才會是個 “事件”這是因為我們的 probability measure $\\mathcal{P}:\\mathcal{F}\\rightarrow [0,1]$, 是定義在 $\\mathcal{F}$ (事件的 $\\sigma$-algebra) 上 Week 1.5: Trajectories and finite-dimensional distributions[Def]: Stochastic process &emsp;Stochastic process 是一個 mapping $X:T\\times\\Omega\\rightarrow\\mathbb{R}$, 而通常 $T=\\mathbb{R}^+$, 且滿足: &emsp;$\\forall t \\in T$, we have $X_t=X(t,\\cdot)$ is a random variable on probability space $(\\Omega,\\mathcal{F},\\mathcal{P})$ [Def]: Trajectory (sample path, or path) of a stochastic process &emsp;對一個 stochastic process $X$ 來說, fixed $\\omega\\in\\Omega$, 我們得到 $X(\\cdot,\\omega)$ 為 $T$ 的函數, 這就是 trajectory [Def]: Finite dimensional distributions &emsp;對一個 stochastic process $X$ 來說, 我們根據時間可以拿到 $n$ 個 random variables: &emsp;$(X_{t_1}, X_{t_2}, ..., X_{t_n})$, where $t_1, t_2,...,t_n \\in \\mathbb{R}$ 在 probability theory 裡面都是將這 $n$ 個 r.v.s 視為獨立, 但在 stochastic processes 裡面不能. 這是很大的不同. 所以就算是 finite dimensional distribution, 在 stochastic processes 也是很挑戰的. video 的試題: Week 1.6: Renewal process. Counting process可參考詳細解說: https://www.randomservices.org/random/renewal/Introduction.html第一個事件發生的時間為 $T_1$, 第二個事件發生的時間為 $T_2$, …每一個事件要隔多久發生都是從一個 random variable $X_i$ 決定的因此我們會有一個 sequence of interarrival times, $X=(X_1,X_2,…)$所以 sequence of arrival times 就會是 $T = (T_1, T_2, …)$, 其中$T_n=\\sum_{i=1}^nX_i$ , $n\\in\\mathbb{N}$它們的關係用圖來看如下:最後我們可以定義一個 random variable $N_t$ 表示到時間 $t$ 為止有多少個”事件”到達了:$N_t=\\sum_{n=1}^\\infty \\mathbf{1}(T_n\\leq t)$, $t\\in[0,\\infty)$其中 $\\mathbf{1}(\\cdot)$ 表示 indicator function.又或者可以用課程上的定義:$N_t=\\arg\\max_n\\{T_n\\leq t\\}$所以可以定義 counting process 為一個 random process $N=(N_t:t\\geq 0)$我們可以將 $N$ 這個 random processes 的 trajectory (path) 畫出來.這個意思就是 fixed 一個 sample space 的值, 例如固定 $X’=(X_1=0.5,X_2=0.11,…)$, 然後對每個時間點的 $N_t$ 的值隨著時間畫出來. 我們會發現是如下的 increasing step function:$T_n\\leq t$ 意思就是 $n$ 個事件發生的時間比 $t$ 小. 等同於到時間 $t$ 為止至少有 $n$ 個事件已經發生, i.e. $N_t\\geq n$ [Properties]: counting variables $N_t$ 與 arrival times $T_n$ 的關聯如下:&emsp;1. ${N_t\\geq n}={T_n\\leq t}$ or ${N_t\\leq n}={T_n\\geq t}$&emsp;2. $\\{N_t=n\\}=\\{T_n\\leq t\\} \\cap \\{T_{n+1}&gt;t\\}$ Week 1.7: Convolution我們有兩個互為獨立的 r.v.s $X\\bot Y$, 且已知 $X\\sim F_X$, $Y\\sim F_Y$ (in c.d.f.) 或是寫成 $X\\sim P_X$, $Y\\sim P_Y$ (in p.d.f.). $F_X$ and $F_Y$ 是 cumulated distribution function of $X$ and $Y$$P_X$ and $P_Y$ 是 probability density function of $X$ and $Y$$F_X(x)=P_X(X&lt;x)$ 則 convolution of two independent random variables 記做: $F_X\\ast F_Y$ (convolution in terms of distribution function): $F_{X+Y}(x)=\\int_\\mathbb{R} F_X(x-y)dF_Y(y)$ 或 $P_X \\ast P_Y$ (convolution in terms of density function): $P_{X+Y}(x)=\\int_\\mathbb{R} P_X(x-y)P_y(y)dy$ 💡 Convolution 同樣都是用 $\\ast$ 表示, 但根據是 c.d.f. or p.d.f. 會有不同定義, 然而兩者為等價 把 convolution 擴展到 $n$ 個 i.i.d. r.v.s $\\{X_1,X_2,...,X_n\\}$, 其中 $X_i \\sim F$, 則:$S_n=X_1+...+X_n\\sim {\\color{orange}{F^{n\\ast}=\\underbrace{F\\ast ...\\ast F}_\\text{n times}}}$ 注意到這裡的 convolution, $\\ast$, is in terms of distribution function 有幾個特性: $F^{n\\ast}(x)\\leq F^n(x)$, if $F(0)=0$這是因為$$\\{X_1 + ... + X_n \\leq x\\}\\subseteq\\{X_1\\leq x, ..., X_n\\leq x\\} \\\\ \\therefore P\\{X_1 + ... + X_n \\leq x\\}\\leq \\prod_{i=1}^n P\\{X_i\\leq x\\} \\\\ =F^{n\\ast}(x)\\leq \\prod_{i=1}^n F(x)=F^n(x)$$ $F^{n\\ast}(x)\\geq F^{(n+1)\\ast}(x)$這是因為$\\{X_1 + ... + X_n \\leq x\\}\\supseteq\\{X_1 + ... + X_n + X_{n+1} \\leq x\\}$ [Thm] Expectation of counting process equals to renewal function $\\mathcal{U}(t)$:&emsp;考慮 renewal process: $T_n=T_{n-1}+X_n$, 且 $X_1,X_2,…$ are i.i.d. r.v.s with distribution function $F$. $\\mathcal{U}(t):=\\sum_{n=1}^\\infty F^{n\\ast}(t) &lt; \\infty$ 課程略過證明. 此定理告訴我們該序列收斂. $F^{n\\ast}(t)$ 的意思是 $n$ 個 i.i.d. r.v.s 相加的 CDF, i.e. $P(X_1+…+X_n\\leq t)=P(T_n\\leq t)$, 而在 renewal process 指的就是 arrival time $P(T_n\\leq t)$, i.e. 到時間 $t$ 為止至少有 $n$ 個事件已經發生的機率. 然後 $\\sum_{n=1}^\\infty$ 有那種把所有可能的情況都考慮進去的意思, 因此 $\\mathcal{U}(t)$ 可能會跟到時間 $t$ 為止發生”事件”的次數的期望值 ($\\mathbb{E} N_t$) 有關. 而事實上就是. $\\mathcal{U}(t)$ 稱 renewal function $\\mathbb{E} N_t = \\mathcal{U}(t)$&emsp;$$\\mathbb{E}N_t = \\mathbb{E}\\left[ \\#\\{n:T_n\\leq t\\} \\right] =\\mathbb{E}\\left[ \\sum_{n=1}^\\infty \\mathbf{1}(T_n\\leq t) \\right] \\\\ =\\sum_{n=1}^\\infty \\mathbb{E}\\left[ \\mathbf{1}(T_n\\leq t) \\right] =\\sum_{n=1}^\\infty P(T_n\\leq t) =\\sum_{n=1}^\\infty F^{n\\ast}(t)$$ 雖然 $\\mathbb{E} N_t = \\mathcal{U}(t) =\\sum_{n=1}^\\infty F^{n\\ast}(t)$ 我們可以明確寫出來, 但對於 $F$ 是比較 general form 的話, 幾乎很難算出來. 因為要算 convolution, 又要求 sequence 的 limit. 課程試題: 附上 exponential distributino 定義 Week 1.8: Laplace transform. Calculation of an expectation of a counting process-1[Def]: Laplace transform&emsp;Given $f:\\mathbb{R}^+\\rightarrow\\mathbb{R}$, Laplace transform 定義為如下積分:&emsp;$\\mathcal{L}_f(s)=\\int_{x=0}^\\infty e^{-sx}f(x)dx$ 有幾個主要的 properties: 令 $f$ 是 p.d.f. of some random variable $\\xi$則 $\\mathcal{L}_f(s)=\\mathbb{E}\\left[ e^{-s\\xi} \\right]$ 給定任兩個 functions (不一定要是 p.d.f. or c.d.f.) $f_1$, $f_2$, 則 $\\mathcal{L}_{f_1\\ast f_2}(s)=\\mathcal{L}_{f_1}(s)\\cdot\\mathcal{L}_{f_2}(s)$其中 $\\ast$ 是 convolution in terms of density[Proof]:Let $f(x)=f_1(x)*f_2(x)$ 則 $$\\mathcal{L}_f(s)=\\int_{x=0}^\\infty e^{-sx}\\left(\\int_{y=0}^\\infty f_1(x-y)f_2(y)dy\\right)dx \\\\ =\\int_{y=0}^\\infty\\left(\\int_{x=0}^\\infty e^{-sx}f_1(x-y)dx\\right)f_2(y)dy \\\\ =\\int_{y=0}^\\infty\\left( e^{-sy}\\int_{x-y=y}^\\infty e^{-s(x-y)}f_1(x-y)d(x-y) \\right) f_2(y)dy \\\\ =\\mathcal{L}_{f_1}(s)\\cdot\\int_{y=0}^\\infty e^{-sy} f_2(y)dy = \\mathcal{L}_{f_1}(s)\\cdot\\mathcal{L}_{f_2}(s)$$ 令 $F$ 是 c.d.f. 且 $F(0)=0$, $p=F’$ is p.d.f., 則: $\\mathcal{L}_F(s)=\\frac{\\mathcal{L}_p(s)}{s}\\\\$ [Proof]: 使用分部積分, integration by part $$l.h.s.=-\\int_{\\mathbb{R}^+}F(x)\\frac{d(e^{-sx})}{s} = -\\left[F(x)e^{-sx}/s\\right]|_{x=0}^\\infty+\\frac{1}{s}\\int_{\\mathbb{R}^+}e^{-sx}dF(x) \\\\ = 0+\\frac{1}{s}\\int_{\\mathbb{R}^+}p(x)e^{-sx}dx = r.h.s.$$ [Example 1]: 令 $f(x) = x^n$, 求 $\\mathcal{L}_f(s)$&emsp;[sol]: 也是使用分部積分, integration by part$$\\mathcal{L}_{x^n}(s) = \\int_{\\mathbb{R}^+}x^n e^{-sx}dx =-\\int_{\\mathbb{R}^+}x^n\\frac{d(e^{-sx})}{s} \\\\ = \\frac{n}{s} \\int_{\\mathbb{R}^+}x^{n-1} e^{-sx}dx=...\\\\ =\\frac{n}{s}\\cdot\\frac{n-1}{s}\\cdot...\\cdot\\frac{1}{s}\\int_{\\mathbb{R}^+}e^{-sx}dx \\\\ =\\frac{n!}{s^n}\\cdot\\left[ -\\left.\\frac{1}{s}e^{-sx} \\right |_0^\\infty \\right] = \\frac{n!}{s^{n+1}}$$ [Example 2]: 令 $f(x) = e^{ax}$, 求 $\\mathcal{L}_f(s)$&emsp;[sol]: 答案為 $\\mathcal{L}_{e^{ax}}(s)=\\frac{1}{s-a}$, if $a&lt;s$&emsp;略… Week 1.9: Laplace transform. Calculation of an expectation of a counting process-2現在我們要來計算 $\\mathbb{E}N_t$, 回顧一下 $N_t$ 是一個 r.v. 表示到時間 $t$ 為止有多少個事件到達了而我們也證明了 $\\mathbb{E} N_t = \\mathcal{U}(t) =\\sum_{n=1}^\\infty F^{n\\ast}(t)$, 現在我們在仔細分析一下$$\\mathbb{E}N_t=\\mathcal{U}(t)=\\sum_{n=1}^\\infty F^{n\\ast}(t) \\\\ = F(t) + \\left( \\sum_{n=1}^\\infty F^{n\\ast} \\right) \\ast F(t) \\\\ = F(t) + \\mathcal{U}(t)\\ast F(t)$$因此我們得到: $\\mathcal{U}=F+\\mathcal{U}\\ast F$, 其中 convolution, $\\ast$, is in terms of distribution function然後我們對等號兩邊套用 Laplace transfrom, 但是我們注意到 Laplace transfrom 套用的 convolution 必須是 density function, 因此要轉換其實我們從定義可以知道 $\\mathcal{U}\\ast_{cdf}F=\\mathcal{U}\\ast_{pdf}p$ 已知 $p=F’$$\\int_{\\mathbb{R}}\\mathcal{U}(x-y)dF(y) = \\int_{\\mathbb{R}}\\mathcal{U}(x-y)p(y)dy$ 所以$\\mathcal{U}=F+\\mathcal{U}\\ast_{cdf} F = F+\\mathcal{U}\\ast_{pdf} p$, 然後就可以套用 Laplace transfrom, 並利用上面提到的 properties 2 &amp; 3$$\\mathcal{L}_\\mathcal{U}(s) = \\mathcal{L}_F(s) + \\mathcal{L}_\\mathcal{U}(s) \\cdot \\mathcal{L}_p(s) \\\\ =\\frac{\\mathcal{L}_p(s)}{s} + \\mathcal{L}_\\mathcal{U}(s) \\cdot \\mathcal{L}_p(s)$$因此$\\mathcal{L}_\\mathcal{U}(s) = \\frac{\\mathcal{L}_p(s)}{s(1-\\mathcal{L}_p(s))} \\ldots (\\star)$所以雖然無法直接計算出 $\\mathbb{E} N_t = \\mathcal{U}(t)$, 但我們可以迂迴地透過以下三個步驟來估計: 從 $F$ 算出 $\\mathcal{L}_p$ 利用 $(\\star)$ 從 $\\mathcal{L}_p$ 算出 $\\mathcal{L}_\\mathcal{U}$ 反推什麼樣的 $\\mathcal{U}$ 會得到 $\\mathcal{L}_\\mathcal{U}$, 這一步是最困難的 下一段課程將會給出一個如何使用上面三步驟的範例 Week 1.10: Laplace transform. Calculation of an expectation of a counting process-3[Example]: 假設我們有一個 renewal process, $S_n=S_{n-1}+\\xi_n$&emsp;其中 $\\xi_1, \\xi_2, ... \\sim p(x)=\\frac{e^{-x}}{2}+e^{-2x}$&emsp;我們要如何計算 $\\mathbb{E}N_t$?, 我們使用上面所述的三步驟:&emsp;&emsp;在最後一步的時候我要需要找出&emsp;什麼樣的 $\\mathcal{U}(t)$ 會有 $\\mathcal{L}_\\mathcal{U}(s)=\\frac{1}{s}$? Ans: $1$&emsp;什麼樣的 $\\mathcal{U}(t)$ 會有 $\\mathcal{L}_\\mathcal{U}(s)=\\frac{1}{s^2}$? Ans $t$&emsp;什麼樣的 $\\mathcal{U}(t)$ 會有 $\\mathcal{L}_\\mathcal{U}(s)=\\frac{1}{2s+3}$? Ans $\\exp\\{-\\frac{3}{2}t\\}$ Week 1.11: Limit theorems for renewal processes考慮一個 renewal process, $S_n=S_{n-1}+\\xi_n$, 其中 $\\xi_1,\\xi_2,...$ are i.i.d. &gt;0 almost surelySLLN 為 Strong Law of Large Number; CLT 為 Central Limit TheoremThm1 直觀上可以理解, 因為 $N_t$ 表示到時間 $t$ 為止共有多少個事件發生了. 當 $t$ 很大的時候, 每單位時間發生的事件次數, i.e. $\\frac{N_t}{t}$, 應該會十分接近頻率, i.e. $\\frac{1}{\\mu}$Thm2 就不好直接理解了, 其中 $\\xrightarrow[]{d}$ 表示 convergence in distribution. 注意到 $\\frac{1}{\\mu}$ 表示單位時間是間發生的次數, 所以乘上 $t$ 就是事件發生的次數, 注意到這是期望值, 而 $N_t$ 是發生次數的 random variable. 因此可以想像 mean 應該就是 $t/\\mu$. 困難的是 variance 是什麼, 以及這剛好會 follow normal distribution.而這兩個分別跟 SLLN (Strong Law of Large Number) and CLT (Central Limit Theorem) 相關. 以下給出證明 Thm1 的證明: Thm2 的證明: 開頭的第一行:$\\mathcal{P}\\left\\{ \\frac{\\xi_1 + \\xi_2 + ... + \\xi_n -n\\cdot\\mu}{\\sigma\\sqrt{n}} \\leq x \\right\\} = \\mathcal{P}\\left\\{ \\frac{S_n-n\\cdot\\mu}{\\sigma\\sqrt{n}} \\leq x \\right\\}$是從 CLT 出發第二行到第三行用到了, $\\mathcal{P}\\left\\{ S_n\\leq t \\right\\} = \\mathcal{P}\\left\\{ N_t \\geq n \\right\\}$然後由 $t=n\\mu+\\sigma\\sqrt{n}x\\Rightarrow n=\\frac{t}{\\mu}-\\frac{\\sigma\\sqrt{n}}{\\mu}x$ 並將 $n \\approx t/\\mu$ 帶入 r.h.s. 得到$n=\\frac{t}{\\mu}-\\frac{\\sigma\\sqrt{t}}{\\mu^{3/2}}x$, 然後代回到 $\\mathcal{P}\\left\\{ N_t \\geq n \\right\\}$, 再整理一下得到最後一行證明最後一行是 (被擋到):$\\mathcal{P}\\left\\{ Z_t \\leq x \\right\\} = \\mathcal{P}\\left\\{ Z_t &gt; -x \\right\\} \\rightarrow 1-\\Phi(-x) = \\Phi(x)$或可以參考 https://www.randomservices.org/random/renewal/LimitTheorems.html 的 The Central Limit Theorem Applications of the Renewal ProcessesApplications of the Renewal Processes.pdf","tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://bobondemon.github.io/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"https://bobondemon.github.io/tags/Stochastic-Processes/"},{"name":"Probability Space","slug":"Probability-Space","permalink":"https://bobondemon.github.io/tags/Probability-Space/"},{"name":"Renewal Process","slug":"Renewal-Process","permalink":"https://bobondemon.github.io/tags/Renewal-Process/"},{"name":"Counting Process","slug":"Counting-Process","permalink":"https://bobondemon.github.io/tags/Counting-Process/"}]},{"title":"Stochastic Processes Week 0 一些預備知識","date":"2021-12-11T12:01:14.000Z","path":"2021/12/11/Stochastic-Processes-Week-0-一些預備知識/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 (本文) Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes 本篇回顧一些基礎的機率複習, 這些在之後課程裡有用到.強烈建議閱讀以下文章: [測度論] Sigma Algebra 與 Measurable function 簡介 [機率論] 淺談機率公理 與 基本性質 A guide to the Lebesgue measure and integration Measure theory in probability 以下回顧開始: 回顧機率知識 $e^x=\\sum_{k=0}^\\infty\\frac{x^k}{k!}$. Proof link. Independent of Random Variables $X\\perp Y \\Longleftrightarrow \\mathcal{P}(XY)=\\mathcal{P}(X)\\mathcal{P}(Y)$ $Cov(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]$[Proof]: $$Cov(X,Y)=\\mathbb{E}[(X-\\mu_x)(Y-\\mu_y)] \\\\ =\\mathbb{E}[XY]-\\mu_x\\mathbb{E}[Y]-\\mu_y\\mathbb{E}[X]+\\mu_x\\mu_y \\\\ =\\mathbb{E}[XY]-\\mu_x\\mu_y$$ $Var(X)=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2$[Proof]: $$Var(X)=Cov(X,X)=\\mathbb{E}[XX]-\\mathbb{E}[X]\\mathbb{E}[X] \\\\ = \\mathbb{E}[X^2]-(\\mathbb{E}[X])^2$$ $X,Y$ uncorrelated $\\Longleftrightarrow Cov(X,Y)=0 \\Longleftrightarrow \\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ $X\\perp Y \\Rightarrow$ $X,Y$ uncorrelated $\\Rightarrow \\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ Covariance 是線性的: $Cov(aX+bY,cZ)=acCov(X,Z)+bcCov(Y,Z)$[Proof]: $$Cov(aX+bY,cZ)=\\mathbb{E}[(aX+bY)cZ]-\\mathbb{E}[aX+bY]\\mathbb{E}[cZ] \\\\ = ac\\mathbb{E}[XZ]+bc\\mathbb{E}[YZ]-ac\\mathbb{E}[X]\\mathbb{E}[Z]-bc\\mathbb{E}[Y]\\mathbb{E}[Z] \\\\ = ac(\\mathbb{E}[XZ]-\\mathbb{E}[X]\\mathbb{E}[Z])+bc(\\mathbb{E}[YZ]-\\mathbb{E}[Y]\\mathbb{E}[Z]) \\\\ = acCov(X,Z)+bcCov(Y,Z)$$ [Characteristic Function Def]: For random variable $\\xi$, 定義 characteristic function $\\Phi:\\mathbb{R}\\rightarrow \\mathbb{C}$ 為 $$\\Phi_\\xi(u) = \\mathbb{E}\\left[ e^{iu\\xi} \\right]$$ 如果 $\\xi_1\\perp\\xi_2$, 則 $\\Phi_{\\xi_1+\\xi_2}(u)=\\Phi_{\\xi_1}(u)\\Phi_{\\xi_2}(u)$ [Proof]: $$\\Phi_{\\xi_1+\\xi_2}(u)=\\mathbb{E}[e^{iu(\\xi_1+\\xi_2)}]=\\mathbb{E}[e^{iu\\xi_1}e^{iu\\xi_2}] \\\\ = \\int_{\\xi_1,\\xi_2} \\mathcal{P}(\\xi_1,\\xi_2)e^{iu\\xi_1}e^{iu\\xi_2} d\\xi_1 d\\xi_2 \\\\ = \\int_{\\xi_1,\\xi_2} \\left(\\mathcal{P}(\\xi_1)e^{iu\\xi_1}\\right)\\left(\\mathcal{P}(\\xi_2)e^{iu\\xi_2}\\right) d\\xi_1 d\\xi_2 \\\\ =\\mathbb{E}[e^{iu\\xi_1}]\\mathbb{E}[e^{iu\\xi_2}] \\\\ =\\Phi_{\\xi_1}(u)\\Phi_{\\xi_2}(u)$$ $\\mathbb{E}[X+Y]=\\mathbb{E}[X]+\\mathbb{E}[Y]$. 跟 $X,Y$ 是否獨立或不相關無關. $|Cov(X,Y)|\\leq\\sqrt{Var(X)}\\sqrt{Var(Y)}$[Proof]: $Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)$[Proof]: $$Var(X+Y)=Cov(X+Y,X+Y)\\\\ =Cov(X,X)+2Cov(X,Y)+Cov(Y,Y)\\\\ =Var(X)+Var(Y)+2Cov(X,Y)$$ 如果 $X,Y$ uncorrelated (所以 $X\\perp Y$ 也成立), 則 $Var(X+Y)=Var(X)+Var(Y)$ Normal distribution of one r.v. $$X\\sim\\mathcal{N}(\\mu,\\sigma^2), \\text{ for }\\sigma&gt;0,\\mu\\in\\mathbb{R} \\\\ p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$ The characteristic function of normal distribution is: $\\Phi(u)=e^{iu\\mu-\\frac{1}{2}u^2\\sigma^2}$ 獨立高斯分佈之和仍為高斯分佈, mean and variance 都相加[Proof]: $(X_1,...,X_n) \\text{ where } X_k\\sim\\mathcal{N}(\\mu_k,\\sigma_k^2),\\forall k=1,...,n$ 我們知道 $$X_k\\sim\\mathcal{N}(\\mu_k,\\sigma_k^2)\\longleftrightarrow \\Phi_k(u)=e^{iu\\mu_k-\\frac{1}{2}u^2\\sigma_k^2}$$ 則 $$\\sum_k X_k \\longleftrightarrow \\prod_k \\Phi_k(u) = e^{iu(\\sum_k \\mu_k)-\\frac{1}{2}u^2(\\sum_k \\sigma_k^2)}$$ 由特徵方程式與機率分佈一對一對應得知 $\\sum_k X_k \\sim \\mathcal{N}\\left(\\sum_k \\mu_k, \\sum_k \\sigma_k^2\\right)$ $K:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$ is symmetric positive semi-definite: 給定 $t_1&lt;t_2&lt;…&lt;t_n$ 一個很有用的技巧為:可以變成以下這些 disjoint 區段的線性組合 $(t_2-t_1),...,(t_n-t_{n-1})$ 具體如下: $$\\sum_{k=1}^n \\lambda_k B_{t_k} = \\lambda_n(B_{t_n}-B_{t_{n-1}}) + (\\lambda_n+\\lambda_{n-1})B_{t_{n-1}} + \\sum_{k=1}^{n-2} \\lambda_k B_{t_k} \\\\ = \\sum_{k=1}^n d_k(B_{t_n}-B_{t_{n-1}})$$ 會想要這樣轉換是因為課程會學到 independent increment 特性, 表明 disjoint 區段的 random variables 之間互相獨立, 因此可以變成互相獨立的 r.v.s 線性相加 Calculating Moments with Characteristic Functions (wiki)) Brownian Motion referenceBrownian Motion by Hartmann.pdf Lebesgue Measure and IntegrationA guide to the Lebesgue measure and integrationLebesgue integration from wiki[測度論] Sigma Algebra 與 Measurable function 簡介 Probability Theory[機率論] 淺談機率公理 與 基本性質Measure theory in probabilityDistribution function $F$ defined with (probability) measure $\\mu$ (ref)$F$ 又稱 cumulative distribution function (c.d.f.), 或稱 cumulative function其微分稱為 probability density function (p.d.f.), 或簡稱 density function. 注意到 density 不是 (probability) measure $\\mu$!而期望值可以用 Lebesgue measure or in probability measure 來看待:Given Probability space: $(\\Omega,\\Sigma,\\mathcal{P})$, 期望值定義為 所有 event $\\omega\\in\\Sigma$ 的 probability measure $\\mathcal{P}(\\omega)$, 乘上該 random variable 的值 $X(\\omega)$所有 outcome $\\omega\\in\\Omega$ 的 probability measure $\\mathcal{P}(d\\omega)$, 乘上該 random variable 的值 $X(\\omega)$ Lebesgue’s Dominated Convergence Theorem: Exchanging $\\lim$ and $\\int$Let $(f_n)$ be a sequence of measurable functions on a measure space $(S,\\Sigma,\\mu)$. Assume $(f_n)$ converges pointwise to $f$ and is dominated by some (Lebesgue) integrable function $g$, i.e. $|f_n(x)|\\leq g(x), \\qquad \\forall n,\\forall x\\in S$ Then $f$ is (Lebesgue) integrable, i.e. $\\int_S |f|d\\mu&lt;\\infty$and $$\\lim_{n\\rightarrow\\infty}\\int_S |f_n-f|d\\mu=0 \\\\ \\lim_{n\\rightarrow\\infty}\\int_S f_nd\\mu = \\int_S\\lim_{n\\rightarrow\\infty}f_nd\\mu = \\int_S f d\\mu$$","tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://bobondemon.github.io/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"https://bobondemon.github.io/tags/Stochastic-Processes/"}]},{"title":"MCMC by Gibbs and Metropolis-Hasting Sampling","date":"2021-10-27T13:53:41.000Z","path":"2021/10/27/MCMC-by-Gibbs-and-Metropolis-Hasting-Sampling/","text":"PRML book sampling (chapter 11) 開頭把動機描述得很好, 也引用來當這篇文章的前言.在用 machine learning 很多時候會遇到需要計算某個 function $f(x)$ 的期望值, 當 $x$ follow 某個 distribution $p(x)$ 的情況, i.e. 需計算 $$\\begin{align} \\mu:=\\mathbb{E}_p[f]=\\int f(x)p(x)dx \\end{align}$$ 例如 EM algorithm 會需要計算 $\\mathbb{E}_{p(z|x)}[f(x,z)]$, 參考 ref 的式 (23), (28)又或者我們要做 Bayesian 的 prediction 時, 參考 ref 的式 (2) 這些情況大部分都無法有 analytical form. 不過如果我們能從給定的 distribution $p(x)$ 取 $L$ 個 sample 的話, 式 (1) 就能如下逼近 $$\\begin{align} \\mathbb{E}_p[f] \\approx \\hat f:= \\frac{1}{L}\\sum_{l=1}^L f(x_l) \\end{align}$$ 我們先來看一下 $\\hat f$ 這個估計的期望值是什麼: $$\\begin{align} \\mathbb{E}_p[\\hat f]=\\mathbb{E}_p\\left[ \\frac{1}{L}\\sum_{l=1}^L f(x_l) \\right] = \\frac{1}{L}\\sum_{l=1}^L\\mathbb{E}_p\\left[ f(x_l) \\right] = {E}_p [f] = \\mu \\end{align}$$ 得到一個好消息是我們只要估超多次的話, $\\hat f_1, \\hat f_2, …$ 這些估計的平均就是我們要的值 其實這等同於估一次就好, 但用超大的 $L$ 去估計. 問題是 $L$ 要多大才夠 ? 如果變數 $x$ 的維度增加, 需要的 $L$ 是否也要增加才會準確 ? i.e. 會不會有維度爆炸的問題 ? (參考 Curse of dimensionality [1]) 我們可以證明 (see Appendix): $$\\begin{align} var[\\hat f]=\\frac{1}{L}var(f) \\end{align}$$ 這告訴我們, 隨著 sample 數量 $L$ 愈大, 我們估出來的 $\\hat f$ 的”變化”會愈來愈小 (成反比). 更重要的是, 這跟 input dimension 無關! 所以不會有維度爆炸的問題. 課本說通常 $L$ 取個 10 個 20 個估出來的 $\\hat f$ 就很準了. (其實很好驗證) 所以剩下要解決的問題便是, 要怎麼從一個給定的 distribution 取 sample ? 本篇正文從這開始 先說明 1-d 情況下的 r.v. 怎麼 sampling 再來說明如何用 Markov chain sampling, 也就是大名鼎鼎的 MCMC (Markov Chain Monte Carlo) 最後介紹兩個實作方法 Gibbs and Metropolis-Hasting sampling. 以下文章內容絕大多數都是從 Coursera: Bayesian Methods for Machine Learning 課程來的非常推薦這門課程! 從 1-D 說起Discrete case先討論 discrete distribution 的情形, 我們總是可以取 samples from uniform distribution [0, 1], i.e. $\\text{sample} \\sim \\mathcal{U}[0,1]$所以若要從下圖例子的 discrete distribution 取 samples 其實很容易, 若落在 [0, 0.6) 就 sample $a_1$, 落在 [0.6, 0.7) 取 $a_2$, 落在 [0.7, 1) 取 $a_3$. Gaussian case如果是 continuous distribution 呢?考慮如下的 standard Gaussian distribution $\\mathcal{N}(0,1)$ 可以使用 Central Limit Theorem. 舉例來說我們可以從 $n$ 個 I.I.D. 的 $\\mathcal{U}[0,1]$ 取 samples, 然後平均起來. CLT 告訴我們當 $n$ 很大的時候, 結果分布會接近 $\\mathcal{N}(0,1)$ General continuous case那如果是 general case 呢? 方法是找一個已知會 sampling 的分布乘上 constant value 使它成為 upper bound例如利用 $2q(x)=2\\mathcal{N}(1,9)$ 可以變成 $p(x)$ 的 upper bound 因此我們可以 sample $\\tilde{x}$ from $2q(x)$, 舉例來說很有可能 $\\tilde{x}=0$ 因為在 $0$ 附近的機率最大. 但是對於我們真實想要 samping 的 $p(x)$ 來說, $0$ 反而機率比較小. 因此我們要有一些 rejection 機制. 所以流程就是, 首先先從已知的 $q(x)$ sample 出 $\\tilde{x}$, 由於 $2q(x)$ 是 $p(x)$ 的 upper bound, 因此我們可以根據比例來決定這一次的 $\\tilde{x}$ 是否接受. 上圖紅色為 rejection 而綠色為 acception. 因此 acception 機率為: $$\\begin{align} \\frac{p(x)}{2q(x)} \\end{align}$$ 我們解釋一下為何這方法可以運作, 首先注意到所有取出來的 $\\tilde{x}$ (還沒拒絕之前) 是均勻分布在 $2q(x)$ curve 下的 (見下圖). 而一旦引入我們 rejection 的方法, 取出來的點就是均勻分布在我們要的 $p(x)$ curve 下了. 從上面的說明可以看出, accept 的比例其實就是藍色的比例, 因此 upper bound 愈緊密效果愈好.所以如果 $p(x)\\leq Mq(x)$, 則平均 accept $1/M$ points. 這是因為 $p,q$ 都是機率分布, 所以 area under curve 都是 $1$. 因此比例就是 $1/M$.最後, 這個方法可以用在不知道 normalization term $Z$ 的情形. 例如我們只知道 $\\hat{p}(x)$, 但我們仍然可以找到一個 distribution $q(x)$ 乘上 constant $\\tilde{M}$ 後是 upper bound: $$\\hat{p}(x) \\leq \\tilde{M}q(x) \\\\ \\Longrightarrow p(x)=\\frac{\\hat{p}(x)}{Z} \\leq Mq(x)$$ 總解一下此法 結論就是雖然對大部分 distribution 都可以用, 但效率不好. 尤其在維度高的時候會大部分都 reject.那有什麼方法可以對付高維度呢? 下面要介紹的 MCMC with Gibbs/Metropolis-Hastings 就能處理. Markov Chains Monte Carlo這裡假設大家已經熟悉 Markov chain 了, 不多做介紹.使用 Markov chain 的策略為以下幾個步驟: 重點在如何設計一個 Markov chain (這裡等同於設計 transition probability $T$), 收斂的 stationary distribution 正好就是我們要的 $p(x)$首先不是每個 Markov chain 都會收斂, 但有一些充分條件如下圖 Theorem: 對照 Stochastic Processes 裡的筆記 (之後補 link), 這裡的 theorem 隱含了此 Markov chain 為 ergodic, i.e. 1-equivalence class, recurrent, and aperiodic. 而 ergodic Markov chain 必定存在 stationary distribution. Gibbs sampling上面提到使用 Markov chain 取 sample 的話, 怎麼樣的 $T$ 會讓它收斂到 desired $p(x)$Gibbs sampling 可以想成一種特殊的 $T$ 的設計方法, 可以確保收斂至 $p(x)$假設我們有一個 3-dim 的 P.D.F., 可以不知道 normalization term $Z$: $$\\begin{align} p(x_1,x_2,x_3)=\\frac{\\hat{p}(x_1,x_2,x_3)}{Z} \\end{align}$$ 從 $(x_1^0, x_2^0, x_3^0)$ 開始, e.g. $(0,0,0)$先對第一維取 sample: $$\\begin{align} x_1^1 \\sim p(x_1 | x_2=x_2^0, x_3=x_3^0) \\\\ = \\frac{\\hat{p}(x_1,x_2^0,x_3^0)}{Z_1} \\end{align}$$ 針對 1-d distribution 取 sample 是很容易的, 可以使用上一節的做法接著對第二維取 sample: $$\\begin{align} x_2^1 \\sim p(x_2 | x_1=x_1^{\\color{red}{1}}, x_3=x_3^0) \\end{align}$$ 最後對第三維取 sample: $$\\begin{align} x_3^1 \\sim p(x_3 | x_1=x_1^{\\color{red}{1}}, x_2=x_2^{\\color{red}{1}}) \\end{align}$$ 以上便是一次的 iteration, 所以: 顯而易見, 這個方法不能 parallel, 之後會說怎麼加速 (利用 Metropolis-Hastings) 證明收斂至 desired distribution現在要證明這樣的採樣方式定義了一個 Markov chain 且會收斂到 desired distribution $p(x)$, which is stationary!Markov chain 的 states 定義為 $p(x)$ 的 domain, 我們以 $n$-dim 來說就是 $(x_1,x_2,…,x_n)$Transition probabilities $p_T(x\\rightarrow x’)$ , i.e. 從 state $x$ 到 $x’$ 的機率, 使用 Gibbs sampling 來定義: $$\\begin{align} p_T(x\\rightarrow x&apos;)=p(x_1&apos;|x_2,x_3,...,x_n)p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) \\end{align}$$ 這裡我們做個假設, 令 $p_T(x\\rightarrow x’)&gt;0,\\forall x,x’$, 則由定理知道此 Markov chain 必 $\\exists !$ stationary distribution. 所以現在問題是該 stationary distribution 是我們要的 $p(x)$ 嗎?要證明 $p(x)$ 是 stationary, 我們只需證明: $$\\begin{align} p(x&apos;)=\\sum_x p(x\\rightarrow x&apos;)p(x) \\end{align}$$ 這表示 $p(x)$ 經過 1-step transition 後, 分布仍然是 $p(x)$所以再來就是用 $p_T(x\\rightarrow x’)$ 代入, 驗證看看對不對 $$\\begin{align} \\sum_x p_T(x\\rightarrow x&apos;)p(x) \\\\ = \\sum_x p(x_1&apos;|x_2,...,x_n)p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) p(x) \\\\ =p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_x p(x_1&apos;|x_2,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n)p(x) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_2,..,x_n} p(x_1&apos;|x_2,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\sum_{x_1}p(x) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_2,..,x_n} {\\color{orange}{p(x_1&apos;|x_2,...,x_n)}} ...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) {\\color{orange}{p(x_2,...,x_n)}} \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_2,..,x_n} {\\color{orange}{p(x_1&apos;,x_2,...,x_n)}} p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\ldots(\\star) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_3,..,x_n} {\\color{orange}{p(x_1&apos;,x_3,...x_n)}}p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_3,..,x_n} {\\color{orange}{p(x_1&apos;,x_2&apos;,x_3,...,x_n)}}p(x_3&apos;|x_1&apos;,x_2&apos;,x_4,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\ldots(\\square) \\end{align}$$ 觀察 $(\\star)$ 到 $(\\square)$, 是消耗掉 $x_2$ 的 summantion, 同時也消耗掉對 $x_2$ 的 gibbs sampling step. 因此我們可以對 $(\\square)$ 做一樣的事情, 去消耗掉 $x_3$ 的 summantion 以及對 $x_3$ 的 gibbs step.重複做會得到: $$\\begin{align} = p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;)\\sum_{x_n}p(x_1&apos;,...,x_{n-1} &apos;,x_n) \\\\ = p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) p(x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) \\\\ = p(x_1&apos;,x_2&apos;,...,x_n&apos;)=p(x&apos;) \\end{align}$$ Q.E.D. 總結大致上有兩個前提: 固定其他維度, 對某一維度取 samples 是很容易的 $p(x_i|x_1,...,x_{i-1}, x_{i+1}, ..., x_n)&gt;0$, 這保證了我們透過 Gibbs sampling 產生的 Markov chain 一定收斂到 desired $p(x)$ 優點為: 將 multi-dimensional sampling 化簡為 1-d sampling 容易實作 缺點為: Highly correlated samples, 這使得我們跑到 stationary distribution 後, 也不能連續的取 sample 點 Slow convergence (mixing) Not parallel (接下來介紹的 Metropolis Hastings 幫忙可以改善) Metropolis-HastingsGibbs sampling 缺點是 samples are too correlated, 且不能平行化. 注意到在 Gibbs sampling 方法裡, 已經定義好某一個特別的 Markov chain 了. Metropolis-Hastings 則可以定義出一個 famliy of Markov chain 都收斂到 desired distribution. 因此可以選擇某一個 Markov chain 可能收斂較快, 或是 less correlated.Metropolis-Hastings 中心想法就是 “apply rejection sampling to Markov chains” Algorithm 其中 $Q(x^k\\rightarrow x)$ 是任意事先給定的一個 transition probabilities (注意到需滿足 $&gt;0,\\forall x,x’$, 這樣才能保證唯一收斂)$A(x^k\\rightarrow x)$ 表示 given $x^k$ accept $x$ 的機率, 稱為 critic演算法流程為: 先從 $Q(x^k\\rightarrow x)$ 取樣出 $x’$, $x’$ 有 $A(x^k\\rightarrow x’)$ 的機率被接受, 一旦接受則 $x^{k+1}=x’$ 否則 $x^{k+1}=x^k$, 然後 iterate 下去使用這種方式的話, 我們其實可以算出 transition probability $T(x\\rightarrow x’)$, 如上圖所以關鍵就是, 怎麼選擇 $A(x^k\\rightarrow x)$ 使得這樣的 Markov chain 可以收斂到 desired probability $\\pi(x)$ 怎麼選擇 Critic $A$ 使得 Markov chain 收斂到 $\\pi$我們先介紹一個充分條件 (所以有可能 $\\pi(x)$ 是 stationary 但是不滿足 detailed balance equation) [Detailed Balance Equation]:若 $\\pi(x)T(x\\rightarrow x’)=\\pi(x’)T(x’\\rightarrow x), \\forall x,x’$, 則 $\\pi(x)$ 為 stationary distribution, i.e. $\\pi(x&apos;)=\\sum_x \\pi(x)T(x\\rightarrow x&apos;)$ [Proof]: $$\\begin{align} \\sum_x \\pi(x)T(x\\rightarrow x&apos;) \\\\ \\text{by assumption} = \\sum_x \\pi(x&apos;)T(x&apos;\\rightarrow x) \\\\ = \\pi(x&apos;)\\sum_x T(x&apos;\\rightarrow x) = \\pi(x&apos;) \\end{align}$$ 所以只要選擇的 $A(x\\rightarrow x’)$ 能夠讓 $T(x\\rightarrow x’)$ 針對 $\\pi(x)$ 滿足 detailed balance 特性就能保證 Markov chain 收斂到 $\\pi(x)$因此我們計算一下, 只需考慮 $x\\neq x’$ 的情形 (因為 $x=x’$ 一定滿足 detailed balance equation, 這不是廢話嗎) $$\\begin{align} \\pi(x)T(x\\rightarrow x&apos;)=\\pi(x&apos;)T(x&apos;\\rightarrow x) \\\\ \\Longleftrightarrow \\pi(x)Q(x\\rightarrow x&apos;)A(x\\rightarrow x&apos;) = \\pi(x&apos;)Q(x&apos;\\rightarrow x)A(x&apos;\\rightarrow x) \\\\ \\Longleftrightarrow \\frac{A(x\\rightarrow x&apos;)}{A(x&apos;\\rightarrow x)} = \\frac{\\pi(x&apos;)Q(x&apos;\\rightarrow x)}{\\pi(x)Q(x\\rightarrow x&apos;)} =: \\rho \\end{align}$$ 所以當 $\\rho&lt;1$ 我們設定 $$\\begin{align} \\left\\{ \\begin{array}{r} A(x\\rightarrow x&apos;)=\\rho \\\\ A(x&apos;\\rightarrow x)=1 \\end{array} \\right. \\end{align}$$ 而如果 $\\rho&gt;1$ 我們設定 $$\\begin{align} \\left\\{ \\begin{array}{r} A(x\\rightarrow x&apos;)=1 \\\\ A(x&apos;\\rightarrow x)=1/\\rho \\end{array} \\right. \\end{align}$$ 總結來說 $A$ 可以這麼設定 $$\\begin{align} A(x\\rightarrow x&apos;)=\\min\\left\\{ 1, \\frac{\\pi(x&apos;)Q(x&apos;\\rightarrow x)}{\\pi(x)Q(x\\rightarrow x&apos;)} \\right\\} \\end{align}$$ 注意到 $\\rho$ 是可以直接算出來的, 因為 $Q,\\pi$ 都是事先給定已知的, 因此我們就能設定出對應的 acceptance distribution $A$. 同時如果我們只有 unnormalized distribution, i.e. $\\hat\\pi(x)$, 由 $A$ 的設定可以看出不受影響 $$\\begin{align} A(x\\rightarrow x&apos;)=\\min\\left\\{ 1, \\frac{ {\\color{orange}{\\hat\\pi(x&apos;)}} Q(x&apos;\\rightarrow x)}{ {\\color{orange}{\\hat\\pi(x)}} Q(x\\rightarrow x&apos;)} \\right\\} \\end{align}$$ 怎麼選擇 $Q$首先需滿足 $Q(x\\rightarrow x’)&gt;0,\\forall x,x’$. 這樣才會有以上的推論.$Q$ 會希望能走”大步”一點, 也就是 transition 不要只圍繞在相鄰的點. 好處是產生的 sample 會比較無關.但如果走太大步, critic $A$ 就有可能一直 reject (why?) 導致效率太差 想像如果 $x$ 已經在機率很高的地方了, 例如 local maximum point. 如果 $Q$ 走太大步到 $x’$, 則容易 $\\pi(x’)&lt;&lt;\\pi(x)$, 造成 $A$ 太小容易 reject所以如果 $Q$ 走小步一點, $x’$ 還是圍繞在 $x$ 附近, 相對來說可能機率就不會那麼低 Example of Metropolis-Hastings1-d case toy example 告訴我們 proposal 的 distribution 選擇也是很重要的. 最後可以使用 Metropolis Hastings 來平行化 Gibbs sampling!我們使用如下圖 “錯誤的” Gibbs sampling 方法, 並將這方法視為 Metropolis Hastings 的 proposal $Q(x\\rightarrow x’)$因此可以平行對每個維度取 sample! (好聰明!) 結語MCMC 被譽為 20 世紀十個偉大的演算法發明之一 [3]. 找知乎的文章可以看到這個討論: 有什么理论复杂但是实现简单的算法？[4] 果然 MCMC 理論不是一般人能做的.後續對於 Metropolis-Hastings 的改進有一個算法是 Metropolis-adjusted Langevin algorithm [5] (MALA). 該方法提出使用 Langevin dynamics [6] 當作 proposal, 這會使得 random walk 會走向機率比較高的地方, 因此被拒絕機率較低. 但是 MALA 我實在看不懂, 只知道跟 Langevin dynamics sampling [7] 有關 在 Generative Modeling by Estimating Gradients of the Data Distribution [8] 的 Langevin dynamics 段落裡提到 MALA 可以只根據 score function ($\\nabla_x \\log p(x)$) 就從 P.D.F. $p(x)$ 取 samples! 會看到 MALA 是因為除了 GAN 之外最近很熱門的 generative models: DPM [9]), 其核心技術之一用到它.看來要全部融會貫通目前會先卡關在這了. MALA 你等著! 別跑啊, 不要以為我怕了你, 總有一天我 #$@^#@$Q (逃~) Appendix證明 $var[\\hat f]=\\frac{1}{L}Var(f)$ 如下: 首先兩個 independent r.v.s $X,Y$ 我們知道其 covariance 為 $0$: $$\\begin{align} 0 = Cov[XY] = \\mathbb{E}\\left[ (X-\\mu_x)(Y-\\mu_y) \\right] \\\\ = \\mathbb{E}[XY-X\\mu_y-\\mu_xY+\\mu_x\\mu_y] = \\mathbb{E}[XY] - \\mu_x\\mu_y \\\\ \\Rightarrow \\mathbb{E}[XY] = \\mu_x\\mu_y \\ldots(\\star) \\end{align}$$ 且有 variance 的性質: $Var(X)=\\mathbb{E}[X^2]-\\mu_x^2\\ldots(\\star\\star)$接著開始計算: $$\\begin{align} Var[\\hat f]=\\mathbb{E}[(\\hat f - \\mathbb{E}[\\hat f])^2] = \\mathbb{E}[(\\hat f - \\mu)^2] = \\mathbb{E}[\\hat f^2] - \\mu^2 \\\\ = \\mathbb{E}\\left[ \\frac{1}{L}\\sum_k f(x_k) \\frac{1}{L}\\sum_m f(x_m) \\right] - \\mu^2 \\\\ = \\frac{1}{L^2}\\sum_k\\sum_m\\left[ \\mathbb{E}[f(x_k)f(x_m)] - \\mu^2 \\right] \\\\ \\text{by }(\\star)= \\frac{1}{L^2}\\sum_k \\left[ (\\mathbb{E}[f(x_k)^2]-\\mu^2)+(L-1)(\\mu^2-\\mu^2) \\right] \\\\ \\text{by }(\\star\\star) = \\frac{1}{L^2}\\sum_k Var(f(x_k)) \\\\ = \\frac{1}{L} Var(f) \\end{align}$$ Reference Curse of Dimensionality — A “Curse” to Machine Learning Coursera: Bayesian Methods for Machine Learning The Best of the 20th Century: Editors Name Top 10 Algorithms 有什么理论复杂但是实现简单的算法？ Metropolis-adjusted Langevin algorithm: wiki Langevin dynamics: wiki 抽样理论中有哪些令人印象深刻(有趣)的结论? Generative Modeling by Estimating Gradients of the Data Distribution What are Diffusion Models?","tags":[{"name":"MCMC","slug":"MCMC","permalink":"https://bobondemon.github.io/tags/MCMC/"},{"name":"Metropolis Hastings","slug":"Metropolis-Hastings","permalink":"https://bobondemon.github.io/tags/Metropolis-Hastings/"},{"name":"Coursera","slug":"Coursera","permalink":"https://bobondemon.github.io/tags/Coursera/"},{"name":"Markov Chain","slug":"Markov-Chain","permalink":"https://bobondemon.github.io/tags/Markov-Chain/"},{"name":"Gibbs Sampling","slug":"Gibbs-Sampling","permalink":"https://bobondemon.github.io/tags/Gibbs-Sampling/"}]},{"title":"Gumbel-Max Trick","date":"2021-08-07T10:41:01.000Z","path":"2021/08/07/Gumbel-Max-Trick/","text":"我們在介紹 VAE 的時候有說明到 re-parameterization trick, 大意是這樣的 $y$ 是 sampling from distribution $\\alpha$, i.e., $y=\\text{Sampling}(\\alpha)$, 其中 $\\alpha=\\text{NN}_1(a;\\theta)$由於我們有採樣, 因此 loss 採用期望值. Loss function 為: $$\\begin{align} L = \\mathbb{E}_{y\\sim\\alpha}[\\text{NN}_2(y;\\nu)] \\end{align}$$ Loss 對 $\\theta$ 偏微分的時候會失敗, 主要是因為: $$\\begin{align} \\nabla_\\theta L = \\nabla_\\theta \\mathbb{E}_{y\\sim\\alpha}[\\text{NN}_2(y;\\nu)] \\\\ \\neq \\mathbb{E}_{y\\sim\\alpha}[\\nabla_\\theta \\text{NN}_2(y;\\nu)] \\end{align}$$ 微分不能跟 Expectation 互換是因為 sampling 的 distribution $\\alpha$ 其實也是 depends on $\\theta$. 因此在 VAE 那邊的假設就是將 $\\alpha$ 定義為 Gaussian pdf. 因此可以變成: $$\\begin{align} \\nabla_\\theta L = \\nabla_\\theta \\mathbb{E}_{y\\sim\\alpha}\\left[ \\text{NN}_2(y;\\nu) \\right] \\\\ = \\nabla_\\theta \\mathbb{E}_{\\varepsilon\\sim N(0,I)}\\left[ \\text{NN}_2(\\mu+\\sigma\\varepsilon; \\nu) \\right] \\\\ = \\mathbb{E}_{\\varepsilon\\sim N(0,I)}\\left[ \\nabla_\\theta \\text{NN}_2(\\mu+\\sigma\\varepsilon; \\nu) \\right] \\end{align}$$ 採樣變成從一個 跟 $\\theta$ 無關的分布, 因此微分跟期望值就能互換, 所以可以做 backprop. 現在的情況是如果是 Gaussian 的情形很好做變換, 但如果是 categorical distribution 該怎麼辦呢? 什麼情況會遇到 categorical distribution? 在 reinforcement learning 時, $\\text{NN}_1$ predict 出例如 4 個 actions 的機率, 我們需要隨機採樣一種 action, 然後傳給後面的 NN 去計算 reward.(其實我不熟 RL, 看網路上的文章說的) Gumbel max trick 就提供了解法! Gumbel Distribution and Gumbel Max Sampling這一篇文章 The Humble Gumbel Distribution 提供了非常清晰的解釋, 十分推薦閱讀 假設我們經由一個 network 算出 logits $(x_k)_k$, 一般我們如果要 sampling 的話還必須過 softmax 讓它變成機率 $(\\alpha_k)_k$, 然後在用例如 np.random.choice 根據機率採樣出結果. 現在 sampling 流程改為: 先從標準 Gumbel 分佈 (先不管這分佈長什麼樣) 採樣出 $N$ 個值, 令為 $(G_k)_k$, 讓它跟 logits 相加: $z_k=x_k+G_k$, 然後 $\\text{argmax}_k (z_k)$ 就是我們這次的採樣結果 圖示為: 注意到我們唯一的一個採樣動作完全跟 network 的參數 $\\theta$ 無關! 因此 re-parameterization trick 就能用上. (先假設 $\\text{argmax}_k (z_k)$ 可微, 因此可以 backprop, 這等下會說)剩下唯一不確定的就是, 這樣的採樣行為出來的結果, 會跟使用 $(\\alpha_k)_k$ 的機率分佈採樣出來一樣嗎 ?換句話說, $\\text{argmax}_k (z_k)$ 出來的結果, 其結果的分佈是不是符合 $(\\alpha_k)_k$ ?程式驗證可參考 The Humble Gumbel Distribution, 將最主要的部分修短擷取後如下: 123456789101112131415161718192021222324252627282930313233343536373839# Modified from http://amid.fish/humble-gumbelimport numpy as npimport matplotlib.pyplot as plt# Assign categorical probabilities, for example:probs = [0.13114754, 0.01639344, 0.21311475, 0.24590164, 0.19672131, 0.06557377, 0.13114754]n_classes = len(probs)logits = np.log(probs) # logits is log probability (with constant offset)n_samples = 10000 # experimental number of samplingdef gumbel_sampling(logits): noise = np.random.gumbel(size=len(logits)) sample = np.argmax(logits + noise) return samplesamples_with_gumbel_max_trick = [gumbel_sampling(logits) for _ in range(n_samples)]samples_from_true_distribution = np.random.choice(np.arange(n_classes), size=n_samples , p=probs)# Plotting area, comparing `samples_with_gumbel_max_trick` and `samples_from_true_distribution`def plot_estimated_probs(samples, n_classes): estd_probs, _, _ = plt.hist(samples, bins=np.arange(n_classes + 1), align='left', edgecolor='white', density=True) plt.xlabel(\"Category\") plt.ylabel(\"Estimated probability\") return estd_probsplt.figure()plt.subplot(1, 2, 1)plot_estimated_probs(samples_from_true_distribution, n_classes)plt.title('Sampling from true pdf')plt.subplot(1, 2, 2)estd_probs = plot_estimated_probs(samples_with_gumbel_max_trick, n_classes)plt.title('Sampling with Gumbel-max trick')plt.tight_layout()plt.show() 可以看到用 Gumbel-max trick 採樣出來的 samples 其分佈跟真實的機率分佈十分接近.事實上可以證明會是一樣的, 在下一節我們將證明寫出來.再囉嗦一下, 不要忘記了, 使用 np.random.choice 對真實分佈採樣是沒有辦法做 backprop 的 (見 eq (2) (3))而透過 Gumbel-max trick 我們可以從一個與要 optimize 的參數 $\\theta$ 無關的分佈 (Gumbel distribution) 進行採樣, 才能利用 re-parameterization trick 做 backprop (例如 eq (4)~(6) 的概念) 其實我少講了一件事, np.argmax 不可微, 所以不能 backprop. 因此一個實際的做法是使用 softmax (with temperature) 近似: $$\\begin{align} \\text{softmax}(z_k,\\tau)=\\frac{\\exp(z_k/\\tau)}{\\sum_{i=1}^N\\exp(z_i/\\tau)} \\end{align}$$ 實作上會先讓 temperature $\\tau$ 從比較大的值開始 (比較不那麼凸顯值之間大小的差異), 之後慢慢變小接近 $0$ (等同於 argmax). 參考 paper 的圖: Proof of Gumbel-Max Trick for Discrete Distributions其實完全參考 The Gumbel-Max Trick for Discrete Distributions, 但最後一行的推導用看的實在沒看出來, 因此自己補齊完整一點 Math warning, 很枯燥 Gumbel PDF: $f(z;\\mu)=\\exp\\left[-(z-\\mu)-\\exp\\left[-(z-\\mu)\\right]\\right]$ $f(z;0)=\\exp\\left[-z-\\exp\\left[-z\\right]\\right]$ Gumbel CDF: $F(z;\\mu)=\\exp\\left[-\\exp\\left[-(z-\\mu)\\right]\\right]$ $F(z;0)=\\exp\\left[-\\exp\\left[-z\\right]\\right]$ Categorical distribution 例如分成 $N$ 類, NN 通常最後會輸出一個 logits vector, $(x_k)_k$, $k=1…N$ $z_k=x_k+G_k$, 其中 $G_k$ 是一個標準 Gumbel distribution (mean=0, scale=1) $$\\begin{align} \\Pr(k\\text{ is largest}|\\{x_i\\},z_k) = \\Pr(\\max_{i\\neq k}z_i&lt;z_k) \\\\ =\\prod_{i\\neq k}\\Pr(z_i&lt;z_k) = \\prod_{i\\neq k}\\Pr(x_i+G_i&lt;z_k) \\\\ =\\prod_{i\\neq k}\\Pr(G_i&lt;z_k-x_i) \\\\ =\\prod_{i\\neq k}F(z_k-x_i;0) \\\\ =\\prod_{i\\neq k}\\exp\\{-\\exp\\{-z_k+x_i\\}\\} \\end{align}$$ $$\\begin{align} \\therefore \\Pr(k\\text{ is largest}|\\{x_i\\})=\\int\\Pr(z_k)\\Pr(k\\text{ is largest}|\\{x_i\\},z_k)dz_k \\\\ = \\int f(z_k-x_k;0)\\prod_{i\\neq k}\\exp\\{-\\exp\\{-z_k+x_i\\}\\} \\\\ = \\int \\left(\\exp\\{-z_k+x_k-e^{-z_k+x_k}\\}\\right) \\prod_{i\\neq k}\\exp\\{-e^{-z_k+x_i}\\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k\\}\\prod_{i=1}^N{ \\exp\\{-e^{-z_k+x_i}\\} } dz_k \\\\ = \\int \\exp\\{-z_k+x_k\\} \\cdot \\exp\\{-\\sum_{i=1}^Ne^{-z_k+x_i}\\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k-\\sum_{i=1}^Ne^{-z_k+x_i} \\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k-e^{-z_k} {\\color{orange}{\\sum_{i=1}^Ne^{x_i}}} \\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k- {\\color{orange}A} e^{-z_k} \\} dz_k \\end{align}$$ 這裡我們為了方便定義 $A=\\sum_{i=1}^N e^{x_i}$ $$\\begin{align} =\\int \\exp\\{-z_k+x_k - {\\color{orange}{e^{\\ln A}}} e^{-z_k} \\} dz_k \\\\ = e^{x_k} \\int \\exp\\{-z_k-e^{-z_k + \\ln A}\\} dz_k \\\\ = e^{x_k} \\int \\exp\\{-z_k {\\color{orange}{+\\ln A-\\ln A}} -e^{-z_k + \\ln A}\\} dz_k \\\\ = e^{x_k}\\cdot e^{-\\ln A} \\int \\exp\\{-(z_k-\\ln A)-e^{-(z_k-\\ln A)}\\} dz_k \\\\ = \\frac{e^{x_k}}{A} \\int f(z_k;\\ln A) dz_k \\\\ = \\frac{e^{x_k}}{\\sum_{i=1}^N e^{x_i}} \\end{align}$$ Reference The Humble Gumbel Distribution The Gumbel-Max Trick for Discrete Distributions The Gumbel-Softmax Trick for Inference of Discrete Variables 【一文学会】Gumbel-Softmax的采样技巧 Categorical Reparameterization with Gumbel-Softmax","tags":[{"name":"Gumbel distribution","slug":"Gumbel-distribution","permalink":"https://bobondemon.github.io/tags/Gumbel-distribution/"},{"name":"Gumbel max trick","slug":"Gumbel-max-trick","permalink":"https://bobondemon.github.io/tags/Gumbel-max-trick/"},{"name":"Gumbel max sampling","slug":"Gumbel-max-sampling","permalink":"https://bobondemon.github.io/tags/Gumbel-max-sampling/"},{"name":"Re-parameterization trick","slug":"Re-parameterization-trick","permalink":"https://bobondemon.github.io/tags/Re-parameterization-trick/"}]},{"title":"Noise Contrastive Estimation (NCE) 筆記","date":"2021-06-05T02:15:04.000Z","path":"2021/06/05/Noise-Contrastive-Estimation-NCE-筆記/","text":"之前聽人介紹 wav2vec [3] 或是看其他人的文章大部分都只有介紹作法, 直到有一天自己去看論文才發現看不懂 CPC [2] (wav2vec 使用 CPC 方法). 因此才決定好好讀一下並記錄. 先將這些方法關係梳理一下, NCE –&gt; CPC (infoNCE) –&gt; wav2vec. 此篇筆記主要紀錄 NCE (Noise Contrastive Estimation) 在做 ML 時常常需要估計手上 training data 的 distribution $p_d(x)$. 而我們通常會使用參數 $\\theta$, 使得參數的模型跟 $p_d(x)$ 一樣. 在現在 DNN 統治的年代可能會說, 不然就用一個 NN 來訓練吧, 如下圖: 給 input $x$, 丟給 NN 希望直接吐出 $p_\\theta(x)$. 上圖的架構是 $x$ 先丟給參數為 $\\theta_f$ 的 NN, 該 NN 最後一層的 outputs 再丟給參數為 $w$ 的 linear layer 最後吐出一個 scalar 值, 該值就是我們要的機率.而訓練的話就使用 MLE (Maximum Likelihood Estimation) 來求參數 $\\theta$. 恩, 問題似乎很單純但真正實作起來卻困難重重. 一個問題是 NN outputs 若要保持 p.d.f. 則必須過 softmax, 確保 sum 起來是 1 (也就是要算 $Z_\\theta$). $$\\begin{align} p_\\theta(x)=\\frac{u_\\theta(x)}{Z_\\theta}=\\frac{e^{G(x;\\theta)}}{Z_\\theta} \\\\ \\text{where } Z_\\theta = \\sum_x u_\\theta(x) \\end{align}$$ 式 (1) 為 energy-based model, 在做 NN classification 時, NN 的 output 就是 $G(x;θ)$, 也就是常看到的 logit, 經過 softmax 就等同於式 (1) 在做的事 而做這件事情在 $x$ 是 discrete space 但數量很多, 例如 NLP 中 LM vocabulary 很大時, 計算資源會消耗過大.或是 $x$ 是 continuous space 但是算 $Z_\\theta$ 的積分沒有公式解的情形會做不下去. (不然就要用 sampling 方法, 如 MCMC) NCE 巧妙的將此 MLE 問題轉化成 binary classification 問題, 從而得到我們要的 MLE 解. 不過在此之前, 我們先來看看 MLE 的 gradient 長什麼樣. MLE 求解寫出 likelihood: $$\\begin{align} \\text{likilhood}=\\prod_{x\\sim p_d} p_\\theta(x) \\end{align}$$ Loss 就是 negative log-likelihood $$\\begin{align} -\\mathcal{L}_{mle}=\\mathbb{E}_{x\\sim p_d}\\log p_{\\theta}(x)= \\mathbb{E}_{x\\sim p_d}\\log \\frac{u_\\theta(x)}{Z_\\theta}\\\\ \\end{align}$$ 計算其 gradient: $$\\begin{align} -\\nabla_{\\theta}\\mathcal{L}_{mle}= \\mathbb{E}_{x\\sim p_d} \\left[ \\nabla_{\\theta}\\log{u_\\theta(x)} - \\color{orange}{\\nabla_{\\theta}\\log{Z_\\theta}} \\right] \\\\ \\color{orange}{\\nabla_{\\theta}\\log{Z_\\theta}} = \\frac{1}{Z_\\theta}\\nabla_{\\theta}Z_\\theta = \\frac{1}{Z_\\theta} \\sum_x \\nabla_{\\theta} e^{G(x;\\theta)} \\\\ =\\frac{1}{Z_\\theta} \\sum_x e^{G(x;\\theta)} \\nabla_{\\theta}G(x;\\theta) = \\sum_x \\left[ \\frac{1}{Z_\\theta}e^{G(x;\\theta)} \\right] \\nabla_{\\theta}G(x;\\theta) \\\\ =\\sum_x p_{\\theta}(x) \\nabla_{\\theta} \\log u_{\\theta}(x) = \\mathbb{E}_{x \\sim p_{\\theta}} \\nabla_{\\theta} \\log u_{\\theta}(x) \\\\ \\therefore \\text{ } -\\nabla_{\\theta}\\mathcal{L}_{mle} = \\mathbb{E}_{x\\sim p_d} \\left[ \\nabla_{\\theta} \\log u_{\\theta}(x) - \\color{orange}{\\mathbb{E}_{x \\sim p_{\\theta}} \\nabla_{\\theta} \\log u_{\\theta}(x)} \\right] \\\\ = \\mathbb{E}_{x\\sim p_d} \\nabla_{\\theta} \\log u_{\\theta}(x) - \\mathbb{E}_{x \\sim p_{\\theta}} \\nabla_{\\theta} \\log u_{\\theta}(x)\\\\ = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\nabla_{\\theta} \\log u_{\\theta}(x) \\\\ \\end{align}$$ 從 (11) 式可以看到, 估計的 pdf 與 training data 的 pdf 差越大 gradient 愈大, 當兩者相同時 gradient 為 0 不 update. Sigmoid or Logistic Function在說明 NCE 之前先談一下 sigmoid function. 假設現在我們做二分類問題, 兩個類別 $C=1$ or $C=0$. 令 $p$ 是某個 input $x$ 屬於 class 1 的機率 (所以 $1-p$ 就是屬於 class 0 的機率)定義 log-odd 為 (其實也稱為 logit): $$\\begin{align} \\text{log-odd} = \\log \\frac{p}{1-p} \\end{align}$$ 我們知道 sigmoid function $\\sigma(x)=\\frac{1}{1+e^{-x}}$ 將實數 input mapping 到 0 ~ 1 區間的函式. 若我們將 log-odd 代入我們很容易得到: $$\\begin{align} \\sigma(\\text{log-odd})=...=p \\end{align}$$ 發現 sigmoid 回傳給我們的是 $x$ 屬於 class 1 的機率值, i.e. $\\sigma(\\text{log-odd})=p(C=1|x)$. 所以在二分類問題上, 我們就是訓練一個 NN 能 predict logit 值. NCE 的 Network 架構首先 NCE 引入了一個 Noise distribution $q(x)$. 論文提到該 $q$ 只要滿足當 $p_d(x)$ nonzero 則 $q(x)$ 也必須 nonzero 就可以. 二分類問題為, 假設要取一個正例 (class 1), 就從 training data pdf $p_d(x)$ 取得. 而若要取一個反例 (class 0) 則從 noise pdf $q(x)$ 取得.我們可以取 $N_p$ 個正例以及 $N_n$ 個反例, 代表 prior 為: $$\\begin{align} p(C=1)=\\frac{N_p}{N_p+N_n} \\\\ p(C=0)=1-p(C=1) \\\\ \\end{align}$$ 因此就可以得到一個 batch 共 $N_p+N_n$ 個 samples, 丟入下圖的 NN structure 做二分類問題: Network 前半段還是跟原來的 MLE 架構一樣, 只是我們期望 $NN_{\\theta}$ 吐出來的是 logit, 由上面一個 section 我們知道經過 sigmoid 得到的會是 $x$ 屬於 class 1 的機率. 因此很容易就用 xent loss 優化. 神奇的來了, NCE 告訴我們, optimize 這個二分類問題得到的 $\\theta$ 等於 MLE 要找的 $\\theta$! $$\\begin{align} \\theta_{nce} = \\theta_{mle} \\end{align}$$ 且 NN 計算的 logit 直接就變成 MLE 要算的 $p_{\\theta}(x)$. 同時藉由換成二分類問題, 也避開了很難計算的 $Z_{\\theta}$ 問題.為了不影響閱讀流暢度, 推導過程請參照 Appendix 所以我們可以透過引入一個 Noise pdf 來達到估計 training data 的 generative model 了. 這也是為什麼叫做 Noise Contrastive Estimation. Representation由於透過 NCE 訓練我們可以得到 $\\theta$, 此時只需要用 $\\theta_f$ 的 NN 來當作 feature extractor 就可以了. 總結最後流程可以總結成下面這張圖: 最後聊一下 CPC (Contrastive Predictive Coding) [2]. 我覺得跟 NCE 就兩點不同: 我們畫的 NCE 圖裡的 $w$, 改成論文裡的 $c_t$, 所以變成 network 是一個 conditioned 的 network 不是一個二分類問題, 改成 N 選 1 的分類問題 (batch size $N$, 指出哪一個是正例), 因此用 categorical cross-entorpy 當 loss 所以文章稱這樣的 loss 為 infoNCE loss 同時 CPC [2] 論文中很棒的一點是將這樣的訓練方式也跟 Mutual Information (MI) 連接起來.證明了最小化 infoNCE loss 其實就是在最大化 representation 與正例的 MI (的 lower bound). 這些背後數學撐起了整個利用 CPC 在 SSL (Self-Supervised Learning) 的基礎. 簡單講就是不需要昂貴的 label 全部都 unsupervised 就能學到很好的 representation.而近期 facebook 更利用 SSL 學到的好 representation 結合 GAN 在 ASR 達到了 19 年的 STOA WER. 論文: Unsupervised Speech Recognition or see [9] SSL 好東西, 不試試看嗎? AppendixPrior pdf:$$\\begin{align} p(C=1)=\\frac{N_p}{N_p+N_n} \\\\ p(C=0)=1-p(C=1) \\\\ \\end{align}$$ Generative pdf:$$\\begin{align} p(x|C=1)=p_{\\theta}(x) \\\\ p(x|C=0)=q(x) \\end{align}$$ 因此 Posterior pdf:$$\\begin{align} p(C=1|x)=\\frac{p(C=1)p(x|C=1)}{p(C=1)p(x|C=1)+p(C=0)p(x|C=0)}=\\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_r q(x)} \\\\ p(C=0|x)=\\frac{p(C=0)p(x|C=0)}{p(C=1)p(x|C=1)+p(C=0)p(x|C=0)}=\\frac{N_r q(x)}{p_{\\theta}(x)+N_r q(x)} \\\\ \\end{align}$$其中 $N_r=\\frac{N_n}{N_p}$ 因此 likelihood 為:$$\\begin{align} \\text{likilhood}=\\prod_{t=1}^{N_p} p(C_t=1|x_t) \\cdot \\prod_{t=1}^{N_n} p(C_t=0|x_t) \\end{align}$$ Loss 為 negative log-likelihood:$$\\begin{align} - \\mathcal{L}_{nce} = \\sum_{t=1}^{N_p} \\log p(C_t=1|x_t) + \\sum_{t=1}^{N_n} \\log p(C_t=0|x_t) \\\\ = N_p \\left[ \\frac{1}{N_p} \\sum_{t=1}^{N_p} \\log p(C_t=1|x_t) \\right] + N_n \\left[ \\frac{1}{N_n} \\sum_{t=0}^{N_n} \\log p(C_t=0|x_t) \\right] \\\\ \\propto \\left[ \\frac{1}{N_p} \\sum_{t=1}^{N_p} \\log p(C_t=1|x_t) \\right] + N_r \\left[ \\frac{1}{N_n} \\sum_{t=0}^{N_n} \\log p(C_t=0|x_t) \\right] \\end{align}$$ 當固定 $N_r$ 但是讓 $N_p\\rightarrow\\infty$ and $N_n\\rightarrow\\infty$. 意味著我們固定正負樣本比例, 但取無窮大的 batch. 重寫上式成:$$\\begin{align} - \\mathcal{L}_{nce} = \\mathbb{E}_{x\\sim p_d} \\log p(C=1|x) + N_r \\mathbb{E}_{x\\sim q} \\log p(C=0|x) \\\\ \\therefore \\text{} -\\nabla_{\\theta}\\mathcal{L}_{nce} = \\nabla_{\\theta}\\left[ \\mathbb{E}_{x\\sim p_d} \\log \\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_rq(x)} + N_r\\mathbb{E}_{x\\sim q} \\log \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\right] \\\\ = \\mathbb{E}_{x\\sim p_d} \\color{orange}{\\nabla_{\\theta} \\log \\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_rq(x)}} + N_r \\mathbb{E}_{x\\sim q} \\color{green}{\\nabla_{\\theta} \\log \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} } \\end{align}$$ 計算橘色和綠色兩項, 之後再代回來: $$\\begin{align} \\color{orange}{\\nabla_{\\theta} \\log \\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_rq(x)}} = \\nabla_{\\theta}\\log\\frac{1}{1+N_r\\frac{q(x)}{p_{\\theta}(x)}} = -\\nabla_{\\theta}\\log \\left( 1+\\frac{N_rq(x)}{p_{\\theta}(x)} \\right) \\\\ = -\\frac{1}{1+\\frac{N_rq(x)}{p_{\\theta}(x)}}\\nabla_{\\theta}\\frac{N_rq(x)}{p_{\\theta}(x)} = -\\frac{N_rq(x)}{1+\\frac{N_rq(x)}{p_{\\theta}(x)}}\\nabla_{\\theta}\\frac{1}{p_{\\theta}(x)} \\\\ = -\\frac{N_rq(x)}{1+\\frac{N_rq(x)}{p_{\\theta}(x)}} \\frac{-1}{p_{\\theta}^2(x)} \\nabla_{\\theta} p_{\\theta}(x) \\\\ = \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\left[ \\frac{1}{p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) \\right] \\\\ = \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta} \\log p_{\\theta}(x) \\end{align}$$ $$\\begin{align} \\color{green}{\\nabla_{\\theta} \\log \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)}} = -\\nabla_{\\theta} \\log\\left( 1+\\frac{p_{\\theta}(x)}{N_rq(x)} \\right) = -\\frac{1}{1+\\frac{p_{\\theta}(x)}{N_rq(x)}} \\nabla_{\\theta} \\frac{p_{\\theta}(x)}{N_rq(x)} \\\\ = -\\frac{1}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) \\\\ = -\\frac{p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\left[ \\frac{1}{p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) \\right] \\\\ = -\\frac{p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} \\log p_{\\theta}(x) \\end{align}$$ 將 (34), (38) 代回去 (29) 得到: $$\\begin{align} - \\nabla_{\\theta}\\mathcal{L}_{nce} = \\mathbb{E}_{x\\sim p_d} {\\color{orange}{\\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta} \\log p_{\\theta}(x)}} - N_r \\mathbb{E}_{x\\sim q} {\\color{green}{\\frac{p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} \\log p_{\\theta}(x)}} \\\\ = \\sum_x \\left[ p_d(x) \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta} \\log p_{\\theta}(x) \\right] - \\sum_x \\left[ q(x) \\frac{N_r p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} \\log p_{\\theta}(x)\\right] \\\\ = \\sum_x \\frac{(p_d(x)-p_{\\theta}(x))N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ = \\sum_x \\frac{(p_d(x)-p_{\\theta}(x))q(x)}{\\frac{p_{\\theta}(x)}{N_r}+q(x)} \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ \\end{align}$$ 當 $N_r\\rightarrow\\infty$ 意味著我們讓負樣本遠多於正樣本, 上式變成:$$\\begin{align} \\lim_{N_r\\rightarrow\\infty} - \\nabla_{\\theta}\\mathcal{L}_{nce} = \\sum_x \\frac{(p_d(x)-p_{\\theta}(x))q(x)}{0+q(x)} \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ = \\sum_x (p_d(x)-p_{\\theta}(x)) \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\left( \\nabla_{\\theta}\\log u_{\\theta}(x) -\\nabla_{\\theta}\\log Z_{\\theta} \\right) \\end{align}$$ 此時我們發現這 gradient 也與 Noise pdf $q(x)$ 無關了! 最後我們將 MLE and NCE 的 gradient 拉出來對比一下:$$\\begin{align} -\\nabla_{\\theta}\\mathcal{L}_{mle} = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\nabla_{\\theta} \\log u_{\\theta}(x) \\\\ -\\nabla_{\\theta}\\mathcal{L}_{nce} = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\left( \\nabla_{\\theta}\\log u_{\\theta}(x) -\\nabla_{\\theta}\\log Z_{\\theta} \\right) \\end{align}$$ 我們發現 MLE and NCE 只差在一個 normalization factor (or partition) $Z_{\\theta}$.最魔術的地方就在於 NCE 論文 [1] 證明最佳解本身的 logit 已經是 probability 型式, 因此也不需要 normalize factor. 論文裡說礙於篇幅沒給出證明, 主要是來自 Theorem 1 的結果: 所以我們不妨將 $Z_{\\theta}=1$, 結果有: $$\\begin{align} \\color{red} {\\nabla_{\\theta}\\mathcal{L}_{mle} = \\nabla_{\\theta}\\mathcal{L}_{nce}} \\\\ \\color{red} {\\Rightarrow \\theta_{mle} = \\theta_{nce}} \\\\ \\end{align}$$ Reference 2010: Noise-contrastive estimation: A new estimation principle for unnormalized statistical models 2019 DeepMind infoNCE/CPC: Representation learning with contrastive predictive coding 2019 FB: wav2vec: Unsupervised pre-training for speech recognition 2020 MIT &amp; Google: Contrastive Representation Distillation Noise Contrastive Estimation 前世今生——从 NCE 到 InfoNCE “噪声对比估计”杂谈：曲径通幽之妙 [译] Noise Contrastive Estimation The infoNCE loss in self-supervised learning High-performance speech recognition with no supervision at all","tags":[{"name":"Noise Contrastive Estimation","slug":"Noise-Contrastive-Estimation","permalink":"https://bobondemon.github.io/tags/Noise-Contrastive-Estimation/"},{"name":"NCE","slug":"NCE","permalink":"https://bobondemon.github.io/tags/NCE/"},{"name":"infoNCE","slug":"infoNCE","permalink":"https://bobondemon.github.io/tags/infoNCE/"}]},{"title":"Distributed Data Parallel and Its Pytorch Example","date":"2020-12-20T04:19:38.000Z","path":"2020/12/20/Distributed-Data-Parallel-and-Its-Pytorch-Example/","text":"訓練時候的平行化可分為: Model Parallel: 所有 GPUs 跑同一個 batch 但是各自跑模型不同部分 Data Parallel: GPUs 跑不同的 batches, 但跑同一個完整的模型 由於 Data Parallel 跑同一個完整模型且各 GPU 都用自己複製的一份, 在 update 參數時要如何確保更新一致? 可分為 synchronous 和 asynchronous update. (文章後面會詳細討論) 本文討論 Data Parallel with Synchronous update. 既然要做 data parallel, 第一件事情便是如何對不同 GPU 分派不同的 batches, 接下來我們就使用 PyTorch 做這件事. 指派不同 Batch 給不同 GPU直接上一個 toy example (minimal_distributed_data_example.py) 123456789101112131415161718192021222324252627282930313233343536# file: minimal_distributed_data_example.pyimport ...class SimpleDataset(torch.utils.data.Dataset): def __init__(self, start, end): assert(start &lt; end) self.start, self.end, self.data_num = start, end, end - start def __len__(self): return self.data_num def __getitem__(self, idx): return idx + self.startif __name__ == '__main__': # ===== Distributed Settings world_size = int(os.environ.get('WORLD_SIZE', 1)) local_rank = 0 is_distributed = world_size &gt; 1 if is_distributed: torch.distributed.init_process_group(backend='nccl') local_rank = torch.distributed.get_rank() torch.cuda.set_device(local_rank) device = torch.device(\"cuda\", local_rank) # ===== Dataset/DataLoader Settings dataset = SimpleDataset(0, 4*6) sampler = DistributedSampler(range(4*6), shuffle=False, seed=1111) # Shuffle here (set True) if needed rather than in DataLoader print(f'========== device:&#123;device&#125;') data_parallel_dl = DataLoader(dataset, batch_size=4, num_workers=8, shuffle=False, sampler=sampler) # since we use sampler, so we set shuffle to False (default) in DataLoader # ===== Traverse All Data arr = [] for sample_batch in data_parallel_dl: arr += sample_batch.tolist() t = np.random.randint(100)/100.0 sample_batch.to(device) print('sleep &#123;:.2f&#125;; device:&#123;&#125;\\t&#123;&#125;'.format(t, device, sample_batch)) time.sleep(t) print(f'device:&#123;device&#125;\\n&#123;np.sort(np.array(arr))&#125;') [Line 23~27 有關 Dataset/DataLoader] Line 24 dataset 只是一個 0 到 23 的 int list. Line 27 DataLoader 在分配 batches 給不同 GPUs 時只需要將 sampler 使用 DistributedSampler 創建就可以. DistributedSampler 在分配一個 batch 除了會指定資料是那些 index 之外, 還會指定該筆 batch 是要分到哪個 gpu. [Line 14~22 有關 Distributed Settings]在執行這個檔案的時候, 我們會使用 torch.distributed.launch, 範例指令如下: 1CUDA_VISIBLE_DEVICES=2,3 python -m torch.distributed.launch --nproc_per_node=2 --use_env minimal_distributed_data_example.py 此時 PyTorch 會開啟兩個 processes 去執行你的 .py, 這裡注意不是 threads, 這是因為 python Global Interpreter Lock (GIL) 的原因, 使用 thread 效率會不高. 另外使用 --use_env 則會在各自的 process 裡設定環境變數: WORLD_SIZE (範例 = 2) LOCAL_RANK (範例 = 0 or 1) 因此 line 17 我們便可藉由 world_size 得知是否為 distributed 環境. 是的話 line 20 就可以拿到這個 process 的 local_rank (可以想成是 worker 的編號, 也就是第幾個平行的單位), 接著 line 21, 22 就可以根據 local_rank 設置 gpu. [Line 28~36 有關 go through all data] 在執行時, 各個 process 會拿到相對應個 batches. Line 35 模擬處理該筆資料所花的時間. Line 36 為確認自己這個 process 總共拿到那些 batches. 以範例來說, 兩個 gpus 應該要拿到 exclusive 的兩個 sets 其聯集是 {0,1, …, 23}. 結果如下: Good Job! 現在我們會把每個 GPU 都分配不同的 batches 了, 不過還有一個關鍵的問題: 該怎麼各自計算 gradients 然後 update? 這就開始討論 update 的兩種 case, synchronous and asynchronous update. Asynchronous Update Synchronous: 每一次模型 update 要等到所有 device 的 batch 都結束, 統合後 update Asynchronous: 每個 device 算完自己的 batch 後即可直接 update 可以想像非同步的化可以更新的比較有效率, 但可能效果會不如同步的方式.Asynchronous 會遇到的狀況是算完 gradient 後要 update parameters 時, parameters 已經被其他 process update 過了, 那為什麼還可以 work? Asynchronous 狀況 1範例假設兩個 GPU (1&amp;2) 其參數空間都在 $\\theta_a$. Step 1. 假設 GPU2 先算完 $\\Delta P_2(\\theta_a)$ 並且 update 到 $\\theta_b$: $$\\begin{align} \\theta_b = \\theta_a + \\Delta P_2(\\theta_a) \\end{align}$$ Step2. 這時候 GPU1 算完 gradient 了, 由於當時算 gradient 是基於 $\\theta_a$, 因此 gradient 為 $\\Delta P_1(\\theta_a)$, 但是要 update 的時候由於已經被 GPU2 更新到 $\\theta_b$ 了, 所以會更新到 $\\theta_c$: $$\\begin{align} \\theta_c = \\theta_b + \\Delta P_1(\\theta_a) \\end{align}$$ 這裡讀者可能會疑問, 計算 gradient 與 update 時根據的參數是不同, 這樣 update 會不會出問題? 以上面這個例子來說, 還剛好沒事. 原因是其實等同於 synchronous update: $$\\begin{align} \\theta_c = \\theta_a + \\left[ \\Delta P_2(\\theta_a) + \\Delta P_1(\\theta_a) \\right] \\end{align}$$ 那可能會繼續問, 這只是剛好, 如果一個 GPU 比另一個慢很多, 會怎樣? 我們看看 case 2 Asynchronous 狀況 2GPU2 太快了… 已經 update 好幾輪 好吧… 想成類似有 momentum 效果吧 實務上會在幾次的 update 過後強制 synchronize update 一次, 可以想像如果一些條件成立 (譬如 gradients 是 bounded), 應該能保證收斂 (這邊我沒做功課阿, 純粹猜測) Synchronous Update每個 gpu 都算完各自 batch 的 gradients 後, 統一整理 update parameters, 常見兩種方式: Parameter Server Ring Allreduce 接著介紹的這兩種方法圖片主要從 Baidu: Bringing HPC techniques to deep learning [Andrew Gibiansky] 筆記下來. Parameter Server 的 Synchronous Update一次 Update 分兩步驟 GPU 0 全部都拿到 GPU 1~4 的 Gradients 後, 更新 parameters GPU 0 把 model 發送給 GPU 1~4 假設有 $N$ 個 GPU, 通信一次花費時間 $K$, 則 PS 方法成本為: Gradients passing: $(N-1)K$ Model passing: $(N-1)K$ Total $2K(\\color{orange}{N}-1)$, 跟 GPU 數量正比 Ring Allreduce 比較多圖, 特別拉出一個 section 說明 Ring Allreduce 的 Synchronous Update每一個 GPU 都分別有一個傳送和接收的對象 GPU, 分配起來正好形成一個環. 假設每個 GPUs 都算好 gradients 了, 並且我們將 gradients 分成跟 GPU 數量一樣的 $N$ 個 chunks: 這方法分兩步驟: Scatter Reduce All Gather 1. Scatter Reduce 做完 $N-1$ 次 iteration 後可以發現每張 GPU 都會有一個是完整的 chunk. 2. All Gather 做完 $N-1$ 次 iteration 後可以發現每張 GPU 都拿到所有完整的 chunk. All Gather 流程跟 Scatter Reduce 是一樣, 只是將累加行為變成取代而已. 成本每個 GPUs 都得到統合後的 gradients, 因此 各個 GPU 上的 model 可以各自 update (gradients 相同, 所以 update 後的 models 也相同) 假設有 $N$ 個 GPU,則成本為: 通信一次花費時間 $K/N$ (因為我們分成 $N$ 個 chunks 同時傳輸) Scatter reduce: $(N-1)K/N$ All gather: $(N-1)K/N$ Total $2K(\\color{orange}{N}-1)/\\color{orange}{N}$, 跟 GPU 數量無關 PyTorch: Model with DDP還記得最開頭的範例嗎? 我們做到了把每個 GPU 都分配不同的 batches, 但還不會將各自計算 gradients 統合然後 update. 其實我們只需要針對上面範例的 minimal_distributed_data_example.py 做點修改就可以. 針對 model 作如下改動: 1model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank) 這樣就使得 model 的 backward() 成為 sync op. 也就是在呼叫 loss.backward() 會等到每張 GPU 的 gradient 都算完且 sync 了 (PS or All Gather 都可以) 才會接下去執行. 注意事項 由於每個 process 都有自己的 optimizer(scheduler), 而 momentum 會根據當前的 gradient update, 如何確保每個 optimizers 都相同?Ans: 由於 .backward() 是 sync op, 因此 opt.step() 時每個 processes 的 gradients 已經同步了, 所以 momentum 會根據相同的 gradient update Batch-norm 的 statistics 同步?Ans: See torch.nn.SyncBatchNorm Save checkpoint 時在一張卡上存就可以 (通常用 LOCAL_RANK=0 的那個 process) 怎麼確保每個 process 上的 model random initial 相同的 weights?Ans: DistributedDataParallel 在 init 時就會確保 parameters/buffers sync 過了, see here model 經過 DistributedDataParallel 包過後 name 會多一個前綴 module., 如果訓練和加載模型一個使用 DDP 一個沒有 load_state_dict 有可能會因此出錯, 需自行處理 一些 metrics 如 accuracy/loss 由於在各個 GPUs 計算, 可以利用 torch.distributed.all_reduce, torch.distributed.all_gather 等來 syncSee DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED and Appendix 有一個不錯的 DDP 範例 [2] 如果可以的話, 推薦使用 PyTorch Lightning, 直接幫你把這些繁瑣的細節包好, 告訴它要用幾張 GPUs 就結束了. Reference[1] Bringing HPC Techniques to Deep Learning[2] A good example of DDP in PyTorch Appendix使用 torch.distributed.all_reduce 來同步不同 GPU 之間的 statistics與本文上面的範例 codes 雷同, 主要增加 AvgMetric 當範例說明 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import math, time, os, fireimport numpy as npimport torchfrom torch.utils.data import DataLoaderfrom torch.utils.data.distributed import DistributedSamplerfrom torch.utils.data import RandomSampler, SequentialSampler, Samplerimport torch.distributed as distclass AvgMetric: def __init__(self): self._acc = torch.zeros([]) self._num = torch.zeros([], dtype=torch.long) def reset(self): self._acc = 0 self._num = 0 def update(self, value_arr): self._acc += value_arr.sum().item() self._num += value_arr.numel() def summarize(self): acc = self._acc.clone().cuda() num = self._num.clone().cuda() if dist.is_available() and dist.is_initialized(): dist.all_reduce(acc, op=dist.ReduceOp.SUM) # all tensors in each local rank have final results dist.all_reduce(num, op=dist.ReduceOp.SUM) # all tensors in each local rank have final results # dist.reduce(acc, 0, op=dist.ReduceOp.SUM) # only tensor in local rank 0 has final results # dist.reduce(num, 0, op=dist.ReduceOp.SUM) # only tensor in local rank 0 has final results return acc.item() / num.item()class SimpleDataset(torch.utils.data.Dataset): def __init__(self, start, end): assert start &lt; end self.start = start self.end = end self.data_num = end - start def __len__(self): return self.data_num def __getitem__(self, idx): return idx + self.startdef run(shuffle=False): print(f\"shuffle=&#123;shuffle&#125;\") # ===== Distributed Settings world_size = int(os.environ.get(\"WORLD_SIZE\", 1)) local_rank = 0 is_dist = world_size &gt; 1 if is_dist: torch.distributed.init_process_group(backend=\"nccl\") local_rank = torch.distributed.get_rank() torch.cuda.set_device(local_rank) # makes tensor.cuda() to the specified cuda device device = torch.device(\"cuda\", local_rank) # ===== Dataset/DataLoader Settings num_workers = 16 dataset = SimpleDataset(0, 4 * 6) indices = list(np.arange(4 * 6)) print(indices) sampler = ( DistributedSampler(indices, shuffle=shuffle, seed=1111) # DistributedSubsetSampler(indices, shuffle=shuffle, seed=1111) if is_dist else RandomSampler(indices) if shuffle else SequentialSampler(indices) ) print(f\"========== device:&#123;device&#125;\") data_parallel_dl = DataLoader(dataset, batch_size=4, num_workers=num_workers, shuffle=False, sampler=sampler) avg_metric = AvgMetric() # ===== Traverse all data arr = [] for sample_batched in data_parallel_dl: arr += sample_batched.tolist() t = np.random.randint(100) / 100.0 sample_batched.to(device) print(\"sleep &#123;:.2f&#125;; device:&#123;&#125;\\t&#123;&#125;\".format(t, device, sample_batched)) avg_metric.update(sample_batched) time.sleep(t) print(f\"device:&#123;device&#125;\\t&#123;np.sort(np.array(arr))&#125;\\tavg=&#123;avg_metric.summarize()&#125;\")# ========== [Entry Point] ==========if __name__ == \"__main__\": # Usage: # Single GPU: `CUDA_VISIBLE_DEVICES=2 python practice.py --shuffle=False` # Multiple GPU: `CUDA_VISIBLE_DEVICES=2,3 python -m torch.distributed.launch --nproc_per_node=2 --use_env practice.py --shuffle=True` fire.Fire(run) Output 為: Cheers! 👏","tags":[{"name":"Distributed Data Parallel (DDP)","slug":"Distributed-Data-Parallel-DDP","permalink":"https://bobondemon.github.io/tags/Distributed-Data-Parallel-DDP/"},{"name":"PyTorch","slug":"PyTorch","permalink":"https://bobondemon.github.io/tags/PyTorch/"}]},{"title":"Quantization 的那些事","date":"2020-10-03T01:35:24.000Z","path":"2020/10/03/Quantization-的那些事/","text":"NN 在做 quantization 時採用的是非對稱的方式, real ($r$) 和 quantized ($q$) values 對應關係如下: 其中 zero point $Z$ 會跟 $q$ 相同 type, 例如 int8, 而 scaling value $S$ 則會跟 $r$ 相同, 例如 float. 以 uint3 (0~7) 做 quantization, 如下圖所示: 本篇討論以下兩點: 同一個 real 值如何在不同的 $Z$/$S$ 做轉換, e.g.: $q_1$ with ($Z_1$/$S_1$) 如何對應到 $q_2$ with ($Z_2$/$S_2$) PyTorch 的 Quantization Aware Training (QAT) 討論 在不同 $Z$/$S$ 轉換有兩個常見理由: 在做 NN 的 quantization 時候, 每個 layer 的 output domain 都不同, 這導致了使用不同的 $Z$/$S$. 又或者丟給 NN 做 inference 之前, mfcc/mfb 需要先轉換到 NN input 的 $Z$/$S$ quantized domain 上. 額外提一點 PyTorch 的 quantized Tensor 其實就只是比原本的 Tensor 多了 $Z$ and $S$. 例如給定 $Z$ and $S$, torch.quantize_per_tensor 會將一個正常的 tensor 從 $r$ 轉成 $q$, 官網範例: 12345&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)tensor([-1., 0., 1., 2.], size=(4,), dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()tensor([ 0, 10, 20, 30], dtype=torch.uint8) 以下我們都以 uint8 當作 quantized 的 type, real value 以 float (4 bytes) 為準. 而 int 為 4 bytes. 先使用 Float 轉換要將第一個 domain ($Z_1$/$S_1$) 的數值轉換到第二個 domain ($Z_2$/$S_2$) 最簡單的方法就是先把第一個 domain 的 $r_1$ 算出來, 再利用第二個 domain 的 $Z_2$/$S_2$ 求得 $q_2$ $$\\begin{align} \\color{orange}{r_1}=(float)\\left( \\left( (int32)q_1-Z_1 \\right)*S_1 \\right) \\\\ q_2=\\text{uint8_saturated_round}\\left( \\frac{\\color{orange}{r_2}}{S_2}+Z_2 \\right) \\end{align}$$ 由於 $r_2=r_1$ 因此 (2) 可計算出 $q_2$. 但這樣計算還是用到 float, 其實我們可以完全使用 integer 運算來達成. 純用 Integer 運算 其中 $M&gt;1.0$ 是沒有意義的, e.g. $S_1&gt;S_2$. 如下圖舉例來說, data domain 分布只會在 8 個點位置上, 使用更細的 resolution 去存沒意義. $M_0$ 很明顯可以用 Q0.31 的 int32 來保存, 所以 $M_0$ 與 $(q_1-Z_1)$ 相乘的時候使用 fractional multiplication, 最後 $2^{-n}$ 使用 shift 即可. 什麼是 fractional multiplication? 一張圖表示就知道: 最後我們要驗證的話其實可以跟上一段講的 Float 版本對比就可以. 矩陣運算的 Quantization 轉換其實 convolution 裡的矩陣運算只是原來的 $r_2=r_1$ 變成 $r_3=r_1r_2$ 的關係而已, 其餘都相同. 貼一張論文的內容即可. 更多內容可以參考論文 ref [1], 例如使用 ReLU6 替代 ReLU, 因為如果我們使用 uint8 的話由於 ReLU6 將 domain 限制在 [0,6] 之間, 這樣 8 bits 可以用 $Z=0$, $S=1.0/2^5=0.03125$ 來表示. 同時最後再轉換成 quantization model 時可以直接拿掉 ReLU6 (因為直接使用 quantization 就好) Symmetric Fixed Point傳統上常見的 fixed point 採用的是 symmetric quantization, 例如 Q4.3 這種 int8 的表示方式 (-8.0 ~ 7.875). 但它其實只是 asymmetric quantization 的特例. Q4.3 基本上就是 $Z=0$ 和 $S=1.0/2^3=0.125$ 的 asymmetric quantization. PyTorch 的 Quantization Aware Training (QAT) 筆記PyTorch 1.7.0 quantization doc 一開始要先對你的 NN Module 先作如下改動: 在自己定義的 NN Module 裡, 所有用到 torch.nn.functional 的 op 都轉換成 torch.nn.Module 在自己定義的 NN Module 裡, forward 時先將 input 過 QuantStub(), 然後最後 output 過 DeQuantStub(). QuantStub() 會將正常的 input tensor 變成 quantized tensor (裡面包含 $Z$/$S$), 然後 DeQuantStub() 會將 quantized tensor 轉換成正常的 tensor. 在自己定義的 NN Module 裡, 使用 torch.quantization.fuse_modules 定義你的 fuse_model function. 目前 PyTorch 只支援有限種 modules fusion (see function fuse_known_modules in fuse_modules.py). 接著 QAT 為以下幾個步驟: 將 NN 的 object (net) 設定為 net.train() (如果只是做 post-quantization 則用 net.eval()).這是因為 QAT 要在 training 時模擬 inference 的 quantization precision loss, 所以要插入很多 fake-quantization 的 op. 可以參考論文 ref [1] 的 Figure C.4 到 Figure C.8. 而如果只是 post-quantization 則在原來正常的 floating trianing 完後, 將 net.eval() 設定好直接就 fuse model 了 (torch.quantization.fuse_modules 對是 train or eval 有不同的 fuse 行為). 呼叫 net.fuse_model().例如假設我們要 fuse [&#39;conv1&#39;, &#39;bn1&#39;, &#39;relu1&#39;], PyTorch 會將第一個 Module 變成 fused Module, 剩下的兩個為 Identity() Module 將 net 設定 attribute qconfig.例如: net.qconfig= torch.quantization.get_default_qat_qconfig(&#39;fbgemm&#39;) 呼叫 torch.quantization.prepare_qat(net, inplace=True).此 function 主要幫你做兩件事情: a. propagate qconfig: 對所有子 Module 設定相對應的 qconfig (因為步驟3我們只針對 root Module 設定 qconfig) b. add observer/fake-quantization: observer 為簡單的 min/max 線性量化方式(或 histogram 方式等). 將圖需要 quantization 的地方安插好這些 observer/fake-quantization. 執行一般 training 流程.在 training 的過程中就會順便統計好對應的 min/max 等, 然後每個 tensor 的 $Z$/$S$ 也會對應得到 (通常用 moving average 方式做 smoothing). 最後轉換成 quantized model torch.quantization.convert(net, inplace=True) 以上一個最小範例如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import torchimport torch.nn as nnfrom torch.quantization import QuantStub, DeQuantStubimport torch.quantizationclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.bn1 = nn.BatchNorm2d(6) self.relu1 = nn.ReLU() self.quant = QuantStub() self.dequant = DeQuantStub() def forward(self, x): x = self.quant(x) x = self.relu1(self.bn1(self.conv1(x))) x = self.dequant(x) return x # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization # This operation does not change the numerics def fuse_model(self): torch.quantization.fuse_modules(self, ['conv1', 'bn1', 'relu1'], inplace=True)net = Net()print('===== Before fuse_model:')print(net)print('===== After fuse_model:')net.train()net.fuse_model()print(net)print('===== Setting qconfig:')# Specify quantization configuration# Start with simple min/max range estimation and per-tensor quantization of weightsnet.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')print(net.qconfig)print('===== After torch.quantization.prepare:')torch.quantization.prepare_qat(net, inplace=True)print(net)# Do your regular trainingtraining_loop(net)print('===== After torch.quantization.convert:')torch.quantization.convert(net, inplace=True)print(net) 最後附上一個很棒的 convolution and batchnorm fusion 解說 [連結], 作者是 Nenad Markuš Reference Paper: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference (BETA) STATIC QUANTIZATION WITH EAGER MODE IN PYTORCH Nenad Markuš: Fusing batch normalization and convolution in runtime","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"https://bobondemon.github.io/tags/PyTorch/"},{"name":"Quantization Aware Training (QAT)","slug":"Quantization-Aware-Training-QAT","permalink":"https://bobondemon.github.io/tags/Quantization-Aware-Training-QAT/"},{"name":"Asymmetric Quantization","slug":"Asymmetric-Quantization","permalink":"https://bobondemon.github.io/tags/Asymmetric-Quantization/"},{"name":"Symmetric Quantization","slug":"Symmetric-Quantization","permalink":"https://bobondemon.github.io/tags/Symmetric-Quantization/"}]},{"title":"TF Notes (7), Some TF2.x Eager Mode Practices","date":"2020-06-26T02:52:18.000Z","path":"2020/06/26/TF-Notes-some-TF2-x-eager-mode-practices/","text":"為了學習 TF2.x 只好把以前練習的一些 projects 重寫一次, 但後來時間斷斷續續的, 所以只做了一部分. 總之先記錄一下目前的練習進度吧. TFDataset 練習: jupyter notebook if map() has random ops: dataset.shuffle().batch().cache().map().prefetch() map() has NO random ops: dataset.shuffle().batch().map().cache().prefetch() NeuralStyleTransfer: jupyter notebook 練習 optimization 變數是 input x 而不是原來的 weights w TSLearning ToyExample: jupyter notebook 固定某一部分的 model 最原始的 distillation AutoEncoder jupyter notebook: decoder 部分使用 deconvolution jupyter notebook: 全部 FC 但 decoder 是 encoder 的 transpose (share weights) GAN jupyter notebook: MMGAN jupyter notebook: WGAN jupyter notebook: WGAN-div Adversarial Domain Adaptation jupyter notebook 和 介紹及實驗結果 還有很多沒練習到, VAE, seq2seq, transformer 等….只好再說了","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://bobondemon.github.io/tags/TensorFlow/"}]},{"title":"CTC Model and Loss","date":"2020-05-31T10:07:24.000Z","path":"2020/05/31/CTC-Model-and-Loss/","text":"CTC model 是一個 decoder 部分為簡單的 (independent) linear classifer 的 seq2seq model. 因此 input frame 有 $T$ 個, 就會有 $T$ 個 output distribution vectors. 正常來說 (ex: ASR) output token 數量 $N&lt;T$, 所以會有 alignment 問題. 以往的 alignment (HMM) 強迫每個 frame index 都需對應到一個 phone’s state, 但 CTC 允許對應到 “空” 的 state (null or blank). 這讓 CTC 的 alignment 比 HMM 更有彈性. RNN-T 是另一種比 CTC 更有彈性的 alignment 表達方式. CTC 的 gradient 可以非常有效率的用 dynamic programming 求得 (forward/backward 演算法, 下圖). 因此採用 gradient-based optimization 方法就很合適. 本文會詳細介紹上面提到的幾點. Decoding 部分不介紹. CTC Model and Loss: PDF Slide 連結 Link PDF","tags":[{"name":"CTC","slug":"CTC","permalink":"https://bobondemon.github.io/tags/CTC/"}]},{"title":"Exp of Adversarial Domain Adaptation","date":"2020-05-17T09:15:21.000Z","path":"2020/05/17/Exp-of-Adversarial-Domain-Adaptation/","text":"Domain Adaptation 是希望在 source domain 有 label 但是 target domain 無 label 的情況下, 能針對 target domain (或同時也能對 source domain) 進行分類任務. “Adversarial” 的意思是利用 GAN 的 “對抗” 想法: Label predictor 雖然只能保證 source domain 的分類. 但由於我們把 feature 用 GAN 消除了 domain 之間的差異, 因此我們才能期望這時候的 source domain classifier 也能作用在 target domain. 這篇文章 張文彥, 開頭的圖傳達的意思很精確, 請點進去參考. 接著嘗試複現了一次 Domain-Adversarial Training of Neural Networks 的 mnist(source) to mnist_m(target) 的實驗. 上一篇說明 GAN 的 framework: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} D(x) - E_{x\\sim P_G}f^*(D(x)) \\right] \\\\ G^*=\\arg\\min_G{Div\\left(P_d\\|P_G\\right)} + reg(G) \\\\ \\end{align}$$ 對於 Adversarial Domain Adaptation 來說只要在正常 GAN 的 training 流程中, update $G$ 時多加一個 regularization term $reg(G)$ 就可以了. 而 $reg(G)$ 就是 Label Predictor 的 loss, 作用就是 train $G$ 時除了要欺騙 $D$, 同時要能降低 prediction error. 實驗 source domain 為標準的 mnist, 而 target domain 是 modified mnist, 如何產生可以參考Daipuwei/DANN-MNIST. 下圖是 mnist_m 的一些範例: 我們先來看一下分佈, 藍色的點是 mnist, 紅色是 mnist_m, 用 tSNE 跑出來的結果明顯看到兩個 domain 分佈不同: 我們之前說過, 不用管 GRL (Gradient Reversal Layer), 就一般的 GAN 架構, 加上 regularization term 就可以. 聽起來很容易, 我就隨手自己用了幾個 CNN 在 generator, 幾層 fully connected layers 給 classifier 和 discriminator 就做了起來. 發現怎麼弄都訓練不起來! 產生下面兩種情形: GAN too weak:重新調整了一下 $reg(G)$ 的比重後…. GAN too strong:兩個 domain 的 features 幾乎完全 overlap, 然後 classifier 幾乎無作用 (也看不出有10個分群). 話說, 這圖很像腦的紋路? 貪食蛇? 迷宮? 肚子裡的蛔蟲? 後來在嘗試調了幾個參數後仍然訓練不起來. 這讓我感到很挫折. 實在受不了後, 參考了網路上的做法改成以下幾點: WGAN 改成用 MMGAN RMSProp(1e-4) 改成 Adam(1e-3) 使用網路上一個更簡單的架構 github 改成用 MMGAN 後, 去掉 BN layer 就能訓練起來 然後就可以訓練起來了(翻桌xN), 訓練後的結果如下: 可以看到在 mnist 辨識率 ~99% 的情形下, mnist_m 能夠有 83.6% 的辨識率 (沒做 adaptation 只有約50%) Feature 的分布如下圖 (藍色的點是 mnist, 紅色是 mnist_m): 雖然還有一些 feature 沒有完全 match 到, 但已經很重疊了. 同時我們也能明顯到到 10 群的分類. 結論雖然理論上的理解很容易, 但實作起來卻發現很難調整. GAN 就是那麼難搞阿…. Reference GAN framework Domain-Adversarial Training of Neural Networks 參考產生 mnist_m 的 codes Daipuwei/DANN-MNIST Domain-Adversarial Training of Neural Networks with TF2.0: lancerane/Adversarial-domain-adaptation 張文彥 Domain-adaptation-on-segmentation 自己實驗的 jupyter notebook","tags":[{"name":"GAN","slug":"GAN","permalink":"https://bobondemon.github.io/tags/GAN/"},{"name":"ADDA","slug":"ADDA","permalink":"https://bobondemon.github.io/tags/ADDA/"}]},{"title":"Framework of GAN","date":"2020-05-11T12:29:12.000Z","path":"2020/05/11/Note-for-Framework-of-GAN/","text":"說來汗顏, 自從17年三月筆記完 WGAN 後, 就沒再碰 GAN 相關的東西了. 惡補了一下 李宏毅GAN 的課程和其他相關資料, 因此筆記一下. MMGAN(最原始的GAN), NSGAN(跟MMGAN差別在 G 的 update 目標函式有點不同), f-GAN, WGAN, ADDA (Adversarial Discriminative Domain Adaptation), infoGAN, VAE-GAN 等… 這些全部都是 follow 下面這樣的 framework: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} D(x) - E_{x\\sim P_G}f^*(D(x)) \\right] \\\\ G^*=\\arg\\min_G{Div\\left(P_d\\|P_G\\right)} + reg(G) \\\\ \\end{align}$$ 其中 $P_d$ 為 real data pdf, $P_G$ 為 generator 產生的 data pdf. $f^*$ 帶入不同的定義會產生不同的 divergence, 這之後會再說明. 式 (1) 定義了 $P_G$ 與 $P_d$ 的 divergence, 其中這個 divergence 的值為藉由解這個最佳化問題求得的. 式 (2) 表示要找的 $G$ 就是 divergence 最小的那個. Divergence 最小 ($=0$) 同時也表示 $P_G=P_d$ (生成器鍊成). 如果同時考慮 regularization term, $reg(G)$, 則會有很多變化產生, 如 ADDA, infoGAN, VAE-GAN… 我們接著來看 MMGAN, NSGAN, f-GAN, WGAN, ADDA, infoGAN, VAE-GAN 這些怎麼 fit 進這個框架. MMGANMMGAN 是 MinMax GAN 的縮寫, 指的是最原始的 GAN. 將 (1) 中的 $D(x)$ 使用 $\\log D(x)$ 替換, 並且 $f^*(t)=-\\log(1-exp(t))$ 替換得到如下式子: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} \\log D(x) - E_{x\\sim P_G}[-\\log(1-D(x))] \\right] \\\\ \\end{align}$$ 稍微再整理一下: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} \\log D(x) + E_{x\\sim P_G}[\\log(1-D(G(z)))] \\right] \\\\ \\end{align}$$ 這就是 GAN discriminator 原始的式子. 而我們知道給定 $G$ 上述的最佳解為 \\( D_G^*(x) = \\frac{P_d(x)}{P_d(x)+P_G(x)} \\), 並帶入 (4) 我們得到: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = -\\log4+KL(p_d\\vert\\frac{p_d+p_g}{2})+KL(p_g\\vert\\frac{p_d+p_g}{2}) \\\\ =-\\log4+2JSD(p_d \\vert p_g) \\end{align}$$ 因此 discriminator 的最大化目的是計算出 JS divergence. 而 generator $G$ 求解沒什麼好說, 直接對 (3) 最小化: $$\\begin{align} G^*=\\arg\\min_G E_{x\\sim P_G}[\\log(1-D(x))] \\end{align}$$ 注意到與 (2) 對比, MMGAN 只是沒有 regularization term 而已. NSGANNSGAN 為 Non-Saturating GAN 縮寫, 與 MMGAN 只差在 generator $G$ 求解式子不同, 原本是希望在一開始 generator 比較差的情形下用 (7) 算的 gradient 會太小, 因此改成下式, 使得 gradient 能在一開始的時候比較大, 讓 update 動起來. NSGAN generator $G$ 為: $$\\begin{align} G^*=\\arg\\min_G E_{x\\sim P_G}[-\\log(D(x))] \\end{align}$$ 如果我們將 \\( D_G^*(x) = \\frac{P_d(x)}{P_d(x)+P_G(x)} \\) 帶入並整理, 我們會發現: $$\\begin{align} G^*=\\arg\\min_G E_{x\\sim P_G}[-\\log(D^*(x))] \\\\ =\\arg\\min_G \\left[ KL(P_G\\|P_d)-2JSD(P_d\\|P_G) \\right] \\end{align}$$ 產生了兩個互相 trade-off 的 objective funtion… 這造成了矛盾 詳細推導請參考 令人拍案叫绝的Wasserstein GAN 一文, 非常棒的文章. 引用文章內的說明: 一句话概括：最小化第二种生成器loss函数，会等价于最小化一个不合理的距离衡量，导致两个问题，一是梯度不稳定，二是collapse mode即多样性不足。 f-GAN我們在 MMGAN 時提到 “將 (1) 中的 $D(x)$ 使用 $\\log D(x)$ 替換, 並且 $f^*(t)=-\\log(1-exp(t))$ 替換” 則會得到 discriminator 就是在求解 JS divergence. 那麼有沒有其他設定會產生其他 divergence 呢? 有的, 藉由 f-GAN 的定義可以囊括各式各樣的 divergence. 使用李老師的說明流程筆記: 首先定義 f-divergence, 可以發現 JSD, KL, reverse-KL, Chi square 等等都屬於其中的特例. 接著說明 convex function 的 conjugate function. 最後才說明怎麼跟 GAN 產生關聯 (神奇的連結). f-divergence$$\\begin{align} Div_f(P\\|Q)=\\int_x q(x)f\\left( \\frac{p(x)}{q(x)} \\right) dx \\\\ \\text{where } f \\text{ is }\\color{orange}{convex} \\text{ and } f(1)=0 \\end{align}$$ 明顯知道 $p(x)=q(x)$ 時 $Div_f(P|Q)=0$, 同時可以證明 $Div_f(P|Q)\\geq 0$, 因此滿足 divergence 定義(search “Divergence (statistics) wiki” for definition): $$\\begin{align} Div_f(P\\|Q)=\\int_x q(x)f\\left( \\frac{p(x)}{q(x)} \\right) dx \\\\ \\geq f\\left( \\int_x q(x)\\frac{p(x)}{q(x)} dx \\right)=f(1)=0 \\\\ \\end{align}$$ $f$ 是 convex 這點很重要, 才能將 (13) 到 (14) 使用 Jensen’s inequality. 定義不同 $f$ 會產生不同 divergence, 常見的為(李老師slide): 由於 $f$ 是 convex, 而每一個 convex function 都會有一個 conjugate function $f^*$ (它也是 convex), 利用這個特性最後可以跟 GAN 連起來. 因此以下先說明 conjugate function. Fenchel ConjugateEvery convex function $f$ has a conjugate function $f^*$: $$\\begin{align} f^*(t)=\\max_{x\\in dom(f)}\\{xt-f(x)\\} \\end{align}$$ 老師的投影片非常形象的表示出 $f$ 與 $f^*$ 的關係L 還具體舉了個當 $f(x)=x\\log x$ 的例子: 與 GAN 的關聯這是我覺得非常厲害的地方. 首先 $f^*$ 的 conjugate 就變回 $f$ 了, 它們互為 conjugate. $$\\begin{align} f^*(t)=\\max_{x\\in dom(f)}\\{xt-f(x)\\}\\longleftrightarrow f(x)=\\max_{t\\in dom(f^*)}\\{xt-f^*(t)\\} \\end{align}$$ 將 (11) 利用 conjugate 的關係重新表示一下 $$\\begin{align} Div_f(P\\|Q)=\\int_x q(x)f\\left( \\frac{p(x)}{q(x)} \\right) dx \\\\ =\\int_x q(x) \\left( \\max_{t\\in dom(f^*)} \\left[ \\frac{p(x)}{q(x)}t - f^*(t) \\right] \\right) dx \\end{align}$$ 厲害的地方來了…. 假設我們有一個 function $D$ 可以直接幫我們解出 (18) 的那個 $t$ 是什麼, 也就是: $$\\begin{align} D(x)=\\hat{t}=\\arg\\max_{t\\in dom(f^*)} \\left[ \\frac{p(x)}{q(x)}t - f^*(t) \\right] \\end{align}$$ 那麼 $Div_f(P||Q)$ 直接就是 $$\\begin{align} Div_f(P||Q)=\\int_x q(x) \\left[ \\frac{p(x)}{q(x)}\\hat{t} - f^*(\\hat{t})) \\right] dx \\end{align}$$ 實作上 $D$ 的表達能力有限, 同時讓我們找到最準的那個叫做 $\\hat{D}$, 因此只能求得一個下界並整理一下得到: $$\\begin{align} Div_f(P||Q)\\geq \\int_x q(x) \\left[ \\frac{p(x)}{q(x)}\\hat{D}(x) - f^*(\\hat{D}(x))) \\right] dx \\\\ \\approx \\int_x q(x) \\left[ \\frac{p(x)}{q(x)}\\hat{D}(x) - f^*(\\hat{D}(x))) \\right] dx \\\\ = \\int_x {p(x)\\hat{D}(x)}dx - \\int_x{q(x)f^*(\\hat{D}(x))} dx \\\\ = E_{x\\sim P}\\left[ \\hat{D}(x) \\right] - E_{x\\sim Q}\\left[ f^*( \\hat{D}(x) ) \\right] \\\\ = \\max_D \\left[ E_{x\\sim P}\\left[ D(x) \\right] - E_{x\\sim Q}\\left[ f^*( D(x) ) \\right] \\right] \\\\ \\end{align}$$ 請把 (25) 跟 (1) 比較, 其實就一模一樣. 因此, 只要 $f$ 是 convex function , 且 $f(1)=0$, discriminator $D$ 的最佳化問題 ((1) 用 $f$ 的 conjugate, $f^*$, 帶入) 就是在計算兩個分布的 f-divergence. 論文直接給出各種 f-divergence 的 $f$ and $f^*$ 因此我們可以發現 MMGAN 和 LSGAN 都是 f-GAN 的一種特例. WGAN具體請參考之前自己筆記的文章 李老師的講義對於 Earth Mover’s Distance (或稱 Wasserstein distance) 講解得很清楚, 其中的一個參考連結更解釋了 Wasserstein distance 如何轉換成求解 $\\max_D$ 且 $D$ 必須限制在 Lipschitz 條件下. 總之這裡要說的是, Wasserstein distance 不屬於 f-divergence, 但也完全 follow 我們一開始說的 (1) &amp; (2) 的架構: 令 $f^*(x)=x$ 同時多一個限制是 $D\\in k-Lipschitz$ $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_{D\\in k-Lipschitz}\\left[ E_{x\\sim P_d} D(x) - E_{x\\sim P_G}D(x) \\right] \\\\ \\end{align}$$ 求解 discriminator 的最佳化問題其實就是在估算兩個分布的 divergence. 原始論文針對 $D\\in k-Lipschitz$ 的限制直接用很暴力的 weight clipping 方法解掉. 因此後面有一篇 WGAN-GP (Gradient Panelty) 的方式補強. 這裡不展開討論, 因為我也沒什麼研究, 簡單帶過一點待讀的論文. 另外有一篇 SN-GAN “Spectral Normalization for Generative Adversarial Networks“ 看起來是一種主流訓練 WGAN 的方式, 事先就將 gradient 都限制 norm&lt;=1. 這篇文章大致整理各種變體, 參考連結. 關於 regularization term, $reg(G)$Adversarial Domain Adaptation我們先說 Domain-Adversarial Training of Neural Networks 這篇經典的文章. Generator 現在做的是 feature extractor 的工作, 而我們希望 target domain 的 feature 能跟 source domain 的 feature 分佈一樣, 這樣在 source domain (有 label) 訓練好的 model, 就能直接在 target domain (無 label) 上作用. 要做到無法區分出這個 feature 是 source or target domain 這件事情….正好就可以用 GAN 的方式達到. 不看 Label Predictor 的部分的話, 就是一個典型的 GAN. 作用就是把 source and target 的 feature 投影到共同的空間, 並且分不開. 但缺少 Label Predictor 有可能造成 feature extractor 產生 trivial solution (例如全部 map 到 constant) 這樣也能使 discriminator 分不開. 因此加上 Label Predictor 除了避免這件事外, 也保證在 source domain 能夠很好的完成我們的分類任務. 注意, 因為 label 只有在 source domain, 因此 label predictor 只能保證 source domain 的分類. 但由於我們把 feature 用 GAN 消除了 domain 之間的差異, 因此我們才能期望這時候的 source domain classifier 也能作用在 target domain. 論文使用了一個叫做 Gradient Reversal Layer (GRL), 其實我們可以忽略這件事情, 因為這只是 discriminator and generator 一個 maximize 另一個 minimize, 而使得要 update generator 時當時算的 discriminator gradient 要取負號. 我們照正常的 GAN training 就可以了. Label Predictor 的 loss 具體就是 (2) 的 regularization term, $reg(G)$. 這是希望我們 train $G$ 的時候除了要欺騙 $D$, 同時要能降低 $reg(G)$ (prediction loss). 後續有一篇 Advesarial Discriminative Domain Adaptation 算是豐富了這種架構. 論文裡對 source and target 的 feature extractor 使用不同的 neural networks. 並且一開始的 source domain feature extractor 是事先訓練好的. 然後後面的 GAN 部分訓練的時候, target domain 的 feature extractor 要去匹配 source domain 的. 這樣做的好處是至少一邊的分佈是固定住的, 比較容易訓練. 同時也簡化了訓練流程, 見下圖: infoGAN詳細就不解釋了, 事實上推導較複雜但實作上卻異常容易, 之後有機會再記錄一下. 總之在原始 GAN 架構上多了一個 Decoder, 用來還原 generator input 中所指定的部分($c$). Decoder 希望能將 $c$ 無損的還原, 那麼什麼叫無損? 指的就是 Mutual Information of $c$ and $\\hat{c}$ 最大. 其中 $\\hat{c}$ 表示由 Decoder 還原出來的結果. 還原的 loss term 基本就是 $reg(G)$, 同樣的理解, $G$ 除了要騙過 $D$ 之外, 多了一個任務就是使得還原的 loss 愈小愈好. 附上李宏毅教授課程的兩張圖片: VAE-GAN直接上老師的 slides 以 GAN 的角度來看, $G$ 除了要欺騙 $D$ 之外, 還多了 VAE 的 loss ($reg(G)$) 用來 reconstruct 原本的 input image. 對 GAN 來說是有好處的, 因為 GAN 雖然能夠產生夠真的 image, 但是會自己”捏造”, 因此多了 VAE 的 $reg(G)$ 會讓捏造的情況降低. 以 VAE 的角度來看, GAN 的 loss 變成了 regularization term 了. 也就是說 VAE 除了要產生跟原本接近的 image (pixel-level), 還要能騙過 $D$. 這是為了補足 VAE 的缺點, 原始 VAE 的目標函式是 pixel-level 的 l2-norm, 這跟人類認為的真實不真實不一致, 因此 AVE 會產生模糊的 image. 用 GAN 的 loss 當成 regularization term 則補足了 VAE 這點. 因此 VAE-GAN 這是個互惠的結構, 很漂亮. 這個結構新的一篇 Adversarial Latent Autoencoders 粗略講也是 VAE-GAN 架構, 只是 reconstruction 不是再 image, 而是在 latent space. 論文結果十分驚艷, github. 結論本篇開頭說明的 framework 基本可以解釋了上述各種 GAN. 但由於本魯才疏學淺, 還有一大堆沒看的變種, EBGAN, BEGAN, CycleGAN, …etc. 只能說之後讀到的時候, 看看能否試著這麼解釋. GAN 實在太多了, 可以看看 GAN Zoo 有多少用 GAN 來命名的架構(似乎停止更新). Reference 李宏毅GAN f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization 令人拍案叫绝的Wasserstein GAN WGAN筆記 Wasserstein GAN and the Kantorovich-Rubinstein Duality Spectral Normalization for Generative Adversarial Networks GAN论文阅读笔记3：WGAN的各种变体 by 林小北 InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets Domain-Adversarial Training of Neural Networks Advesarial Discriminative Domain Adaptation Autoencoding beyond pixels using a learned similarity metric Adversarial Latent Autoencoders","tags":[{"name":"GAN","slug":"GAN","permalink":"https://bobondemon.github.io/tags/GAN/"},{"name":"ADDA","slug":"ADDA","permalink":"https://bobondemon.github.io/tags/ADDA/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://bobondemon.github.io/tags/Generative-Model/"},{"name":"fGAN","slug":"fGAN","permalink":"https://bobondemon.github.io/tags/fGAN/"},{"name":"WGAN","slug":"WGAN","permalink":"https://bobondemon.github.io/tags/WGAN/"},{"name":"infoGAN","slug":"infoGAN","permalink":"https://bobondemon.github.io/tags/infoGAN/"},{"name":"VAE-GAN","slug":"VAE-GAN","permalink":"https://bobondemon.github.io/tags/VAE-GAN/"}]},{"title":"Notes for (conditional/cross-)Entropy, Mutual-information, ...","date":"2020-05-02T03:37:12.000Z","path":"2020/05/02/Notes-for-conditional-cross-Entropy-Mutual-information/","text":"整理下 entropy 的一些東西, 不然久沒看老是忘記. Entropy of a r.v. $X$: $H(X)$ Conditional Entropy of $Y$ given $X$: $H(Y|X)$ Cross(Relative) Entropy of two pdf, $p$ and $q$: $D(p\\Vert q)$ Mutual Information of two r.v.s: $I(X;Y)$ 文章會明確定義每一項, 然後在推導它們之間關係的同時會解釋其物理意義. 最後其實就可以整理成類似集合關係的圖 (wiki) Entropy$$\\begin{align} H(X) = \\sum_{x\\in X}{p(x)\\log{\\frac{1}{p(x)}}} \\end{align}$$ 一個 r.v. $X$ 假設配給他的 pdf 為 $p$, 則可以算出相對應的 entropy $H(X)$, 所以我們其實可以看成是 $H(p)$.可以知道當 $p$ 是 uniform distribution 時 entropy 達到最大 (後面再證明). 同時該定義可以很容易看出 $H(X)\\geq 0$. 直觀解釋: 因為 uniform distribution 時最大 (最無法確定哪一個 outcome 最有可能), 另一個極端為 $X$ 為 constant (r.v. 只有一個 outcome) 所以我們可以理解為 entropy 為量測一個 r.v. $X$ 的不確定性 如果要對每一個 outcome 決定要用多少 bit 來代表, 我們希望平均花的 bits 數能最小, 直觀上機率愈大的 outcome 用愈小的 bits 來表達. 因此 outcome $x_i$ 我們用 $\\log{\\frac{1}{p(x_i)}}$ 這麼多 bits 表示, 則 entropy 代表了 encode 所有 outcome 所需要的平均 bits 數, 則這個數是最小的. (這可以從下面的 cross entropy 得到證明) 我們可以用 entropy 來表示該 r.v. 所包含的資訊量. 因為比 entropy 更多的資訊量都是 reduandant 的 (由上一點可看出) 為了方便我們會交叉使用 資訊量 或是 encode 的最小平均 bits 數 來表示 entropy 的物理意義. Cross(Relative) Entropy為量測兩個 pdfs, $p(x)$ and $q(x)$ 之間的 “divergence”, 也稱為 KL divergence. 注意 divergence 不需滿足三角不等式也不需滿足對稱性, 因此不是一個 “metric”. 但也足夠當成是某種”距離”來看待 (不是指數學上的 norm) $$\\begin{align} D(p\\Vert q) = \\sum_x{p(x)\\log{\\frac{p(x)}{q(x)}}} \\end{align}$$ 由 Jensen’s inequality 可以推得 $D(p\\Vert q) \\geq 0$, 另外 $D(p\\Vert q)=0$ iff $p=q$ $$\\begin{align} D(p\\Vert q) = - \\sum_x{p(x)\\log{\\frac{q(x)}{p(x)}}} \\\\ \\text{by Jensen&apos;s inequality: } \\geq \\log\\sum_x{p(x)\\frac{q(x)}{p(x)}} = 0 \\end{align}$$ 重寫一下 (2):$$\\begin{align} 0\\leq D(p\\Vert q) = \\sum_x{p(x)\\log{\\frac{1}{q(x)}}} - H(p) \\end{align}$$ 這說明了對於 outcome $x_i$ (true distribution 為 $p(x)$) 我們用 $\\log{\\frac{1}{\\color{red}{q}(x_i)}}$ 這麼多 bits 表示是浪費的. 所以證明了上面 Entropy 第二種解釋. 直觀解釋: $D(p\\Vert q)$ 表示使用錯誤的 distribution $q$ 來 encode 要多花的平均 bits 數量 Conditional Entropy$$\\begin{align} H(Y|X) = \\sum_{x\\in X}{p(x)H(Y|x=X)} \\\\ =\\sum_x{p(x)\\sum_y{p(y|x)\\log{\\frac{1}{p(y|x)}}}} \\end{align}$$ $H(Y|x=X)$ 解釋為 given $x$ encode $Y$ 的最小平均 bits 數 (就是 entropy 只是又重複說了一次), 但是每一個 $x$ 有自己的機率, 因此對 $x$ 再取一次平均. case 1:如果 $Y$ 完全可以由 $X$ 決定, 因此一旦給定 $x$, $Y$ 就是一個 constant, 所以 $H(Y|x=X)=0$. 再對 $X$ 算期望值仍然是 0. case 2:如果 $X$ and $Y$ independent. $$\\begin{align} (7)=\\sum_x{p(x)\\sum_y{p(y)\\log{\\frac{1}{p(y)}}}}=\\sum_x{p(x)H(Y)}=H(Y) \\end{align}$$ 得到結論 $H(Y|X)=H(Y)$ Case 1 and 2 說明了 $X$ and $Y$ 在完全依賴和完全無關時 $H(Y|X)$ 分別為 $0$ and $H(Y)$. 直覺上我們可以認為 $0\\leq H(Y|X) \\leq H(Y)$, 因為剛好是兩個極端情形. 但直覺是不夠的, 我們證明一下. 使用定義, 我們可以很容易推導出 $$\\begin{align} \\color{orange}{I(X;Y)\\equiv}H(Y)-H(Y|X)=\\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)p(y)}}} \\\\ =D(p(x,y)\\Vert p(x)p(y)) \\geq 0 \\end{align}$$ 我們這裡先偷跑出了 mutual information $I(X;Y)$ 的定義, 下面會再詳細講. 直觀解釋:$H(Y|X)$ 表示給了 $X$ 的資訊後, $Y$ 剩下的資訊量. 其中 $0\\leq H(Y|X) \\leq H(Y)$ Chain Rule for Entropy我們在把 (7) 做個運算: $$\\begin{align} H(Y|X)=(7)=-\\sum_x{p(x)\\sum_y{p(y|x)\\log{p(y|x)}}} \\\\ =-\\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)}}} \\\\ =-\\sum_{x,y}{p(x,y)\\log{p(x,y)}}+\\sum_{x,y}{p(x,y)\\log{p(x)}} \\\\ =H(X,Y) - H(X) \\end{align}$$ 直觀解釋:給定 $X$ 後 $Y$ 剩下的資訊量 ($H(Y|X)$) 就等於 $X,Y$ 整體的資訊量扣掉單獨 $X$ 部分的資訊量 Mutual Information(9) 已經先給出了定義, 我們這裡重複一次: $$\\begin{align} I(X;Y)\\equiv\\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)p(y)}}} =D(p(x,y)\\Vert p(x)p(y))\\\\ \\text{(用 entropy 定義得到) }=H(Y)-H(Y|X) \\\\ \\text{(用 entropy 定義得到) }=H(X)-H(X|Y) \\end{align}$$ 在用 Chain Rule for Entropy: $H(Y)=H(X,Y)-H(X|Y)$ 代進 (16) 得到 $$\\begin{align} I(X;Y)=H(X,Y)-H(Y|X)-H(X|Y) \\end{align}$$ 我知道這麼多式子一定眼花了….好在完全可以用集合的 Venn diagram 表示! 所有式子的直觀表達 Reference wiki mutual information Dr. Yao Xie, ECE587, Information Theory, Duke University 本篇內容只是 lecture 1 ~ 4 的範圍","tags":[{"name":"Entropy","slug":"Entropy","permalink":"https://bobondemon.github.io/tags/Entropy/"},{"name":"Conditional Entropy","slug":"Conditional-Entropy","permalink":"https://bobondemon.github.io/tags/Conditional-Entropy/"},{"name":"Cross Entropy","slug":"Cross-Entropy","permalink":"https://bobondemon.github.io/tags/Cross-Entropy/"},{"name":"Mutual Information","slug":"Mutual-Information","permalink":"https://bobondemon.github.io/tags/Mutual-Information/"}]},{"title":"Determinant of Covariance Matrix","date":"2019-07-15T13:41:13.000Z","path":"2019/07/15/Determinant-of-Covariance-Matrix/","text":"筆記 covariance matrix $R$ 的 determinant 意義以及他的 bound. 這是在讀 Time-delay estimation via linear interpolation and cross correlation 時的 appendix 證明. 覺得有用就筆記下來. 開門見山, $det(R)$ 可以想成 volumn (等於所有 eigenvalues 相乘), 然後 upper bound 就是所有對角項元素相乘. $$\\begin{align} det(R)=\\prod_i \\lambda_i \\leq \\prod_i r_{ii} \\end{align}$$ $\\lambda_i$ 是 i-th eingenvalue. 事實上只要 $R$ 是 square matrix, 則 $|det(R)|$ 等於用每個 row vector 做出來的 “平行六面體” 的體積 [ref] 以下筆記論文中證明 $det(R)$ 的 upper bound, 從這個 bound 我們也能看出物理意義. Upper bound of the determinant of positive definite matrix[Theorem]: 令 $H$ 為 $L$ by $L$ 正定矩陣, 則$$\\begin{align} det(H) \\leq \\prod_{i=1}^{L} h_{ii} \\end{align}$$ [Proof]:先將 $H$ 作如下拆解$$\\begin{align} H=\\left( \\begin{array}{cc} \\tilde{H} &amp; h \\\\ h^T &amp; h_{LL} \\end{array} \\right) \\end{align}$$ 其中 $\\tilde{H}$ 是 $L-1$ by $L-1$ 矩陣.從 Determinant of block matrices 我們知道:$$\\begin{align} det(H)=det(\\tilde{H})(h_{LL}-h^T \\tilde{H}^{-1}h) \\end{align}$$ 因為 $H$ 是正定, 所以 $\\tilde{H}$ 也是正定, 包含其 inverse Every principal submatrix of a positive definite matrix is positive definite. 正定的 $det&gt;0$, 以及正定的二次式 $&gt;0$, 帶入到 (4) 就不難發現$$\\begin{align} det(H)\\leq h_{LL}det(\\tilde{H}) \\end{align}$$ 重複此步驟就能推導出 (2) Determinant of covariance matrix我們知道 covariance matrix $R$ 是正定 (嚴格上為半正定, 如果沒有兩個完全 linear depedent 的維度的話, 就是正定), 因此符合 upper bound (2). 觀察當 coordinate 之間為 independent 時, 表示非對角項都是 $0$, 只剩下對角項 (每個維度的 variance). 這時 (2) 的不等式變成等式, 對角項相乘意義相當於算 volumn 可以看出兩點結論 covariance matrix 對角項的相乘總是會比 $det$ 大 coordinate 之間是 independent 則 covariance matrix 對角項的相乘會等於 $det$ Correlation matrix我們知道 correlation matrix 對角項都是 $1$, 且是正定根據以上的討論知道:$$\\begin{align} 0\\leq det(\\mbox{corr}(X))\\leq 1 \\end{align}$$ Take Home Messages令 $R$ 為 covariance matrix, $\\tilde{R}$ 是 correlation matrix $R$ 對角項的相乘總是會比 $det(R)$ 大 coordinate 之間是 independent 則 $R$ 對角項的相乘等於 $det(R)$ $det(\\tilde{R})$ 在 0 和 1 之間 (包含) 令 $A$ 為 square matrix, 則 $|det(A)|$ 等於以每個 row vector 做出來的 “平行六面體” 的體積 [ref] Reference Time-delay estimation via linear interpolation and cross correlation Determinants and Volumes","tags":[{"name":"Covariance matrix","slug":"Covariance-matrix","permalink":"https://bobondemon.github.io/tags/Covariance-matrix/"},{"name":"Correlation matrix","slug":"Correlation-matrix","permalink":"https://bobondemon.github.io/tags/Correlation-matrix/"},{"name":"determinant","slug":"determinant","permalink":"https://bobondemon.github.io/tags/determinant/"}]},{"title":"TF Notes (6), Candidate Sampling, Sampled Softmax Loss","date":"2019-07-02T12:34:12.000Z","path":"2019/07/02/TF-Notes-Candidate-Sampling/","text":"NN 做分類最後一層通常使用 softmax loss, 但如果類別數量很大會導致計算 softmax 的 cost 太高, 這樣會讓訓練變得很慢. 假如總共的 class 數量是 10000 個, candidate sampling 的想法就是對於一個 input $x$ 採樣出一個 subset (當然需要包含正確的 label), 譬如只用 50 個 classes, 扣掉正確的那個 class, 剩下的 49 個 classes 從 9999 個採樣出來. 然後計算 softmax 只在那 50 個計算. 那麼問題來了, 這樣的採樣方式最終訓練出來的 logits 會是對的嗎? 它與未採樣前 (full set) 的 logtis 有何對應關係? 採用 candidate sampling 方式的 softmax loss 在 tensorflow 中已經直接有 op 了, 參考 tf.nn.sampled_softmax_loss. 文檔裡最終推導得到如下的一個式子: $$\\begin{align} \\log(P(y|x_i,C_i))=\\log(P(y|x_i))-\\log(Q(y|x_i))+K&apos;(x_i,C_i) \\end{align}$$ 推導過程自行看文檔就可以, 重要的是了解式子的物理意義.$C_i$ 是對 input $x_i$ 採樣出的 subset, 包含了 一個正確的類別標籤 和 其他採樣出的類別 $S_i$. $Q(y|x_i)$ 是基於 input $x_i$, label $y$ 被選中成為 $S_i$ 的機率. $K’$ 是跟 $y$ 無關的, 所以對於式子來說是 constant. 注意到式子的變數是 $y$ 代表了是 softmax 的哪一個 output node. 式 (1) 的解釋為: “在 candidate set $C_i$ 下的 logits 結果” 等於 “在 full set 下的 logtis 結果減去 $\\log Q(y|x_i)$”, $K’$ 會直接被 $\\log P(y|x_i)$ 吸收, 因為 logits 加上 constant 對於 softmax 來說會分子分母消掉, 所以不影響. 以下我們順便複習一下, 為什麼 logits 可以寫成 “$\\mbox{const}+\\log P(y|x)$” 這種形式. (包含複習 Entropy, cross-entropy, softmax loss) Entropy 定義$$\\begin{align} \\sum_i{q(x_i)\\log{\\frac{1}{q(x_i)}}} \\end{align}$$ 對於 input $x_i$, 其機率為 $q(x_i)$, 若我們使用 $\\log{\\frac{1}{q(x_i)}}$ 這麼多 bits 的數量來 encode 它的話, 則上面的 entropy 代表了 encode 所有 input 所需要的平均 bits 數, 而這個數是最小的. 用錯誤的 encoding 方式我們假設用 $\\log{\\frac{1}{p(x_i)}}$ 這麼多 bits 的數量來 encode 的話, 則平均 encode bits 數為: $$\\begin{align} \\sum_i{q(x_i)\\log{\\frac{1}{p(x_i)}}} \\end{align}$$ 這個數量一定會比 entropy 來的大, 而大出來的值就是我們使用錯誤的 encoding 造成的代價 (cross-entropoy). Cross-entropy如上面所說, 錯誤的 encoding 方式造成的代價如下: $$\\begin{align} \\mbox{Xent}(p,q)\\triangleq\\sum_i{q(x_i)\\log{\\frac{1}{p(x_i)}}} - \\sum_i{q(x_i)\\log{\\frac{1}{q(x_i)}}} \\\\ =\\sum_i{q(x_i)\\log{\\frac{q(x_i)}{p(x_i)}}} \\\\ \\end{align}$$ Sparse softmax loss最常見的情形為當只有 $q(x_j)=1$ 而其他 $x\\neq x_j$ 時 $q(x)=0$ 的話 ($q$ 變成 one-hot), 上面的 corss-entropy 變成: $$\\begin{align} \\mbox{SparseSoftmaxLoss}\\triangleq\\mbox{Xent}(p,q\\mbox{ is one-hot})=-\\log p(x_j) \\\\ =-\\log\\frac{e^{z_j}}{\\sum_i{e^{z_i}}}=-\\log e^{z_j} + \\log\\sum_i{e^{z_i}} \\\\ =-z_j + \\log\\sum_i{e^{z_i}} \\end{align}$$ 其中 $z_i$ 表示 i-th logtis, 參考 tf.nn.sparse_softmax_cross_entropy_with_logits Logits 的解釋j-th logtis $z_j$ 可解釋為 “const + class $j$ 的 log probability”. $$\\begin{align} z_j = \\mbox{cosnt} + \\log p(j) \\end{align}$$ 為什麼呢? 這是因為 logtis 經過 softmax 後會變成機率, 我們假設經過 softmax 後 node $j$ 的機率為 $p’(j)$, 計算一下這個值: $$\\begin{align} p&apos;(j)=\\frac{e^{z_j}}{\\sum_i e^{z_i}} \\\\ =\\frac{e^{\\log p(j)}e^{\\mbox{const}}}{e^{\\mbox{const}}\\sum_i e^{\\log p(i)}} \\\\ =\\frac{p(j)}{\\sum_i p(i)} \\\\ =p(j) \\end{align}$$ 這時候我們再回去對照開始的式 (1), 就能清楚的解釋 candidate sampling 的 logtis 和 full set 的 logits 之間的關係了. Sampled softmax loss由式 (1) 我們已經知道 candidate sampling 的 logtis 和 full set 的 logits 之間的關係. 因此在訓練的時候, 正常 forward propagation 到 logits 時, 這時候的 logits 是 full set 的. 但由於我們計算 softmax 只會在 candidate set 上. 因此要把 full set logits 減去 $\\log Q(y|x_i)$, 減完後才會是正確的 candiadtes logits. 對於 inference 部分, 則完全照舊, 因為原本 forward propagation 的結果就是 full set logits 了. 這也是 tf 官網範例這麼寫的原因: 123456789101112131415if mode == \"train\": loss = tf.nn.sampled_softmax_loss( weights=weights, biases=biases, labels=labels, inputs=inputs, ..., partition_strategy=\"div\")elif mode == \"eval\": logits = tf.matmul(inputs, tf.transpose(weights)) logits = tf.nn.bias_add(logits, biases) labels_one_hot = tf.one_hot(labels, n_classes) loss = tf.nn.softmax_cross_entropy_with_logits( labels=labels_one_hot, logits=logits) Reference tf.nn.sampled_softmax_loss Candidate Sampling tf.nn.sparse_softmax_cross_entropy_with_logits","tags":[{"name":"Entropy","slug":"Entropy","permalink":"https://bobondemon.github.io/tags/Entropy/"},{"name":"Candidate sampling","slug":"Candidate-sampling","permalink":"https://bobondemon.github.io/tags/Candidate-sampling/"},{"name":"Sampled softmax loss","slug":"Sampled-softmax-loss","permalink":"https://bobondemon.github.io/tags/Sampled-softmax-loss/"}]},{"title":"SphereFace Paper Study and Implementation Notes","date":"2019-06-18T13:13:46.000Z","path":"2019/06/18/SphereFace-paper-study-and-implementation-notes/","text":"SphereFace: Deep Hypersphere Embedding for Face Recognition 使得訓練出來的 embeddings 可以很好的使用 cosine similarity 做 verification/identification. 可以先網路上搜尋一下其他人的筆記和討論, 當然直接看論文最好.一般來說我們對訓練集的每個人用 classification 的方式訓練出 embeddings, 然後在測試的時候可以對比兩個人的 embeddings 來判斷是否為同一個人. 使用 verification 當例子, 實用上測試的人不會出現在訓練集中, 此情形稱為 openset 設定. 注意到 embedding 是使用 classification 方式訓練出來, 也就是說, 如果訓練集有 1000 個人, 最後一層的 softmax 就有 1000 個 nodes. 然後 embedding 一般取 softmax 前一層 (前兩層也可).測試時常見的做法就是計算兩個 embeddings 的 cosine similarity, 直觀上相同的人他們的 embedding 會接近, 因此夾角小 (cosine 大), 而不同的人夾角大 (cosine 小).但問題來了, 當初訓練 embedding 時並沒有針對 classification 用夾角來分類, 也就不能保證 softmax loss 對於使用 cosine similarity 是最有效的. Modified softmax loss (M-softmax loss) 和 Angular softmax loss (A-softmax loss) 就能針對這種情形 (測試時使用 cosine similarity) 計算 loss. A-softmax loss 比 M-softmax loss 條件更嚴苛, 除了希望針對 angular 做分類外, 還希望同一類的夾角能聚再一起, 不同類的夾角能盡量分開. 下面就說明一下 softmax loss, M-softmax loss and A-softmax loss, 然後以 tensorflow 的實作來說明 Softmax Loss其實沒什麼好說明的, 公式如下 Decision boundary 以兩類來看如下: $$\\begin{align} (W_1 - W_2)x+b_1 - b_2=0 \\end{align}$$ M-Softmax Loss如果我們將 $W_j$ 的 norm 限制為 1, 且去掉 biases, $b_j=0$, 則原來的 softmax loss 變成如下: Decision boundary 以兩類來看如下: $$\\begin{align} \\parallel x \\parallel (\\cos \\theta_1 - \\cos \\theta_2)=0 \\Rightarrow \\cos \\theta_1 = \\cos \\theta_2 \\end{align}$$ 我們可以發現 decision boundary 完全由夾角來決定了! 論文使用 toy example 來說明 M-softmax loss 造成的現象: A-Softmax Loss以兩類來說明, M-softmax loss 將 $x$ 分類成 class 1 的條件為 $\\cos \\theta_1 &gt; \\cos \\theta_2$, 也就是 $\\theta_1 &lt; \\theta_2$. A-softmax loss 則讓這個條件更嚴格, 它希望 $m$ 倍的 $\\theta_1$ 都還小於 $\\theta_2$, 因此條件為 $\\cos m\\theta_1 &gt; \\cos \\theta_2$. 論文中以幾何的方式說明很清楚: 因此 A-softmax loss 如下: 論文使用 toy example 來說明 A-softmax loss 造成的現象: 可以看到相比於 M-softmax loss, A-softmax loss 會使得 margin 增大 這種 within class 靠近, between class 拉遠就如同 LDA 的概念. A-softmax 也能造成這種效果且是在 angular 的 measure 下. 而常見的情形都是針對 euclidean distance, 例如使用 triplet loss (推薦這篇 blog 說明具體且 tensorflow 實現非常厲害). 原則上我們希望與 class $i$ 的夾角 $\\theta_i$ 愈小, 所算出來的 logits 也就是 $\\cos\\theta_i$ 要愈大, 所以放大 $m$ 倍的夾角所算出來的 logits, $\\cos m\\theta_i$ 必須要變小.但由於 $\\cos$ 是 periodic function, 一旦 $m\\theta_i$ 超過 $2\\pi$ 就反而可能使得 logits 變大, 這就適得其反了. 精確來說 $\\cos m\\theta_i &lt; \\cos\\theta_i$ 只會在 $\\theta_i$ 屬於 $[0,\\pi/m]$ 區間範圍內成立. 因此我們必須對 A-softmax loss 作如下改動: 其中 $$\\begin{align} \\psi(\\theta)=(-1)^k \\cos(m\\theta)-2k\\\\ \\mbox{where }\\theta\\in[\\frac{k\\pi}{m},\\frac{(k+1)\\pi}{m}]\\mbox{ and }k\\in[0,m-1] \\end{align}$$ 我們將 $\\psi$ 畫出來: 兩個觀察: 首先 $\\psi$ 的確會隨著角度變大而變小, 這符合我們要的 logits 的行為. 再來要計算出正確的 $\\psi(\\theta)$ 必須要先知道 $k$, 也就是需要知道 $\\theta$ 落在哪個區間才行. 第二點可能比較棘手, 我們思考一下怎麼在 tensorflow 的 graph 中實現 …. hmm…. 好像有點麻煩 Tensorflow Implementation A-softmax Loss其實網路上就很多 tensorflow 的實現了, 不看還好, 一看才發現 A-softmax loss 的 $\\psi$ 實現步驟如下: 這什麼操作?! 怎麼跟原來理解的 (3) and (4) 長相差這麼多! 網路上幾乎大家都直接拿來用, 也沒什麼說明. 不過我們仔細分析一下, 還是能發現端倪.首先注意到這樣的實現是基於 $m=4$ 做的. (論文的實驗最後在這個設定有不錯的效果) 因此將 $m=4$ 套入 (3)(4) 得: $$\\begin{align} \\psi(\\theta)=(-1)^k \\cos(\\color{red}{4}\\theta)-2k\\\\ \\mbox{where }\\theta\\in[\\frac{k\\pi}{\\color{red}{4}},\\frac{(k+1)\\pi}{\\color{red}{4}}]\\mbox{ and }k\\in[0,\\color{red}{3}] \\end{align}$$ 接著我們作如下分析: 發現 $s3=(-1)^k$ 和 $s4=-2k$, 因此 $$\\begin{align} \\psi(\\theta)=\\color{green}{(-1)^k} \\cos(4\\theta)\\color{blue}{-2k} = \\color{green}{s3}[1-8\\cos^2\\theta +8\\cos^4\\theta]\\color{blue}{+s4} \\end{align}$$ 而 $\\cos\\theta$ 則因為 weights $W$ 的 norm 限制為 1, 所以只需要 $Wx$ 再除以 $x$ 的 norm 即可. 到這裡最麻煩的實作問題分析完畢, 依樣畫葫蘆也可以做出 $m=2$, $m=3$. SummaryTake home messages: M-softmax loss 算出來的 embeddings 在 test 階段可以直接用 cosine measure A-softmax loss 更進一步使得各類別之間的角度拉更開, 達到 large margin 效果 A-softmax loss 實作上不好訓練, 可以使用論文中提到的訓練方法, 一開始偏向原來的 softmax loss, 然後漸漸偏向 A-softmax loss M-softmax loss 簡單實用, 經過 weight norm = 1 的條件, 論文中說明能去掉 prior 分布 Reference SphereFace: Deep Hypersphere Embedding for Face Recognition Blog: Triplet loss","tags":[{"name":"SphereFace","slug":"SphereFace","permalink":"https://bobondemon.github.io/tags/SphereFace/"},{"name":"Angular softmax loss","slug":"Angular-softmax-loss","permalink":"https://bobondemon.github.io/tags/Angular-softmax-loss/"},{"name":"Modified softmax loss","slug":"Modified-softmax-loss","permalink":"https://bobondemon.github.io/tags/Modified-softmax-loss/"}]},{"title":"Adaptive Filters 簡介 (2) Fast Convolution and Frequency Domain","date":"2019-06-08T15:35:35.000Z","path":"2019/06/08/Adaptive-Filters-Notes-2/","text":"上一篇說明了 time domain 的 adaptive filters, 由於是 sample-by-sample 處理, 因此太慢了不可用, 真正可用的都是基於 frequency domain. 不過在深入之前, 一定要先了解 convolution 在 input 為 block-by-block 的情況下如何加速. 本文內容主要參考 Partitioned convolution algorithms for real-time auralization by Frank Wefers (書的介紹十分詳盡). Convolution 分類如下: 我們就針對最常使用的情形介紹: Input (UP) and Filter (0). 這是因為實際應用 input 是 infinite length, 所以需要 block-by-block 給定, 而 filter 通常都是 finite length, 可以選擇不 partition, 或 uniformly partitioned 以便得到更低的延遲效果. 針對 block-based input 的 convolution, 我們有兩種架構: OverLap-and-Add (OLA) OverLap-and-Save (OLS) OLAOLA 相對來說很好理解的. 每一個新來的 data block $x_i$ (長度為 $M$), 都與 filter $h$ (長度為 $N$) 做 linear convolution, 產生的 output $y_i$ (長度為 $M+N-1$) 開頭的 $N-1$ 個結果與前一個output block 重疊的部分疊加 (“add”), 所以稱 overlap-and-ADD. 示意圖如下: OLSOLS 則從 output 角度來看. 根據現在的 output 來決定需要用到那些 input 做 linear convolution. 舉例 input block $x_i$ 長度為 $B=3$, filter $h$ 長度為 $N=4$, 則 output block $y_i$ 的結果可以從下圖來看出來: 注意到, 我們一開始先將 $h$ 右邊補上 $B-1=2$ 個 $0$, 而 input block $x_i$ 左邊補上 $N-1=3$ 個舊的 input data. 目的是把 $x_i$ 和 $h$ 都湊成 $B+N-1$ 這麼長.則我們可以發現, 針對增長後的 input and filter 做 lineaer convolution, 雖然會得到長度為 $2*(B+N-1)-1$ 的 output, 但這其中有 $B$ 個結果是我們要的! 因此我們只需要 “save” 需要的這 $B$ 個 output, 其他都丟較即可. 所以稱 overlap-and-SAVE. 如何有效率的做 linear convolution?不管是 OLA 或 OLS 都需要對兩個固定長度 (通常使用 padding $0$ 成等長) 的 signal 做 linear convolution. 怎麼有效率的做 linear convolution 就變得十分重要.我們都知道頻域的相乘相當於時域的 circular convolution. 因此如果能用 ciruclar convolution 來做出 linear convolution 的話, 我們就能轉到頻域上再相乘就可以了.Circular convolution 的定義如下[1], 其實概念也很容易: 我們只需要適當地 padding zeros, 就可以使得 padding 後的 signals 做 circular convolution 會等於原來的 singals 做 linear convolution. 如下圖[1] 因此使用 FFT-domain 的 circular convolution 來實現 fast linear convolution 流程如下 Fast Conv with OLA在 OLA 架構中使用 FFT-domain 的 circular convolution 如下: Padding zeros 不管在前還是在後都可以, 只要滿足 $K=\\geq M+B-1$ 避免 aliasing 即可. Fast Conv with OLS在 OLS 架構中使用 FFT-domain 的 circular convolution 如下: Input signal 不是 padding zeros, 而是在左邊 padding 之前的 input 訊號 (參考本篇上面的 OLS 段落), 用這樣的 padding 方式來看 circular convolution 的話, 每一次我們就 “save” output 的最後 $B$ 個結果即可. 在實作上通常會將 $B=N$, 並且設定 $K=2B=2N$, 這樣我們每一次只需要保留前一次的 input block, 並且 padding 給新來的 input block. Frequncy Domain Adaptive FilterFrequency Domain Adaptive Filter (FDAF) 請參考 [2], 整理的非常好, 所以這裡就不多描述, 完全可以照著實作出來! 我們會發現其實它採用的是我們上面說過的 Fast Convolution with OLS 架構, 只是 filter 必須 adaptive 更新. 以下是 python implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# In the frequency domain methods, notations are defined as:# x: reference signal, [-1, 1]# d: desired signal, [-1, 1]# step_size: step size# alpha: the alpha filter for tracking the energy for each bin# w: the retruned filter# e: the error signal, of size (itr_num,)# ========== FDAF (Frequency Domain Adaptive Filters)def FDAF(x,d,step_size,N=512,alpha=0.9): iter_num = len(d)//N-1 assert(iter_num&gt;0) # Init W = np.zeros(2*N,dtype=complex) pow_lambda = np.ones(2*N)*np.finfo(np.float32).eps rtn_e = np.zeros((iter_num-1)*N) # Main Iteration for itridx in range(1,iter_num): x_2blocks = x[(itridx-1)*N:(itridx+1)*N] # (2N) d_block = d[itridx*N:(itridx+1)*N] # (N) X = fft(x_2blocks) # (2N) Y = np.einsum('i,i-&gt;i',X,W) y = ifft(Y) # (2N) y = y[N:] # (N), discard first half block # print (y) # e = np.real(d_block - y) # (N) e = d_block - y # (N) # print(len(rtn_e)) rtn_e[(itridx-1)*N:itridx*N] = np.real(e) e = np.concatenate([np.zeros([N]),e]) # (2N) E = fft(e) # (2N) pow_lambda = alpha*pow_lambda + (1-alpha)*(np.abs(X)**2) # scale error signal, just like NLMS E = E/pow_lambda # Set the upper bound of E, to prevent divergence m_errThreshold = 0.2 Enorm = np.abs(E) # (2N) # print(E) for eidx in range(2*N): if Enorm[eidx]&gt;m_errThreshold: E[eidx] = m_errThreshold*E[eidx]/(Enorm[eidx]+1e-10) # Constraint Part gradient = np.einsum('i,i-&gt;i',X.conj(),E) # (2N) gradient = ifft(gradient) gradient[N:] = 0 gradient = fft(gradient) # (2N) # Update Part W = W + step_size*gradient return rtn_e Summary我們介紹了針對 input 是 block-by-block 給定時, 計算 linear convolution 的兩種架構: OLA, OLS. 而如何加速 linear convolution 我們則介紹了使用 circular convolution 來等價地完成 linear convolution. Circular convolution 可以利用頻域相乘來加快速度 (得益於 FFT 的效率). 除了對 input 切 block 之外, 我們也還可以對 filter $h$ 切 block, 這樣的好處是計算量可以在更低, 且 latency 也會降低. 這部分請參考書的 Ch5, 附上一張書本裡的架構圖: 這種方式其實很重要, 原因是 webrtc 中的 AEC 採用的是 Partitioned Block Frequency Domain Adaptive Filter (PBFDAF) [3], 就是 filter 也是 uniformly partitioned. 最後我們利用 OLA 和 fast convolution, 列出來 frequency domain AF 的架構圖. 同時如果想要進一步降低 latency 則需使用 PBFDAF[3] (filter 也 partition). Reference Partitioned convolution algorithms for real-time auralization by Frank Wefers Block Adaptive Filters and Frequency Domain Adaptive Filters by Prof. Ioan Tabus On the implementation of a partitioned block frequency domain adaptive filter (PBFDAF) for long acoustic echo cancellation","tags":[{"name":"Adaptive Filters","slug":"Adaptive-Filters","permalink":"https://bobondemon.github.io/tags/Adaptive-Filters/"},{"name":"OLA","slug":"OLA","permalink":"https://bobondemon.github.io/tags/OLA/"},{"name":"OLS","slug":"OLS","permalink":"https://bobondemon.github.io/tags/OLS/"},{"name":"circular convolution","slug":"circular-convolution","permalink":"https://bobondemon.github.io/tags/circular-convolution/"},{"name":"linear convolution","slug":"linear-convolution","permalink":"https://bobondemon.github.io/tags/linear-convolution/"}]},{"title":"Adaptive Filters 簡介 (1) Time Domain","date":"2019-05-14T14:03:03.000Z","path":"2019/05/14/Adaptive-Filters-Notes/","text":"粗略筆記 time domain adaptive filters, frequency domain adaptive filters 會在下一篇筆記. 應用以 Acoustic Echo Cancellation (AEC) 來說明. Motivation直接使用 wiki. AEC 要解決的是如下的情形 各個訊號關聯如下: $$\\begin{align} y(n)=h(n)\\ast x(n) \\\\ d(n)=y(n)+v(n) \\\\ \\hat{y}(n)=\\hat{h}(n)\\ast x(n)\\\\ e(n)=d(n)-\\hat{y}(n) \\end{align}$$ 目的是找到 $\\hat{h}$ 滿足下式: $$\\begin{align} \\hat{y}(n)\\approx y(n)\\Rightarrow e(n)\\approx v(n) \\end{align}$$ 對於第 $n$ 個 sample 點來說, 我們通常使用過去(含自己) $p$ 個 samples. 下面小寫粗體表示 vector, 大寫粗體表示 matrix. Optimal Solution也就是 Wiener solution. 但在真實世界中, 不管 reference signal ($x(n)$) or desired signal ($d(n)$) 都是 non-stationary 的. 最直接且暴力的想法就是每隔一段時間重新算一次 Wiener solution. 不過想當然爾這是行不通的. 因此就必須採用 Stochastic update 方式. Stochastic Update 上面紅色的式子就是典型的 LMS algorithm. 另外我們知道 optimization 還可以使用 second-moment, 也就是使用二階導函數 (Hessian Matrix). 這就是 Newton’s method: 針對 $\\mathbf{R}_{xx} \\mbox{ , } \\mathbf{R}_{xd}$ 使用不同的 approximation 方式就會得到不同演算法, 例如: 上面紅色的式子就是典型的 NLMS algorithm. 用這樣的方式還可推出 e-NLMS, leaky-LMS, RLS 等等… 實作上 NLMS 在 reference signal $x(n)$ 很小的時候, 由於公式上分母會除以 $x^Hx$, 而分子只有一次方 $x$, 因此除下來會導致 gradient 容易變大, 所以發散. 這是 NLMS 實作上要考慮的情形. Misadjustment我們知道最佳解為 Wiener solution, 但由於我們採用 stochastic gradient 方式, 也就是說 update 的 gradient 本身存在誤差, 這些 gradient 的 variance 就直接影響了最終收斂的效果跟 Wiener solution 的收斂結果之間的差距. 此差距我們稱 misadjustment 或稱 Excess Meam Square Error 直接擷取 Ali Sayed, Adaptive Filters p230 的定義: 為什麼要說這個呢? 是因為實作上有兩個因素會直接影響最終收斂效果的好壞, 分別是 tap length 和 step size. 從理論分析和實作經驗來說, tap length 太小會無法有效模擬 RIR, 而太大會導致 EMSE 提高 (收斂效果反而變差), 因此選取的 tap length 必須要根據 sampling rate 和要消除的 echo path 來計算一下. LMS or NLMS 的 EMSE 可猜考 Ali Sayed, Adaptive Filters p249 and p253. (條件已簡化在 ref and desired signals 為 stationary 情況) 另外 step size 較小會有較好的收斂效果, 但是收斂速度會慢且 tracking 能力較差. 一個有效的方式為使用 Practical Variable Step Size (PVSS) 方法, 具體可參考 待補 好了, time domain 到這就差不多了, 缺點也很明顯, 慢!, 因為是 sample-by-sample 處理. 接著稍微梳理一下 frequency domain 方法. Reference wiki Least mean squares filter Ali Sayed, Adaptive Filters","tags":[{"name":"Adaptive Filters","slug":"Adaptive-Filters","permalink":"https://bobondemon.github.io/tags/Adaptive-Filters/"},{"name":"AEC","slug":"AEC","permalink":"https://bobondemon.github.io/tags/AEC/"},{"name":"LMS","slug":"LMS","permalink":"https://bobondemon.github.io/tags/LMS/"},{"name":"NLMS","slug":"NLMS","permalink":"https://bobondemon.github.io/tags/NLMS/"}]},{"title":"Far Field Notes (4) How Spatial Feature Clusters","date":"2019-04-12T13:36:17.000Z","path":"2019/04/12/Far-Field-Notes-4-How-Spatial-Feature-Clusters/","text":"這是 far field 筆記系列第四篇, 寫這篇是因為做 CGMM-MVDR 時, 很好奇為何 spatial features 聚類的結果可以對應不同方向的聲源. 因此記錄下自己的一點想法. 假設我們有 $M$ 個麥克風, 則在 stft (short-time fourier transform) 上來說, $\\mathbf{f}_{\\omega,t}$ 表示一個頻率 $\\omega$, 時間 $t$ 的 $M$ 維向量. 對於某一個 $\\theta$ 方向的 narrowband 訊號, ideally 我們可以這麼表示 $$\\begin{align} \\mathbf{f}_{\\omega,t}^{\\theta}=f(\\omega)\\mathbf{\\upsilon}(\\theta)=f(\\omega) \\left[ \\begin{array}{clr} e^{-j\\omega\\tau_0} \\\\ e^{-j\\omega\\tau_1} \\\\ \\vdots \\\\ e^{-j\\omega\\tau_{M-1}} \\end{array} \\right] \\end{align}$$ $\\tau_i$ 表示由 $\\theta$ 產生的第 $i$ 個 mic 的 time delay. 因此 spatial feature 每個維度之間的 phase offset 關係是固定的, 由 $\\mathbf{\\upsilon}(\\theta)$ 決定. 所有如果有兩個方向 $\\theta_1$ and $\\theta_2$ 的聲源, phase offset 關係各自是 $\\mathbf{\\upsilon}(\\theta_1)$ 和 $\\mathbf{\\upsilon}(\\theta_2)$. 問題是要用什麼樣的 cluster 能對相同 phase offset 關係的 complex vector 聚類在一起, 而對不同 phase offset 關係能分開呢? 關鍵的答案就是 Circularly Symmetric Gaussian Distribution Circularly Symmetric Gaussian Distribution直接引用 slides 裡的一段定義 A complex Gaussian random vector $Z$ is circularly symmetric if $e^{j\\phi}Z$ has the same distribution as $Z$ for all real $\\phi$. 意思就是如果我們乘上固定的 phase offset $\\phi$ (聲源有 time delay), 這相當於不改變維度之間的 phase offset 關係 (不改變聲源方向 $\\theta$), 這樣的話它們會是同一個機率分佈, 而這種特性完全符合我們的需求! 我們直接擷取 slide 中的 Circularly Symmetric Gaussian Distribution 的定義: 詳細請見 [1] 的 slides. Reference Circularly Symmetric Gaussian Random Vectors","tags":[{"name":"CGMM","slug":"CGMM","permalink":"https://bobondemon.github.io/tags/CGMM/"},{"name":"Spatial","slug":"Spatial","permalink":"https://bobondemon.github.io/tags/Spatial/"}]},{"title":"懷舊篇, 單通道降噪, MMSE-STSA, MMSE-LSA 方法","date":"2019-03-20T13:04:18.000Z","path":"2019/03/20/MMSE-STSA-and-LSA/","text":"記錄一下單通道降噪的一個經典方法, MMSE-STSA, MMSE-LSA, 已經是 1984 左右的文章了. 單通道降噪 OMLSA 也從這衍生出來的. 我們先從 MMSE-STSA 說起, 全名是 minimum mean-square error short time spectral amplitude.$y(t)=x(t)+d(t),0\\leq t\\leq T$$x$, $d$, $y$ 分別是 speech, noise, 和收到的 noisy signal, 其中 $x$, $d$ 相互獨立. 相對應的第 $k$ 個 frequency bin 如下:$$X_k=A_k\\exp(j\\alpha_k) \\\\ D_k \\\\ Y_k=R_k\\exp(j\\theta_k)$$ MMSE-STSA $^{[1]}$目標函式為$$\\begin{align} \\arg\\min_{\\hat{A}_k}{\\mathbb{E}\\left[\\left(A_k-\\hat{A}_k\\right)^2\\vert y(t),0\\leq t\\leq T\\right]} \\end{align}$$ 最佳解為$$\\begin{align} \\hat{A}_k=\\mathbb{E}\\left[A_k\\vert y(t),0\\leq t \\leq T\\right] \\end{align}$$ 但關鍵是我們不知道 clean speech 的 amplitude $A_k$, 那該怎麼估呢? 首先我們對每個 frequency bin 的分布假設為 Gaussian distribution (complex). 引用原文 “Since the Fourier coefficient is, after all, a weighted sum (or integral) of random variables resulting from the random process samples”, 在一個短時的 frame 中大致上是 stationary, 因此可以看作是一個 WSS 的 ramdom process, 再加上 cental limit theorem, 就當作高斯分布吧. 套用 Guassian distribution 假設, 做如下推導$$\\begin{align} \\hat{A}_k=\\mathbb{E}\\left[A_k\\vert y(t),0\\leq t \\leq T\\right]=\\mathbb{E}\\left[A_k\\vert Y_0,Y_1,...\\right] \\\\ =\\mathbb{E}\\left[A_k\\vert Y_k\\right] \\\\ =\\int_0^{\\infty}\\int_0^{2\\pi}a_k p(a_k,\\alpha_k\\vert Y_k)d\\alpha_k d a_k = \\int_0^{\\infty}\\int_0^{2\\pi}a_k \\frac{p(a_k,\\alpha_k,Y_k)}{p(Y_k)}d\\alpha_k d a_k \\\\ =\\frac{ \\int_0^{\\infty}\\int_0^{2\\pi}a_k p(Y_k\\vert a_k,\\alpha_k) p(a_k,\\alpha_k) d\\alpha_k d a_k }{ \\int_0^{\\infty}\\int_0^{2\\pi} p(Y_k\\vert a_k,\\alpha_k) p(a_k,\\alpha_k) d\\alpha_k d a_k } \\end{align}$$ 其中 (3) 到 (4) 我們假設每個 frequency bin 是獨立的由於我們假設每個 frequency bin 都是 complex Gaussian distribution, 因此 (6) 的機率分佈如下定義:$$\\begin{align} p(Y_k\\vert a_k,\\alpha_k)=\\frac{1}{\\pi\\lambda_d (k)}\\exp\\left[ -\\frac{1}{\\lambda_d (k)}\\vert Y_k - a_k e^{j\\alpha_k} \\vert^2 \\right] \\\\ p(a_k,\\alpha_k)=\\frac{1}{\\pi\\lambda_x (k)}\\exp\\left[-\\frac{a_k^2}{\\lambda_x (k)}\\right] \\end{align}$$ 注意到 (7) 能這麼寫是因為我們知道 $x$ and $d$ 互相獨立, 因此在給定 $x$ 的情形下, 只是改變 mean 的位置, 其 variance 仍由 $d$ 來決定. 另外:$$\\begin{align} \\lambda_x (k)=\\mathbb{E}\\left[\\vert X_k \\vert ^2\\right]=A_k^2 \\\\ \\lambda_d (k)=\\mathbb{E}\\left[\\vert D_k \\vert ^2\\right] \\end{align}$$ 表示第 $k$ 個 bin 的 speech and noise 的 variance將 (7) and (8) 帶入 (6) 並感謝偉大的作者推導得到:$$\\begin{align} \\hat{A}_k=\\Gamma(1.5)\\frac{\\sqrt{\\upsilon_k}}{\\gamma_k}M(-0.5;1;-\\upsilon_k)R_k \\\\ \\hat{A}_k=\\Gamma(1.5)\\frac{\\sqrt{\\upsilon_k}}{\\gamma_k}\\exp\\left(-\\frac{\\upsilon_k}{2}\\right)\\left[(1+\\upsilon_k)I_0(\\frac{\\upsilon_k}{2})+\\upsilon_k I_1(\\frac{\\upsilon_k}{2})\\right]R_k \\end{align}$$ 其中 $\\Gamma$ 表示 gamma function, $\\Gamma(1.5)=\\sqrt{\\pi}/2$; $M(a;c;x)$ 是 confluent hypergeometric function (這是外星符號吧), $I_0$ and $I_1$ 是 modified Bessel funciton of zero and first order. 總之就是能帶入計算的東西, 最重要, 也是需要我們估計的變數如下:$$\\begin{align} \\upsilon_k\\triangleq \\frac{\\xi_k}{1+\\xi_k}\\gamma_k \\\\ \\color{orange}{ \\xi_k\\triangleq\\frac{\\lambda_x (k)}{\\lambda_d (k)} } \\\\ \\color{orange}{ \\gamma_k\\triangleq\\frac{R_k^2}{\\lambda_d (k)} } \\\\ \\end{align}$$ $\\xi_k$ 和 $\\gamma_k$ 分別稱為 prior SNR 和 posterior SNR. 總之如能估出 $\\xi_k$ 和 $\\gamma_k$, 我們就能計算出 gain 值, 之後的方法如 LSA, OMLSA 也都如此. 文章後面會使用 MCRA 來估算這兩個 SNR. 現在就算傳統方法一般也很少使用 MMSE-STSA, 至少會使用 LSA 取代. LSA 有近似的計算方式, 因此我們也不糾結 (12) 到底怎麼算出來. MMSE-LSA $^{[2]}$大致想法跟流程跟上面一樣(只是我算不出來), 只是目標函數針對 log 值來計算$$\\begin{align} \\arg\\min_{\\hat{A}_k}{\\mathbb{E}\\left[\\left(\\log A_k-\\log\\hat{A}_k\\right)^2\\vert y(t),0\\leq t\\leq T\\right]} \\end{align}$$ 同樣經過不是人類的計算後得到:$$\\begin{align} \\hat{A}_k=\\frac{\\xi_k}{1+\\xi_k}\\exp\\left[\\frac{1}{2}\\int_{\\upsilon_k}^{\\infty}\\frac{e^{-t}}{t}dt\\right]R_k \\end{align}$$ [3] 給出了一個好算的近似結果$$\\begin{align} \\int_{\\upsilon_k}^{\\infty}\\frac{e^{-t}}{t}dt\\approx \\left\\{ \\begin{array}{rcl} -2.31\\log_{10}(\\upsilon_k)-0.6\\mbox{ for }\\upsilon_k&lt;0.1 \\\\ -1.544\\log_{10}(\\upsilon_k)+0.166\\mbox{ for }0.1\\leq\\upsilon_k\\leq 1 \\\\ 10^{-(0.52\\upsilon_k+0.26)}\\mbox{ for }\\upsilon_k&gt;1 \\\\ \\end{array}\\right. \\end{align}$$ 另外還有 optimally-modified log-spectral amplitude (OMLSA) [4] 方法, 作者有提供 MATLAB codes. 這算單通道降噪標配了, 但實驗結果對聽覺有幫助, 對 WER 不一定降低. 總之不管哪一種方法, 都必須很好的估出 prior and posterior SNR. MCRA Prior/Posterior SNR 估計針對 STFT 時間 $l$, frequency bin $k$ 來說, 假設我們已估出來 speech presence probability $p(k,l)$, 我們可以這麼 update noise 的 variance:$$\\begin{align} \\hat{\\lambda}_d(k,l+1)=\\hat{\\lambda}_d(k,l)p(k,l)+\\left[\\alpha_d\\hat{\\lambda}_d(k,l)+(1-\\alpha_d)|Y(k,l)|^2\\right](1-p(k,l)) \\end{align}$$ 這很好理解, 如果有 speech 的話, noise variance 就沿用原來舊的, 而如果沒有 speech, nosie vaiance 就要用當前 frame 透過 $\\alpha_d$ 平滑地更新一下 (就稱這樣的平滑為 $\\alpha$ 平滑). 估計 $p(k,l)$ 之前, 文章的做法是都先針對 time and frequency 做平滑. frequency 可選用一個 window (可用類似 Gaussian window), 而時間上的平滑可使用 $\\alpha$ 平滑. 令 $S(k,l)$ 為我們平滑後的 spectrum power, 然後對每個 bin 都 tracking 一小段時間的最小值, 令為 $S’(k,l)$. 則很明顯如果 $S(k,l)&gt;\\delta S’(k,l)$, 我們就可以認為有 speech, 機率為 1, 否則為 0. 這樣的 speech 機率過了 $\\alpha$ 平滑的結果就是 $p(k,l)$. 明確一點寫下為:$$\\begin{align} p(k,l)=\\alpha_p p(k,l-1)+(1-\\alpha_p)\\mathbf{I}[S(k,l)&gt;\\delta S&apos;(k,l)] \\end{align}$$ 其中 $\\mathbf{I}[.]$ 為 indicator function MCRA 有哪些調整的參數實際情形有一些需要調整的參數, 列在下面 $\\alpha_d$: noise variance smoothing $\\alpha_p$: speech probability smoothing STFT 的 time and frequency smoothing 參數 $\\delta$: 判斷當前 frame and bin 是否為 speech 的 threshold tracking minimal power $S’(k,l)$ 的參數, 譬如要用多少個 frame 來找 minimum 待做些實驗才會知道效果… Reference Speech Enhancement Using a Minimum Mean-Square Error Short-Time Spectral Amplitude Estimator by Yariv Ephraim and David Malah Speech Enhancement Using a Minimum Mean-Square Error Log-Spectral Amplitude Estimator by Yariv Ephraim and David Malah [A Noise Reduction Pre-processor for Mobile Voice Communication] by R. Martin … Speech enhancement for non-stationary noise environments by Israel Cohen and Baruch Berdugo","tags":[{"name":"MMSE-STSA","slug":"MMSE-STSA","permalink":"https://bobondemon.github.io/tags/MMSE-STSA/"},{"name":"MMSE-LSA","slug":"MMSE-LSA","permalink":"https://bobondemon.github.io/tags/MMSE-LSA/"},{"name":"OMLSA","slug":"OMLSA","permalink":"https://bobondemon.github.io/tags/OMLSA/"},{"name":"MCRA","slug":"MCRA","permalink":"https://bobondemon.github.io/tags/MCRA/"}]},{"title":"Far Field Notes (3) Equivalence of MWF, MaxSNR, and MVDR Filters","date":"2019-03-18T12:33:46.000Z","path":"2019/03/18/Far-Field-Notes-3-MWF-MaxSNR-MVDR-Filters/","text":"這是 far field 筆記系列第三篇, 主要為自己學習用, 如有錯誤還請指正. 主要參考 Microphone Array Signal Processing Ch6 和 Speech Processing in Modern Communication: Challenges and Perspectives Ch9.3.4 在 narrow-band 的情形下, Multi-channel Wiener Filter (MWF), maximum SNR (MSNR) 和 Minimum Variance Distortionless Response (MVDR) 三者求出來的 filter 解只差在 norm 大小不同. 但反應在最後的 full-bank 行為仍然不同. 這部分可看書. 本篇主要紀錄 narrow-bank 下三者為何 equivalent. 算是書本的摘要筆記吧. Signal Model在 frequency doamin 下, 我們有如下的關係 $$\\begin{align} Y_n(j\\omega)=G_n(j\\omega)S(j\\omega)+V_n(j\\omega) \\\\ =X_n(j\\omega)+V_n(j\\omega)\\mbox{, }n=1,2,...,N \\\\ \\end{align}$$ $N$ 是麥克風數量, $S(j\\omega)$ 是原始訊號, $G_n(j\\omega)$ 是聲源到 mic $n$ 的 impluse response, $V_n(j\\omega)$ 是 noise, 而 $X_n(j\\omega)$ 是 mic $n$ 的訊號. 我們希望還原的是 $X_1(j\\omega)$ 而不是 $S(j\\omega)$.排成 vector 形式如下$$\\begin{align} Z(j\\omega)=\\mathbf{h}^H(j\\omega)\\mathbf{y}(j\\omega) \\\\ =\\mathbf{h}^H(j\\omega)[\\mathbf{x}(j\\omega)+\\mathbf{v}(j\\omega)] \\\\ \\end{align}$$ 其中$$\\begin{align} \\mathbf{y}(j\\omega)=[Y_1(j\\omega),Y_2(j\\omega),...,Y_N(j\\omega)]^T \\\\ \\mathbf{x}(j\\omega)=S(j\\omega)[G_1(j\\omega),G_2(j\\omega),...,G_N(j\\omega)]^T=S(j\\omega)\\mathbf{g}(j\\omega) \\\\ \\mathbf{v}(j\\omega)=[V_1(j\\omega),V_2(j\\omega),...,V_N(j\\omega)]^T \\\\ \\mathbf{h}(j\\omega)=[H_1(j\\omega),H_2(j\\omega),...,H_N(j\\omega)]^T \\\\ \\end{align}$$ 注意到$$\\begin{align} \\Phi_{xx}(j\\omega)=\\mathbb{E}\\left[\\mathbf{x}(j\\omega)\\mathbf{x}^H(j\\omega)\\right]=\\phi_{ss}(j\\omega)\\mathbf{g}(j\\omega)\\mathbf{g}^H(j\\omega) \\end{align}$$ MWF將 error term 寫出來 $$\\begin{align} \\mathcal{E}(j\\omega)=Z(j\\omega)-X_1(j\\omega) \\\\ =\\mathbf{h}^H(j\\omega)\\mathbf{v}(j\\omega)+[\\mathbf{h}(j\\omega)-\\mathbf{u}]^H\\mathbf{x}(j\\omega)\\\\ =\\color{orange}{\\mathcal{E}_v(j\\omega)}+\\color{blue}{\\mathcal{E}_x(j\\omega)} \\\\ \\end{align}$$ 其中 $\\mathbf{u}$ 是一個 $N\\times 1$ 的 vector, 只有第一個是1, 其他是0. Error term 可以拆成兩項, 分別對應了 noise reduction 程度和 speech distortion 程度MWF 的目標函式如下:$$\\begin{align} J_{MWF}[\\mathbf{h}(j\\omega)]=\\mathbb{E}\\left[| \\mathcal{E}(j\\omega) |^2\\right]\\\\ = \\color{orange}{ \\mathbb{E}\\left[| \\mathcal{E}_v(j\\omega) |^2\\right] } + \\color{blue}{ \\mathbb{E}\\left[| \\mathcal{E}_x(j\\omega) |^2\\right] } \\end{align}$$ 可以看成 noise reduction 和 speech distortion 同等重要情況下去求解最好的 $\\mathbf{h}$微分等於零求解得到如下:$$\\begin{align} \\Phi_{yy}(j\\omega)\\mathbf{h}_W(j\\omega)=\\Phi_{yx}(j\\omega)\\mathbf{u}=\\Phi_{xx}(j\\omega)\\mathbf{u} \\end{align}$$上式最後推導是由於 $x$ and $v$ 是 independent. 因此最後的 MWF 解為:$$\\begin{align} \\mathbf{h}_W(j\\omega)=\\Phi_{yy}^{-1}(j\\omega)\\Phi_{xx}(j\\omega)\\mathbf{u} \\\\ =\\left[ \\mathbf{I}_{N\\times N} - \\Phi_{yy}^{-1}(j\\omega)\\Phi_{vv}(j\\omega) \\right]\\mathbf{u} \\end{align}$$ MVDRMVDR 要解的問題如下:$$\\min \\color{orange}{ \\mathbb{E}\\left[| \\mathcal{E}_v(j\\omega) |^2\\right] }\\\\ \\mbox{subject to } \\color{blue}{\\mathbb{E}\\left[| \\mathcal{E}_x(j\\omega) |^2\\right]}=0$$ 這也可以看出 MVDR 為什麼叫 MVDR.首先先將 constraint 改寫成 (改寫 speech distortion error term, 定義在 (11), (12)):$\\left[\\mathbf{u}-\\mathbf{h}(j\\omega)\\right]^H\\mathbf{x}(j\\omega)=0 \\\\$ 並利用 $\\mathbf{x}(j\\omega)=S(j\\omega)\\mathbf{g}(j\\omega)$ 可得到$\\mathbf{h}^H(j\\omega)\\mathbf{g}(j\\omega)=G_1(j\\omega)$因此 MVDR 問題的通常如下表達:$$\\min \\mathbf{h}^H(j\\omega) \\Phi_{vv}(j\\omega) \\mathbf{h}(j\\omega) \\\\ \\mbox{subject to } \\mathbf{h}^H(j\\omega)\\mathbf{g}(j\\omega)=G_1(j\\omega) \\\\$$ 這個最佳化問題正好就是上一篇的 LCMV, 所以說 MVDR 是 LCMV 的一種 case.用 Lagrange multipliers 求解得到$$\\begin{align} \\mathbf{h}_{MVDR}(j\\omega)=G_1^{\\ast}(j\\omega)\\frac{\\Phi_{vv}^{-1}(j\\omega)\\mathbf{g}(j\\omega)}{\\mathbf{g}^H(j\\omega)\\Phi_{vv}^{-1}(j\\omega)\\mathbf{g}(j\\omega)} \\end{align}$$ 對 (18) 進一步推導, 為了精簡以下 ${j\\omega}$ 省略不寫$$\\begin{align} \\mathbf{h}_{MVDR}=\\frac{\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}G_1^{\\ast}}{tr\\left[\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}\\mathbf{g}^H\\right]}=\\frac{\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}\\mathbf{g}^H\\mathbf{u}}{tr\\left[\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}\\mathbf{g}^H\\right]} \\\\ =\\frac{ \\Phi_{vv}^{-1}\\Phi_{xx}\\mathbf{u} }{ tr\\left[\\Phi_{vv}^{-1}\\Phi_{xx}\\right] } \\\\ =\\frac{ \\Phi_{vv}^{-1}(\\Phi_{yy}-\\Phi_{vv})\\mathbf{u} }{ tr\\left[ \\Phi_{vv}^{-1}(\\Phi_{yy}-\\Phi_{vv}) \\right] } \\\\ =\\frac{ (\\Phi_{vv}^{-1}\\Phi_{yy}-\\mathbf{I})\\mathbf{u} }{ tr\\left[\\Phi_{vv}^{-1}\\Phi_{yy}\\right]-N } \\end{align}$$ (19) 到 (20) 使用了 (9).(22) 的形式書本說很重要, 因為避免了很難估計的 $\\mathbf{g}$, 取而代之的是我們要估計出 $\\Phi_{vv}$ MWF 與 MVDR 等價情形同樣為了精簡以下 ${j\\omega}$ 省略不寫, 首先我們知道$$\\begin{align} \\Phi_{yy}=\\Phi_{vv} + \\phi_{ss}\\mathbf{g}\\mathbf{g}^H \\end{align}$$ 使用 Woodbury’s identity 可得:$$\\begin{align} \\Phi_{yy}^{-1}=\\Phi_{vv}^{-1}-\\frac{ \\Phi_{vv}^{-1}\\Phi_{xx}\\Phi_{vv}^{-1} }{ 1+tr\\left[\\Phi_{vv}^{-1}\\Phi_{xx}\\right] } \\end{align}$$ 將 (24) 帶入到 (17) 並經過一些代數替換我們得到$$\\begin{align} \\mathbf{h}_{W}=\\frac{ \\Phi_{vv}^{-1}\\Phi_{xx} }{ 1+tr\\left[\\Phi_{vv}^{-1}\\Phi_{xx}\\right] }\\mathbf{u} \\end{align}$$ 這個式子與 MVDR 的 (20) 比較一下我們發現$$\\begin{align} \\mathbf{h}_{W}(j\\omega)=c(\\omega)\\mathbf{h}_{MVDR}(j\\omega) \\end{align}$$ 其中 $c(\\omega)$ 是與 $\\omega$ 相關的一個 scalar. 因此 MWF 與 MVDR 解只差在一個 $\\omega$ 相關的常數項 Maximum SNR (MSNR)同樣為了精簡以下 ${j\\omega}$ 省略不寫, output SNR 定義為:$$\\begin{align} \\mbox{oSNR}\\left[\\mathbf{h}\\right]=\\frac{ \\mathbf{h}^H \\Phi_{xx} \\mathbf{h} }{ \\mathbf{h}^H \\Phi_{vv} \\mathbf{h} } \\end{align}$$ 這個等同於 generalized eigenvalue problem.$$\\begin{align} \\Phi_{xx}\\mathbf{h}=\\lambda\\Phi_{vv}\\mathbf{h} \\end{align}$$ 所以$\\mathbf{h}_{MSNR}\\mbox{ is eigenvector w.r.t max eigenvalue of matrix } \\Phi_{vv}^{-1}\\Phi_{xx}$eigenvector 乘上一個 scalar 仍然是 eigenvector, 因此通常都會將 $\\mathbf{h}_{MSNR}$ 的 norm 定為 1 接著我們說明 $\\mathbf{h}$ 的解具有以下形式:$$\\begin{align} \\mathbf{h}\\propto\\Phi_{vv}^{-1}\\mathbf{g} \\end{align}$$ 先假定 (29) 為等式:$$\\begin{align} \\mathbf{h}=\\Phi_{vv}^{-1}\\mathbf{g} \\end{align}$$ 利用 (9) 和 (30) 得到以下的推導$$\\begin{align} \\Phi_{vv}^{-1}\\Phi_{xx}\\mathbf{h}=\\Phi_{vv}^{-1} \\phi_{ss}\\mathbf{g}\\mathbf{g}^H \\mathbf{h}=\\left(\\Phi_{vv}^{-1}\\mathbf{g}\\right)\\left(\\phi_{ss}\\mathbf{g}^H\\mathbf{h}\\right)=\\mathbf{h}\\lambda \\end{align}$$ 我們發現 $\\mathbf{h}$ 如有 (30) 的形式, 則為 maximum SNR (27) 的解. 當然 eigenvector 乘上 scalar 仍然是 eigenvector, 所以 (29) 為 maximum SNR 的解.最後由於 $\\Phi_{xx}$ 為 rank 1 所以只會有一個 nonzero eigenvalue, 因此 maximum SNR 所有解的形式必然為 (29) 的形式. MVDR 與 MSNR 等價情形檢查下 (18) 的 MVDR 解, 很快就發現滿足 (29) MSNR 的解的形式, 因此$$\\begin{align} \\mathbf{h}_{MVDR}(j\\omega)=d(\\omega)\\mathbf{h}_{MSNR}(j\\omega) \\end{align}$$ 結論MWF, MVDR, MSNR 三個問題的解在 narrowband 上只差在 scalar. 但以 fullband 來說, 表現還是不同的. Reference Microphone Array Signal Processing by Jocab Benesty Speech Processing in Modern Communication: Challenges and Perspectives","tags":[{"name":"MVDR","slug":"MVDR","permalink":"https://bobondemon.github.io/tags/MVDR/"},{"name":"MWF","slug":"MWF","permalink":"https://bobondemon.github.io/tags/MWF/"},{"name":"MSNR","slug":"MSNR","permalink":"https://bobondemon.github.io/tags/MSNR/"}]},{"title":"Far Field Notes (2) LCMV filter and Frost's algorithm","date":"2019-03-02T09:36:58.000Z","path":"2019/03/02/Far-Field-Notes-2-LCMV-and-Frost/","text":"這是 far field 筆記系列第二篇, 主要為自己學習用, 如有錯誤還請指正. 主要參考 Microphone Array Signal Processing Ch4 和 Frost’s algorithm 上一篇最後雖然使用 fixed beamformer 得到了 response-invariant beamformer, 但這個方法限制是 filter 一旦設計好就寫死了, 沒辦法自己 update (所以才叫 “fixed” beamformer). 這引入一個問題是, 如果剛好有一個 inteference noise 在衰減不那麼大的角度時, 就無法壓得很好. 而這篇要介紹的 LCMV (Linear Constrained minimum variance) filter 以及 Frost’s beamformer 能針對給定的方向抽取訊號, 並且對其他方向的 inteference nosie 壓抑的最好. 注意 sound source 方向必須給定, LCMV 求得的 weights 會想辦法對其他方向的 inteference 壓抑. 如同 LCMV 字面上的意思一樣. 會將整個問題轉換成 minimize variance subject to some linear constraints. 另外相當經典的 Frost’s beamformer (1972年呢!) 則將 filter 的 optimal 求解改成使用 stochastic gradient descent 方式, 所以非常適合實際的 real time 系統, 這些下文會詳細說明. 架構設定和 Signal Model架構如下圖 (圖片來源:ref), 第一步是一個 delay stage, 這相當於是針對一個 steering direction 補償每個 mic 之間的 time delay (訊號對齊好). 第二步才是 beamformer, 我們知道 time domain 使用 filter-and-sum 架構, 如果是 frequency domain 則使用拆頻的架構. 忘了可參考第一篇. 本文以 filter-and-sum 來筆記, 另外 signal model 以下推導將會使用 anechoic model, 第一篇有定義可回去查閱. 同時本文接下來的 notation 會與圖中的不同. 上圖只是用來顯示 filter-and-sum 架構. Notations一些 notations 我們先定義起來. $N$ 是麥克風數量, $L$ 是 filter tap 數量, 我們 aligned 好的 anechoic model 如下:$$\\begin{align} \\mathbf{y}(k)=s(k)\\mathbf{\\alpha}+\\mathbf{v}(k) \\end{align}$$其中$$\\begin{align} \\mathbf{y}(k)=[y_1(k),...,y_N(k)]^T \\\\ \\mathbf{v}(k)=[v_1(k),...,v_N(k)]^T \\\\ \\mathbf{\\alpha}=[\\alpha_1,\\alpha_2,...,\\alpha_N]^T \\end{align}$$ $s(k)$ 是時間 $k$ 的聲源訊號, $\\alpha$ 是 $N\\times 1$ 的 attenuation factors, $\\mathbf{v}(k)$ 是時間 $k$ 的 $N\\times 1$ noise 訊號向量, 因此 $\\mathbf{y}(k)$ 是時間 $k$ 的 $N\\times 1$ 麥克風收到的訊號向量. 注意到由於我們先 align 好 delay 了, 所以原先的 anechoic model 可以簡化成上面的表達. 考慮到 filter-and-sum 架構, 我們將整個 $N$ 個 mic 每個 mic 都有 $L$ 個值以下圖(圖片來源:ref)的順序串成一個 $NL$ vector因此我們得到這些向量$$\\begin{align} \\mathbf{y}_{NL}(k)=[\\mathbf{y}^T(k), \\mathbf{y}^T(k-1), ..., \\mathbf{y}^T(k-L+1)]^T \\\\ \\mathbf{x}_{NL}(k)=[s(k)\\mathbf{\\alpha}^T, s(k-1)\\mathbf{\\alpha}^T, ..., s(k-L+1)\\mathbf{\\alpha}^T]^T \\\\ \\mathbf{v}_{NL}(k)=[\\mathbf{v}^T(k), \\mathbf{v}^T(k-1), \\mathbf{v}^T(k-L+1)]^T \\end{align}$$所以整體的 signal model 改寫 (1) 後可得:$$\\begin{align} \\mathbf{y}_{NL}(k)=\\mathbf{x}_{NL}(k) + \\mathbf{v}_{NL}(k) \\end{align}$$ Filter-and-sum 的 filter $\\mathbf{h}$ 也用這個順序定義如下, 因此是一個長度為 $NL$ 的向量$$\\begin{align} \\mathbf{h}=[\\mathbf{h}_0^T, \\mathbf{h}_1^T, \\mathbf{h}_{L-1}^T]^T \\end{align}$$最後整個 beamformer 的輸出 $z(k)$ 就可以這麼寫$$\\begin{align} z(k)=\\mathbf{h}^T\\mathbf{y}_{NL}(k) = \\color{orange}{ \\mathbf{h}^T\\mathbf{x}_{NL}(k) } + \\color{blue}{ \\mathbf{h}^T\\mathbf{v}_{NL}(k) } \\end{align}$$ Problem DefinitionLCMV 的主要想法就圍繞在 (10) 的橘色和藍色兩個部分上面: 我們希望橘色部分能夠還原出原始訊號 $s(k)$ 且藍色部分能夠愈小愈好 (代表著 noise 愈小愈好). 首先我們將橘色部分作如下推導: $$\\begin{align} \\color{orange}{ \\mathbf{h}^T\\mathbf{x}_{NL}(k) } =\\mathbf{h}^T \\left[ \\begin{array}{clr} s(k)\\mathbf{\\alpha} \\\\ s(k-1)\\mathbf{\\alpha} \\\\ \\vdots \\\\ s(k-L+1)\\mathbf{\\alpha} \\end{array} \\right] = sum\\left( \\left[ \\begin{array}{clr} \\mathbf{h}_0^T\\mathbf{\\alpha}\\cdot s(k) \\\\ \\mathbf{h}_1^T\\mathbf{\\alpha}\\cdot s(k-1) \\\\ \\vdots \\\\ \\mathbf{h}_{L-1}^T\\mathbf{\\alpha}\\cdot s(k-L+1) \\end{array} \\right] \\right) \\\\ = sum\\left( \\color{red}{ \\left[ \\begin{array}{clr} u_0\\cdot s(k) \\\\ u_1\\cdot s(k-1) \\\\ \\vdots \\\\ u_{L-1}\\cdot s(k-L+1) \\end{array} \\right] } \\right) \\end{align}$$ (12) 為引入的條件, 藉由這樣的條件來還原原始訊號.$u$ ($L$長度的向量) 定義了我們希望在時間 $k$ 的還原結果, 是原始訊號的權重和定義一個 matrix (size of $NL\\times L$) 如下: $$\\begin{align} \\mathbf{C}_{\\mathbf{\\alpha}}= \\left[ \\begin{array}{clr} \\mathbf{\\alpha} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{\\alpha} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{\\alpha} \\\\ \\end{array} \\right] = \\left[ \\begin{array}{clr} \\mathbf{c}_{\\alpha,0} &amp; \\mathbf{c}_{\\alpha,1} &amp; \\cdots &amp; \\mathbf{c}_{\\alpha,L-1} \\\\ \\end{array} \\right] \\end{align}$$ 觀察 (11) and (12) 並利用 $\\mathbf{C_{\\alpha}}$ 可以將 constraint 明確寫出如下: $$\\begin{align} \\mathbf{C_{\\alpha}}^T\\mathbf{h}=\\mathbf{\\mathbf{u}} \\end{align}$$ 藍色部分代表最後的 noise 成分, 希望愈小愈好計算藍色部分的能量為 $$\\begin{align} \\mathbf{h}^T \\mathbb{E} \\left[ \\mathbf{v}_{NL}(k)\\mathbf{v}_{NL}^T(k) \\right] \\mathbf{h}=\\mathbf{h}^T\\mathbf{R}_{\\mathbf{v},\\mathbf{v}}\\mathbf{h} \\end{align}$$ 但關鍵是我們無法得知實際的 noise signal, 我們有的只有 observation $\\mathbf{y}_{NL}(k)$, 那該怎麼辦呢?LCMV 很厲害的一點是, 由於上面剛提到的 constraints, 導致橘色部分的能量是 constant, 因此以下兩個問題是等價的 $$\\begin{align} \\min_{\\mathbf{h}}{ \\mathbf{h}^T\\mathbf{R}_{\\mathbf{v},\\mathbf{v}}\\mathbf{h} } \\equiv \\min_{\\mathbf{h}}{ \\mathbf{h}^T\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} } \\end{align}$$ 到這裡我們可以寫出完整的最佳化問題$$\\begin{align} \\begin{array}{clr} \\color{blue}{ \\min_{\\mathbf{h}}{ \\mathbf{h}^T\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} } } \\\\ \\color{orange}{ \\mbox{subject to }\\mathbf{C_{\\alpha}}^T\\mathbf{h}=\\mathbf{\\mathbf{u}} } \\end{array} \\end{align}$$ Optimal Solution要解問題 (17), 基本上使用 Lagrange function 求解就可以, 解如下:$$\\begin{align} \\mathbf{h}=\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}^{-1}\\mathbf{C_{\\alpha}} \\left( \\mathbf{C_{\\alpha}}^T \\mathbf{R}_{\\mathbf{y},\\mathbf{y}}^{-1} \\mathbf{C_{\\alpha}} \\right)^{-1} \\mathbf{u} \\end{align}$$ 但是重點來了, 以上這些推導全部都假設是 stationary, 實際情況一定是 non-stationary 怎麼辦? 最直覺的想法就是, 我們每隔一段時間就用 (18) 重新算一下 $\\mathbf{h}$. 但很明顯這非常沒效率 (covariance估計, inverse運算) 根本不可行. 因此必須改成 iteratively update $\\mathbf{h}$ 的方式.Frost’s algorithm 的一個重要貢獻也就是在這, 使用 stochastic gradient descent 方式 update $\\mathbf{h}$! Frost’s Algorithm問題 (17) 的 Lagrange function 如下:$$\\begin{align} \\mathcal{L}(\\mathbf{h},\\mathbf{\\lambda}) = \\frac{1}{2} \\mathbf{h}^T\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} + \\mathbf{\\lambda}^T(\\mathbf{C_{\\alpha}}^T\\mathbf{h}-\\mathbf{\\mathbf{u}}) \\end{align}$$因此 gradient 如下:$$\\begin{align} \\nabla_{\\mathbf{h}}\\mathcal{L} = \\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} + \\mathbf{C_{\\alpha}}\\mathbf{\\lambda} \\end{align}$$gradient descent update 式子如下:$$\\begin{align} \\mathbf{h}_{t+1} = \\mathbf{h}_{t} - \\mu \\left( \\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h}_t + \\mathbf{C_{\\alpha}}\\mathbf{\\lambda}_t \\right) \\end{align}$$由於有 constraint, 必須滿足 update 後仍然滿足條件, 因此:$$\\begin{align} \\mathbf{u}=\\mathbf{C_{\\alpha}}^T\\mathbf{h}_{t+1} \\end{align}$$將(21)帶入(22)整理得到$\\lambda_t$, 接著再將$\\lambda_t$帶回(21)得到結果如下, 並不困難只是一些代數運算:$$\\begin{align} \\mathbf{h}_{t+1} = \\mathbf{h}_{t} - \\mu \\left[ \\mathbf{I} - \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1}\\mathbf{C}^T \\right] \\mathbf{R}_{\\mathbf{y},\\mathbf{y}} \\mathbf{h}_{t} + \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1} \\left[ \\mathbf{u}-\\mathbf{C}^T\\mathbf{h}_t \\right] \\end{align}$$定義兩個 matrix $\\mathbf{A}$, $\\mathbf{B}$ 如下 (注意到這兩個 matrix 是事先計算好的):$$\\begin{align} \\mathbf{A} \\triangleq \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1}\\mathbf{u} \\\\ \\mathbf{B} \\triangleq \\mathbf{I} - \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1}\\mathbf{C}^T \\end{align}$$因此可以改寫(23)如下:$$\\begin{align} \\mathbf{h}_{t+1} = \\mathbf{B}[\\mathbf{h}_t - \\mu \\mathbf{R}_{\\mathbf{y},\\mathbf{y}} \\mathbf{h}_t] + \\mathbf{A} \\end{align}$$由於使用 stochastic 方式, 因此 expectation 使用最新的一次 sample 即可:$$\\begin{align} \\mathbf{R}_{\\mathbf{y},\\mathbf{y}} = \\mathbb{E} \\left[ \\mathbf{y}_{NL}(t)\\mathbf{y}_{NL}^T(t) \\right] \\thickapprox \\color{green}{ \\mathbf{y}_{NL}(t)\\mathbf{y}_{NL}^T(t) } \\end{align}$$將(27)帶入(26)並用(10)替換一下, 我們得到最終的 update 式子:$$\\begin{align} \\color{red}{ \\mathbf{h}_{t+1} = \\mathbf{B}[\\mathbf{h}_t - \\mu z(t)\\mathbf{y}_{NL}(t)] + \\mathbf{A} } \\end{align}$$由於 $\\mathbf{A}$ 和 $\\mathbf{B}$ 是固定的, 跟原來的 optimal 解比較 (18), 可以明顯知道速度上會快非常多.另外 $\\mathbf{h}_0$ 只需要選擇一個 trivial 的 feasible point 即可:$$\\begin{align} \\mathbf{h}_{0} = \\mathbf{A} \\end{align}$$ 結論本篇記錄了 filter-and-sum 架構的 beamformer, LCMV 的問題和其最佳解. LCMV 可以針對給定的一個方向, 找出 filter $\\mathbf{h}$ 使得抽取看的方向的訊號同時壓抑其他方向的訊號.實作上直接套用最佳解太慢不可行, 而 Frost’s algorithm 提供了一個 stochastic gradeint update 方法更新 $\\mathbf{h}$, 這使得 real-time system 變得可行. Reference Microphone Array Signal Processing by Jocab Benesty Frost’s algorithm","tags":[{"name":"LCMV","slug":"LCMV","permalink":"https://bobondemon.github.io/tags/LCMV/"},{"name":"Frost","slug":"Frost","permalink":"https://bobondemon.github.io/tags/Frost/"},{"name":"MVDR","slug":"MVDR","permalink":"https://bobondemon.github.io/tags/MVDR/"}]},{"title":"Far Field Notes (1), Beampattern","date":"2019-02-26T12:22:55.000Z","path":"2019/02/26/Far-Field-Notes-1-Beampattern/","text":"這是 far field 筆記系列第一篇, 主要為自己學習用, 如有錯誤還請指正. 主要參考 Optimum Array Processing Ch2 以及 Microphone Array Signal Processing Ch3. Beampattern 就是希望能得到如下圖 [ref Fig3.3] 的表示, 說明經過一個麥克風陣列的處理後, 每個角度所得到的增益情形. 因此可以看出主要保留哪些方向的訊號, 以及抑制哪些方向的訊號. Geometry Settings遠場一般假設 plan wave, 和 narrow band. 實際處理語音等 broadband 時我們會採取 fft 分頻. 我們定義如下的 geometry, 其中重要的兩個角度為 $\\theta$ 和 $\\phi$ (如圖紅圈). $\\mathbf{a}$ 表示聲源的入射單位向量. 以下符號如果是粗體表示為向量或矩陣, 否則就是 scalar. Signal Model我們定義 $f(t)$ 為聲源訊號, $v_n(t)$ 為 nth mic 的噪聲源 Anechoic Model 我們以下的介紹都是基於 Anechoic Model, 並且先做如下的簡化 Reverberant Model 相當於將 attenuation factor $\\alpha$ 改成 impulse response, 所以相乘改成 convolution. Time Delay為了方便推導麥克風之間的 time delay, 我們先將 Anechoic Model 做如下簡化 因此對於 plan wave 假設和聲源的入射單位向量 $a$ 來說, 我們很容易就得到 time delay 如下 Uniform Lineary Array (ULA) Circular Array以一個 6 mic 的 circular array 來說, 有如下的 time delay Array Manifold Vector由於 time delay $\\tau$ 在 freqeuncy $\\omega$ 只是乘上 $e^{-j\\omega\\tau}$, 因此我們可以得到一個 compact 的表示 重複一遍這裡得到的重要式子 $$\\begin{align} \\color{red}{ \\mathbf{F}(\\omega,\\mathbf{p})=F(\\omega)\\mathbf{\\upsilon}_k(\\mathbf{k}) } \\end{align}$$ 我們稱 $\\mathbf{\\upsilon}_k(\\mathbf{k})$ 為 Array Manifold Vector. 要注意的是, 其實也可以用 time delay $\\tau$ 來表示, 這時我們這麼寫 (如上圖灰色的部分)$$\\mathbf{\\upsilon}_\\tau (\\mathbf{\\tau})= \\left[ \\begin{array}{clr} e^{-j\\omega\\tau_0} \\\\ e^{-j\\omega\\tau_1} \\\\ \\vdots \\\\ e^{-j\\omega\\tau_{N-1}} \\end{array} \\right]$$或甚至入射角度 $\\theta$ 如果可以完全表達 $\\tau$ 的話, 我們也能這麼寫 $\\mathbf{\\upsilon}_\\theta(\\mathbf{\\theta})$. Array Signal Processing早期的 array processing (narrow band) 是對每個麥克風有各自的 weights, 然後再總合起來, 這種作法叫做 weight-and-sum. 而對於 broadband 訊號來說, 相當於拆頻乘很多 narrow band, 因此在每個頻帶上, 都有 N 個麥克風的 weights. 這在時域上等價於每個麥克風都有各自的 filters, 稱 filter-and-sum. 以下介紹 filter-and-sum 和頻域的架構. Filter-and-Sum 在實作上通常採用 FIR filter, 因此架構如下:符號有點不同, 這是因為圖是採用另一本書 Microphone Array Signal Processing Frequency Domain針對 filter-and-sum 做 frequency transform 得到如下: 實際架構圖如下:一樣符號有點不同, 這是因為圖是採用另一本書 Microphone Array Signal Processing Frequency-wavenumber Response Function針對 frequency domain 的 array processing, 我們可以帶入先前推得的 (1) 得到如下: 所以 $\\Upsilon(\\omega,\\mathbf{k})$ 物理意義就是針對 frequency $\\omega$ 和 wavenumber $\\mathbf{k}$ (控制了聲源入射角度 $\\theta$ 等等的物理量) 的 response. Beampatternwavenumber $\\mathbf{k}$ 比較抽象, 如果我們換成角度 $\\theta$, $\\phi$ 就會直觀很多, 而 beampattern 只是針對 $\\Upsilon(\\omega,\\mathbf{k})$ 換成用角度而已. 所以 $B(\\omega:\\theta,\\phi)$ 物理意義就是針對 frequency $\\omega$ 和入射角度 $\\theta$, $\\phi$ 的 response. Delay-and-sum BeampatternDelay-and-sum 想法很簡單, 就是補償每個 mic 的 time delay 而已. 因此所需要的 filter $H(\\omega)$ 就是 array manifold vector 的 conjugate 即可. 如下圖: 但這麼做有個缺點, 就是高頻時雖然針對聲源方向的 mainlobe 變窄了, 但同時 sidelobe 卻變多了. 也就是在高頻時, 某些方向的聲源消不掉. 如下圖: Fixed Beampattern為了修正上述 DS beamformer 的問題, 我們希望得到 response-invariant broadband beamformer. 希望能有下圖的結果: 中心思想很簡單, 針對某個頻率 $\\omega$ 來求出相對應的 $H$ 使得 beampattern 會與我們 desired beampattern 有 least-sqaure 差異. 以下 $H(\\omega)$ 會省略 $\\omega$ 不寫 結論到這裡我們討論了遠場的 signal model, 針對 anechoic model 我們最終導出了 beampattern. 做為例子我們使用簡單的 delay-and-sum (DS) beamformer 來看它的 beampattern 長什麼樣. 可以看到在高頻時很多方向還是無法壓抑, 因此使用 least-square 方法找出每個頻率需要的 spatial filter 來逼近我們需要的 beampattern. Reference Optimum Array Processing: Part IV of Detection, Estimation, and Modulation Theory by Harry L. Van Trees Microphone Array Signal Processing by Jocab Benesty Direction of Arrival Estimation Using the Parameterized Spatial Correlation Matrix","tags":[{"name":"array manifold vector","slug":"array-manifold-vector","permalink":"https://bobondemon.github.io/tags/array-manifold-vector/"},{"name":"beampattern","slug":"beampattern","permalink":"https://bobondemon.github.io/tags/beampattern/"},{"name":"anechoic model","slug":"anechoic-model","permalink":"https://bobondemon.github.io/tags/anechoic-model/"},{"name":"wavenumber","slug":"wavenumber","permalink":"https://bobondemon.github.io/tags/wavenumber/"}]},{"title":"Bayesian Learning Notes","date":"2018-12-20T14:39:42.000Z","path":"2018/12/20/Bayesian-Learning-Notes/","text":"枉費我學習 ML 這麼久, 最近才完整了解 Bayesian learning 大架構, 以及與 MLE, MAP, Variational Inference, Sampling 之間的關聯. 這才終於有了見樹又見林的港覺阿! 筆記整理如下 … 圖片來自 wiki, 我也好想要這個裝飾燈. 就這麼一個 Baye’s Rule, 撐起了統計機器學習的基石! Bayesian Learning給定訓練集 ($X,Y$) 和一個 probabilistic classifier $p(y|x,\\theta)$, 同時定義好 prior distribution $p(\\theta)$. 根據 Baye’s rule, Training stage 如下: $$\\begin{align} p(\\theta|X,Y)=\\frac{p(Y|X,\\theta)p(\\theta)}{\\color{red}{\\int p(Y|X,\\theta)p(\\theta)\\,d\\theta}} \\end{align}$$ Testing stage 如下: $$\\begin{align} p(y^*|x^*,X,Y)=\\color{red}{\\int p(y^*|x^*,\\theta)p(\\theta|X,Y)\\,d\\theta} \\end{align}$$ 注意到關鍵的兩個紅色積分通常都是不容易算, 或根本算不出來. 此時我們有兩種選擇: 使用 Variational Inference 找出一個 $q(\\theta)$ 來逼近 $p(\\theta|X,Y)$ 使用 sampling 方法. 理解一下這個積分的形式, 可以發現這是在算根據某個機率分佈$p(x)$計算$f(x)$的期望值. 因此, 如果我們直接根據 $p(x)$ sample 出 $M$ 個 $x$, 就可以用如下的平均算出近似值了. $$\\begin{align} \\int p(x)f(x) \\,dx \\simeq \\frac{1}{M}\\sum_{i=1}^M f(x_i)\\mbox{, where }x_i \\sim p(x) \\end{align}$$ 我們可能會想, 是不是可以將 Bayesian learning 做些簡化來避掉上述紅色積分? 是的, MLE 和 MAP 就是簡化了完整的 Bayesian learning 過程. 下面介紹. MLE and MAPBaye’s rule (式 (1)), 在 ML 中舉足輕重, 幾乎是所有的根本. 重新列出來並用不同顏色做強調 $$\\begin{align} \\color{orange}{p(\\theta|X,Y)}=\\frac{\\color{blue}{p(Y|X,\\theta)}\\color{green}{p(\\theta)}}{\\color{red}{\\int p(Y|X,\\theta)p(\\theta)\\,d\\theta}} \\end{align}$$ 橘色稱為 posterior distribution, 藍色為 likelihood, 而綠色為 prior distribution. 注意到紅色的期望值基本算不出來, 在這種情況下, 我們要怎麼得到 posterior? MLEMLE (Maximum Likelihood Estimation) 的想法是, 既然 posterior 算不出來, 那乾脆直接用一個 $\\theta^*$ 代表整個 $p(\\theta|X,Y)$ 分布算了. 至於要找哪一點呢, 就找對 likelihood 最大的那點吧! 數學這麼寫: $$\\begin{align} \\theta_{MLE}=\\arg\\max_\\theta p(Y|X,\\theta) \\end{align}$$ 既然已經用一個點來代表整個 posterior 了, 因此原來的 testing (2) 就不需要積分了, testing stage 直接就是: $$\\begin{align} p(y^*|x^*,\\theta_{MLE}) \\end{align}$$ MAPMAP (Maximum A Posterior) estimation 跟 MLE 相同, 也使用一個點來代表整個 posterior: $$\\begin{align} \\theta_{MP}=\\arg\\max_\\theta p(\\theta|X,Y) \\end{align}$$ 意思是 MAP 直接使用 mode 來代表整個 posterior. 因此 testing stage 也如同 MLE 情形: $$\\begin{align} p(y^*|x^*,\\theta_{MP}) \\end{align}$$ 不過聰明的讀者應該會覺得很疑惑, posterior 不是很難計算, 或根本算不出來, 這樣怎麼可能找的到 mode? 是的, 一般情形下是找不出來, 但有一個特殊情況叫做 conjugate prior. conjugate prior 指的是 prior 與 posterior 屬於同一個 distribution family, 等於是告訴我們 posterior 是什麼樣的 distribution, 因此算不出來的紅色期望值(式(4))也根本沒必要去計算, 只不過是個 normalization constant. 因此明確知道 posterior 是什麼樣的 distribution, 找 mode 就容易多了. 所以對於 MAP 來說有哪些 distribution 是互為 conjugate 變得很重要. 我們可以從 wiki 上查到明確資料. 基本上 exponential family 都是. 完全避掉紅色積分項了嗎?很多模型都具有 latent variable (一般都用 $z$ 表示) 的形式 稍微說明下, 一般說的 latent variable 會隨著 data 變大而變大, 而 parameter $\\theta$ 不會. 以 GMM 為例子, latent variable 指每一個 observation 是哪一個 Gaussian 產生出來的那個 index, 而 parameter 是 Gaussian components 的 mean, var, 和 mixture weights 集合. 可以使用 EM algorithm 來找出 MLE 或 MAP . 其中 E-step 為 “令 $q(z)$ 等於 $p(z|x,\\theta^{odd})$”, 這又回到如同式 (1) 求 posterior 會遇到分母積分項的問題. 如果我們的 $z$ 的值有限個的 (如 GMM, $z$ 的值就是 component 的 index), $p(z|x,\\theta^{odd})$ 可以直接算出來. 但複雜一點就不行了, 所以情況又變得跟原來的 Bayesian learning 一樣, 兩種選擇: 使用 Variational Inference, 這時稱為 Variational EM. 使用 sampling 方法. Sampling 通常採用 MCMC 方式, 這時稱為 MCMC EM. Summary擷取自 Coursera 的 Bayesian Methods for Machine Learning 課程投影片如下: (圖中的 $T$ 指的是 latent variable)","tags":[{"name":"Bayesian Learning","slug":"Bayesian-Learning","permalink":"https://bobondemon.github.io/tags/Bayesian-Learning/"},{"name":"Conjugate Prior","slug":"Conjugate-Prior","permalink":"https://bobondemon.github.io/tags/Conjugate-Prior/"},{"name":"MLE","slug":"MLE","permalink":"https://bobondemon.github.io/tags/MLE/"},{"name":"MAP","slug":"MAP","permalink":"https://bobondemon.github.io/tags/MAP/"}]},{"title":"Gaussian Process used in Bayesian Optimization","date":"2018-12-09T10:46:36.000Z","path":"2018/12/09/Gaussian-Process-used-in-Bayesian-Optimization/","text":"上了 Coursera 的 Bayesian Methods for Machine Learning, 其中最後一週的課程介紹了 Gaussian processes &amp; Bayesian optimization 覺得很有收穫, 因為做 ML 最痛苦的就是 hyper-parameter tuning, 常見的方法就是手動調, grid search or random search. 現在可以有一個較 “模型” 的作法: Bayesian optimization. 為了瞭解這個過程, 我們會介紹如下內容並同時使用 GPy and GPyOpt 做些 toy example: Random Process and Gaussian Process Stationary and Wide-Sense Stationary (WSS) GP for regression GP for bayesian optimization 讓我們進入 GP 的領域吧 Random Process (RP) and Gaussian Process (GP)Random process (RP) 或稱 stochastic process 定義為 [Def]: For any $x\\in\\mathbb{R}^d$ assign random variable $f(x)$ 例如 $d=1$ 且是離散的情形, ($x\\in\\mathbb{N}$) 定義說明對於每一個 $x$, $f[x]$ 都是一個 r.v. 所以 $f[1]$, $f[2]$, … 都是 r.v.s. 此 case 通常把 $x$ 當作時間 $t$ 來看. 而 Gaussian Process 定義為 [Def]: Random process $f$ is Gaussian, if for any finite number points, their joint distribution is normal. Stationary and Wide-Sense Stationary (WSS)Stationary一個 RP 是 stationary 定義如下: [Def]: Random process is stationary if its finite-dimensional distributions depend only on relative position of the points 簡單舉例: 取三個 r.v.s $(x_1,x_2,x_3)$ 他們的 joint pdf 會跟 $(x_1+t,x_2+t,x_3+t)$ 一模一樣$$\\begin{align} p(f(x_1),f(x_2),f(x_3))=p(f(x_1+t),f(x_2+t),f(x_3+t)) \\end{align}$$所以 pdf 只與相對位置有關, 白話講就是我們觀察 joint pdf 可以不用在意看的是哪個區段的信號, 因為都會一樣. 進一步地, 如果這個 RP 是 GP 的話, 我們知道 joint pdf 是 normal, 而 normal 只由 mean and variance totally 決定, 因此一個 GP 是 stationary 只要 mean and variance 只跟相對位置有關就會是 stationary. 基於這樣的條件我們可以寫出一個 stationary GP 的定義: [Def]: Covariance matrix or Kernel 只跟相對位置有關, 以下為三種常見的定義方式不管怎樣, 通常相對位置近的 r.v. 都會假設比較相關, (這也符合實際狀況, 譬如聲音訊號時間點相近的 sample 會較相關), 也因此 kernel 都會長類似下面的樣子 支線 Wide-Sense Stationary (WSS)本段可跳過, 主要是為了更深地理解 stationary 做的補充.WSS 定義為 [Def]: Random Process is WSS if its finite-dimensional distribution’s mean and variance depend only on relative position of the points 注意 WSS 與 Stationary 的定義差異. 準確來說 Stationary 要求所有的 moments 都只與相對位置有關, 但 WSS 只要求到 first and second order moments. 說明了 WSS 將 stationary 的條件放寬. 注意到 WSS 不一定是 stationary 的 GP, 這是因為 WSS 沒有要求 distribution 必須是 Normal.WSS, Stationary GP, Stationary RP 之間的關係可以這麼描述:$$\\begin{align} \\mbox{Stationary GP}\\subset\\mbox{Stationary RP}\\subset\\mbox{WSS} \\end{align}$$ 其實 WSS 與本篇主要討論的 Bayesian Optimization 沒有直接關係, 會想介紹是因為滿足 WSS 的話, 能使我們更直覺的 “看出” 一個訊號是否可能是 stationary. (另外 WSS 在 Adaptive Filtering 非常重要, 相當於基石的存在)首先使用課程的 stationary 範例: 中間的圖明顯不是 stationary 因為 mean 隨著位置改變不是 constant, 但左邊和右邊就真的不是那麼容易看出來了. 那麼究竟有什麼方法輔助我們判斷 stationary 呢?WSS 的 power-spectral density property 說明了一個 signal 如果是 WSS, 則它的 Covariance matrix or Kernel (訊號處理通常稱 auto-correlation) 的 DTFT 正好代表的物理意義就是 power spectral density, 而因為 kernel 不會因位置改變, 這導致了不管我們在哪個區段取出一個 window 的訊號, 它們的 power spectral density 都會長一樣. 這個性質可以讓我們很方便的 “看出” 是否是 stationary. (簡單講就是看 signal 的 frequency domain 是否因為隨著時間而變化, 變的話就一定不是 stationary) 好了, 接著回到主線去. GP for regression直接節錄課程 slides, 因為 stationary GP 的 mean 是 const, 因此我們扣掉 offset 讓其為 0, 之後再補回即可.做 Regression 的目的就是 given $x$ 如何預測 $f(x)$, 而我們有的 training data 為 $x_1, …, x_n$ 以及它們相對的 $f(x_1),…,f(x_n)$. GP 就可以很漂亮地利用 conditional probability 預測 $f(x)$ 由於是 GP, 導致上面藍色部分結果仍是 Gaussian, 因此我們得到Regression 公式: 有時候我們的 observation $f(x)$ 是 noisy 的, 此時簡單地對 $f(x)$ 加上一個 random Gaussain noise 會使得我們的 model robust 些. 上面公式都不用改, 只要針對 kernel 作如下更動即可 GPy toolkit example我們使用 Gaussian process regression tutorial 的範例, 使用上算是很直覺, 讓我們簡單實驗一下 12345678910111213import numpy as npimport GPyimport matplotlib.pyplot as plt# Generate data points, where Y is noisyX = np.random.uniform(-3.,3.,(20,1))Y = np.sin(X) + np.random.randn(20,1)*0.2# Define kernel, we use RBFkernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)# Define GP regression modelm = GPy.models.GPRegression(X,Y,kernel,noise_var=1.)# See plotfig = m.plot()plt.show() Codes 的結果如下 [Note]: 上圖的 Mean 和 Confidence 指的是 Regression 公式的 $\\mu$ and $\\sigma^2$ (前幾張有紅框的圖), 另外使用 GPy 算 regression 結果的話這麼使用1mu, sigma2 = m.predict(np.array([[1.0]])) 基本上 input 是一個 shape=(batch_size,in_dim) 的 array 可以看到就算是 data point 附近, 所顯示的 y 還是有非常大的 uncertainty, 這是因為 observation noise 的 variance 可能太大了, 我們從 1.0 改成 0.04 (正確答案) 看看 可以看到有 data point 的地方不確定性降低很多, 且不確定性看起來很合理 (當然, 因為我們用正確答案的 noise var)接著我們改 kernel 的 lengthscale (控制平滑程度) 從 1 縮小成 0.5 應該可以預期 regression 的 mean 會扭曲比較大, 結果如下 看看一個極端情況, 將 kernel 的 lengthscale 降到非常小, 這會導致 kernel 退化成 delta function, 也就是除了自己大家互不相關. 查看 regression 公式 (前幾張有紅框的圖), 由於 kernel 退化成 delta function, k 向量趨近於 0, 所以 regression 公式的 mean 趨近於 0, variance 趨近於 prior K(0). 我們將 lengthscale 調整成 0.02 得到如下的圖 可以看到 x 稍微離開 data point 的地方, 基本 mean 就回到 0, 且 variance 回到 prior K(0) 從這簡單的實驗我們發現, 這三個控制參數:-RBF variance-RBF lengthscale-GPRegression noise_var 設置不好, 基本很悲劇, 那怎麼才是對的? 下一段我們介紹使用最佳化方式找出最好的參數. Learning the kernel parameters由於都是 normal, 因此 MLE 目標函式可微, 可微就使用 gradient ascent 來求參數. 課程 slide 如下 GPy 的使用就這麼一行 m.optimize(messages=True) 結果如下 GP for bayesian optimization問題再描述一下, 就是說模型有一大堆參數 (稱 $x$) 要調整, 選擇一組參數可以得到一個 validation accuracy (稱 $f(x)$), 我們希望找到一組參數使得 $f(x)$ 最大. 這個麻煩之處就在於我們對 $f(x)$ 的表示一無所知, 只能透過採樣得到 $f(x)$, 而每次要得到一個採樣的 $f(x)$ 都要經過漫長的訓練才能得到. 在這種情形下, 怎麼透過最少採樣點得到較大的 $f(x)$ 值呢? 大絕就是用 GP 來 approximate $f(x)$ 合理嗎? 我們這麼想, 由於 kernel 一般的定義會使得相近的採樣點有較高的相關值, 也就是類似的參數會得到較相關的 validation accuracy. 這麼想的話多少有些合理. 另一個好處是使用 GP 可以帶給我們機率分布, 這使得我們可以使用各種考量來決定下一個採樣點. 例如: 我們可以考慮在那些不確定性較大的地方試試看, 因為說不定有更高的$f(x)$, 或是在已知目前估測的 GP model 下有較高的 mean 值那裏採樣. 這兩種方式稱為 “Exploration” and “Exploitation” 所以 Bayesian optimization 主要就兩個部分, Surrogate model (可以使用 GP) 和 Acquisition function: 演算法就很直覺了, 根據 Acquisition function 得到的採樣點 $x$ 計算出來 $f(x)$ 後, 重新更新 GP (使用 MLE 找出最好的 GP 參數更新), 更新後繼續計算下個採樣點. 我們還未說明 Acquisition function 怎麼定義, 常用的方法有下面三種: Maximum probability of improvement (MPI) Upper confidence bound (UCB) Expected improvement (EI), 網路上說最常被用 Expected improvement (EI) 可以參考這篇 blog 搭配這個 implementation, 這裡就不重複了. 值得一提的是 Acquisition function 通常有個參數 $\\xi$ 可以控制 Exploration 和 Exploitation 的 tradeoff. 接著我們使用 GPyOpt 練習一個 toy example. GPyOpt toy example類似上面的 GP for regression 的範例, $x$ and $f(x)$ 我們簡單定義如下: 1234sample_num = 20offset = 10def probeY(x): # i.e. f(x) return np.sin(x) + np.random.randn()*0.2 + offset 這次我們將 $f(x)$ 故意加了一個 offset, 雖然對於 GP regression 的假設是 mean=0, 不過 GPyOpt 預設會對 $f(x)$ 去掉 offset, 所以其實我們可以很安全的使用 API. 接著關鍵程式碼如下 12345678bounds = [&#123;'name': 'var_1', 'type': 'continuous', 'domain': (-3.0,3.0)&#125;] # domain definitionmyBopt = GPyOpt.methods.BayesianOptimization(f=probeY, # function to optimize domain=bounds, # box-constraints of the problem normalize_Y=True, # normalize Y to mean = 0 (default) acquisition_type='EI', # acquisition function type (default) maximize=False, # do maixmization? (default=False) exact_feval = False)myBopt.run_optimization(max_iter) bounds 描述了每一個 probeY 的 arguments. 特別要說一下 exact_feval = False, 這是說明 $f(x)$ 是 noisy 的, 所以會有 noise variance 可以 model (參考 3. GP for regression 的 regression 公式含noise的情形), 這在實際情況非常重要. 更多 BayesianOptimization 參數描述 參考這 使用如下指令看 acquisition 和 regression function 的結果1myBopt.plot_acquisition() 才花1x個採樣求出來的 regression function 就很逼近真實狀況了, 還不錯. 另外注意到這裡的 $f(x)$ 已經去掉 offset 了.再來使用如下指令看每一次採樣值之間的差異, 以及採樣點的 $f(x)$1myBopt.plot_convergence() 第9次採樣開始, 採樣點 $x$ 以及 $f(x)$ 之間基本沒什麼差異了.若要拿到採樣過程的 $x$ and $f(x)$, 可使用 myBopt.X 和 myBopt.Y XGBoost parameter tuning針對 XGBoost 的參數進行最佳化 (可參考這篇 blog), 關鍵就在於上面的 $probeY$ 需替換成計算某個採樣的 evaluation accuracy (因此還需要訓練). 關鍵程式碼如下: 123456789101112131415161718192021222324252627282930313233343536from xgboost import XGBRegressorfrom sklearn.model_selection import cross_val_score... Some codes here# Score. Optimizer will try to find minimum, so we will add a \"-\" sign.def probeY(parameters): parameters = parameters[0] score = -cross_val_score( XGBRegressor(learning_rate=parameters[0], max_depth=int(parameters[2]), n_estimators=int(parameters[3]), gamma=int(parameters[1]), min_child_weight = parameters[4]), X, y, scoring='neg_mean_squared_error').mean() score = np.array(score) return score# Bounds (NOTE: define continuous variables first, then discrete!)bounds = [ &#123;'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)&#125;, &#123;'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)&#125;, &#123;'name': 'max_depth', 'type': 'discrete', 'domain': (1, 50)&#125;, &#123;'name': 'n_estimators', 'type': 'discrete', 'domain': (1, 300)&#125;, &#123;'name': 'min_child_weight', 'type': 'discrete', 'domain': (1, 10)&#125; ]np.random.seed(777)optimizer = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds, acquisition_type ='MPI', acquisition_par = 0.1, exact_eval=True)max_iter = 50max_time = 60optimizer.run_optimization(max_iter, max_time)optimizer.plot_convergence()optimizer.X[np.argmin(optimizer.Y)] 課程讓我們測試了 sklearn.datasets.load_diabetes() dataset, 使用 GPyOpt 可以讓 XGBoost 比預設參數有 9% 的提升! 還是很不錯的. 再來就很期待是否能真的在工作上對 DNN 套用了. Reference GPy and GPyOpt GPyOpt’s documentation can find APIs GPyOpt tutorial 很好的 GPy and GPyOpt 數學和範例 blog Acquisition function implementation WSS power-spectral density property https://www.imft.fr/IMG/pdf/psdtheory.pdf","tags":[{"name":"Gaussian Process","slug":"Gaussian-Process","permalink":"https://bobondemon.github.io/tags/Gaussian-Process/"},{"name":"Bayesian Optimization","slug":"Bayesian-Optimization","permalink":"https://bobondemon.github.io/tags/Bayesian-Optimization/"},{"name":"Stationary","slug":"Stationary","permalink":"https://bobondemon.github.io/tags/Stationary/"}]},{"title":"CTC Implementation Practice","date":"2018-10-16T12:25:10.000Z","path":"2018/10/16/CTC-Implementation-Practice/","text":"Credit 是此篇 DingKe ipynb 的, 他完整呈現了 CTC loss 以及 gradient 的計算, 非常棒!此筆記加入自己的說明, 並且最後使用 tensorflow 來驗證.這篇另一個主要目的為改成可以練習的格式 (#TODO tag). 因為我相信最好的學習方式是自己造一次輪子, 所以可以的話, 請試著把 #TODO tag 的部分做完吧.我們只專注在 CTC loss 的 forward, backwark and gradient. Decoding 部分請參考原作者的 ipynb. 最後使用 tf.nn.ctc_loss and tf.gradients 與我們的計算做對比 完成以下步驟 完成 CTC_Practice.ipynb #TODO tag 參考 CTC_Practice_Answer.ipynb Reference DingKe ipynb Sequence Modeling With CTC Graves CTC","tags":[{"name":"CTC","slug":"CTC","permalink":"https://bobondemon.github.io/tags/CTC/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://bobondemon.github.io/tags/TensorFlow/"}]},{"title":"Variational Inference and VAE Notes","date":"2018-09-18T14:21:05.000Z","path":"2018/09/18/Variational-Inference-Notes/","text":"前一陣子學習了 Variational Inference, 因為自己記性只有 LSTM 沒有 L, 所以趕快記下筆記. 學得還是很粗淺, 又是一個大坑阿.監督學習不外乎就是 training 和 testing (inference). 而 inference 在做的事情就是在計算後驗概率 $p(z|x)$. 在 PGM 中通常是 intractable, 或要找到 exact solution 的計算複雜度太高, 這時 VI 就派上用場了. VI 簡單講就是當 $p(z|x)$ 不容易得到時, 可以幫你找到一個很好的近似, $q(z)$. 放上一張 NIPS 2016 VI tutorial 的圖, 非常形象地表示 VI 做的事情: 將找 $p(z|x)$ 的問題轉化成一個最佳化問題. 怎麼看作最佳化問題?我們要找到一個 $q(z)$ 去逼近 $p(z|x)$, 因此需要計算兩個機率分佈的距離, 而 KL-divergence 是個很好的選擇 (雖然不滿足數學上的距離定義). 所以我們的目標就是希望 $KL(q(z)\\Vert p(z|x))$ 愈小愈好, 接著我們對 KL 定義重新做如下的表達: $$\\begin{align} KL\\left(q(z)\\Vert p(z|x)\\right)=-\\sum_z q(z)\\log\\frac{p(z|x)}{q(z)}\\\\ =-\\sum_z q(z)\\left[\\log\\frac{p(x,z)}{q(z)}-\\log p(x)\\right]\\\\ =-\\sum_z q(z)\\log\\frac{p(x,z)}{q(z)}+\\log p(x) \\end{align}$$ 得到這個非常重要的式子: $$\\begin{align} \\log p(x)=KL\\left(q(z)\\Vert p(z|x)\\right)+ \\color{red}{ \\sum_z q(z)\\log\\frac{p(x,z)}{q(z)} } \\\\ =KL\\left(q(z)\\Vert p(z|x)\\right)+ \\color{red}{ \\mathcal{L}(q) } \\\\ \\end{align}$$ 為什麼做這樣的轉換呢? 這是因為通常 $p(z|x)$ 很難得到, 但是 complete likelihood $p(z,x)$ 通常很好求.觀察 (5), 注意到在 VI 的設定中 $\\log p(x)$ 跟我們要找的 $q(z)$ 無關, 也就造成了 $\\log p(x)$ 是固定的. 由於 $KL\\geq 0$, 讓 $KL$ 愈小愈好等同於讓 $\\mathcal{L}(q)$ 愈大愈好. 因此 VI 的目標就是藉由最大化 $\\mathcal{L}(q)$ 來迫使 $q(z)$ 接近 $p(z|x)$. $\\mathcal{L}(q)$ 可以看出來是 marginal log likelihood $\\log p(x)$ 的 lower bound. 因此稱 variational lower bound 或 Evidence Lower BOund (ELBO). ELBO 的 gradient我們做最佳化都需要計算 objective function 的 gradient. 讓要找的 $q$ 由參數 $\\nu$ 控制, i.e. $q(z;\\nu)$, 所以我們要找 ELBO 的 gradient 就是對 $\\nu$ 微分. $$\\begin{align} \\mathcal{L}(\\nu)=\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\\\ \\Rightarrow \\nabla_{\\nu}\\mathcal{L}(\\nu)=\\nabla_{\\nu}\\left(\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\right)\\\\ \\mbox{Note }\\neq \\mathbb{E}_{z\\sim q}\\left(\\nabla_{\\nu}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\right)\\\\ \\end{align}$$ 注意 (8) 不能將 Expectation 與 derivative 交換的原因是因為要微分的 $\\nu$ 與要計算的 Expectation 分布 $q$ 有關. 下面會提到一個很重要的技巧, Reparameterization trick, 將 Expectation 與 derivative 交換, 而交換後有什麼好處呢? 下面提到的時候再說明. 回到 (7) 展開 Expectation 繼續計算 gradient, 直接用 NIPS slide 結果如下: 計算一個機率分佈的 Expectation 可用 Monte Carlo method 採樣, 例如採樣 $T$ 個 samples$$\\begin{align} \\mathbb{E}_{z\\sim q}f(z)\\approx\\frac{1}{T}\\sum_{t=1}^Tf(z)\\mbox{, where }z\\sim q \\end{align}$$ 因此 gradient 可以這麼大致找出來, 不過這方法找出來的 gradient 與真實的 gradient 存在很大的誤差, 換句話說, 這個近似的 gradient variance 太大了. 原因兩個 $q$ 本身就還在估計, 本身就不準確了 Monte Carlo method 採樣所造成的誤差 下一段的 reparameterization trick 就可以去除掉上面第一個誤差, 因此估出來的 gradient 就穩定很多. Reparameterization Trick我們用 Gaussian 舉例, 令 $q$ 是 Gaussian, $q(z;\\mu,\\sigma)=\\mathcal{N}(\\mu,\\sigma)$, 其中 $\\nu=${$\\mu,\\sigma$}, 而我們其實可以知道 $z=\\mu+\\sigma \\epsilon$, where $\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})$. 因此:$$\\begin{align} \\mathcal{L}(\\nu)=\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z)-\\log q(z;\\nu)\\right]\\\\ =\\mathbb{E}_{ \\color{red}{ \\epsilon\\sim \\mathcal{N}(0,\\mathbf{I}) } }\\left[\\log p(x, \\color{red}{ \\mu+\\sigma \\epsilon } )-\\log q( \\color{red}{ \\mu+\\sigma \\epsilon } ;\\nu)\\right] \\end{align}$$ 這時候我們計算 ELBO 的 gradient 時, 我們發現 $\\nu$ 與 Expectation 的分佈, $\\mathcal{N}(0,\\mathbf{I})$, 無關了! 因此 (7) 套用上面的 trick 就可以將 Expectation 與 derivative 交換. 結果如下: $$\\begin{align} \\nabla_{\\mu}\\mathcal{L}(\\nu)=\\mathbb{E}_{\\epsilon\\sim \\mathcal{N}(0,\\mathbf{I})}\\left[\\nabla_{\\mu}\\left(\\log p(x,\\mu+\\sigma \\epsilon) - \\log q(\\mu+\\sigma \\epsilon;\\nu)\\right)\\right]\\\\ \\approx\\frac{1}{T}\\sum_{t=1}^T \\nabla_{\\mu}\\left( \\log p(x,\\mu+\\sigma \\epsilon) - \\log q(\\mu+\\sigma \\epsilon;\\nu) \\right)\\mbox{, where }\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})\\\\ \\end{align}$$ 在上一段計算 ELBO gradient 所造成誤差的第一項原因就不存在了, 因此我們用 reparameterization 得到的 gradient 具有很小的 variance. 這個 github 做了實驗, 發現 reperameterization 的確大大降低了估計的 gradient 的 variance. $$\\begin{align} \\nabla_{\\mu}\\left(\\log p(x,\\mu+\\sigma \\epsilon) - \\log q(\\mu+\\sigma \\epsilon;\\nu)\\right) \\end{align}$$ 怎麼計算呢? 我們可以使用 Tensorflow 將要計算 gradient 的 function 寫出來, tf.gradients 就能算 VAEVariational Inference 怎麼跟 Neural Network 扯上關係的? 這實在很神奇.我們先來看看 ELBO 除了 (6) 的寫法, 還可以這麼表示: $$\\begin{align} \\mathcal{L}(\\nu)=\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\\\ =\\mathbb{E}_{z\\sim q}\\left[ \\log p(x|z) + \\log p(z) - log q(z;\\nu) \\right]\\\\ =\\mathbb{E}_{z\\sim q}\\left[ \\log p(x|z)\\right] + \\mathbb{E}_{z\\sim q}\\left[ \\log \\frac{p(z)}{q(z;\\nu)}\\right]\\\\ =\\mathbb{E}_{z\\sim q}\\left[ \\log p(x|z)\\right] - KL(q(z;\\nu)\\|p(z))\\\\ \\end{align}$$ 我們讓 $p(x|z)$ 被參數 $\\theta$ 所控制, 所以最後 ELBO 如下:$$\\begin{align} \\mathcal{L}(\\nu,\\theta)=\\mathbb{E}_{z\\sim q}\\left[ \\log \\color{orange}{ p(x|z,\\theta) } \\right] - KL( \\color{blue}{ q(z;\\nu) } \\|p(z))\\\\ \\end{align}$$ 讓我們用力看 (19) 一分鐘接著在用力看 (19) 一分鐘最後在用力看 (19) 一分鐘 有看出什麼嗎? … 如果沒有, 試著對照下面這張圖 Encoder 和 Decoder 都同時用 NN 來學習, 這裡 $\\nu$ 和 $\\theta$ 分別表示 NN 的參數, 而使用 Reparameterization trick 來計算 ELBO 的 gradient (14) 就相當於在做這兩個 NN 的 backprop. 但是上圖的 Encoder 產生的是一個 pdf, 而給 Decoder 的是一個 sample $z$, 這該怎麼串一起? VAE 的做法就是將 $q(z)$ 設定為 diagonal Gaussian, 然後在這個 diagonal Gaussian 採樣出 $T$ 個 $z$ 就可以丟給 Decoder. 使用 diagonal Gaussian 有兩個好處: 我們可以用 reparameterization trick, 因此採樣只在標準高斯上採樣, 自然地 Encoder 的 output 就是 $\\mu$ 和 $\\sigma$ 了. (19)的 KL 項直接就有 closed form solution, 免掉算 expectation (假設$p(z)$也是Gaussian的話) 根據1, 架構改動如下: 將原來的 ELBO (10) 轉成 (19) 來看的話, 還可以看出一些資訊.當最大化 (19) 的時候 RHS 第一項要愈大愈好 (likelihood 愈大愈好), 因此這一項代表 reconstruct error 愈小愈好. RHS 第二項, 也就是 $KL(q(z;\\nu)\\Vert p(z))$ 則要愈小愈好. 因此會傾向於讓 $q(z;\\nu)$ 愈接近 $p(z)$ 愈好. 這可以看做 regularization. 但是別忘了一開始說 VI 的做法就是藉由最大化 ELBO 來迫使 $q(z;\\nu)$ 接近 $p(z|x)$, 而上面才說最大化 ELBO 會傾向於讓 $q(z;\\nu)$ 接近 $p(z)$.這串起來就說 $q(z;\\nu)$ 接近 $p(z|x)$ 接近 $p(z)$. 在 VAE 論文裡就將 $p(z)$ 直接設定為 $\\mathcal{N}(0,\\mathbf{I})$. 因此整個 VAE 訓練完的 Encoder 的 $z$ 分布會有高斯分布的情形. Conditional VAE (CVAE)原來的 VAE 無法控制要生成某些類別的圖像, 也就是隨機產生 $z$ 不知道這會對應到哪個類別. CVAE 可以根據條件來產生圖像, 也就是除了給 $z$ 之外需要再給 $c$ (類別) 資訊來生成圖像. 怎麼辦到的呢? 方法簡單到我嚇一跳, 看原本論文有點迷迷糊糊, 但這篇文章解釋得很清楚! 簡單來說將原來的推倒全部加上 condition on $c$ 的條件. 從 (4) 出發修改如下: $$\\begin{align} \\log p(x \\color{red}{ | c } ) =KL\\left(q(z \\color{red}{ | c } )\\Vert p(z|x, \\color{red}{ c } )\\right)+ \\sum_z q(z \\color{red}{ | c } )\\log\\frac{p(x,z \\color{red}{ | c } )}{q(z \\color{red}{ | c } )} \\\\ \\end{align}$$ 用推導 VAE 一模一樣的流程, 其實什麼都沒做, 只是全部 conditioned on $c$ 得到 (19) 的 condition 版本 $$\\begin{align} \\mathcal{L}(\\nu,\\theta \\color{red}{ | c } )=\\mathbb{E}_{z\\sim q}\\left[ \\log \\color{orange}{ p(x|z,\\theta, \\color{red}{ c } ) } \\right] - KL( \\color{blue}{ q(z;\\nu \\color{red}{ | c } ) } \\|p(z))\\\\ \\end{align}$$ 這說明了我們在學 Encoder 和 Decoder 的 NN 時必須加入 conditioned on $c$ 這個條件! NN 怎麼做到這點呢? 很暴力, 直接將 class 的 one-hot 跟原來的 input concate 起來就當成是 condition 了. 因此 CVAE 的架構如下: 實作細節就不多說了, 直接參考 codes 由於我們的 condition 是 one-hot, 如果同時將兩個 label 設定為 1, 是不是就能 conditioned on two classes 呢? 實驗如下 conditioned on ‘0’ and ‘4’ conditioned on ‘1’ and ‘3’ 另外, 如果給的 condition 值比較小, 是不是就可以產生比較不是那麼確定的 image 呢? 我們嘗試 conditioned on ‘4’ 且值從 0.1 (weak) 到 1.0 (strong), 結果如下: 這個 condition 值大小還真有反應強度呢! Neural network 真的很神奇阿~ Mean Field VI讓我們拉回 VI. Mean Field 進一步限制了 $q$ 的範圍, 它假設所有控制 $q$ 的參數 {$\\nu_i$} 都是互相獨立的, 這樣所形成的函數空間稱為 mean-field family. 接著採取 coordinate ascent 方式, 針對每個 $\\nu_i$ 獨立 update. 這種 fatorized 的 $q$ 一個問題是 estimate 出來的分布會太 compact, 原因是我們使用的指標是 $KL(q|p)$, 詳細參考 PRML Fig 10.2. 放上 NIPS 2016 slides, 符號會跟本文有些不同, 不過總結得很好: 另外想了解更多 Mean Field VI 或是透過例子了解, 推薦以以下兩個資料: Variational Inference tutorial series by Chieh Wu Variational Coin Toss by Björn Smedman Reference Variational Inference tutorial series by Chieh Wu Variational Inference: Foundations and Modern Methods (NIPS 2016 tutorial) Reparameterization Trick Goker Erdogan 有很好的 VAE, VI 文章 Conditional VAE 原論文 Conditional VAE 好文章 Variational Coin Toss by Björn Smedman My CVAE TF Practice Appendix: EM 跟 VI 很像阿在一般 EM 的設定上, 我們是希望找到一組參數 $\\tilde{\\theta}$ 可以讓 marginal likelihood $\\log p(x|\\theta)$ 最大, formally speaking: $$\\begin{align} \\tilde{\\theta}=\\arg\\max_\\theta \\log p(x|\\theta) \\end{align}$$ 如同 (4) 和 (5), 此時要求的變數不再是 $q$, 而是 $\\theta$: $$\\begin{align} \\log p(x|\\theta)=KL\\left(q(z)\\Vert p(z|x,\\theta)\\right)+\\sum_z q(z)\\log\\frac{p(x,z|\\theta)}{q(z)}\\\\ =KL\\left(q(z)\\Vert p(z|x,\\theta)\\right)+ \\color{orange}{ \\mathcal{L}(q,\\theta) } \\\\ \\end{align}$$ 此時的 $\\log p(x|\\theta)$ 不再是固定的 (VI是), 而是我們希望愈大愈好. 而我們知道 $\\mathcal{L}(q,\\theta)$ 是它的 lower bound 這點不變, 因此如果 lower bound 愈大, 則我們的 $\\log p(x|\\theta)$ 就當然可能愈大. 首先注意到 (23) 和 (24) 針對任何的 $q$ 和 $\\theta$ 等式都成立, 我們先將 $\\theta$ 用 $\\theta^{old}$ 以及 $q(z)$ 用 $p(z|x,\\theta^{old})$ 代入得到: $$\\begin{align} \\log p(x|\\theta^{old})= KL\\left(p(z|x,\\theta^{old})\\Vert p(z|x,\\theta^{old})\\right)+\\mathcal{L}(p(z|x,\\theta^{old}),\\theta^{old})\\\\ =0+\\mathcal{L}(p(z|x,\\theta^{old}),\\theta^{old})\\\\ \\leq\\max_{\\theta}\\mathcal{L}(p(z|x,\\theta^{old}),\\theta)\\\\ \\end{align}$$ 接著求$$\\begin{align} \\theta^{new}=\\arg\\max_{\\theta} \\mathcal{L}(p(z|x,\\theta^{old}),\\theta) \\end{align}$$ 如此 lower bound 就被我們提高了.(28) 就是 EM 的 M-step, 詳細請看 PRML Ch9.4 或參考下圖理解 “$q(z)$ 用 $p(z|x,\\theta^{old})$ 代入” 這句話其實有問題, 因為關鍵不就是 $p(z|x,\\theta)$ 很難求嗎? 這似乎變成了一個雞生蛋蛋生雞的情況. (就我目前的理解) 所以通常 EM 處理的是 discrete 的 $z$, 然後利用 $\\sum_z p(x,z|\\theta)$ 算出 $p(x|\\theta)$, 接著得到我們要的 $p(z|x,\\theta)$. 等於是直接簡化了, 但 VI 無此限制.","tags":[{"name":"Variational Inference","slug":"Variational-Inference","permalink":"https://bobondemon.github.io/tags/Variational-Inference/"},{"name":"ELBO","slug":"ELBO","permalink":"https://bobondemon.github.io/tags/ELBO/"},{"name":"Variational Auto Encoder (VAE)","slug":"Variational-Auto-Encoder-VAE","permalink":"https://bobondemon.github.io/tags/Variational-Auto-Encoder-VAE/"}]},{"title":"Ensemble Algorithm Summary Notes","date":"2018-09-03T13:45:08.000Z","path":"2018/09/03/Ensemble-Algorithm-Summary-Notes/","text":"這是用自己理解的方式整理了林軒田老師 ML 課程. 其中 Decision tree and Random Forest 沒紀錄. 以前第一次接觸到 Adaboost 的時候就被它深深著迷了, 當時 face detection 可商用算法無不採用經典的 Viola and Jones adaboost method. 在現在 DNN 成主流的時候, 雖然 adaboost 光環已退去, 但在 data mining, data science 領域 boosting 方法仍是最成功的算法之一. 基本上在 Kaggle 比賽可以看到主要兩大方法, 舉凡聲音影像文字等等的辨識就是 DNN, 其他凡是 data mining 相關的就屬 boosting (xgboost).有趣的是, 近年也有研究人員用 ensemble 的角度看待 DNN, 從這角度就能理解為何一路從 highway network –&gt; skip layer resent –&gt; resnext 的架構演變, 以及為何效果這麼好. 可以參考 “深度神经网络中深度究竟带来了什么？” 很精彩的解釋, 或是 MSR 2017 這篇論文 Deep Convolutional Neural Networks with Merge-and-Run Mappings 筆記內容如下: Bagging (or bootstrap) Adaboost 演算法2.1 Adaboost large margin 解釋2.2 Adaboost exponential error 解釋 Additive Model (a framework) Gradient Boosting Adaboost as an additive model Gradient Boost Decision Tree (GBDT) 待研究: XGBoost (Kaggle 比賽神器) Bagging (or bootstrap)還記得我們在 Why-Aggregation-Work 這篇提到, 當我們有很多 weak learner ${g_t}$ 時, 要得到一個 strong learner $G$ 最簡單的方法就是投票(或平均). 所以一個關鍵問題是要怎麼產生很多的 $g_t$?Bagging (or bootstrap) 提供了一個簡單的方法: 假設 dataset $D$ 有 $N$ 筆資料, bagging 就是從 $D$ 中重複採樣出 $N’$ 筆, 我們稱 $D’$, 然後 $g_t$ 就可以用 $D’$ 訓練出來.既然現在可以方便地產生很多 ${g_t}$, 然後就 $G$ 就採用平均方式, ensemble algorithm 就結束了?! 當然沒有, 別忘了有一個很關鍵的特性是, 當 ${g_t}$ 意見愈分歧時產生出來的 $G$ 效果愈好!那我們就問了, bagging 不就採樣嗎? 我怎麼知道這次採樣出來的 $D’$ 所訓練出來的 $g_t$ 會跟之前一次的意見分歧?我們就是能知道! (神奇吧) 要了解為什麼, 我們必須先將 bagging 擴展一下, 想成是對 weighted $D$ 採樣, 其中每一筆資料 $x_n$ 的 weight $u_n$ 代表抽中的機率. 如果 bagging 是對 weighted $D$ 採樣的話, 在第 t 輪的 $g_t$ 得到方式如下: $$\\begin{align} g_t=\\arg\\min_{h\\in \\mathcal{H}}\\left(\\sum_{n=1}^N u_n^{(t)} \\mathbb{I}[y_n\\neq h(x_n)] \\right) \\end{align}$$ 其中 $\\mathbb{I}[…]$ 表示 indicator function, 條件為 true 則 return 1, otherwise return 0.想法就是我們要設計一組新的權重, 讓新的權重對於 $g_t$ 來說相當於亂猜, 這樣用新權重找出的 $g_t+1$ 就會跟之前的意見分歧了. 具體來說, 新權重要有以下的效果: $$\\begin{align} \\frac{\\sum_{n=1}^N{u_n^{(t+1)} \\mathbb{I}[y_n\\neq g_t(x_n)]}}{\\sum_{n=1}^N{u_n^{(t+1)}}}=\\frac{1}{2} \\end{align}$$ 物理意義就是對於 $g_t$ 來說$$\\begin{align} \\mbox{for weak learner }g_t\\mbox{: }\\left(\\mbox{total }u_n^{(t+1)}\\mbox{ of incorrect}\\right)= \\left(\\mbox{total }u_n^{(t+1)}\\mbox{ of correct}\\right) \\end{align}$$ 所以新的權重調整方式其實很簡單, 用一個例子解釋. 假如 $u_n^t$ incorrect 合是 300, $u_n^t$ correct 合是 500. 我們只要把之前的 $u_n^t$ incorrect部分都乘 500, 而 correct 部分乘 300就可以了.或者我們這麼寫, 定義 $\\epsilon_t=300/(300+500)$, 則$$\\begin{align} u_n^{(t+1)}=u_n^{(t)}(1-\\epsilon_t) \\mbox{, if } y_n\\neq g_t(x_n)\\\\ u_n^{(t+1)}=u_n^{(t)}\\epsilon_t \\mbox{, if } y_n = g_t(x_n)\\\\ \\end{align}$$ 或通常也可以這麼計算 所以目前為止, 我們可以用 bagging 的方式 (對 weighted data) 產生出看似相當意見不同的 $g_t$, 那最後的 $G$ 用平均就可以了嗎? 可能不大好, 因為 $g_t$ 是針對某一種權重的 dataset 好, 不代表對原來沒有權重 (或uniform權重) 的 dataset 是好的.既然直接平均可能不夠好, 不如就用 linear combination 方式組合 $g_t$ 吧, 不過組合的 coefficients 是需要巧思設計的. 而 Adaboost 就設計出了一種組合方式, 能證明這種組合方式會使得 training error 收斂至0. (另一種用 additive model 的解釋方式為這樣的 coefficient 設計方式相當於用 steepest descent 並選擇最佳的步長). 這些會在文章下面說明. Adaboost 演算法 Adaboost large margin 解釋一般來說, model 愈複雜愈容易 overfit, 不過很特別的是 adaboost 隨著 iteration 結合愈多 weak learners 反而不會有容易 overfit 的現象. 其中一種解釋方式是 adaboost 具有類似 SVM 的 large margin 效果.我們首先分析一下第 t+1 次 iteration, dataset 的 weights$$\\begin{align} u_n^{(t+1)}=u_n^{(t)}\\diamond_t^{-y_n g_t(x_n)}\\\\ =u_n^{(t)}\\exp (-y_n \\alpha_t g_t(x_n)) \\end{align}$$ 我們這裡使用 binary classification 來說明, 其中 $y_n,g_t(x_n)\\in${-1,+1}, 式 (6) 可以從上一段 “Adaboost 演算法” 的圖中步驟2的 update 式子看出. 而式 (7) 從 $\\diamond_t$ 定義得到.上式可以一路展開到開頭 (iteration 1), 如下: $$\\begin{align} u_n^{(T+1)}=u_n^{(1)}\\prod_{t=1}^T \\exp (-y_n \\alpha_t g_t(x_n)) \\\\ =\\frac{1}{N}\\exp\\left(-y_n \\color{orange}{ \\sum_{t=1}^T \\alpha_t g_t(x_n) } \\right) \\end{align}$$ 有發現嗎? 橘色的部分其實就是我們的 $G$ $$\\begin{align} G(x_n)=sign\\left( \\color{orange}{ \\sum_{t=1}^T \\alpha_t g_t(x_n) } \\right) \\end{align}$$ 而如果將 $\\alpha_t$ 看成是 t-th coefficient, $g_t(x_n)$ 看成是 t-th 維度的特徵, 橘色部分就等同於 unnormalized margin. (除以 coefficients 的 norm 就是 margin了)Adaboost 可以證明 (with exponential decay) $$\\begin{align} \\sum_{n=1}^N u_n^{(t)}\\rightarrow 0\\mbox{, for }t\\rightarrow 0 \\end{align}$$ 這意味著什麼? 說明了隨著 iteration 增加, 橘色的值會愈大, 等同於我們的 $G$ 對於資料的 margin 會愈大.證明可參考 李航 統計學習方法 p142 Adaboost exponential error 解釋其實單看式 (9) 我們完全可以把它當成 error function. 重寫一下: $$\\begin{align} u_n^{(T+1)}=\\frac{1}{N}\\exp\\left(-y_n \\color{orange}{ \\sum_{t=1}^T \\alpha_t g_t(x_n) } \\right)\\\\ =\\frac{1}{N}\\exp\\left(-y_n \\color{orange}{ f_T(x_n) } \\right) \\end{align}$$ 怎麼說呢? 其實橘色部分我們可想成是該筆資料 $x_n$ 的分數, 記做 $f_T(x_n)$, 當 $y_n=+1$ 時, 如果 $f_T(x_n)$ 很小則會導致 $\\exp(-y_n f_T(x_n))$ 會很大, 同理當 $y_n=-1$ 時, 如果 $f_T(x_n)$ 很大則會導致 $\\exp(-y_n f_T(x_n))$ 會很大. 因此 $\\exp(-y_n f_T(x_n))$ 可以當成 error function 來 minimize.而它跟 0-1 error function 有如下的關係: 而我們知道 Adaboost 滿足式 (11), 等同於說明 exponential error 收斂. 由於 upper bound 的關係也導致了 0-1 error 收斂.聽到有個方法可以使 error 迅速收斂到 0, 這不是太完美了嗎? 別高興得太早, 因為這個 error 是 inside error. 有學過 ML 的童鞋就應該會警覺到當 inside error 為 0, 意味著非常容易 overfit! 好在實作上 Adaboost 卻不是那麼容易 (原因在上一段 large margin 的解釋), 這就帶來了一個好處, 就是在使用 Adaboost 的時候, 我們可以很放心的直接訓練多次 iteration, 甚至到 inside error 接近 0, 最後的 outside test 也不會壞掉. 這特性倒是挺方便的. AdaBoost 小結論 我們希望藉由融合很多 {$g_t$} 來得到強大的 $G$, 同時我們知道 {$g_t$} 之間意見愈分歧愈好.每一個 $g_t$ 都是根據當前 weighted dataset 得到的. 利用調整資料權重的方式來讓上一次的 $g_t$ 表現很差, 這樣新權重的 dataset 訓練出來的 $g$ 就會跟之前的看法分歧.Adaboost 再利用一種頗為巧思的線性組合方式來融合 {$g_t$}, 最終得到強大的 $G$ Additive Model (a framework)這是非常重要的一個框架, Adaboost 在這框架下可視為它的一個 special case, 同時著名的 Gradient Boost Decision Tree (GBDT) 也是基於此框架下的演算法. 通常 supervised learning 就是在學習 input and output 之間的 mapping function $f$, 簡單講, 直接學一個好的 $f$ 可能很困難, 所以不如使用 greedy 方式, 就是從目前的 $f_t$ 出發, 考慮怎麼修正現在的 $f_t$ 來使得 error 更小. 嚴謹一點數學描述如下: 考慮 additive model$$\\begin{align} f_T(x)=\\sum_{t=1}^T \\alpha_t g_t(x) \\end{align}$$ 其中, $g_t(x)$ 為第 t 次學到的 base learner, $\\alpha_t$ 為它的權重.定義 $L(y,f(x))$ 為 loss (or error) function, 所以我們要找的修正的 mapping function 如下: $$\\begin{align} (\\alpha_T,g_T)=\\arg\\min_{\\eta,h}\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta h(x_n)) \\end{align}$$ 用上式的方法找到要修正的 mapping function 因此 mapping function 更新如下: $$\\begin{align} f_T(x)=f_{T-1}(x)+\\alpha_T g_T(x) \\end{align}$$ 我們可以想成是在函數空間做 gradient descent. 每一次就是找一個 descent direction, 在這裡就是 $h$, 然後設定合適的步長 $\\eta$, 這麼想就是最佳化的 gradient descent 了. Gradient BoostingAdditive model framework 很簡單, 難的地方在那個 $\\arg\\min$ 式 (15). 而 Gradient Boosting 可以說是一種明確實現 Additive model 的方式, 我們可以將 $\\eta$ 和 $h$ 分開找, 例如先找 $h$: $$\\begin{align} &amp;\\min_h\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta h(x_n))\\\\ &amp;\\mbox{by Taylor: }\\simeq \\min_h\\sum_{n=1}^N\\left(L(y_n,f_{T-1}(x_n))+\\eta h(x_n) \\color{red}{ \\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right) _{f=f_{T-1}} } \\right)\\\\ \\end{align}$$ Taylor 展開式那邊可以這麼想 $$\\begin{align} &amp;\\mbox{將 }L(y_n, \\color{green}{ f_{T-1}(x_n) }+ \\color{blue}{ \\eta h(x_n) } )\\mbox{ 看作 }\\hat{L}( \\color{green}{ \\tilde{x} }+ \\color{blue}{ \\delta } )\\\\ &amp;\\mbox{因此 by Taylor } \\simeq \\hat{L}( \\color{green}{\\tilde{x}} )+ \\color{blue}{\\delta} \\left(\\frac{\\partial \\hat{L}(x) }{\\partial x}\\right)_{x= \\color{green}{\\tilde{x}} } \\end{align}$$ 上面紅色部分在計算的時候是一個固定值, 我們先令為 $$\\begin{align} \\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right) _{f=f_{T-1}}= \\color{red}{-\\tilde{y}_n} \\end{align}$$ 所以 (18) 變成 $$\\begin{align} &amp;= \\min_h\\sum_{n=1}^N\\left(L(y_n,f_{T-1}(x_n)) \\color{red}{-} \\eta h(x_n) \\color{red}{ \\tilde{y_n} } \\right)\\\\ &amp;\\mbox{去掉與}h\\mbox{無關項並補上}2=\\min_h \\sum_{n=1}^N \\left(-2h(x_n)\\tilde{y}_n\\right) \\end{align}$$ 很明顯, 如果 $h$ 無限制, 則解為 $h=\\infty$, 這顯然不是我們要的, 在 optimization 的時候, 我們需要的只是 gradient 的方向, 而不是大小, 大小可以由 stepsize 控制. 不過如果加上 $norm(h)=1$ 條件並使用 Lagrange Multipliers 會較複雜, 實作上我們就直接將 $norm(h)$ 當作一個 penality 加在 loss 裡就可以. 因此 (23) 修改如下: $$\\begin{align} =\\min_h \\sum_{n=1}^N \\left(-2h(x_n)\\tilde{y}_n+(h(x_n))^2\\right) \\end{align}$$ 湊齊平方項會變成 (之前加的2是為了這裡湊平方項) $$\\begin{align} =\\min_h \\sum_{n=1}^N \\left( \\mbox{const}+\\left(h(x_n)-\\tilde{y}_n\\right)^2 \\right) \\end{align}$$ OK! 到這裡我們發現了一個重要的解釋, $h$ 的找法就是對 $\\tilde{y}_n$ 做 sqaure error regression! 得到 $g_T=h$ 後, 那麼步長 $\\eta$ 呢? $$\\begin{align} \\alpha_T=\\min_{\\eta}\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta g_T(x_n))\\\\ \\end{align}$$ 這個解通常很好算, 令 $L$ 微分為 0 即可, 是個單變量求解. 到目前為止, 我們可以將整個 Gradient Boost 演算法列出來了: $$\\begin{align} &amp;\\mbox{1. Init }g_0(x)\\\\ &amp;\\mbox{2. For }t=1~T\\mbox{ do:}\\\\ &amp;\\mbox{3. }\\tilde{y}_n=-\\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right)_{f=f_{t-1}}\\mbox{, n=1~N}\\\\ &amp;\\mbox{4. }g_t=\\arg\\min_h\\left(h(x_n)-\\tilde{y}_n\\right)^2\\\\ &amp;\\mbox{5. }\\alpha_T=\\arg\\min_{\\eta}\\sum_{n=1}^N L\\left(y_n,f_{t-1}(x_n)+\\eta g_t(x_n)\\right)\\\\ &amp;\\mbox{6. }f_t(x)=f_{t-1}(x)+\\alpha_t g_t(x) \\end{align}$$ Adaboost as an additive model將 Adaboost 套用 additive model framework 時會是什麼情況?首先 loss 是 exponential loss, 然後一樣用 binary classification 來說明, 其中 $y_n,g_t(x_n)\\in${-1,+1}, 則我們要找的 $h$ 如下 (對照 (12) and (13) 並使用 additive model (14) 的架構): $$\\begin{align} g_T=\\min_h\\sum_{n=1}^N\\exp\\left(-y_n\\left(f_{T-1}(x_n)+\\eta h(x_n)\\right)\\right)\\\\ =\\min_h\\sum_{n=1}^N u_n^{(T)}\\exp(-y_n\\eta h(x_n))\\\\ \\simeq\\min_h\\sum_{n=1}^N u_n^{(T)}(1-y_n\\eta h(x_n))\\\\ =\\min_h\\sum_{n=1}^N u_n^{(T)}(-y_n h(x_n))\\\\ \\end{align}$$ (33) 到 (34) 使用 $u_n^{(T)}$ 的定義, 參考 (13). 而最後的 (36) 表明了實際上就是選擇讓 training data 在新的 weighted dataset 下表現最好的那個 $h$, 具體原因看下圖.這不正是 Adaboost 選擇 weak learner 的方式嗎? 最後別忘了 stepsize, 將 (34) 換一下變數, $h$ 變 $\\eta$: $$\\begin{align} \\alpha_T=\\arg\\min_{\\eta}\\sum_{n=1}^N u_n^{(T)}\\exp(-y_n \\eta g_t(x_n))\\\\ \\end{align}$$ 兩種情況:$$\\begin{align} \\mbox{1. }y_n=g_t(x_n)\\mbox{: }u_n^{(T)}\\exp(-\\eta)\\\\ \\mbox{2. }y_n\\neq g_t(x_n)\\mbox{: }u_n^{(T)}\\exp(+\\eta)\\\\ \\end{align}$$ 所以$$\\begin{align} \\alpha_T=\\arg\\min_{\\eta}\\left(\\sum_{n=1}^N u_n^{(T)}\\right) \\cdot \\left(\\left(1-\\epsilon_T\\right)\\exp\\left(-\\eta\\right)+\\epsilon_T\\exp\\left(+\\eta\\right)\\right) \\end{align}$$ 令微分為 0, 我們可以很容易得到 $$\\begin{align} \\alpha_T = \\ln\\sqrt{\\frac{1-\\epsilon_T}{\\epsilon_T}} \\end{align}$$ 這正好也就是 adaboost 所計算的方式! 總結一下, Adaboost 在 additive model 框架下, 相當於使用 steepest gradient descent 方式在函數空間找 weaker learner, 並且將 stepsize 指定為最佳步長. Gradient Boost Decision Tree (GBDT)Gradient Boost 很棒的一個特性是 error function 沒限定, 例如使用 exponential error 就是 adaboost, 而另一個常用的是 sqaure error.當使用 square error 時, $\\tilde{y}_n$ 就會變成 $(y_n-x_n)$ 也就是 residual. 對照 GradientBoost (27)~(32) 來看, 我們發現整個演算法變成對每一次 iteration 的 residual 做 regression.另外在實務上 base learner 常常使用 Decision Tree (因為 decision tree 有很多好處: 可解釋性、訓練快、可處理缺失資料…), 不過這就要特別注意了, 因為如果長成 fully growed tree 就直接把 residual regression 到 0 了. 因此, decision tree 需要 regularization, 而實務上採用 pruned tree. 整個 GBDT 節自課程 slide 如下: XGBoost這篇文章 XGBoost的原理 介紹得很好 幾個重點整理, XGBoost 基本上也是 gradient boost 的一種, 比較特別的是泰勒展展開 (18) 使用到二階導函數: $$\\begin{align} &amp;\\min_h\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta h(x_n))\\\\ &amp;\\simeq \\min_h\\sum_{n=1}^N\\left(L(y_n,f_{T-1}(x_n))+\\eta h(x_n) \\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right) _{f=f_{T-1}}\\\\ \\color{red} { +\\eta^2h^2(x_n)\\left(\\frac{\\partial^2 L(y_n,f)}{\\partial^2 f}\\right)_{f=f_{T-1}} } \\right)\\\\ &amp;=\\min_h\\sum_{n=1}^N \\left( L(y_n,f_{T-1}(x_n)) + \\eta h(x_n)\\mbox{Gradient}_n + \\frac{\\eta^2h^2(x_n)}{2}\\mbox{Hessian}_n \\right)\\\\ &amp;=\\min_h\\sum_{n=1}^N \\left( \\eta h(x_n)\\mbox{Gradient}_n + \\frac{\\eta^2h^2(x_n)}{2}\\mbox{Hessian}_n \\right)\\\\ \\end{align}$$ 最後再加上一個 regularization term $$\\begin{align} &amp;=\\min_h\\sum_{n=1}^N \\left( \\eta h(x_n)\\mbox{Gradient}_n + \\frac{\\eta^2h^2(x_n)}{2}\\mbox{Hessian}_n \\right) + \\Omega(h)\\\\ \\end{align}$$ 針對 (46) 要找到最好的 $h$, 如果使用 Decision Tree, $\\Omega(h)$ 可以使用樹的深度、葉子數量、葉子值的大小等等計算. 但關鍵是如何有效率地找到很好的 $h$, 而在 Decision Tree 此問題相當於如何有效率的對 Tree 做 splitting. XGBoost 文章使用非常有效率的近似方法, 並且該方法可以很好的並行加速. 對於 xgboost 就只粗淺的了解到這了, 也還沒有真的有什麼調整的經驗, 就把這個課題放在 todo list 吧. Reference 林軒田老師 ML 課程 李航 統計學習方法 Why-Aggregation-Work 以前 Adaboost and face detection paper survey 其中Rapid object detection using a boosted cascade of simple features, 2001, cited 17597 深度神经网络中深度究竟带来了什么？ Deep Convolutional Neural Networks with Merge-and-Run Mappings XGBoost的原理 XGBoost: A Scalable Tree Boosting System","tags":[{"name":"bagging","slug":"bagging","permalink":"https://bobondemon.github.io/tags/bagging/"},{"name":"Adaboost","slug":"Adaboost","permalink":"https://bobondemon.github.io/tags/Adaboost/"},{"name":"Gradient Boost","slug":"Gradient-Boost","permalink":"https://bobondemon.github.io/tags/Gradient-Boost/"}]},{"title":"TF Notes (5), GRU in Tensorflow","date":"2018-07-30T15:29:01.000Z","path":"2018/07/30/TF-Notes-GRU-in-Tensorflow/","text":"小筆記. Tensorflow 裡實作的 GRU 跟 Colah’s blog 描述的 GRU 有些不太一樣. 所以做了一下 TF 的 GRU 結構. 圖比較醜, 我盡力了… XD TF 的 GRU 結構 u 可以想成是原來 LSTM 的 forget gate, 而 c 表示要在 memory cell 中需要記住的內容. 這個要記住的內容簡單講是用一個 gate (r) 來控制之前的 state 有多少比例保留, concate input 後做 activation transform 後得到. 可以對照下面 tf source codes. TF Source Codesrnn_cell_impl.py 12345678910111213141516171819def call(self, inputs, state): \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\" gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._gate_kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias) value = math_ops.sigmoid(gate_inputs) r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1) r_state = r * state candidate = math_ops.matmul( array_ops.concat([inputs, r_state], 1), self._candidate_kernel) candidate = nn_ops.bias_add(candidate, self._candidate_bias) c = self._activation(candidate) new_h = u * state + (1 - u) * c return new_h, new_h","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://bobondemon.github.io/tags/TensorFlow/"},{"name":"GRU","slug":"GRU","permalink":"https://bobondemon.github.io/tags/GRU/"}]},{"title":"(what is) Probabilistic Graphical Models","date":"2018-06-16T02:27:30.000Z","path":"2018/06/16/what-is-Probabilistic-Graphical-Model/","text":"本篇主要介紹什麼是 PGM, 以及一個很重要的應用 Part-of-Speech tagging. PGM 的部分主要圍繞在 “它是什麼?” 也就是 Koller 課程的 Representation. Inference 不討論, 因為自己也沒讀很深入 (汗), 而 Learning 就相當於 ML 裡的 training, 會在介紹 POS 時推導一下. 文章結構如下: What is Probabilistic Graphical Model (PGM)? What is Bayesian Network (BN)? What is Markov Network (MN)? (or Markov Random Field) What is Conditional Random Field (CRF)? Part-of-Speech (POS) Tagging References 文長… What is Probabilistic Graphical Model (PGM)?它是描述 pdf 的一種方式, 不同的描述方式如 directed/undirected graphical model, or Factor Graph 所能描述的 pdf 範圍是不同的. (ref: PRML) 其中 P 代表所有 distributions 的集合, U 和 D 分別表示 undirected 和 directed graphical models. 以有向無向圖來分如下: Directed Acyclic Graph (DAG): Bayesian Network Undirected Graph: Markov Network 注意 BN 除了 directed 之外, 還需要 acyclic. 有了圖之後, 怎麼跟 distribution 產生連結的? 下面我們分別介紹 BN and MN. What is Bayesian Network (BN)?對於一個任意的 distibution over variables $x_1,…,x_V$ 我們可以用基本的 chain rule 拆解如下: $$\\begin{align} p(x_{1:V})=p(x_1)p(x_2|x_1)p(x_3|x_{1:2})...p(x_V|x_{1:V-1}) \\end{align}$$ 變數的 order 可以任意排列, 舉例來說 $$\\begin{align} p(x,y,z)\\\\ =p(x)p(y|x)p(z|x,y)\\\\ =p(x)p(z|x)p(y|x,z) \\end{align}$$ 基於這種拆解我們可以很自然地想到, 不如把每個變數都當成 nodes, 再將 conditioning 的關係用 edges 連起來. 因此基本可以這麼表達: $$\\begin{align} p(x_{1:V}|G)=\\prod_{t=1}^{V}p(x_t|pa(x_t)) \\end{align}$$ 其中 $pa(x_t)$ 表示 node $x_t$ 的 parent nodes. 我們用式 (3) 和 (4) 當例子就可以畫出如下的圖: 可以發現同一個 pdf 可以畫出多個 BN, 因此表達方式不是唯一. Conditioning V.S. Independency接著我們可能會想, 如果我們希望對 pdf 加一些獨立條件呢, 譬如如果希望 $x \\perp y$, 是不是可以直接將圖中的 $x$ 和 $y$ 的 edge 拔掉就可以了呢? 先破題, 答案是不行. 同樣以上面的例子解釋, 如果我們用拔掉 edge 的話, 圖變成: 事實上這兩個圖已經各自表示不同的 distribution 了. 特別要注意在右圖中拔掉 $x$ and $y$ 的 edge 沒有造成 $x \\perp y$. 解釋如下: 那究竟該如何從一個圖直接看出變數之間是否獨立? 為了解答這個問題, 我們先從簡單的三個 nodes 開始 Flow of Influence三個 nodes 的 DAG 圖本質上就分以下三類, 其中 given 的變數我們通常以實心圓表示 我們就個別討論 需要特別注意的是 case 3 的 v-structure, 行為跟其他兩種相反. 一種好記的方式是, 我們假設 given 的變量是一個石頭, 而 edges 可以想成是水流, 所以 given 變量就把水流擋住, 因此會造成獨立. 唯一個例外就是 v-structure, 行為剛好相反. Active Trail in BN我們可以很容易將三個 nodes 的 trail 擴展成 $V$ 個 nodes 的 trail. 因此可以很方便的觀察某條 trail 起始的 node 能否影響到最後的 node. d-separation繼續擴展! 我們假設在 BN $G$ 上 node $x$ and $y$ 有 $N$ 條 trails. 我們則可以藉由檢查每條 trail 是否 active 最終就會知道 $x$ 能否影響到 $y$. 需要注意的是, 這些 d-separation 條件我們都可以直接從給定的 $G$ 上直接讀出來 (對於 distribution 沒有任何假設), 為了方便我們定義以下兩個 terms $$\\begin{align} CI(G)=\\{\\textbf{d-sep}(x,y|z)|x,y,z\\textbf{ in }G\\}\\\\ CI(p)=\\{(x \\perp y|z)|x,y,z\\textbf{ in }G\\}\\\\ \\end{align}$$ $CI(G)$ 所列出的 statements 是由 d-sep 所提供, 也就是說從 $G$ 直接讀出來的, 而 $CI(p)$ 才是真的對於 distribution $p$ 來說所有條件獨立的 statements. OK, 到目前為止, 給定一個 BN $G$, 和一個 distribution $p$ (注意 $p$ 不一定可以被 $G$ 所表示), 他們之間的關係到底是什麼? 下面就要引出非常重要的定理 Factorization and Independent 的關係來說明 Factorization and Independent 白話文: 假設 $p$ 剛好可以寫成 $G$ 的 factorization 型式 (式 (5)), 則所有 $G$ 指出需要 $\\perp$ 的 statements (根據 d-sep 所列), $p$ 都滿足 白話文: 假設所有 $G$ 指出需要 $\\perp$ 的 statements (根據 d-sep 所列), $p$ 都滿足, 則 $p$ 可以寫成 $G$ 的 factorization 型式 (式 (5)) 我們用 PRML book 裡一個具體的描述來說明 Thm1 and Thm2 之間的關係 給定一個 $G$, 就好像一個篩子一樣, 根據兩種方式篩選 distribution $p$ 剛好可以寫成 $G$ 的 factorization 型式 (式 (5)) $G$ 指出需要 $\\perp$ 的 statements (根據 d-sep 所列), 剛好 $p$ 都滿足 用上面兩種篩選方式最後篩出來的 distributions 分別稱為 $DF1$ and $DF2$ 兩個 sets. 定理告訴我們它們式同一個集合! Example把下圖的 joint pdf 寫出來: 使用式 (5) 的方式寫一下, 讀者很快就發現, 這不就是 HMM 嗎? What is Markov Network (MN)?Factorization在解釋 MN 之前, 先了解一下什麼是 (maximal) clique. 因此, 我們可以用 maximal cliques 來定義一個 MN. $$\\begin{align} p(x)=\\frac{1}{Z}\\prod_{c\\in\\mathcal{C}}\\psi_c(x_c) \\end{align}$$ $\\mathcal{C}$ 是 maximal cliques 的集合. 然後 $Z$ 是一個 normalization term, 目的為使之成為 distribution. $$\\begin{align} Z=\\sum_x\\prod_{c\\in\\mathcal{C}}\\psi_c(x_c) \\end{align}$$ 舉個例子: 用無向圖的方式來表達 distribution 有一個很大的好處就是判斷 Active Trail 和 separation 變得非常非常簡單! 直接看下圖的說明 如同在 BN 時的討論, 給定一個 MN $H$, 和一個 distribution $p$ (注意 $p$ 不一定可以被 $H$ 所表示), 他們之間的關係可以由 Factorization and Independent 的定理來說明 Factorization and Independent我們直接擷取 Kevin Murphy 書所列的定理, Hammersley-Clifford 定理 跟 BN 一樣, factorization iff independence, 但有一個重要的 assumption, 就是 distribution 必須 strictly positive (如上圖紅色框的部分). 我們一樣用 PRML 篩子的觀念來具體化: 描述就跳過了. Example由於有 $p(x)&gt;0$ 的假設在, 因此如果將 factor functions $\\psi(x_c)$ 都使用 $exp$ 來定義的話, 整個 product 相乘後的 distribution 必定滿足 strictly positive. 因此 $exp$ 就不失為一種方便的 modeling 方式了 喘口氣的結論到這裡, 我們可以 用 graph 簡單的表示出 joint pdf (用 factorization). 也可以從 graph 中看出 conditional independence (用 active tail, separation) 因此我們可以針對要 model 的問題利用 graph 來描述 joint pdf 了. 但是光描述好 model 沒用, 我們還需要 inference (test) and learning (train). Inference 非常推薦看 PRML ch8, 講如何對 tree graph 做 sum-product algorithm (belief propagation) 非常精彩. 接著如何推廣到一般 general graph 則可以使用 junction tree algorithm (推薦看這篇文章, 解釋非常棒!). 上述兩種方式都屬於 exact inference, 對於一些情形仍會需要 exponential time 計算, 因此我們需要 variational inference 或 sampling 的方式算 approximation. 最後有關 learning 我們使用接下來的 POS tagging 當範例推導一下. 但別急, 在講 POS 之前我們得先談一個重要的東西, Conditional Random Field. What is Conditional Random Field (CRF)? 如同上圖的說明, 基本上 CRF 仍舊是一個 MN, 最大的差別是 normalization term 如今不再是一個 constant, 而是 depends on conditioning 的變數 $x$. 一個在 sequence labeling 常用的 CRF 模型是 Linear-Chain CRF 有了這些概念後我們就可以說說 POS 了 Part-of-Speech (POS) Tagging擷取自李宏毅教授上課投影片 基本上就是給定一個 word sequence $x$, 我們希望找出哪一個詞性標註的 sequence $y$ 會使得機率最大. 機率最大的那個 $y$ 就是我們要的詞性標註序列. 使用現學現賣的 PGM modeling 知識, 我們可以使用 BN or MN 的方式描述模型 BN: Hidden Markov Model (HMM) MN: Linear chain CRF with log-linear model 有向圖 HMM 方法一樣擷取自李宏毅教授上課投影片 還記得本文前面講 BN 時的 HMM example 嗎? $y$ 就是詞性, $x$ 就是字. HMM 是在 model 給定詞性序列情形下的字序列 distribution. 了解語音辨識的童鞋門應該再熟悉不過了, 只不過這裡問題比較簡單, 在語音辨識裡, 我們不會針對每個 frame 去標註它是屬於哪一個發音的 state, 因此標註其實是 hidden 的. 但在這裡每個 word 都會有一個對應正確答案的詞性標註, 沒有 hidden 資訊, 因此也不需要 EM algorithm, 簡單的 counting 即可做完訓練. that all … 無向圖 CRF 方法精確說是 Linear chain CRF with log-linear model 我們把 log-linear model 的 factor 帶入 linear chain CRF 中, 注意其中 $\\phi$ 是需要定義的特徵函數, 我們這裡先假設可以抽取出 $K$ 維. 因此可以推導如下 實作上我們會針對時間 share weights, 這是因為句子都是長短不一的, 另一方面這樣做也可以大量減少參數量. 所以最後可以簡化成一個 weigth vector $w$ 和我們合併的特徵向量 $f(x,y)$ 的 log-linear model. Learning 目標函數就是在最大化 CRF 的 likelihood. 採用 gradient method. 而 gradient 的推導事實上也不困難, 只要花點耐心即可了解 但是其實我說不困難只說對了一半, 紅色的地方事實上需要跑 inference 才可以得到, 好在 linear-chain 架構下正好可以用 Viterbi 做前向後算計算, 這部分的式子可以跟 “李航 統計學習方法“ 這本書的 p201 式 (11.34) 銜接上, 該式寫出了前向後向計算. ToolCRF++ 做為語音辨識的後處理十分好用的工具, in c++. ReferencesPGM 博大精深, 這個框架很完整且嚴謹, 值得我後續花時間研讀, 有機會看能否將 Koller 的課程上過一次看看. 通常這麼說就表示 …. hmm…你懂得 Bishop PRML book Kevin Murphy book Junction Tree Algorithm 李航 統計學習方法 李宏毅老師 ML 課程","tags":[{"name":"Probabilistic Graphical Models","slug":"Probabilistic-Graphical-Models","permalink":"https://bobondemon.github.io/tags/Probabilistic-Graphical-Models/"},{"name":"Bayesian Network","slug":"Bayesian-Network","permalink":"https://bobondemon.github.io/tags/Bayesian-Network/"},{"name":"Markov Network","slug":"Markov-Network","permalink":"https://bobondemon.github.io/tags/Markov-Network/"},{"name":"Conditional Random Field","slug":"Conditional-Random-Field","permalink":"https://bobondemon.github.io/tags/Conditional-Random-Field/"},{"name":"POS tagging","slug":"POS-tagging","permalink":"https://bobondemon.github.io/tags/POS-tagging/"}]},{"title":"Kaldi Notes (1), I/O in C++ Level","date":"2018-05-31T15:32:43.000Z","path":"2018/05/31/Kaldi-Notes-IO-in-C-Level/","text":"Kaldi I/O C++ Level 筆記, 主要介紹以下幾點, 以及它們在 Kaldi c++ 裡如何關聯: 標準 low-level I/O for Kaldi Object XXXHolder類別: 一個符合標準 low-level I/O 的類別 Kaldi Table Object: &lt;key,value&gt; pairs 組成的 Kaldi 格式檔案 (scp, ark), 其中 value 為 XXXHolder 類別 標準 low-level I/O for Kaldi ObjectKaldi Object 有自己的標準 I/O 介面:12345class SomeKaldiClass &#123; public: void Read(std::istream &amp;is, bool binary); void Write(std::ostream &amp;os, bool binary) const; &#125;; 因此定義了該 Kaldi Class 如何針對 istream 讀取 (ostream 寫入). 在 Kaldi 中, istream/ostream 一般是由 Input/Output(在 util/kaldi-io.h 裡定義) 這個 class 來開啟的. 那為何不用一般的 c++ iostream 開啟一個檔案呢? 這是因為 Kaldi 想要支援更多樣的檔案開啟方式, 稱為 “Extended filenames: rxfilenames and wxfilenames“. 例如可以從 stdin/stdout, pipe, file 和 file with offset 讀取寫入, 詳細請看文檔的 “Extended filenames: rxfilenames and wxfilenames” 部分. 所以 Input/Ouput Class 會自動解析 rxfilenames/wxfilenames 然後開啟 istream/ostream. 開啟後, Kaldi Object 就可以透過標準的 I/O 介面呼叫 Read/Write 方法了. 官網範例如下:123456789101112&#123; // input. bool binary_in; Input ki(some_rxfilename, &amp;binary_in); my_object.Read(ki.Stream(), binary_in); // you can have more than one object in a file: my_other_object.Read(ki.Stream(), binary_in);&#125;// output. note, \"binary\" is probably a command-line option.&#123; Output ko(some_wxfilename, binary); my_object.Write(ko.Stream(), binary);&#125; 有時候會看到更精簡的寫法如下12345678int main(int argc, char *argv[]) &#123; ... std::string rxfilenames = po.GetArg(1); std::string wxfilenames = po.GetArg(2); SomeKaldiClass my_object; ReadKaldiObject(rxfilenames, &amp;my_object); WriteKaldiObject(my_object, wxfilenames, binary);&#125; 其中 ReadKaldiObject and WriteKaldiObject (defined in util/kaldi-io.h) 的作用只是將 Input/Output 開啟 xfilenames 為 iostream, 並傳給 my_object 的標準 I/O 介面包裝起 來而已. 擷取 define 片段如下: 12345678910111213141516171819template &lt;class C&gt; void ReadKaldiObject(const std::string &amp;filename, C *c) &#123; bool binary_in; Input ki(filename, &amp;binary_in); c-&gt;Read(ki.Stream(), binary_in);&#125;// Specialize the template for reading matrices, because we want to be able to// support reading 'ranges' (row and column ranges), like foo.mat[10:20].// 上面的 class C 如果是 Matrix&lt;float&gt; or Matrix&lt;double&gt; 的話, 使用下面兩個定義// Note: 這種方式是 template 的 specialization, 同樣名稱的 template function or class 可以重複出現，只針對某些 type 客製化template &lt;&gt; void ReadKaldiObject(const std::string &amp;filename, Matrix&lt;float&gt; *m);template &lt;&gt; void ReadKaldiObject(const std::string &amp;filename, Matrix&lt;double&gt; *m);template &lt;class C&gt; inline void WriteKaldiObject(const C &amp;c, const std::string &amp;filename, bool binary) &#123; Output ko(filename, binary); c.Write(ko.Stream(), binary);&#125; Kaldi Table ObjectTable Object 不直接透過標準的 Read/Write 操作, 是因為 Table object 的構成是由 &lt;key,value&gt; pairs 組成的, 而 value 才會是一個符合標準 Read/Write 操作的 object. 這種 table 所需要的讀寫可能有很多方式, 譬如 sequential access, random access 等等, 因此單純的 Read/Write 比較不能滿足需求, 更需要的是要有 Next, Done, Key, Value 等等的操作方式. 例如以下範例: 12345678910111213141516std::string feature_rspecifier = \"scp:/tmp/my_orig_features.scp\", transform_rspecifier = \"ark:/tmp/transforms.ark\", feature_wspecifier = \"ark,t:/tmp/new_features.ark\";// there are actually more convenient typedefs for the types below,// e.g. BaseFloatMatrixWriter, SequentialBaseFloatMatrixReader, etc.TableWriter&lt;BaseFloatMatrixHolder&gt; feature_writer(feature_wspecifier);SequentialTableReader&lt;BaseFloatMatrixHolder&gt; feature_reader(feature_rspecifier);RandomAccessTableReader&lt;BaseFloatMatrixHolder&gt; transform_reader(transform_rspecifier);for(; !feature_reader.Done(); feature_reader.Next()) &#123; std::string utt = feature_reader.Key(); if(transform_reader.HasKey(utt)) &#123; Matrix&lt;BaseFloat&gt; new_feats(feature_reader.Value()); ApplyFmllrTransform(new_feats, transform_reader.Value(utt)); feature_writer.Write(utt, new_feats); &#125;&#125; 主要有幾種 table classes:TableWriter, SequentialTableReader, RandomAccessTableReader 等等, 都定義在 util/kaldi-table.h. 我們就以 SequentialTableReader 來舉例. 上面的範例 feature_reader 就是一個 SequentialTableReader, 他的 &lt;key,value&gt; pairs 中的 value 定義為 BaseFloatMatrixHolder 類別 (一個符合標準 low-level I/O 的 Kaldi Class, 等於是多一層包裝). XXXHolder (如 KaldiObjectHolder, BasicHolder, BasicVectorHolder, BasicVectorVectorHolder, …) 指的是符合標準 low-level I/O 的 Kaldi Object, 因此這些 XXXHolder 都可以統一透過 Read/Write 來呼叫. 這些 Holder 的定義在 util/kaldi-holder.h.另外 kaldi-holder.h 最後一行會 include kaldi-holder-inl.h. “-inl” 意思是 inline, 通常會放在相對應沒有 -inl 的 .h 最後面, 用來當作是 inline implementation 用. SequentialTableReader 的定義在 “util/kaldi-table.h”, 擷取要介紹的片段:1234567891011template&lt;class Holder&gt;class SequentialTableReader &#123; public: typedef typename Holder::T T; inline bool Done(); inline std::string Key(); T &amp;Value(); void Next(); private: SequentialTableReaderImplBase&lt;Holder&gt; *impl_;&#125; Done(), Next(), Key(), and Value() 都可以從 feature_reader 看到如何使用, 應該很直覺, 而 Holder 的解釋上面說了. 剩下要說明的是這行 SequentialTableReaderImplBase&lt;Holder&gt; *impl_;. 在呼叫 SequentialTableReader 的 Next() 時, 他實際上呼叫的是 impl_ 的 Next(). 定義在 util/kaldi-table-inl.h 片段: 12345template&lt;class Holder&gt;void SequentialTableReader&lt;Holder&gt;::Next() &#123; CheckImpl(); impl_-&gt;Next();&#125; impl_ 的 class 宣告是 “SequentialTableReaderImplBase”, 該類別的角色是提供一個父類別, 實際上會根據 impl_ 真正的類別呼叫其對應的 Next(), 就是多型的使用. 現在假設 impl_ 真正的類別是 SequentialTableReaderArchiveImpl. 我們可以在 util/kaldi-table-inl.h 看到他的 Next (line 531) 實作如下:123456789virtual void Next() &#123; ... if (holder_.Read(is)) &#123; state_ = kHaveObject; return; &#125; else &#123; ... &#125;&#125; 到這才真正看到透過 XXXHolder 使用 low-level I/O 的 Read()! Kaldi Codes 品質很高阿, 要花不少時間讀, 果然 c++ 底子還是太差了. References Kaldi Project","tags":[{"name":"Kaldi","slug":"Kaldi","permalink":"https://bobondemon.github.io/tags/Kaldi/"}]},{"title":"TF Notes (4), Deconvolution","date":"2018-05-09T11:59:12.000Z","path":"2018/05/09/TF-Notes-deconvolution/","text":"這篇是個小練習, 就兩點: 了解什麼是 deconvolution, 並在 tensorflow 中怎麼用 實作一個 CNN AutoEncoder, Encoder 用 conv2d, Decoder 用 conv2d_transpose What is deconvolution?破題: Deconvolution 的操作就是 kernel tranpose 後的 convolution. 使用李宏毅老師的上課內容, 如下圖: 其實圖已經十分明確了, 因此不多解釋. 另外在 tensorflow 中, 假設我們的 kernel $W$ 為 W.shape = (img_h, img_w, dim1, dim2). 則 tf.nn.conv2d(in_tensor,W,stride,padding) 會將 (dim1,dim2) 看成 (in_dim, out_dim). 而 tf.nn.conv2d_transpose(in_tensor,W,output_shape,stride) 會將 (dim1,dim2) 看成 (out_dim, in_dim), 注意是反過來的. 有兩點多做說明: tf.nn.conv2d_transpose 會自動對 $W$ 做 transpose 之後再 convolution, 因此我們不需要自己做 transpose. tf.nn.conv2d_transpose 需要額外指定 output_shape. 更多 conv/transpose_conv/dilated_conv with stride/padding 有個 非常棒的可視化 結果參考此 github CNN AutoEncoder結構如下圖 直接將 embedding 壓到 2 維, 每個類別的分布情形如下: embedding 是 128 維, 並使用 tSNE 投影到 2 維畫圖如下: Encoder 如下: 1234567891011121314151617181920def Encoder(x): print('Input x got shape=',x.shape) # (None,28,28,1) # Layer 1 encode: Input = (batch_num, img_height, img_width, cNum). Output = (batch_num, img_height/2, img_width/2, layer_dim['conv1']) layer1_en = tf.nn.relu(tf.nn.conv2d(x, weights['conv1'], strides=[1, 1, 1, 1], padding='SAME')) # Avg Pooling layer1_en = tf.nn.avg_pool(layer1_en, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') print('After Layer 1, got shape=',layer1_en.shape) # (None,14,14,32) # Layer 2 encode: Input = (batch_num, img_height/2, img_width/2, layer_dim['conv1']). Output = (batch_num, img_height/4, img_width/4, layer_dim['conv2']) layer2_en = tf.nn.relu(tf.nn.conv2d(layer1_en, weights['conv2'], strides=[1, 1, 1, 1], padding='SAME')) # Avg Pooling layer2_en = tf.nn.avg_pool(layer2_en, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') print('After Layer 2, got shape=',layer2_en.shape) # (None,7,7,64) # Layer embedded: Input = (batch_num, img_height/4 * img_width/4 * layer_dim['conv2']). Output = (batch_num, layer_dim['embedded']) flatten_in = flatten(layer2_en) embedded = tf.matmul(flatten_in,weights['embedded']) print('embedded has shape=',embedded.shape) return embedded Decoder 如下: 12345678910111213141516171819202122def Decoder(embedded): # API: tf.nn.conv2d_transpose = (value, filter, output_shape, strides, padding='SAME', ...) bsize = tf.shape(embedded)[0] # Layer embedded decode: Input = (batch_num, layer_dim['embedded']). Output = (batch_num, in_dim_for_embedded) embedded_t = tf.matmul(embedded,weights['embedded'],transpose_b=True) embedded_t = tf.reshape(embedded_t,[-1, 7, 7, layer_dim['conv2']]) print('embedded_t has shape=',embedded_t.shape) # Layer 2 decode: Input = (batch_num, 7, 7, layer_dim['conv2']). Output = (batch_num, 14, 14, layer_dim['conv1']) layer2_t = tf.nn.relu(tf.nn.conv2d_transpose(embedded_t,weights['conv2t'],[bsize, 14, 14, layer_dim['conv1']], [1, 2, 2, 1])) print('layer2_t has shape=',layer2_t.shape) # Layer 1 decode: Input = (batch_num, 14, 14, layer_dim['conv1']). Output = (batch_num, 28, 28, cNum) layer1_t = tf.nn.relu(tf.nn.conv2d_transpose(layer2_t,weights['conv1t'],[bsize, 28, 28, cNum], [1, 2, 2, 1])) print('layer1_t has shape=',layer1_t.shape) # Layer reconstruct: Input = batch_num x layer_dim['layer1']. Output = batch_num x img_dim. reconstruct = tf.nn.relu(tf.nn.conv2d(layer1_t, weights['reconstruct'], strides=[1, 1, 1, 1], padding='SAME')) - 0.5 print('reconstruct has shape=',reconstruct.shape) return reconstruct AutoEncoder 串起來很容易: 12345def AutoEncoder(x): embedded = Encoder(x) reconstruct = Decoder(embedded) return [embedded, reconstruct] 完整 source codes 參考下面 reference Reference 李宏毅 deconvolution 解釋 tf.nn.conv2d_transpose 說明 conv/transpose_conv/dilated_conv with stride/padding 可視化: github 本篇完整 source codes","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://bobondemon.github.io/tags/TensorFlow/"},{"name":"Computational Graph","slug":"Computational-Graph","permalink":"https://bobondemon.github.io/tags/Computational-Graph/"}]},{"title":"ROS in Self-driving Car system","date":"2018-04-15T11:05:29.000Z","path":"2018/04/15/ROS-in-Self-driving-Car-system/","text":"這是經歷了漫長的時間, 最後的一哩路了….從2016年12月開始, 到2018年4月中, 花了整整一年五個月. 其實我原先打算半年前就畢業的, 但是中途有狀況, 所以只好 term2 完成後停了半年才開始 term3, 也因此到昨天才剛確定畢業! 而昨天剛好也參加了 Udacity 在中國兩周年的會, 見到了 David Sliver 本人, 算是畢業的一個小紀念! 最後的 project 比較有別於以往, 採用 team work 的方式. 我們的 team 共五人, team lead Franz Pucher 德國, Theodore King 美國, 和我. 疑? 另外兩個呢? 對於 project 完全沒貢獻…我不想說了….= = ROS 簡介關於機器人控制和自動車都會使用 ROS (Robot Operating System), ROS 一定要參考 ROS wiki. 本次作業的 ROS 系統擷取課程圖片如下: 看不懂沒關係, 了解 ROS 主要三個概念: Node, Topic, Msg 就清楚上面的圖在幹嘛了. Node 簡單講類似於 class, 可以訂閱某些 Topic, 和發送 Msg 到指定的 Topic. 舉例來說當有某個 Node A 發送一個 msg M 到一個 topic T 時, 如果 Node B 有訂閱 topic T, 則 Node B 會收到 msg M, 並且執行預先設定好的 call back function. 用以下的程式範例舉例: 1234567891011class TrafficLightDetector(object): def __init__(self): rospy.init_node('tl_detector') # 要在開頭就先 init 好這是 ros node ... # 訂閱了一個 topic '/current_pose', 並且如果有 msg 發送到此 topic, 此 node 會收到並且呼叫 call back function self.pose_cb sub = rospy.Subscriber('/current_pose', PoseStamped, self.pose_cb, queue_size=1) # 此 node 會發送 msg 到 topic '/traffic_waypoint' self.upcoming_red_light_pub = rospy.Publisher('/traffic_waypoint', Int32, queue_size=1) def pose_cb(self, msg): self.pose = msg.pose 要注意的是, 由於 topic 運作方式為一旦有其他 node 發送 msg 到此 topic, 有訂閱此 topic 的 node 的 call back function 都會被呼叫. 這就意謂著 topic 如果發送 msg 太頻繁, 導致訂閱的 node 無法及時消化, 則 msg 會掉包. 一種解決方式為使用 rospy.Rate 控制發送的頻率. 但是其實還有另一種傳送 msg 的方式: Service 簡單講 Service 的概念就是 request and response, 不同於 topic, service 會將兩個 node 直接連接起來, 一個發起 request 後, 會等另一個 node response 才會接著做下去. 一個簡單的舉例如下: 12345678910111213141516171819# 需注意 ServiceClassName 要先在 package 裡的 srv folder 定義好class NodeA(object):... # 拿到該 service service = rospy.ServiceProxy('service_name',ServiceClassName) # 拿到可以 request 的 msg instance msg = ServiceClassNameRequest() # 修改 msg 成需要的狀態 ... # 發起 request 並得到 response response = service(msg) class NodeB(object):... rospy.Service('service_name',ServiceClassName, self.handler_func) ... def handler_func(self, msg): # 收到 request 的 msg, 在此 handler function 負責處理如何 response ... 上面的範例使用了兩個 nodes, node A 負責發起 request, 而 node B 負責 response. 另外筆記一些 ros 常用的指令和功能12345678910111213141516171819202122232425262728293031323334353637383940&gt;&gt; roscore # start ROS master# rosrun 可以指定負責要跑哪個 node&gt;&gt; rosrun package_name node_name# node 一多, 可以使用 roslauch 一次執行多個 nodes, 但是要寫好 launch file&gt;&gt; roslaunch launch/launchfile# 列出 active 的 nodes&gt;&gt; rosnode list# 列出 active 的 topics&gt;&gt; rostopic list# 查看某個 topic&gt;&gt; rostopic info topic_name# 將 publish 到此 topic 的 msgs 都即時顯示在 terminal 上&gt;&gt; rostopic echo topic_name# 一般來說 rospy.loginfo('info msg') 會顯示在 /rosout 這個 topic, 因此適合 debug&gt;&gt; rostopic echo /rosout# 查看某個 msg&gt;&gt; rosmsg info msg_name# build 自定義的 ros package&gt;&gt; cd ~/catkin_ws; catkin_make# 檢查 package 的 dependency&gt;&gt; rosdep install -i package_name# 如果將某個 package 加入到自己的 catkin_ws 時, 需加到 catkin_ws/src 資料夾下, 並且重新 make&gt;&gt; cd ~/catkin_ws/src&gt;&gt; git clone 'some packages'&gt;&gt; cd ~/catkin_ws&gt;&gt; catkin_make# Build 完後, 需要 source 才可以將 catkin_ws/src 下的所有 packages 都加到 ros 中&gt;&gt; source ~/catkin_ws/devel/setup.bash Debug 的話 rospy.loginfo, rospy.logwarn, rospy.logerr, rospy.logfatal 很好用, 它們分別會被記錄在以下幾個地方: Self-Driving Car ROS Nodes因此這最後的 project 主要就分成三個部分 Perception:這部分負責收到 /image_color 這個 topic 的影像後, 來找出 traffic sign 在哪裡並且是哪種燈號. 相當於 term1 的 Vehicle Tracking, 我主要負責此部分, 但是沒有使用當時做 project 的 sliding window + svm 方法. 下面會詳細介紹. Planning:負責根據目前車子的位置以及如果有紅燈的話, 必須規劃好新的路徑, 並將期望的速度一併發送給 Control. 相當於 term3 的 Path Planning Control:根據規畫的路徑和速度, 找出可以實際操控的參數 (throttle, brake, steering). 相當於 term2 的 Model Predictive Control. 但我們團隊沒有用 MPC, 而是使用 PID control. Perception Traffic Light由於小弟我不是做 CV 的, 沒這麼多厲害的能力, 因此一開始我也沒打算訓個 YOLO 之類的方法. 重頭開始訓練的話我只能先想到不如用上次 project 的 semantic segmantation 方法, 將認為是 traffic sign 的部分找出來, 接著用簡單的顏色區分一下好了. training set 我使用 Bosch Traffic Light Dataset, 共有 5093 張 images. 很多張影像完全沒有 traffic sign, 因此我就忽略, 並且有些 traffic sign 實在太小, 那種情況也忽略, 最後篩選出 548 張有 traffic signs 的影像並且 resize 成 600x800, 舉個例如下: 注意到用的 semantic segmentation 方法是 pixel level 的, 也就是說每個 pixel 都會去判別 yes/no traffic sign. 而我們看到就算是都有 traffic sign 的影像了, 實際上 pixel 是 traffic sign 所占的比例還是偏低, 這讓我開始有點懷疑是否 DNN 有能力分辨出來. 但是….還真的可以! 現在有種感覺, 有時候針對資料不平均做了一些方式讓每個 class 平均一些, 但是 DNN 的效果其實都沒啥提升, 感覺 DNN 對資料不平均的問題較不敏感 不過由於模擬器的 traffic sign 跟 Bosch 的差太多, 因此效果不大好. 我只好加入了一些模器器下的影像去訓練, 結果就好很多了. 但還是遇到一個問題, 我的 macbook 沒有 GPU, 跑一張影像花了 120 secs, 而一秒鐘 camera 會傳來 8 張影像! 根本處理不了, 關鍵是也不知道 Udacity 它們用自己 GPU 跑起來會多快. 所以我就將影像長寬各縮小一半, 總體速度會降到原來的 1/4. 就算如此還是無法驗證是否夠快. 我們團隊卡在這個無法驗證的狀況很久, 導致可能需要用到延長四周的情形. 最後在 teammate Theodore King 的幫助下, 我們使用了 tf 的 object detection API, 使用 mobilenet 速度快到靠北飛起來. 連 CPU 處理一張影像都只需要不到1秒的時間! 何況使用 GPU. 最終總算有驚無險過關了. 我之前做那麼辛苦幹嘛 閒聊其實 Udacity 規劃相當棒了, 主要幾個部分都有分別的實作過, 最後來個大一統, 真的很有意思. 但我仍要吐槽的是, 搞環境太麻煩了! 模擬器跑在 virtualbox 上, 而我的 virtualbox window 沒法裝好, 只能裝在 macbook, 但 macbook 又沒有 GPU, 導致使用 deep learning 的方法完全不知夠不夠快! 另外, VM 的環境我還搞不定怎麼跟 host share data, 搞得我只好上傳雲端再下載, 最後衰事接踵而來, VM 也搞不定翻牆 (對, 我在網路長城的牆內)…..80%都在搞環境….真的很痛苦 恩, 終於畢業了… 結束了這漫長的旅程. 原以為我會興奮得不得了, 不過可能是因為最後 project 搞環境太痛苦, 加上這樣子的團隊合作其實沒有約束力 (有兩個完全的壟員), 反而解脫感壓過了高興. 但總結來說, 還是很感謝 Udacity 陪伴了我一年多, 並且有了這麼有趣的經驗! 有機會的話, 我還是會繼續上 Udacity 其他課程的. Reference Our github ROS wiki TrafficLight_Detection-TensorFlowAPI Semantic Segmantation Path Planning Model Predictive Control","tags":[{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"},{"name":"ROS","slug":"ROS","permalink":"https://bobondemon.github.io/tags/ROS/"}]},{"title":"Udacity-Semantic-Segmentation","date":"2018-03-06T11:59:13.000Z","path":"2018/03/06/Udacity-Semantic-Segmentation/","text":"Udacity SDC term 3 第二個 Project 做的是使用 Deep Learning 學習識別 pixel 等級的路面區域. 簡單講就是有如下的 ground truth data, 標示出哪邊是正確的路面, 然後用 Fully Convolutional Network 去對每個 pixel 做識別. Fully-Convolutional-Network (FCN)主要是實作這篇論文 “Fully Convolutional Networks for Semantic Segmentation“ 換言之, 全部都是 convolution layers, 包含使用 1x1 convolution 替換掉原來 Convnet 的 fully-connected-layer, 和使用 deconvolution 做 upsampling. 架構圖如下: 分成 Encoder 和 Decoder 部分. Encoder 使用 pre-trained 好的 VGG16 network, 負責做特徵抽取. 抽取出來的特徵後, 接上 deconvolution layers (需訓練) 搭建而成的 decoder part. 有一個特別之處是使用了 skip 方法. 這個方法是在 decoder 做 upsampling 時, 會加上當初相對應大小的 Encoder layer 資訊. 這樣做論文裡提到會增加整個識別效果. Results效果有點讓我小驚豔, 因為只使用少少的 289 張圖片去訓練而已. 跑出來的測試結果如下: 另外還有一點是縱使已經有 dropout 了, 如果沒有加上 l2 regularization 的話, 會 train 不好! (l2 regularization 真讓我第一次看到有這麼重要), 同樣設定下, 有和沒有 l2 regularization 的差別: Reference Fully Convolutional Networks for Semantic Segmentation Source code github","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://bobondemon.github.io/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"}]},{"title":"Mixtures of Factor Analyzers","date":"2018-02-11T15:23:24.000Z","path":"2018/02/11/Mixtures-of-Factor-Analyzers/","text":"這篇使用 Bishop PRML 的 notations, 同使參考 Zoubin Ghahramani and Geoffrey E. Hinton (沒錯, 就是那位 Hiton, 另外, 第一作者也是神人級別, 劍橋教授, Uber 首席科學家) 1997 年的論文 “The EM Algorithm for Mixtures of Factor Analyzers“, 實作了 Mixtures of Factor Analyzers, 臥槽! 都20年去了! My python implementation, github. 關於 EM 的部分會比較精簡, 想看更多描述推薦直接看 PRML book. 文章主要分三個部分 什麼是 Factor Analysis, 以及它的 EM 解 推廣到 mixtures models 語者識別中很關鍵的 ivector 究竟跟 FA 有什麼關聯? 直接進入正題吧~ Factor Analysis一言以蔽之, sub-space 降維. 假設我們都活在陰魂不散的 Gauss 世界中, 所有 model 都是高斯分布. 我們觀察的資料 $x$ 都是高斯分布, 且都是高維度. 但實際上 $x$ 通常只由少數幾個看不到的變數控制, 一般稱這些看不到的變數為 latent variable $z$. 如下圖舉例: 所以我們的主要問題就是, 怎麼對 Gaussian distribution 建立 sub-space 模型? 答案就是使用 Linear Gaussian Model. 一些 notations 定義: $x$ 表示我們的 observation, 維度是 $D$. $z$ 是我們的 latent variable, 維度是 $K$. 我們一般都期望 $K \\ll D$. $x$ and $z$ follow linear-Gaussian framework, 有如下的關係: $$\\begin{align} p(z)=N(z|0,I) \\\\ p(x|z)=N(x|Wz+\\mu,\\Psi) \\\\ \\end{align}$$ $W$ 是一個線性轉換, 將低維度的 latent space 轉換到高維度的 observation space, 另外 $\\Psi$ 必須是對角矩陣. 由於是對角的關係, 因此 $\\Psi$ 捕捉了 obaservation 維度的各自變異量, 因此稱為 uniquenesses, 而 $W$ 就是負責捕捉共同項, 稱為 factor loading. 書裡有一個簡單明瞭的圖解釋上述的模型, 我就不多說了, 自行看圖: 因為是 linear-Gaussian model, 所以 marginal distribution 也是 Gaussian: $$\\begin{align} p(x)=N(x|\\mu,C) \\\\ \\mbox{where } C=WW^T+\\Psi \\end{align}$$ 同時, 事後機率也是 Gaussian $$\\begin{align} p(z|x)=N(z|GW^T\\Psi^{-1}(x-\\bar{x}),G^{-1}) \\\\ \\mbox{where } G=(I+W^T\\Psi^{-1}W)^{-1} \\end{align}$$ 完整的 lineaer-Gaussian model 公式, from PRML book: 有了 $p(x)$ (式 3) 基本上我們就可以根據 training data 算出 likelihood, 然後找出什麼樣的參數可以最大化它. 但是這裡的問題是含有未知變數 $z$, 這個在 training data 看不到, 因為我們只看的到 $x$. 不過別擔心, EM 演算法可以處理含有未知變數情況下的 maximal likelihood estimation. 忘了什麼是 EM, 可以參考一下這. 很精簡的講一下就是, 找到一個輔助函數 $Q$, 該輔助函數一定小於原來的 likelihood 函數, 因此只要找到一組參數可以對輔助函數最大化, 那麼對於原來的 likelihood 函數也會有提升, 重複下去就可以持續提升, 直到 local maximum.另外輔助函數就是 “complete-data log likelihood and take its expectation with respect to the posterior distribution of the latent distribution evaluated using ‘old’ parameter values”, 我知道很粗略, 還請自行看筆記或是 PRML Ch9. E-StepE-Step 主要算出基於舊參數下的事後機率的一階二階統計量 首先將符號做簡化, 方便後面的式子更簡潔 ($n$ 是訓練資料的 index): $$\\mathbb{E}[z_n]\\equiv\\mathbb{E}_{z_n|x_n}[z_n] \\\\ \\mathbb{E}[z_nz_n^T]\\equiv\\mathbb{E}_{z_n|x_n}[z_nz_n^T] \\\\$$ 事後機率的一階二階統計量如下: $$\\begin{align} \\mathbb{E}[z_n] = GW^T\\Psi^{-1}(x_n-\\mu) \\\\ \\mathbb{E}[z_nz_n^T] = G + \\mathbb{E}[z_n] \\mathbb{E}[z_n]^T \\\\ \\mbox{where } G=(I+W^T\\Psi^{-1}W)^{-1} \\end{align}$$ 因為事後機率是 Gaussian, 所以由式 (5) 可以推得式 (7) 和 式 (8). M-Step這一步就是最大化輔助函數 $Q$, 其中 $\\mu$ 等於 sample mean, 可以直接寫死不需要 iteration. 另外兩個參數 update 如下: $$\\begin{align} W^{new}=\\left[\\sum_{n=1}^N (x_n-\\mu)\\mathbb{E}[z_n]^T\\right]\\left[\\sum_{n=1}^N \\mathbb{E}[z_nz_n^T]\\right]^{-1} \\\\ \\Psi^{new}=\\mbox{diag}\\left[S-W^{new}\\frac{1}{N}\\sum_{n=1}^N \\mathbb{E}[z_n](x_n-\\mu)^T\\right] \\end{align}$$ $S$ 是 sample covariance matrix (除 N 的那個 biased) Toy Example黑色那條線是真正產生資料時的 $W$, 可以當成正確答案. 紅色的是 FA 估計出來的 $W$ 和 $p(x)$. 可以發現 $W$ 沒有跟正確答案一樣, 這是因為我們在做 maximum likelihood 的時候, 只關心 $p(x)$, 因此可以有不同的 latent space 產生相同的 $p(x)$. 範例也一併把 probabilistic PCA 做出來了, 可以發現 PPCA 算的 $W$ 跟正確答案很接近, 這是因為此範例的資料其實是根據 PPCA 的模型產生的, 所以 PPCA 較接近是正常. 同時我們看到 PPCA 估計出來的 $p(x)$ 其實也跟 FA 一樣, 再度佐證 FA 其實也沒算錯, 只是不同的表達方式. Mixtures of Factor Analyzers將 FA 假設有多個 components 組成就變成 MFA 了, 其實就跟 GMM 一樣, 差別在於我們用了 latent space 去各別 model 每個 Gaussian Components 而已! 要注意的是, 這時候的 latent variables 不只有 $z$, 還有 $m$ (=1~M 表示有 $M$ 個 components), 我們用下標 $j$ 表示 component 的 index. 另外, 每一個 component, 會有各自的 latent space, 因此有各自的 $W_j$ 和 $\\mu_j$, 但是全部的 components 共用一個 uniquenesses $\\Psi$. $$\\begin{align} p(x|z,m=j)=N(x|W_j z+\\mu_j,\\Psi) \\end{align}$$ 和 GMM 一樣, 每一個 component 都有一個 weights, $\\pi_j$, 合起來機率是1 E-Step一階和二階統計量如下: $$\\begin{align} \\color{red}{\\mathbb{E}[z_n|m=j]} = G_j W_j^T \\Psi^{-1}(x_n-\\mu_j) \\\\ \\color{red}{\\mathbb{E}[z_nz_n^T|m=j]} = G_j + \\mathbb{E}[z_n|m=j] \\mathbb{E}[z_n|m=j]^T \\\\ \\mbox{where } G_j=(I+W_j^T\\Psi^{-1}W_j)^{-1} \\end{align}$$ 而真正的事後機率為: $$\\begin{align} \\mathbb{E}[m=j,z_n] = h_{nj}\\mathbb{E}[z_n|m=j] \\\\ \\mathbb{E}[m=j,z_nz_n^T] = h_{nj}\\mathbb{E}[z_nz_n^T|m=j] \\\\ \\mbox{where } \\color{red}{h_{nj}}=\\mathbb{E}[m=j|x_n]\\propto p(x_n,m=j) \\end{align}$$ 將 (18) 解釋清楚一下, 基本上就是計算給定一個 $x_n$, 它是由 component $j$ 所產生的機率是多少. 我們可以進一步推導如下: $$\\begin{align} p(x_n,m=j)=p(m=j)p(x_n)\\\\ =\\pi_j N(x_n|\\mu_j,C_j=W_jW_j^T+\\Psi) \\end{align}$$ (19) 到 (20) 的部分可以由 (3) 和 (4) 所知道的 marginal distribution $p(x)$ 得到 到這裡, 所有需要的統計量, 紅色部分, 我們都可以算得了. M-Step通通微分等於零, 通通微分等於零, 通通微分等於零 … 得到: $$\\begin{align} \\pi_j^{new}=\\frac{1}{N}\\sum_{n=1}^N h_{nj} \\\\ \\mu_j^{new}=\\frac{\\sum_{n=1}^N h_{nj}x_n}{\\sum_{n=1}^N h_{nj}} \\\\ W_j^{new}=\\left[\\sum_{n=1}^N h_{nj}(x_n-\\mu_j)\\mathbb{E}[z_n|m=j]^T\\right]\\left[\\sum_{n=1}^N h_{nj}\\mathbb{E}[z_nz_n^T|m=j]\\right]^{-1} \\\\ \\Psi^{new}=\\frac{1}{N}\\mbox{diag}\\left[ \\sum_{nj} h_{nj} \\left( (x_n-\\mu_j) - W_j^{new}\\mathbb{E}[z_n|m=j] \\right)(x_n-\\mu_j)^T \\right] \\end{align}$$ Toy Example 圖應該很清楚了, 有正確 model 到 data 這個 MFA 還真的不容易實作, 寫起來很多要注意的地方, 很燒腦阿! 不過做完了之後頗有成就感~ i-vector其實會想寫這篇主要是因為語者識別中的 ivector, 而 ivector 基本上就是一個 FA. 在計算 ivector 時, 我們會先估計 Universal Background Model (UBM), 其實就是所有語者的所有語音特徵算出來的 GMM. 以下圖為例, UBM 有三個 mixtures, 用淡藍色表示. 而針對某一位 speaker, 其 GMM 為橘色. 傳統上我們將所有 mixture 的 mean 串接成一個長的向量, 則該向量就可以當作是該 GMM 模型的一個代表, 並稱為 supervector 不一起串接 covariance matrix 嗎? weight 呢? 當然也可以全部都串成一個非常長的向量, 但研究表明 mean 向量就足夠了 supervector 維度為 mfcc-dim x mixture數, 很容易有 40x1024 這麼高維! 因此 ivector 就是利用 FA 的方法將 supervector 降維. 那具體怎麼做呢? 首先我們要先用一個小技巧將 “多個 Gaussians” (注意不是 GMM, 因為沒有mixture weight的概念, 每一個 Gaussian都同等重要) 轉換成一個 Gaussain. 見圖如下: 我們可以很容易驗證兩邊是等價的. 轉換成一個 Gaussian 好處就是我們可以直接使用 FA 降維, 而 ivector 就是該 FA 的 latent variable $z$. 如同 (2) 的定義: $$\\begin{align} p(x|z)=N(x|Wz+\\mu,\\Sigma) \\\\ \\end{align}$$ 這裡的 $\\mu$ 是 UBM 的 supervector, $\\Sigma$ 則如同上圖的定義, 是一個 block diagonal matrix, 每一個 block 對應一個 UBM mixture 的 covariance matrix. 因此 $\\mu$ 和 $\\Sigma$ 都是使用 UBM 的參數. 針對式 (25) 去更仔細了解其所代表的物理意義是很值得的, 所以我們多說一點. 由於我們已經知道這樣的一個 Gaussian 實際上代表了原來 mfcc space 的多個 Gaussians. 所以針對某一個特定的 ivector $z^*$ 由式 (25) 得知, 他有可能代表了下圖橘色的三個 Gaussians (也因此可能代表了某一個 speaker 的模型): 到目前為止所描述的 ivector 實際上是根據自己的理解將 2005 年 “Eigenvoice Modeling with Sparse Training Data“ 裡的 Proposition 1 (p348) 的設定描述出來. 如有錯誤還請來信指正. 該設定中, 每一個 mfcc vector 都會事先被歸類好屬於哪一個 mixture, 等於硬分類. 但是其實並不需要, 一個明顯的改進方法就是使用後驗概率來做軟分類. 直接看圖: 目前的 ivector 計算都使用這種方式, 例如 Microsoft Research 的 MSR Identity Toolbox. 該 toolbox 使用 “A Straightforward and Efficient Implementation of the Factor Analysis Model for Speaker Verification“ 的實作方式, 可以由論文的式 (2),(5) 看出使用後驗概率的設定. 最後多說一些語者識別的事情. ivector 主要是針對原高維空間 (mfcc-dim x component數量) 做降維, 而沒有去針對語者的訊息. 所以傳統流程會再經過 WCCN + LDA, 而 LDA 就會針對同一個語者盡量靠近, 而不同語者盡量拉開. 經過 LDA 後就可以用 $cos$ 計算相似度進行語者之間的打分. 但事實上, 更好的做法是用一個 PLDA 做更好的打分. 關於 PLDA 請參考這邊原始文章 “Probabilistic Linear Discriminant Analysis for Inferences About Identity“, 而 PLDA 更是與本篇的 FA 脫離不了關係! 總體來說 FA, MFA 對於目前的語者識別系統仍然十分關鍵, 縱使目前 Kaldi 使用了深度學習替換了 ivector, 但後端仍然接 PLDA. Reference 自己實作的 Python MFA (含 Toy examples) github Zoubin Ghahramani and Geoffrey E. Hinton, The EM Algorithm for Mixtures of Factor Analyzers Bishop PRML 以前的 EM 筆記 以前的 GMM EM 筆記 i-vector 原始論文 PLDA 原始論文 Eigenvoice Modeling with Sparse Training Data A Straightforward and Efficient Implementation of the Factor Analysis Model for Speaker Verification MSR Identity Toolbox","tags":[{"name":"Factor Analysis","slug":"Factor-Analysis","permalink":"https://bobondemon.github.io/tags/Factor-Analysis/"},{"name":"Expectation Maximization","slug":"Expectation-Maximization","permalink":"https://bobondemon.github.io/tags/Expectation-Maximization/"},{"name":"ivector","slug":"ivector","permalink":"https://bobondemon.github.io/tags/ivector/"}]},{"title":"Path-Planning-Udacity-Term3-Project1","date":"2018-02-06T15:38:48.000Z","path":"2018/02/06/Path-Planning-Udacity-Term3-Project1/","text":"停了半年 的 Udacity Self Driving Car (SDC) Program, 終於又開始了. 做為 Term3 的第一個 Project, 我抱著高度的期待. 不過完成後, 有點小失望. 失望的原因是這個 project 跟課程上的連結感覺不是那麼明顯. 例如課程上有講到 A*, hybrid A* 的算法, 但 project 是模擬 highway drive, 因此 A* 比較不適合 (適合在 parking lot 場景). 另外也有提到怎麼降低 jerk (加速度的微分, 主要是不舒適的來源), 當參考內容是很好, 不過在寫 Project 時感覺也不大需要. 這篇就是個紀錄, 會很水. 方法很簡單, 一張圖解決: ego-car 根據自己所在的車道, 最多可以有三條路徑選擇, 路徑就用 spline curve 產生, 確保夠 smooth. 同時有 sensor-fusion 的資料可以知道其他車子的狀況, 然後利用 prediction model 去預測其他車的路徑 (我就單純使用 constant velocity 線性路徑). 如果有 collision 在未來的 1 or 1.5 秒, 該路徑就無效. 另外, 如果有車子太靠近, 就減速. ego-car 的每個路徑都會有各自的 cost, cost 是根據一些喜好, 譬如哪一條車道可以跑得比較快, 哪一條路徑比較不會跟其他車子太接近等等… 這 Project 最麻煩的就是在設計 cost function, 和調整. (還有熟悉 project 的程式碼…麻煩阿) 影片連結 here. Project github here 沒了, 文章水不水 ? 好水阿, 真心虛","tags":[{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"}]},{"title":"Maximum Mutual Information in Speech Recognition","date":"2017-12-16T04:08:44.000Z","path":"2017/12/16/Maximum-Mutual-Information-in-Speech-Recognition/","text":"Maximum Mutual Information (MMI) 序列的鑑別性訓練方法從早期的 GMM-HMM, 到現今就算使用了深度學習同樣十分有用, 如 Kaldi chain model 在 DNN-HMM 的基礎上加上序列鑑別訓練, 性能會再進一步提升. 前一陣子讀了俞棟、鄧力的這本 語音識別實踐, 對我來說整理得滿好的, 就是數學部分的推導有點簡潔了些, 所以這篇就基於該書的推導, 補齊了較詳細的步驟, 並且嘗試使用 Computational graph 的方式理解 MMI 的訓練. 那麼就開始吧! 用自己畫的 MMI 的計算圖譜當封面吧 :) MMI 數學定義定義$o^m=o_1^m,...,o_t^m,...,o_{T_m}^m$是訓練樣本裡第 m 句話的 observation (MFCC,fbank,…) sequence, 該 sequence 有 $T_m$ 個 observation vector. 而$w^m=w_1^m,...,w_t^m,...,w_{N_m}^m$則是該句話的正確 transcription, 有 $N_m$ 個字. 通過 forced-alignment 可以得到相對應的 state sequence$s^m=s_1^m,...,s_t^m,...,s_{T_m}^m$MMI 目的就是希望模型算出的正確答案 sequence 機率愈大愈好, 同時非正確答案 (與之競爭的其他 sequences) 的機率要愈小愈好, 所以正確答案放分子, 非正確放分母, 整體要愈大愈好. 由於考慮了競爭 sequences 的最小化, 所以是鑑別性訓練. 又此種方始是基於整句的 sequence 考量, 因此是序列鑑別性訓練. 數學寫下來如下:$$J_{MMI}(\\theta;S)=\\sum_{m=1}^M J_{MMI}(\\theta\\|o^m,w^m) \\\\ =\\sum_{m=1}^M \\log \\frac{ p(o^m\\|s^m,\\theta)^KP(w^m) }{ \\sum_w p(o^m\\|s^w,\\theta)^K P(w) }$$ 為了簡單化, 我們假設只有一條訓練語音, 所以去掉 $m$ 的標記, 然後 $\\sum_m$ 省略: $$\\begin{align} J_{MMI}(\\theta\\|o,w) =\\log \\frac{ p(o\\|s,\\theta)^KP(w) }{ \\sum_w p(o\\|s^w,\\theta)^K P(w) } \\end{align}$$ 接著我們要算針對 $\\theta$ 的微分, 才可以使用梯度下降算法: $$\\begin{align} \\triangledown_\\theta J_{MMI}(\\theta\\|o,w) =\\sum_t \\triangledown_{z_t^L}J_{MMI}(\\theta\\|o,w)\\frac{\\partial z_t^L}{\\partial\\theta} \\\\ =\\sum_t e_t^L\\frac{\\partial z_t^L}{\\partial\\theta} \\end{align}$$ 其中定義$e_t^L=\\triangledown_{z_t^L}J_{MMI}(\\theta\\|o,w)$ 語音聲學模型 (AM) 傳統上使用 GMM 來 model, 而現在都是基於 DNN, 其中最後的 output layer 假設為第 $L$ 層: $z_t^L$, 過了 softmax 之後我們定義為 $v_t^L$, 而其 index $r$, $v_t^L(r)=P(r|o_t)$ 就是給定某一個時間 $t$ 的 observation $o_t$ 是 state $r$ 的機率. 讀者別緊張, 我們用 Computational graph 的方式將上式直接畫出來: MMI Computational Graph 表達 上圖用 computational graph 清楚的表達了式 (3) 的計算, 因為所有參數 $\\theta$ 在所有的時間 $t$ 上是共享的, 因此要 $\\sum_t$, 也就是要累加上圖所有紅色的 gradient path. 計算 $\\partial z_t^L / \\partial\\theta$ 很容易, 就是 DNN 的計算圖譜的 gradient, 因此重點就在如何計算 $e_t^L$, 而整個 MMI 最核心的地方就是在計算這個了! MMI 數學推導我們把 $e_t^L(i)$ (就是$e_t^L$這個向量的第$i$個element)計算如下: $$\\begin{align} e_t^L(i)=\\triangledown_{z_t^L(i)}J_{MMI}(\\theta\\|o,w) \\\\ =\\sum_r \\frac{\\partial J_{MMI}(\\theta\\|o,w)}{\\partial\\log p(o_t|r)}\\frac{\\partial\\log p(o_t|r)}{\\partial z_t^L(i)} \\end{align}$$ 先解釋一下 $\\log p(o_t|r)$ 這個 term, 可以重寫成$$\\begin{align} \\log p(o_t|r)=\\log \\color{red}{p(r|o_t)} + \\log p(o_t) - \\log p(r) = \\log \\color{red}{v_t^L(r)} + \\log p(o_t) - \\log p(r) \\end{align}$$所以這個 term 是跟 $v_t^L(r)$ 相關的, 而由於 $v_t^L$ 是 $z_t^L$ 經過 softmax 得到, 因此式(5)才會有 $\\sum_r$.根據式 (6), 我們可以很快算得式 (5) 的第二個分子分母項如下:$$\\begin{align} \\frac{\\partial\\left[\\log v_t^L(r) + \\log p(o_t) - \\log p(r)\\right]}{\\partial z_t^L(i)}=\\frac{\\partial \\log v_t^L(r)}{\\partial z_t^L(i)} \\end{align}$$很明顯因為 $\\log p(o_t)$ 和 $\\log p(r)$ 都跟 $z_t^L(i)$ 無關所以去掉.為了計算式 (5) 的第一個分子分母項, 我們把先把式 (1) 的 log 項拆開:$$\\begin{align} J_{MMI}(\\theta\\|o,w)= K\\color{green}{\\log p(o\\|s,\\theta)}+\\color{blue}{\\log p(w)} - \\color{orange}{\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]} \\end{align}$$所以$$\\begin{align} \\frac{\\partial J_{MMI}(\\theta\\|o,w)}{\\partial \\log p(o_t|r)}= K\\color{green}{ \\frac{\\partial\\log p(o\\|s,\\theta)}{\\partial \\log p(o_t|r)} } + \\color{blue}{ \\frac{\\partial\\log p(w)}{\\partial \\log p(o_t|r)} } - \\color{orange}{ \\frac{\\partial\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]}{\\partial \\log p(o_t|r)} } \\end{align}$$ 綠色部分注意到 $\\log p(o|s,\\theta)$ 在 HMM 的情況下, 是給定 state sequence 的觀測機率值, 因此只是每個 state 時間點的 emission probability, 所以$$\\begin{align} \\log p(o\\|s,\\theta)= \\sum_{t&apos;} \\log p(o_{t&apos;}\\|s_{t&apos;},\\theta) \\end{align}$$而只有 $t’=t$ 時與微分項有關, 因此變成$$\\begin{align} \\frac{\\partial\\log p(o\\|s,\\theta)}{\\partial \\log p(o_t\\|r)}= \\frac{\\partial \\log p(o_t\\|s_t,\\theta)}{\\partial \\log p(o_t\\|r)}=\\delta(r=s_t) \\end{align}$$ 藍色部分與微分項無關，因此$$\\begin{align} \\frac{\\partial\\log p(w)}{\\partial \\log p(o_t|r)}=0 \\end{align}$$ 橘色部分$$\\begin{align} \\frac{\\partial\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]}{\\partial \\log p(o_t|r)}= \\frac{1}{\\sum_w p(o\\|s^w,\\theta)^K p(w)}\\times\\frac{\\partial \\sum_w \\color{red}{p(o\\|s^w,\\theta)}^K p(w)}{\\partial \\log p(o_t|r)} \\end{align}$$ 紅色的部分如同上面綠色項的討論, 只有時間點 $t$ 才跟微分項有關, 不同的是這次沒有 $\\log$ 因此是連乘, 如果 $s_t\\neq r$ 整條 sequence 的機率與微分項無關, 因此只會保留 $s_t=r$ 的那些 $w$ sequences.另外,$\\frac{\\partial p(o_t\\|r)^K}{\\partial\\log p(o_t\\|r)} \\mbox{ 可想成 } \\frac{\\partial e^{Kx}}{\\partial x} = Ke^{Kx}$綜合以上討論橘色部分為$$\\begin{align} \\frac{\\partial\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]}{\\partial \\log p(o_t|r)}= K\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)} \\end{align}$$ 全部帶入並整理 $e_t^L(i)$將 (11),(12),(14) 代回到 (9) 我們得到$$\\begin{align} \\frac{\\partial J_{MMI}(\\theta\\|o,w)}{\\partial \\log p(o_t|r)}= K\\left(\\delta(r=s_t)-\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)}\\right) \\end{align}$$繼續將 (15),(7) 代回到 (5) 我們終於可以得到 $e_t^L(i)$ 的結果了!$$\\begin{align} e_t^L(i)=\\sum_r K\\left(\\delta(r=s_t)-\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)}\\right) \\times \\frac{\\partial \\log v_t^L(r)}{\\partial z_t^L(i)} \\\\ = \\sum_r K\\left(\\delta(r=s_t)-\\color{red}{\\gamma_t^{DEN}(r)}\\right) \\times \\frac{\\partial \\log v_t^L(r)}{\\partial z_t^L(i)} \\\\ =K\\left(\\delta(i=s_t)-\\gamma_t^{DEN}(i)\\right) \\end{align}$$其中一個很重要的定義$$\\begin{align} \\gamma_t^{DEN}(r)=\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)} \\end{align}$$物理意義就是時間$t$在狀態$r$的機率! 理論上來說我們要取遍所有可能的 word sequence $w$ 並求和計算, 但實際上只會在 decoding 時的 lattice 上計算, 以節省時間. 到目前為止我們算完了 MMI 最困難的部分了, 得到 $e_t^L(i)$ 後 (式(18))，剩下的就只是 follow 上圖的 MMI computational graph 去做. 有讀者來信詢問式 (17) 如何推導至 (18), 過程如下圖: (抱歉偷懶不打 Latex 了) 結論還有一些其他變種如 boost MMI (bMMI)、MPE、MCE等等, 差別只是在最小化不同的標註精細度, 最重要的還是要先了解 MMI 就可以容易推廣了. 這些都有一個統一的表達法如下:$$\\begin{align} e_t^L(i)=K\\left(\\gamma_t^{DEN}(i)-\\gamma_t^{NUM}(i)\\right) \\end{align}$$注意到正負號跟 (18) 相反, 因為只是一個最大化改成最小化表示而已. 並且多了一個分子的 lattice 計算. Reference 俞棟、鄧力: 語音識別實踐 Ch8 Kaldi chain model","tags":[{"name":"Speech Recognition","slug":"Speech-Recognition","permalink":"https://bobondemon.github.io/tags/Speech-Recognition/"},{"name":"Maximum Mutual Information","slug":"Maximum-Mutual-Information","permalink":"https://bobondemon.github.io/tags/Maximum-Mutual-Information/"},{"name":"Computational Graph","slug":"Computational-Graph","permalink":"https://bobondemon.github.io/tags/Computational-Graph/"}]},{"title":"TF Notes (3), Computational Graph in Tensorflow","date":"2017-11-29T12:36:59.000Z","path":"2017/11/29/TF-Notes-Computational-Graph-in-Tensorflow/","text":"這篇介紹 computational graph (計算圖譜), 主要來源參考自李宏毅教授的課程內容. 並且我們使用 tensorflow 的求導函數 tf.gradients 來驗證 computational graph. 最後我們在 MNIST 上驗證整個 DNN/CNN 的 backpropagation 可以利用 computational graph 的計算方式訓練. Computational Graph主要就截圖李教授的投影片, 一個 computational graph 的 node 和 edge 可以定義如下 對於 chain rule 的話, 我們的計算圖譜可以這麼畫 其實就是 chain rule 用這樣的圖形表示. 比較要注意的就是 case 2 的情況, 由於 $x$ and $y$ 都會被 $s$ 影響, 因此計算 gradients 時要累加兩條路徑. 再來另一項要注意的是如果有 share variables 我們的計算圖譜該怎麼表示呢 ? 舉例來說如下的函式$y=x\\cdot e^{x^2}$ 計算圖譜畫出來長這樣 簡單來說把相同變數的 nodes 上所有的路徑都相加起來. 上面的範例就是計算 $\\frac{\\partial y}{\\partial x}$ 時, 有三條路徑是從 node $x$ 出發最終會影響到 node $y$ 的, 讀者應該可以很容易看出來. 另外如果分別計算這三條路徑, 其實很多 edges 的求導結果會重複, 因此從 $x$ 出發計算到 $y$ 會很沒有效率, 所以反過來 (Reverse mode) 從 root ($y$) 出發, 反向找出要求的 nodes ($x$) 就可以避免很多重複運算. Verify with Tensorflow考慮以下範例, 其中 $&lt;,&gt;$ 表示內積, $a$, $b$, 和 $1$ 都是屬於 $R^3$ 的向量. 它的計算圖譜如下: 在 Tensorflow 中, tf.gradients 可以幫助計算 gradients. 舉例來說如果我們要計算 $\\frac{\\partial e}{\\partial c}$, 我們只要這樣呼叫即可 ge_c=tf.gradients(ys=e,xs=c). 為了方便, 我們將 $\\frac{\\partial y}{\\partial x}$ 在程式裡命名為 gy_x. 下面這段 codes 計算出上圖 5 個 edges 的 gradients: 123456789101112131415161718import tensorflow as tfimport numpy as npa = tf.placeholder(tf.float32, shape=(1,3))b = tf.placeholder(tf.float32, shape=(1,3))c = a + bd = b + 1e = tf.matmul(c,d,transpose_b=True)ge_c, ge_d = tf.gradients(ys=e,xs=[c,d])gc_a, gc_b = tf.gradients(ys=c,xs=[a,b])gd_b = tf.gradients(ys=d,xs=b)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) gec, ged, gca, gcb, gdb = sess.run([ge_c, ge_d, gc_a, gc_b, gd_b],feed_dict=&#123;a:[[2,1,0]],b:[[1,2,3]]&#125;) print('ge_c=&#123;&#125;\\nge_d=&#123;&#125;\\ngc_a=&#123;&#125;\\ngc_b=&#123;&#125;\\ngd_b=&#123;&#125;'.format(gec, ged, gca, gcb, gdb)) 計算結果為 12345ge_c=[[ 2. 3. 4.]]ge_d=[[ 3. 3. 3.]]gc_a=[[ 1. 1. 1.]]gc_b=[[ 1. 1. 1.]]gd_b=[array([[ 1., 1., 1.]], dtype=float32)] 可以自己手算驗證一下, 結果當然是對的 (使用 Jacobian matrix 計算) 所以我們如果要得到 $\\frac{\\partial e}{\\partial b}$, 我們只要算 ge_c*gc_b + ge_d*gd_b 就可以了. 不過這樣自己把相同路徑做相乘，不同路徑做相加, 太麻煩了! 其實有更好的方法. 對於同一條路徑做相乘, tf.gradients 有一個 arguments 是 grad_ys 就可以很容易做到. tf.gradients(ys=,xs=,grad_ys=) 以下圖來說明 但事實上根本也不用這麼麻煩, 除非是遇到很特殊的狀況, 否則我們直接呼叫 tf.gradients(c,a), tensorflow 就會直接幫我們把同樣路徑的 gradients 做相乘, 不同路徑的 gradients 結果做相加了! 所以上面就直接呼叫 tf.gradients(c,a) 其實也就等於 gb_a2 了. 最後回到開始的 e=&lt;a+b,b+1&gt; 的範例, 如果要計算 $\\frac{\\partial e}{\\partial b}$, 照原本提供的 codes 需要計算三條路徑各自相乘後再相加, 其實只要直接呼叫 ge_b=tf.gradients(e,b) 就完成這件事了 (ge_c*gc_b + ge_d*gd_b) MNIST 用計算圖譜計算 back propagationDNN 的計算圖譜為了清楚了解 Neural network 的 backpropagation 如何用 computational graph 來計算 gradients 並進而 update 參數, 我們不使用 tf.optimizer 幫我們自動計算. 一個 3 layers fo MLP-DNN 的計算圖譜我們可以這樣表示: 圖裡的參數直接對應了程式碼裡的命名, 因此可以很方便對照. 其中 tensorflow 裡參數的名字跟數學上的對應如下: $$\\begin{align} gll=\\frac{\\partial \\mbox{loss}}{\\partial \\mbox{logits}} \\\\ gly2=\\frac{\\partial \\mbox{logits}}{\\partial y2}gll \\\\ gy2y1=\\frac{\\partial y2}{\\partial y1}gly2 \\\\ gy1y0=\\frac{\\partial y1}{\\partial y0}gy2y1 \\\\ (glw3,glb3)=(\\frac{\\partial \\mbox{logits}}{\\partial w3}gll,\\frac{\\partial \\mbox{logits}}{\\partial b3}gll) \\\\ (gy2w2,gy2b2)=(\\frac{\\partial y2}{\\partial w2}gly2,\\frac{\\partial y2}{\\partial b2}gly2) \\\\ (gy1w1,gy1b1)=(\\frac{\\partial y1}{\\partial w1}gy2y1,\\frac{\\partial y1}{\\partial b1}gy2y1) \\\\ (gy0w0,gy0b0)=(\\frac{\\partial y0}{\\partial w0}gy1y0,\\frac{\\partial y0}{\\partial b0}gy1y0) \\end{align}$$ 相對應的 tensorflow 代碼如下:12345678910111213141516gll = tf.gradients(ys=loss,xs=logits)gly2 = tf.gradients(ys=logits,xs=y2,grad_ys=gll)gy2y1 = tf.gradients(ys=y2,xs=y1,grad_ys=gly2)gy1y0 = tf.gradients(ys=y1,xs=y0,grad_ys=gy2y1)glw3 = tf.gradients(ys=logits,xs=w3,grad_ys=gll)glb3 = tf.gradients(ys=logits,xs=b3,grad_ys=gll)gy2w2 = tf.gradients(ys=y2,xs=w2,grad_ys=gly2)gy2b2 = tf.gradients(ys=y2,xs=b2,grad_ys=gly2)gy1w1 = tf.gradients(ys=y1,xs=w1,grad_ys=gy2y1)gy1b1 = tf.gradients(ys=y1,xs=b1,grad_ys=gy2y1)gy0w0 = tf.gradients(ys=y0,xs=w0,grad_ys=gy1y0)gy0b0 = tf.gradients(ys=y0,xs=b0,grad_ys=gy1y0) 用圖來表示為: Tensorflow 程式碼用上面一段的方式計算出所需參數的 gradients 後, update 使用最單純的 steepest descent, 這部分程式碼如下 12345678910111213update_w3 = tf.assign_add(w3,-rate*glw3[0])update_b3 = tf.assign_add(b3,-rate*glb3[0])update_w2 = tf.assign_add(w2,-rate*gy2w2[0])update_b2 = tf.assign_add(b2,-rate*gy2b2[0])update_w1 = tf.assign_add(w1,-rate*gy1w1[0])update_b1 = tf.assign_add(b1,-rate*gy1b1[0])update_w0 = tf.assign_add(w0,-rate*gy0w0[0])update_b0 = tf.assign_add(b0,-rate*gy0b0[0])training_operation = [update_w3, update_b3, update_w2, update_b2, update_w1, update_b1, update_w0, update_b0] 完整程式碼如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171import osimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.layers import flattenfrom tensorflow.examples.tutorials.mnist import input_datafrom sklearn.utils import shuffle\"\"\"Data Loading\"\"\"dataPath='../dataset/MNIST_data/'mnist = input_data.read_data_sets(dataPath, one_hot=True)# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]img_width = 28img_height = 28images = mnist.train.imagesimg_num, _ = images.shapeimages = np.reshape(images,(img_num,img_height,img_width))images = images[...,np.newaxis]print('(Input to CNN) Images with shape &#123;&#125;'.format(images.shape))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# load the validation setimages_valid = mnist.validation.imagesimg_num_valid = len(images_valid)images_valid = np.reshape(images_valid,(img_num_valid,img_height,img_width))images_valid = images_valid[...,np.newaxis]labels1Hot_valid = mnist.validation.labelsprint('Having %d number of validation images' % img_num_valid)# plotting sample imagesplt.figure(figsize=(15,5))for i in np.arange(2*7): random_idx = np.random.randint(0,img_num) plt.subplot(2,7,i+1) plt.imshow(images[random_idx][...,0],cmap='gray') plt.title(labels[random_idx][0])\"\"\"First define the hyper-parameters\"\"\"# Hyper-parametersEPOCHS = 30BATCH_SIZE = 512rate = 0.01depth_list = [512, 256, 128]cNum = 1\"\"\"Define the input output tensors\"\"\"# using one-hot decodingx = tf.placeholder(tf.float32, (None, img_height, img_width, cNum))one_hot_y = tf.placeholder(tf.int32, (None, n_classes))#one_hot_y = tf.one_hot(y, n_classes)\"\"\"Define the graph and construct it\"\"\"z0 = flatten(x)w0 = tf.get_variable('w0', shape=[img_width*img_height, depth_list[0]], initializer=tf.random_uniform_initializer(-0.1,0.1))b0 = tf.get_variable('b0', [depth_list[0]], initializer=tf.zeros_initializer)y0 = tf.nn.xw_plus_b(z0, w0, b0)z1 = tf.nn.relu(y0)w1 = tf.get_variable('w1', shape=[depth_list[0], depth_list[1]], initializer=tf.random_uniform_initializer(-0.1,0.1))b1 = tf.get_variable('b1', [depth_list[1]], initializer=tf.zeros_initializer)y1 = tf.nn.xw_plus_b(z1, w1, b1)z2 = tf.nn.relu(y1)w2 = tf.get_variable('w2', shape=[depth_list[1], depth_list[2]], initializer=tf.random_uniform_initializer(-0.1,0.1))b2 = tf.get_variable('b2', [depth_list[2]], initializer=tf.zeros_initializer)y2 = tf.nn.xw_plus_b(z2, w2, b2)z3 = tf.nn.relu(y2)w3 = tf.get_variable('w3', shape=[depth_list[2], n_classes], initializer=tf.random_uniform_initializer(-0.1,0.1))b3 = tf.get_variable('b3', [n_classes], initializer=tf.zeros_initializer)logits = tf.nn.xw_plus_b(z3, w3, b3)cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits)loss = tf.reduce_mean(cross_entropy)\"\"\"Define gradients\"\"\"gll = tf.gradients(ys=loss,xs=logits)gly2 = tf.gradients(ys=logits,xs=y2,grad_ys=gll)gy2y1 = tf.gradients(ys=y2,xs=y1,grad_ys=gly2)gy1y0 = tf.gradients(ys=y1,xs=y0,grad_ys=gy2y1)glw3 = tf.gradients(ys=logits,xs=w3,grad_ys=gll)glb3 = tf.gradients(ys=logits,xs=b3,grad_ys=gll)gy2w2 = tf.gradients(ys=y2,xs=w2,grad_ys=gly2)gy2b2 = tf.gradients(ys=y2,xs=b2,grad_ys=gly2)gy1w1 = tf.gradients(ys=y1,xs=w1,grad_ys=gy2y1)gy1b1 = tf.gradients(ys=y1,xs=b1,grad_ys=gy2y1)gy0w0 = tf.gradients(ys=y0,xs=w0,grad_ys=gy1y0)gy0b0 = tf.gradients(ys=y0,xs=b0,grad_ys=gy1y0)update_w3 = tf.assign_add(w3,-rate*glw3[0])update_b3 = tf.assign_add(b3,-rate*glb3[0])update_w2 = tf.assign_add(w2,-rate*gy2w2[0])update_b2 = tf.assign_add(b2,-rate*gy2b2[0])update_w1 = tf.assign_add(w1,-rate*gy1w1[0])update_b1 = tf.assign_add(b1,-rate*gy1b1[0])update_w0 = tf.assign_add(w0,-rate*gy0w0[0])update_b0 = tf.assign_add(b0,-rate*gy0b0[0])training_operation = [update_w3, update_b3, update_w2, update_b2, update_w1, update_b1, update_w0, update_b0]\"\"\"Define accuracy evaluation\"\"\"# calculate the average accuracy by calling evaluate(X_data, y_data)correct_prediction = tf.equal(tf.argmax(logits, axis=1), tf.argmax(one_hot_y, axis=1))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))def evaluate(X_data, y_data): num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] accuracy = sess.run(accuracy_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y&#125;) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples \"\"\"Run Session\"\"\"### Train your model here.import timeif not os.path.isdir('./models'): os.makedirs('./models')#saver = tf.train.Saver()accumulate_time = 0.0with tf.Session() as sess: sess.run(tf.global_variables_initializer()) num_examples = img_num print(\"Training...\") print() train_accuracy = np.zeros(EPOCHS) validation_accuracy = np.zeros(EPOCHS) for i in range(EPOCHS): stime = time.time() acc_train_accuracy = 0 X_train, y_train = shuffle(images, labels1Hot) for offset in range(0, num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] sess.run(training_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y&#125;) etime = time.time() accumulate_time += etime - stime validation_accuracy[i] = evaluate(images_valid, labels1Hot_valid) print(\"EPOCH &#123;&#125; ...\".format(i+1)) print(\"Validation Accuracy = &#123;:.3f&#125;\".format(validation_accuracy[i])) print() print('Cost time: ' + str(accumulate_time) + ' sec.') 訓練結果如下: 1234567891011121314151617EPOCH 1 ...Validation Accuracy = 0.553EPOCH 2 ...Validation Accuracy = 0.698EPOCH 3 ...Validation Accuracy = 0.771... 略EPOCH 28 ...Validation Accuracy = 0.936EPOCH 29 ...Validation Accuracy = 0.936EPOCH 30 ...Validation Accuracy = 0.936 這速度果然明顯比用 Adam 慢很多, 但至少說明了我們的確使用 Computational graph 的計算方式完成了 back propagation! 結論Tensorflow 使用計算圖譜的框架來計算函數的 gradients, 一旦這樣做, 神經網路的 backprop 很自然了. 事實上, 所有流行的框架都這麼做, 就連 Kaldi 原先在 nnet2 不是, 但到 nnet3 也改用計算圖譜來實作. Reference 李宏毅 Computational Graph tf.gradients說明 Colah’s Blog","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://bobondemon.github.io/tags/TensorFlow/"},{"name":"Computational Graph","slug":"Computational-Graph","permalink":"https://bobondemon.github.io/tags/Computational-Graph/"}]},{"title":"Notes for KKT Conditions","date":"2017-11-14T13:36:40.000Z","path":"2017/11/14/Notes-for-KKT-Conditions/","text":"2011年自己做的筆記, 放上來以免檔案丟失, 也方便隨時參考. 參考自 “Numerical Optimization” by Jorge Nocedal and Stephen J. Wright. 但是打算只用 Lagrange Multiplier Theorem 理解 KKT. :) 就像是一般微積分裡學到的一樣, 對於一個函式 $f(x)$ 若 $x^\\ast$ 為一 minimal/maximum point, 則必要條件為 $f’(x^\\ast)=0$. 而在 constraint optimization 版本必要條件變成 KKT conditions. 說更清楚一點就是, 若 $x^\\ast$ 為一 minimal/maximum point (+滿足某些神秘條件) , 則必要條件為在 $x^\\ast$ 滿足 KKT Conditions. 神秘條件稱為 Constraint Qualifications, 常見的為 LICQ, 在 Convex opt 裡為 Slater’s condition. wiki KKT 具體來說，我們要探討的是對於以下的問題，如果 $x^\\ast$ 為一 minimal point 且滿足式 (2) 的條件, 則會發生什麼事情 (我們找的是必要條件) $$\\begin{align} \\min f(x) \\\\ \\mbox{subject to } \\begin{array}{rcl} c_i(x)=0,i \\in \\mathbf{E} \\\\ c_i(x)\\geq 0, i \\in \\mathbf{I} \\\\ \\end{array} \\end{align}$$ Descent Direction一般這樣的問題不會有 closed form solution, 因此會使用數值最佳化的方式, 找出一個 sequence $(x_k)$ 來逼近 $x^\\ast$. 問題是如何讓這樣的 sequence 逼近一個 (local) minimum? 一個嘗試是至少先讓找到的每一個 $x_k$ 都比前一步更好. 換句話說就是要保證找到的 $x_k$ 滿足$f(x_k)&lt;f(x_{k-1})$ 有了這個想法, 再來就是該怎麼找下一個點, 或是說, 基於現在的點 $x_k$, 該往哪個方向 $d$, 走多遠 $t$? 也因此，我們也就衍生了一個問題, 往哪個方向走函數值會保證下降 (descent direction)? Descent direction 保證了在該方向上只要不要走太大步, 目標函數值一定會下降, 反過來說, 如果 $x^\\ast$ 已經是 (local) minimum 了, 在該點上不應該存在 descent direction. 基於上述的討論, 我們有必要了解清楚 descent direction. 定義如下 [Def]:令 $f \\in C^1(\\mathbb{R}^n,\\mathbb{R})$, 我們稱 $d$ 為 $x_0$ 的一個 descent direction 如果滿足: $\\exists t&gt;0$, such that $f(x_0+sd)&lt;f(x_0),\\forall s \\leq t$ 其實很容易證得: 只要該方向 $d$ 跟 gradient $\\triangledown f(x_0)$ 方向相反, 就會是 $x_0$ 的一個 descent direction [Thm1]:令 $f \\in C^1(\\mathbb{R}^n,\\mathbb{R})$, 如果滿足 $\\triangledown f(x_0)^Td&lt;0$ 則 $d$ 就會是 $x_0$ 的一個 descent direction [Pf]: 由微分的定義出發$$\\lim_{t\\rightarrow 0^+}\\frac{f(x_0+td)-f(x_0)-\\triangledown f(x_0)^Ttd}{\\parallel td \\parallel}=0 \\\\ \\Rightarrow \\lim_{t\\rightarrow 0^+}\\frac{f(x_0+td)-f(x_0)}{t \\parallel d \\parallel}=\\triangledown f(x_0)^T\\frac{d}{\\parallel d \\parallel} \\\\ \\Rightarrow \\lim_{t\\rightarrow 0^+}\\frac{f(x_0+td)-f(x_0)}{t}=\\triangledown f(x_0)^Td&lt;0 \\\\ \\Rightarrow \\exists t&gt;0,s.t.,f(x_0+sd)-f(x_0)&lt;0,for\\forall s \\leq t \\\\ \\Rightarrow \\exists t&gt;0,s.t.,f(x_0+sd)&lt;f(x_0),for\\forall s \\leq t$$ 很明顯 steepest descent $d=-\\triangledown f(x_0)$ 是 descent direction. 其實只要滿足這種形式 $d=-B\\triangledown f(x_0)$ 當 $B$ 是正定，就會是 descent direction. 而當 B 定義為 $\\triangledown ^2 f(x_0)$ (Hessian Matrix 是半正定, 通常是 full rank 就會正定), 這種形式就是牛頓法 $d=−\\triangledown ^2 f(x_0)\\triangledown f (x_0)$ 不過我們今天要處理的是 constrained opt, 會有等式或不等式的條件, 因此我們的搜尋空間只能在滿足這些條件下去搜尋, 稱該空間為 feasible set = {x|x滿足所有(2)式的條件}. 可以想像, 在 feasible set 的限制下, 能搜尋的 direction 會被限制. 因此 “Numerical Optimization” 這本書就展開了一系列的討論和證明, 可以得到在這個 feasible set 下, 這些 能搜尋的方向(我們稱為 limiting direction)所構成的集合 究竟長什麼樣. 且發生在最佳解上的 limiting directions 都不會是 descent direction (合理, 不然就找到更佳的解了). 此外, 看課本的話, 會繞更大一圈才會知道什麼是 KKT Conditions (但是相當嚴謹且豐富). 為了清楚了解 KKT, 我們繞過課本的方法, 完全採用微積分學過的 Lagrange Multiplier Theorem 來說明. 了解 KKT Conditions限制條件為等式其實 KKT 的表達全部圍繞在 Lagrange Multiplier Theorem 上. 一般課本上講的都是等式條件, 我們列出高維課本裏頭的定理:不想打 Latex 了 &gt;&lt;, 貼圖好了 [Thm2]: Lagrange Multiplier Theorem 考慮以下問題 $$\\min f(x) \\\\ \\mbox{subject to } \\begin{array}{rcl} c(x)=0 \\\\ \\end{array}$$ 我們可以得到, 若 $x^\\ast$ 為一個 local minimum, 由 Thm2 知道, 滿足 $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$, for some $\\lambda$此時的 $\\lambda$ 正負都有可能, 也就說明了兩個 gradients 是平行的. 用圖來說明如下: 限制條件為不等式考慮以下問題 $$\\min f(x) \\\\ \\mbox{subject to } \\begin{array}{rcl} c(x)\\geq 0 \\\\ \\end{array}$$ 當 $x^\\ast$ 為一個 local minimum 會發生什麼事? 分成兩種情況討論: $c(x^\\ast)=0$ $c(x^\\ast)&gt;0$ 第一種情況就退化成條件為等式的情形. 因此存在 $\\lambda$ 滿足 $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$. 如果 $\\lambda&lt;0$, 導致 $\\triangledown c(x^\\ast)$ 跟 $\\triangledown f(x^\\ast)$ 反方向的話, $\\triangledown c(x^\\ast)^T\\triangledown f(x^\\ast)&lt;0$ 導致 $\\triangledown c(x^\\ast)$ 變成一個 desent direction.則表示我們可以找到一個方向使得目標函數值下降且同時讓條件函數值上升(因此仍然是 feasible), 那麼與 $x^\\ast$ 是 local minimum 矛盾. 因此得到的結論是 $\\triangledown c(x^\\ast)$ 跟 $\\triangledown f(x^\\ast)$ 同方向, i.e., $\\lambda\\geq 0$. 圖示如下: 第二種情況是退化成 unconstrained opt, 因為 $x^\\ast$ 是在 feasible set 內, 換句話說 $x^\\ast$ 是 feasible set 的 interior point. 既然是 unconstrained opt, 且 $x^\\ast$ 為 local minimum, 則表示 $\\triangledown f(x^\\ast)=0$, 所以當然也可以寫成 $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$ 只不過此時的 $\\lambda=0$ 所以不管是第一種或是第二種情形, 我們都可以寫成 存在 $\\lambda\\geq 0$ 滿足 $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$ KKT Conditions到這裡為止, 我們基本上已經可以列出 KKT 了: [Thm3]: Karush‐Kuhn‐Tucker conditions[Pf]:Condition 1 只是說明具有 Lagrange Multiplier 的表達方式: $\\triangledown f(x^\\ast)=\\sum_i{\\lambda_i\\triangledown c_i(x^\\ast)}$Condition 2,3 是說明 $x^\\ast$ 是 feasible point, 這是廢話Condition 4 說明 若條件為不等式, 相對應的 Lagrange Multipliers 必須大於等於0, 我們在上一段討論了Condition 5 稱為 complementarity slackness (我知道很難念…), 這需要稍微說明一下如果 $c_i$ 是等式條件, 則 $c_i(x^\\ast)=0$, 因此滿足 Condition 5如果 $c_i$ 是不等式條件, 但是 $c_i(x^\\ast)=0$, 同樣也滿足 Condition 5最後一種情況是 $c_i$ 是不等式條件, 且 $c_i(x^\\ast)&gt;0$. 還記得我們上面針對此種情形的討論嗎? 我們會令他的 $\\lambda_i=0$, 所以還是滿足 Condition 5. 這裡沒有提到一件事情就是 LICQ, 全名 Linear Independent Constraint Qualification, 可參考 wiki KKT. LICQ 條件為: 對於某一點 feasible point x, 所有等式條件 (包含那些不等式條件但剛好在 x 變成等式) 在 x 這點的 gradients 都是線性獨立. 這個條件正好可以從 [Thm2]: Lagrange Multiplier Theorem 裡面看出來, Thm2 說明如果等式條件的 gradients 都線性獨立, 可以把 $\\mu=1$, 因此可以寫成 Condition 1: $\\triangledown f(x^\\ast)=\\sum_i{\\lambda_i\\triangledown c_i(x^\\ast)}$, 也因此可以滿足 KKT. 課本裡的證法課本裡的證明實在頗迂迴, 但是提供了很豐富和深刻的理解. 這裡還是努力記錄下來吧! {數學多, 謹慎服用} 我們分成5個步驟來討論: Limiting directions Limiting direction 與 Local minimum 的關聯 Limiting directions 的集合, 就稱為 F 吧 LICQ 成立時, “Limiting directions 都不是 descent direction” 與 “Lagrange Multipliers” 的等價關係 串起來變成 KKT 1. Limiting directions直觀來說, 對於某一點 $x_0$ (當然屬於 feasible set) 用在 feasible set 中的某條路徑去逼近它, 而逼近的最後方向就是 limiting direction. 另外, 一個 sequence ${z_k}$ 都屬於 feasible set , 都不等於 $x_0$, 且最後逼近 $x_0$, 我們稱為 feasible sequence. [Def]:若滿足以下條件稱 $d$ 是 $x_0$ 的 limiting direction. (當然 $x_0$ 是 feasible point)存在一個 feasible sequence $(z_k)_k$ 使得該 sequence 有一個 subsequence$$\\exists (z_{k_j})_j \\mbox{ such that } d = \\lim\\frac{(z_{k_j}-x_0)}{\\parallel z_{k_j}-x_0\\parallel}$$ 從定義上我們可以知道 limiting direction 長度為 1, 因為我們只在乎方向. 另外要特別說存在一個 subsequence 是因為 feasible sequence 不會只有一個 limiting direction. 例子如下: 2. Limiting direction 與 Local minimum 的關聯文章開頭有說明, “如果 $x^\\ast$ 已經是 (local) minimum 了, 在該點上不應該存在 descent direction.” 對於 constrained opt 的版本相當於 “如果 $x^\\ast$ 已經是 (local) minimum 了, 它的 limiting directions 都不能是 descent direction.” 用數學寫出來如下: [Thm4]:已知 $x^\\ast$ 是一個 local minimum, 則它所有的 limiting direction $d$ 都滿足 $\\triangledown f(x^\\ast)^Td \\geq 0$ 直觀上如果不滿足, 我們就可以找到一個 feasible sequence 從而得到該 limiting direction 會是一個 descent direction, 因此與 $x^\\ast$ 是 local minium 矛盾.我們在等下的第4個步驟可以看到此條件 “所有的 limiting direction $d$ 都滿足 $\\triangledown f(x^\\ast)^Td \\geq 0$” 等價於 KKT Conditions 的表達方式. 因此 Thm4 可以重寫成 “已知 $x^\\ast$ 是一個 local minimum, 則滿足 KKT Conditions”, 在最後第5步會串起來. 3. Limiting directions 的集合 (F) [Def]: Active Set對於某一 feasible point $x_0$, 它的 active set $\\mathbf{A}(x_0)$ 定義為$\\mathbf{A}(x_0) = \\mathbf{E} \\cup \\{i \\in \\mathbf{I} | c_i(x_0)=0 \\}$ [Def]: LICQ如果以下集合為線性獨立集, 則稱 LICQ 在 $x_0$ 成立$\\{\\triangledown c_i(x_0), i \\in \\mathbf{A}(x_0) \\}$ 其實我們在上面的討論都有使用這兩個定義, 這裡只不過用數學表示方便等下的討論.某一點它的所有 limiting direction 的集合 ($F$) 如下: [Thm5]:對於某一 feasible point $x_0$,$$F=\\left\\{ \\begin{array}{c|r} d &amp; \\begin{array}{rcl} d^T\\triangledown c_i(x_0)=0,i \\in \\mathbf{E} \\\\ d^T\\triangledown c_i(x_0) \\geq 0, i \\in \\mathbf{A}(x_0) \\cap \\mathbf{I} \\\\ \\parallel d \\parallel = 1\\\\ \\end{array} \\end{array} \\right\\}$$ 為了不模糊焦點, 證明就跳過, 想看的童鞋門就查一下舊的筆記 另外, 定義 $F_1=\\alpha F$, for $\\alpha\\geq 0$ (所以是 convex cone). 因此 $F1$ 只不過是把 $\\parallel d \\parallel =1$ 的條件去調. 4. LICQ 成立時, 關鍵的等價關係我們以下都假設 LICQ 成立, 這麼做就可以很方便地讓 limiting direction 的集合用 $F1$ 來表示. 還記得在 “2. Limiting direction 與 Local minimum 的關聯” 有提到我們希望找到某一 feasible point $x_0$ 的 $F1$ 都不是 descent direction, 因此該點就很有可能是我要找的 local optimum. 而這一個條件 “某一 feasible point $x_0$ 的 $F1$ 都不是 descent direction” 其實與 Lagrange Multipliers 息息相關, 也因此跟 KKT conditions 會產生連結. 下面定理可以證明這個條件可以等價於 KKT condition 的表達方式. [Thm6]:對於某一 feasible point $x_0$, Let $\\mathbf{A}(x_0)=(1…m)$, $\\mathbf{A}^T=[\\triangledown c_1(x_0)…\\triangledown c_m(x_0)]$ 則$$\\triangledown f(x-0)^Td\\geq 0,\\forall d \\in F1 \\Leftrightarrow\\\\ \\exists \\lambda \\in \\mathbb{R}^m \\mbox{ where } \\lambda_i \\geq 0 \\forall i \\in \\mathbf{A}(x_0) \\cap \\mathbf{I} \\mbox{, such that } \\triangledown f(x_0)=\\sum_{i=1}^m \\lambda_i \\triangledown c_i(x_0)=\\mathbf{A}^T$$ 同樣跳過, 證明可查看舊的筆記 我們可以仔細對照一下上面的 Lagrange Multiplier 那個條件, 其實它跟 “[Thm3]: Karush‐Kuhn‐Tucker conditions” 是一樣的, 只差在一個地方就是 complementarity slackness 沒有明確寫出來, 但我們知道一定存在 $\\lambda$ 可以滿足. 因此這個 Lagrange Multiplier 的條件也就是 KKT 的表達方式. 5. 串起來變成 KKT腦袋差不多都打結了, 目前為止到底得到了什麼關係? 我們來整理一下 Thm4 告訴我們一個最佳解一定會使得它的 limiting directions 都不是 descent direction.Thm5 告訴我們 limiting directions 的集合其實就是 $F1$ (or $F$).Thm6 告訴我們對於任一個 $F1$ 的 direction, 都不是 descent direction, 等同於滿足 KKT 的表達方式. 將 Thm4,5,6 串起來變成: 一個最佳解滿足 KKT 的表達方式. (當然前提是有滿足 LICQ) 打這篇累到不想有結論的結論不想有結論了, 乾脆來碎碎念吧. 最佳化是我唸書期間很愛的一門科目, 當時愈是唸它, 愈是不懂. 以前也很愛看 Stephen P. Boyd 的 convex opt 課程, 但現在腦袋裡似乎只剩教授名字和課程名字了. 喔對了, 我還記得一件事情, 就是在 convex 問題時, KKT condition 會變成 充要條件. 至於細節, 恩… [待補充]: 我有找到當時的筆記關於 convex 問題下的 KKT conditions, 以及它的 dual problem 討論. 只能說: 1. 學生可以很自由掌控自己的時間, 工作後的時間都是零碎的阿! 根本沒法長時間死嗑某一樣學科! 2. 當碼農工程師數學真的會退步, 碼農友碼農的好, 但希望自己也別忘記重要的數學觀念了. Reference Numerical Optimization 2nd edition, Jorge Nocedal 筆記 for the proof of Thm5,6","tags":[{"name":"Nonlinear Constraint Optimization","slug":"Nonlinear-Constraint-Optimization","permalink":"https://bobondemon.github.io/tags/Nonlinear-Constraint-Optimization/"},{"name":"KKT","slug":"KKT","permalink":"https://bobondemon.github.io/tags/KKT/"}]},{"title":"TF Notes (2), Speedup and Benchmark with Two GPU Cards","date":"2017-11-01T12:28:49.000Z","path":"2017/11/01/TF-Notes-Speedup-and-Benchmark-with-Two-GPU-Cards/","text":"這篇文章實作了官網的 同步式 data Parallelism 方法 ref，並且與原本只用一張GPU做個比較。實驗只使用兩張卡，多張卡方法一樣。主要架構如下圖 by TF 官網: 兩張卡等於是把一次要計算的 mini-batch 拆成兩半給兩個 (相同的) models 去並行計算 gradients，然後再交由 cpu 統一更新 model。詳細請自行參考官網。下面直接秀 Codes 和結果。 Machine Spec.GPU 卡為 Tesla K40c CPU 為 Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz 單 GPU 跑 MNIST直接上 Codes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201import gzipimport osimport tempfileimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.layers import flattenfrom tensorflow.examples.tutorials.mnist import input_datafrom sklearn.utils import shuffle\"\"\"Data Loading\"\"\"dataPath='../dataset/MNIST_data/'mnist = input_data.read_data_sets(dataPath, one_hot=True)# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]img_width = 28img_height = 28images = mnist.train.imagesimg_num, _ = images.shapeimages = np.reshape(images,(img_num,img_height,img_width))images = images[...,np.newaxis]print('(Input to CNN) Images with shape &#123;&#125;'.format(images.shape))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# load the validation setimages_valid = mnist.validation.imagesimg_num_valid = len(images_valid)images_valid = np.reshape(images_valid,(img_num_valid,img_height,img_width))images_valid = images_valid[...,np.newaxis]labels1Hot_valid = mnist.validation.labelsprint('Having %d number of validation images' % img_num_valid)# plotting sample imagesplt.figure(figsize=(15,5))for i in np.arange(2*7): random_idx = np.random.randint(0,img_num) plt.subplot(2,7,i+1) plt.imshow(images[random_idx][...,0],cmap='gray') plt.title(labels[random_idx][0])\"\"\"First define the hyper-parameters\"\"\"# Hyper-parametersEPOCHS = 30BATCH_SIZE = 512rate = 0.001drop_out_keep_prob = 0.5ksize = 5cnn_depth_list = [16, 32]mlp_depth_list = [256, 128]cNum = 1\"\"\"Define the input output tensors\"\"\"# using one-hot decodingx = tf.placeholder(tf.float32, (None, img_height, img_width, cNum))one_hot_y = tf.placeholder(tf.int32, (None, n_classes))#one_hot_y = tf.one_hot(y, n_classes)keep_prob = tf.placeholder(tf.float32) # probability to keep units\"\"\"Define the graph and construct it\"\"\"class MNISTCNN: def __init__(self, ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes): self._ksize = ksize self._cnn_depth_list = cnn_depth_list self._mlp_depth_list = mlp_depth_list self._img_height = img_height self._img_width = img_width self._cNum = cNum self._n_classes = n_classes self._mu = 0 self._sigma = 0.1 def create(self,x,keep_prob): conv = self._conv(x, self._cNum, self._ksize, self._cnn_depth_list[0], 'conv1') # Pooling. Input = 24x24xlayer_depth['layer_1']. Output = 12x12xlayer_depth['layer_1']. conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') for lidx in range(1,len(self._cnn_depth_list)): conv = self._conv(conv, self._cnn_depth_list[lidx-1], self._ksize, self._cnn_depth_list[lidx], 'conv&#123;&#125;'.format(lidx+1)) conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') fc = flatten(conv) bsize, fc_in_dim = fc.shape fc = self._fc(fc,fc_in_dim,self._mlp_depth_list[0],'fc1') fc = tf.nn.dropout(fc, keep_prob) # dropout for lidx in range(1,len(self._mlp_depth_list)): fc = self._fc(fc,self._mlp_depth_list[lidx-1],self._mlp_depth_list[lidx],'fc&#123;&#125;'.format(lidx+1)) fc = tf.nn.dropout(fc, keep_prob) # dropout with tf.variable_scope('logits') as scope: logits_w = tf.get_variable('logits_w', shape=[self._mlp_depth_list[-1],self._n_classes],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) logits_b = tf.get_variable('logits_b', shape=[self._n_classes],\\ initializer=tf.zeros_initializer) logits = tf.nn.xw_plus_b(fc, logits_w, logits_b, name=scope.name) print(logits.shape) return logits def _conv(self, x, in_depth, ksize, out_depth, scope_name, relu=True): bsize,h,w,cNum = x.shape assert(h-(ksize-1)&gt;=1) assert(w-(ksize-1)&gt;=1) with tf.variable_scope(scope_name) as scope: # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=(ksize, ksize, in_depth, out_depth),\\ initializer=tf.random_normal_initializer(self._mu,self._sigma)) biases = tf.get_variable('biases', shape=(out_depth),initializer=tf.zeros_initializer) out = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding='VALID',name=scope.name) + biases if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out def _fc(self, x, num_in, num_out, scope_name, relu=True): \"\"\"Create a fully connected layer.\"\"\" with tf.variable_scope(scope_name) as scope: # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=[num_in, num_out],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) biases = tf.get_variable('biases', [num_out],\\ initializer=tf.zeros_initializer) # Matrix multiply weights and inputs and add bias out = tf.nn.xw_plus_b(x, weights, biases, name=scope.name) if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out mnistCNN = MNISTCNN(ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes)logits = mnistCNN.create(x,keep_prob)\"\"\"Define loss and optimizer\"\"\"cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits)loss_operation = tf.reduce_mean(cross_entropy)optimizer = tf.train.AdamOptimizer(learning_rate = rate)training_operation = optimizer.minimize(loss_operation)# Define accuracy evaluation# calculate the average accuracy by calling evaluate(X_data, y_data)correct_prediction = tf.equal(tf.argmax(logits, axis=1), tf.argmax(one_hot_y, axis=1))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))def evaluate(X_data, y_data): num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] accuracy = sess.run(accuracy_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y, keep_prob: 1.0&#125;) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples \"\"\"Run Session\"\"\"### Train your model here.import timeif not os.path.isdir('./models'): os.makedirs('./models')#saver = tf.train.Saver()accumulate_time = 0.0with tf.Session() as sess: sess.run(tf.global_variables_initializer()) num_examples = img_num print(\"Training...\") print() train_accuracy = np.zeros(EPOCHS) validation_accuracy = np.zeros(EPOCHS) for i in range(EPOCHS): stime = time.time() acc_train_accuracy = 0 X_train, y_train = shuffle(images, labels1Hot) for offset in range(0, num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] sess.run(training_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y, keep_prob: drop_out_keep_prob&#125;) etime = time.time() accumulate_time += etime - stime validation_accuracy[i] = evaluate(images_valid, labels1Hot_valid) print(\"EPOCH &#123;&#125; ...\".format(i+1)) print(\"Validation Accuracy = &#123;:.3f&#125;\".format(validation_accuracy[i])) print() print('Cost time: ' + str(accumulate_time) + ' sec.') 同步式 data Parallelism 在兩張 GPU 跑 MNIST一樣直接上 Codes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287import gzipimport osimport tempfileimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.layers import flattenfrom tensorflow.examples.tutorials.mnist import input_datafrom sklearn.utils import shuffle\"\"\"Data Loading\"\"\"dataPath='../dataset/MNIST_data/'mnist = input_data.read_data_sets(dataPath, one_hot=True)# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]img_width = 28img_height = 28images = mnist.train.imagesimg_num, _ = images.shapeimages = np.reshape(images,(img_num,img_height,img_width))images = images[...,np.newaxis]print('(Input to CNN) Images with shape &#123;&#125;'.format(images.shape))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# load the validation setimages_valid = mnist.validation.imagesimg_num_valid = len(images_valid)images_valid = np.reshape(images_valid,(img_num_valid,img_height,img_width))images_valid = images_valid[...,np.newaxis]labels1Hot_valid = mnist.validation.labelsprint('Having %d number of validation images' % img_num_valid)plt.figure(figsize=(15,5))for i in np.arange(2*7): random_idx = np.random.randint(0,img_num) plt.subplot(2,7,i+1) plt.imshow(images[random_idx][...,0],cmap='gray') plt.title(labels[random_idx][0])\"\"\"First define the hyper-parameters\"\"\"# Hyper-parametersEPOCHS = 30BATCH_SIZE = 512rate = 0.001drop_out_keep_prob = 0.5ksize = 5cnn_depth_list = [16, 32]mlp_depth_list = [256, 128]cNum = 1\"\"\"Define the graph and construct it\"\"\"class MNISTCNN: def __init__(self, ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes): self._ksize = ksize self._cnn_depth_list = cnn_depth_list self._mlp_depth_list = mlp_depth_list self._img_height = img_height self._img_width = img_width self._cNum = cNum self._n_classes = n_classes self._mu = 0 self._sigma = 0.1 def create(self,x,keep_prob): conv = self._conv(x, self._cNum, self._ksize, self._cnn_depth_list[0], 'conv1') # Pooling. Input = 24x24xlayer_depth['layer_1']. Output = 12x12xlayer_depth['layer_1']. conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') for lidx in range(1,len(self._cnn_depth_list)): conv = self._conv(conv, self._cnn_depth_list[lidx-1], self._ksize, self._cnn_depth_list[lidx], 'conv&#123;&#125;'.format(lidx+1)) conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') fc = flatten(conv) bsize, fc_in_dim = fc.shape fc = self._fc(fc,fc_in_dim,self._mlp_depth_list[0],'fc1') fc = tf.nn.dropout(fc, keep_prob) # dropout for lidx in range(1,len(self._mlp_depth_list)): fc = self._fc(fc,self._mlp_depth_list[lidx-1],self._mlp_depth_list[lidx],'fc&#123;&#125;'.format(lidx+1)) fc = tf.nn.dropout(fc, keep_prob) # dropout with tf.variable_scope('logits') as scope: with tf.device('/cpu:0'): logits_w = tf.get_variable('logits_w', shape=[self._mlp_depth_list[-1],self._n_classes],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) logits_b = tf.get_variable('logits_b', shape=[self._n_classes],\\ initializer=tf.zeros_initializer) logits = tf.nn.xw_plus_b(fc, logits_w, logits_b, name=scope.name) print(logits.shape) return logits def _conv(self, x, in_depth, ksize, out_depth, scope_name, relu=True): bsize,h,w,cNum = x.shape assert(h-(ksize-1)&gt;=1) assert(w-(ksize-1)&gt;=1) with tf.variable_scope(scope_name) as scope: with tf.device('/cpu:0'): # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=(ksize, ksize, in_depth, out_depth),\\ initializer=tf.random_normal_initializer(self._mu,self._sigma)) biases = tf.get_variable('biases', shape=(out_depth),initializer=tf.zeros_initializer) out = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding='VALID',name=scope.name) + biases if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out def _fc(self, x, num_in, num_out, scope_name, relu=True): \"\"\"Create a fully connected layer.\"\"\" with tf.variable_scope(scope_name) as scope: with tf.device('/cpu:0'): # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=[num_in, num_out],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) biases = tf.get_variable('biases', [num_out],\\ initializer=tf.zeros_initializer) # Matrix multiply weights and inputs and add bias out = tf.nn.xw_plus_b(x, weights, biases, name=scope.name) if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out mnistCNN = MNISTCNN(ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes)# Averaging gradients for all tower models on GPUdef average_gradients(tower_grads): \"\"\"Calculate the average gradient for each shared variable across all towers. Note that this function provides a synchronization point across all towers. Args: tower_grads: List of lists of (gradient, variable) tuples. The outer list is over individual gradients. The inner list is over the gradient calculation for each tower. Returns: List of pairs of (gradient, variable) where the gradient has been averaged across all towers. \"\"\" average_grads = [] for grad_and_vars in zip(*tower_grads): # Note that each grad_and_vars looks like the following: # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN)) grads = [] for g, _ in grad_and_vars: # Add 0 dimension to the gradients to represent the tower. expanded_g = tf.expand_dims(g, 0) # Append on a 'tower' dimension which we will average over below. grads.append(expanded_g) # Average over the 'tower' dimension. grad = tf.concat(axis=0, values=grads) grad = tf.reduce_mean(grad, 0) # Keep in mind that the Variables are redundant because they are shared # across towers. So .. we will just return the first tower's pointer to # the Variable. v = grad_and_vars[0][1] grad_and_var = (grad, v) average_grads.append(grad_and_var) return average_grads# Construct model for each GPU, where variables are shared/updated by CPUwith tf.device('/cpu:0'): optimizer = tf.train.AdamOptimizer(learning_rate = rate)# Calculate the gradients for each model tower.tower_grads = []logits_list = []feed_x = []feed_one_hot_y = []keep_prob = tf.placeholder(tf.float32) # probability to keep unitswith tf.variable_scope(tf.get_variable_scope()): for i in range(2): with tf.device('/gpu:%d' % i): x = tf.placeholder(tf.float32, (None, img_height, img_width, cNum)) feed_x.append(x) one_hot_y = tf.placeholder(tf.int32, (None, n_classes)) feed_one_hot_y.append(one_hot_y) logits = mnistCNN.create(x,keep_prob) logits_list.append(logits) cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits) loss_op = tf.reduce_mean(cross_entropy) tf.get_variable_scope().reuse_variables() # Calculate the gradients for each batch of data on this model tower. grads = optimizer.compute_gradients(loss_op) # Keep track of the gradients across all towers. tower_grads.append(grads)with tf.device('/cpu:0'): # We must calculate the mean of each gradient. Note that this is the # synchronization point across all towers. grads = average_gradients(tower_grads) # Apply the gradients to adjust the shared variables. apply_gradient_op = optimizer.apply_gradients(grads) training_op = apply_gradient_op \"\"\"Prediction/Inference Part\"\"\"# Define accuracy evaluation, calculate the average accuracy by calling evaluate(X_data, y_data)# Using model that in the First GPU to calculatecorrect_prediction = tf.equal(tf.argmax(logits_list[0], axis=1), tf.argmax(feed_one_hot_y[0], axis=1))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))def evaluate(X_data, y_data): num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] accuracy = sess.run(accuracy_operation, feed_dict=&#123;feed_x[0]: batch_x, feed_one_hot_y[0]: batch_y, keep_prob: 1.0&#125;) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples \"\"\"Run Session\"\"\"# &#123;feed_x[0]:batch_x_1, feed_x[1]:batch_x_2,\\# feed_one_hot_y[0]:batch_y_1, feed_one_hot_y[1]:batch_y_1, keep_prob:drop_out_keep_prob&#125;def gen_feed_dict(batch_x, batch_y, drop_out_keep_prob): assert(len(batch_x)==len(batch_y)) assert(len(batch_x)%2==0) data_num = int(len(batch_x)/2) rtn_dict = &#123;&#125; rtn_dict[feed_x[0]] = batch_x[:data_num] rtn_dict[feed_x[1]] = batch_x[data_num:] rtn_dict[feed_one_hot_y[0]] = batch_y[:data_num] rtn_dict[feed_one_hot_y[1]] = batch_y[data_num:] rtn_dict[keep_prob] = drop_out_keep_prob return rtn_dict ### Train your model here.import timeif not os.path.isdir('./models'): os.makedirs('./models')#saver = tf.train.Saver()accumulate_time = 0.0with tf.Session() as sess: sess.run(tf.global_variables_initializer()) num_examples = img_num print(\"Training...\") print() train_accuracy = np.zeros(EPOCHS) validation_accuracy = np.zeros(EPOCHS) stime = time.time() for i in range(EPOCHS): stime = time.time() acc_train_accuracy = 0 X_train, y_train = shuffle(images, labels1Hot) for offset in range(0, num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] feed_dict = gen_feed_dict(batch_x, batch_y, drop_out_keep_prob) sess.run(training_op, feed_dict=feed_dict) etime = time.time() accumulate_time += etime - stime validation_accuracy[i] = evaluate(images_valid, labels1Hot_valid) print(\"EPOCH &#123;&#125; ...\".format(i+1)) print(\"Validation Accuracy = &#123;:.3f&#125;\".format(validation_accuracy[i])) print('Cost time: ' + str(accumulate_time) + ' sec.') 幾個注意處: 記得建立 variables (tf.get_variable) 時要使用 with tf.device(&#39;/cpu:0&#39;): 確保變量是存在 cpu 內 可以跑一小段 code: 12with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: sess.run(tf.global_variables_initializer()) 來觀察變量是否正確放在 cpu 上。 延續 2. 若使用 jupyter notebook 可以這樣做 jupyter notebook &gt; outputlog，執行完 2. 的 code 接著 cat outputlog | grep &#39;cpu&#39; 觀察變量是否存在。 使用 collections (如下) 來確認變數有正確分享 (養成好習慣)123trainable_collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)global_collection = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)print('Without Scope: len(trainable_collection)=&#123;&#125;; len(global_collection)=&#123;&#125;'.format(len(trainable_collection),len(global_collection))) Benchmark Results batch size = 128 時，使用兩張 GPU 花的時間為一張的 0.78 倍。而 batch size = 512 時的效果更明顯，為 0.69 倍。 一點小結論這種同步的架構適合在 batch size 大的時候，效果會更明顯。實驗起來兩張卡在 512 batch size 花的時間在一張卡的 0.7 倍。不過相比使用兩張卡，一張卡其實有一點優勢是在變量全部放在 GPU 上，因此省去了 CPU &lt;–&gt; GPU 的傳輸代價。這也是主要只到 0.7 倍，而沒有接近 0.5 倍的關鍵原因。 Reference Tensorflow 官網 同步式 data Parallelism 方法 Tensorflow github cifar10_multi_gpu_train.py","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://bobondemon.github.io/tags/TensorFlow/"}]},{"title":"A Toy Example for Teacher Student Domain Adaptation","date":"2017-10-22T02:19:54.000Z","path":"2017/10/22/A-Toy-Example-for-Teacher-Student-Domain-Adaptation/","text":"看了這篇 2017 Microsoft AI and Research 的文章 “Large-Scale Domain Adaptation via Teacher-Student Learning“ 覺得滿有意思的，加上很容易實作，因此就分析一下這篇的可行性。 設計了一個 MNIST Toy Example 來展示 T/S Learning 的能力，自己也想知道這個方法有多可靠。相關的實驗 code 請參考 github TS Learning Methods介紹一下 TS Learning 的方法。他要解決的問題描述如下 假設我們已經有了一個訓練好的語音辨識模型，現在要辨識遠場的聲音，原 Domain (近場錄製的聲音)，可能效果就會不好。要解決最直接的方法就是重新錄製遠場語料，錄製的過程中很容易可以取得同時有近場和遠場未標記的語料 (放兩個麥克風，一個近一個遠)，不過關鍵是標記成本太高。因此這篇就是想利用未標記的語料直接針對原 Domain 的模型 Adapt 到新的 Domain 上。 以上面論文中的圖來說，左邊 Teacher network 只能在 Source Domain 有好的辨識能力，目標是希望得到右邊的 Student network 能在 Target Domain 針對同樣問題也有好的辨識能力。論文方法是一開始先將 Teacher network 拷貝一份給 Student network ，接著就開始餵 parallel data 給兩個 networks。 所謂 parallel data 意思是相同的資料來源，但是在不同 domain 蒐集，例如同一個人講同一句話，一個近場麥克風蒐集到，另一個遠場蒐集到。目標函式就是希望兩個 network 的後驗概率相同 (兩者的後驗概率平方誤差為0)，而我們只更新 Student network。在實作上不會使用後驗概率來計算兩個 network 的誤差，會使用未經過 softmax 的那層，也就是一般說的 logits 來計算。原因簡單說明如下: softmax 會將同樣是 negative 的類別的機率都壓到很低，但是 negative examples 也有分好壞，讀者可以試試 [10,2,1] 經過 softmax 後， 2 跟 1 之間的差異會被抹平。因此好的做法是，不要使用 softmax ，而是使用 logits。 Hinton 在這篇論文裡修改了 softmax 函式，多了一個 temperature $T$，論文裡推導這樣修改的 softmax，其實跟目標函式使用 logits 的平方誤差是一樣的 (在 “T跟logits差異很大” 且 “logits的分布均值為0” 的條件下) 這樣做的物理意義就相當於，將 “某聲音的遠場表現在 Student network 眼裡”，視為跟 “該聲音的近場表現在 Teacher network 眼裡” 認定為相同一件事情。因此就不需要針對 data 做標記了，只需要拿到這樣的一大堆 parallel data 就可以，而這很容易。 附上論文上的步驟如下: 演算法就這樣而已，很單純吧。但是究竟有多靠普? 好奇心下，就用 MNIST 設計了 toy example，就是接下來的內容囉。 MNIST Toy Example for TS Learning實驗設定 and Teacher Network首先設定兩個 Domain 為: 一個原圖 (原世界)，另一個上下顛倒的圖 (上下顛倒的世界)。 Teacher network 是一個很簡單的 “6” 和 “9” 的辨識器，當然是在原世界訓練好的。如果直接拿 teacher network 去看顛倒的 6，期望它認出一樣是 6 是辨識不出來的! (同樣期望 teacher network 看出顛倒的 9 仍然是 9 也是辦不到的) 之所以會選 6 和 9，是因為就算上下顛倒，顛倒的 6 和正向的 9 看起來仍然是不同的! 同樣的，顛倒的 9 和正向的 6 一樣看起來不同 ! 我們得到的 Teacher network 辨識情況如下: 12345678910Training...EPOCH 1 ...Train Accuracy = 0.967; Flip Accuracy = 0.098EPOCH 2 ...Train Accuracy = 0.999; Flip Accuracy = 0.151EPOCH 3 ...Train Accuracy = 0.999; Flip Accuracy = 0.110 明顯看到辨識率接近 100%，但是一旦上下顛倒，辨識率只剩 10%。有意思的是，由於我們只有兩個類別，對於上下顛倒的辨識率剩10%可以看做: 顛倒的 6，會被認成 9，而顛倒的 9 會被認為 6。但事實上，顛倒的 6 和 9 還是不一樣。 Student network 訓練我們將 MNIST 其他影像上下顛倒，做出 parallel dataset，然後按照論文的做法做 unsupervised training。有趣的是得到結果如下: 12345678910111213141516171819EPOCH 1 ...Acc loss = 3.871242271944786Train Accuracy = 0.156; Flip Accuracy = 0.998EPOCH 2 ...Acc loss = 0.40557907682784994Train Accuracy = 0.101; Flip Accuracy = 0.999EPOCH 3 ...Acc loss = 0.3005437100890939Train Accuracy = 0.103; Flip Accuracy = 0.999EPOCH 4 ...Acc loss = 0.2651689475203995Train Accuracy = 0.097; Flip Accuracy = 0.999EPOCH 5 ...Acc loss = 0.23342516055794454Train Accuracy = 0.116; Flip Accuracy = 0.999 Student network 可以成功辨識 顛倒的 6 和顛倒的 9 了! 注意，我們從來沒有給過 Student network 顛倒的 6 和顛倒的 9 這些訓練資料! 但是現在它有能力辨識這兩種圖了! 但是同樣的，如果給 student network 看一個正向的 6，在他的眼哩，看起來就如同 teacher network 看到 9 一樣。 也就是說，Student network 失去了原 Domain 的辨識能力。 這與論文原作者的結論不大一樣。 用 parallel data 非監督學習到底學到了什麼? 給 T/S 網路看過很多很多的 parallel data 後，Teacher 眼裡的圖，在 Student 眼裡看起來就反過來，反之亦然。因此這時候如果給 Student network 看一個 “正向的6”，它會認為: 啊!這在 Teacher 眼裡看到的是一個顛倒的 6 。(而 teacher network 會將顛倒的 6 看做是 9) 因此我認為，Student netowrk 很容易失去原先 domain 的辨識能力，就像這個例子 student network 無法認出正向的 6 一樣。 Summary如何讓一個 network 同時有原 Domain 和新 Domain 的辨識能力呢 ? 以上面的 toy example 為例，就是辨識兩個 classes class 1: 6 and 顛倒的6class 2: 9 and 顛倒的9 最直覺的做法，就是 T and S models 都跑一次辨識，然後將兩個後驗概率加起來後算 argmax。缺點就是 model size 立馬變成兩倍。 怎麼讓模型 size 不要變成兩倍呢? 簡單想了一個方式，就是讓 student model 改成這樣的模型: 其中 M model 的部分負責將 上下顛倒的 domain 轉換成原 domain 的 input，然後這樣的 input 就可以原封不動地用 teacher model 去辨識。剛好這個問題其實用一個 permuation matrix 可以做上下顛倒，因此實驗上就直接使用一個 linear layer (沒有 activation function)，當然 backprob 算出來的不會正好是 permutation matrix 就是了。 收斂情況如下: 基本上比原先要慢，因為原來是所有的 weights 都可以調整，而現在只能動一個 linear layer 12345678910111213141516171819EPOCH 1 ...Acc loss = 9.52696829760659Train Accuracy = 0.460; Flip Accuracy = 0.806EPOCH 2 ...Acc loss = 3.6580730849143146Train Accuracy = 0.364; Flip Accuracy = 0.955EPOCH 3 ...Acc loss = 2.454553008463332Train Accuracy = 0.304; Flip Accuracy = 0.980EPOCH 4 ...Acc loss = 1.823352760733923Train Accuracy = 0.277; Flip Accuracy = 0.988EPOCH 5 ...Acc loss = 1.4707165408316494Train Accuracy = 0.235; Flip Accuracy = 0.992 這樣做法雖然 model size 小了很多，但是要同時辨識正的和顛倒的仍然要跑兩遍的 model。 有沒有方法結合 TS learning unsupervised 的方式，且同時兼顧兩邊的 domain 辨識能力呢? 就再思考看看囉。 Reference Large-Scale Domain Adaptation via Teacher-Student Learning Distilling the Knowledge in a Neural Network Toy Example github","tags":[{"name":"ML","slug":"ML","permalink":"https://bobondemon.github.io/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://bobondemon.github.io/tags/Deep-Learning/"},{"name":"Adaptation","slug":"Adaptation","permalink":"https://bobondemon.github.io/tags/Adaptation/"}]},{"title":"Word Embeddings (Encoder-Decoder 架構)","date":"2017-09-07T13:22:53.000Z","path":"2017/09/07/Word-Embeddings-and-Encoder-Decoder-Neural-Net/","text":"From Sparse Vector to Embeddings with Encoder–Decoder Structure 求 Embeddings Encoder–Decoder 結構 字典向量若我們字典裡有 $N$ 個 words, 第 $i$ 個字 $w^i$ 應該怎麼表示呢? 通常使用 one-hot vector 來表示: 把 $w^i$ 變成一個長度 $N$ 的向量 $x^i$。 恭喜! 有了 vector 我們就可以套用數學模型了。 問題是這樣的向量太稀疏了，尤其是當字典非常大的時候。 稀疏向量對於模型訓練很沒有效率。 我們需要轉換到比較緊密的向量，通常稱為 embedding。 下圖舉例將 $x$ 對應到它的緊密向量 $e$, 緊密向量有 embed_dim 維度 先假設已知如何對應到緊密向量已知一個 N * embed_dim 的矩陣 $E$，第 $i$ 個 row $e^i$ 就是 $w^i$ 的 embedding。 我們就可以使用 $e$ 來代替原先的稀疏向量 $x$ 進行訓練，讓訓練更好更容易。 以一個語言模型來說，使用 LSTM 模型如下: 恩，這樣大功告成，我們的模型可以順利訓練 …. ?? 不對，$E$ 這個 lookup table 怎麼決定? Lookup Table 使用矩陣相乘答案是讓模型自己訓練決定。要更了解內部運作，我們先將 lookup table 使用矩陣相乘的方式來看。 所以使用 lookup table LSTM 的語言模型變成如下 等等，矩陣相乘不就跟 neural net 一樣嗎? 這樣看起來這個 lookup table $E$ 就是一層的類神經網路而已 (沒有 activation function)。 我們用 LL (Linear Layer) 來代表，$E$ 就是 LL 的 weight matrix。 表示成 neural net 的方式，我們就直接可以 Backprob 訓練出 LL 的 weight $E$ 了。而 $E$ 就是我們要找的 embeddings。 Tensorflow 中做這樣的 lookup table 可以使用 tf.nn.embedding_lookup()。 Embedding 的作法可參考 tf 官網此處。 LL很弱怎麼辦?只用一層線性組合 (LL) 就想把特徵擷取做到很好，似乎有點簡化了。 沒錯，我們都知道，特徵擷取是 Deep neural net 的拿手好戲，所以我們可以將 LL 換成強大的 CNN。 這種先經過一層特徵擷取，再做辨識，其實跟 Encoder – Decoder 的架構一樣。 都是先經過 Encoder 做出 embeddings，接著使用 Embeddings decode 出結果。 Encoder 如果也採用 RNN 的話基本上就是 sequence-to-sequence 的架構了。 基本上拓展一下，對圖或影像做 Encode，而 Decoder 負責解碼出描述的文字。或是語言翻譯，語音辨識，都可以這麼看待。 Reference Embedding tf 官網 link Sequence to sequence learning link Udacity lstm github colah lstm","tags":[{"name":"ML","slug":"ML","permalink":"https://bobondemon.github.io/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://bobondemon.github.io/tags/Deep-Learning/"},{"name":"Embedding","slug":"Embedding","permalink":"https://bobondemon.github.io/tags/Embedding/"}]},{"title":"AutoEncoder","date":"2017-08-26T08:38:22.000Z","path":"2017/08/26/AutoEncoder/","text":"使用 MNIST and notMNIST 做了一個 AutoEncoder with Fully Connected DNN 的實驗。 依序將實驗結果根據如下步驟顯示出來，程式碼可以參考 [github] Data Loading and Plotting AutoEncoder Graph Constructiona. Define the input output tensorsb. Define the graph and construct itc. Define loss and optimizer Run Session Show some reconstructed images Plot Embeddings Do Image Generation by Decoder Data Loading and PlottingMNIST training data 有 55000 筆資料，是一個 28x28 的 image，值的範圍是 [0~1]，因此會對 input 都減去 0.5 正規化。 而 notMNIST 整理過後有 200000 筆，同樣也是 28x28 的 image，但值的範圍已經是 [-0.5~0.5]。值得一提的是，此資料還參雜著一些錯誤，如下圖就可發現，第二列的第二個應為 J，但是標記是 A。因此 notMNIST 相對來說很挑戰，但我們一樣可以看到 AutoEncoder 也會做出一些合理的壓縮。 AutoEncoder Graph ConstructionDefine the input output tensorsInput x 與 Output y 都是一樣 (沒有要做 Denoise AutoEncoder)，其中 code 是建立 Decoder 時的 input tensor。 1234x = tf.placeholder(tf.float32, (None, img_dim))y = tf.placeholder(tf.float32, (None, img_dim))embedding_dim = 2code = tf.placeholder(tf.float32, (None, embedding_dim)) Define the graph and construct it針對 Encoder 和 Decoder 都使用同一組參數，這樣的好處是參數量直接少約一半，同時減少 overfitting 的機會。當然我們沒有理由一定要將參數綁再一起，可以各自用自己的方法 (參數、模型結構) 去 Encode 和 Decocde。結構如下: Define loss and optimizer注意到 loss 的定義除了原來的影像重建誤差之外，還多了一個 embeddings 的 l2-norm。這是為了希望在 embedding space 上 encode 之後都接近 0，減少那種很大的 outliers 出現。參考李宏毅 Deep AutoEncoder 123loss_op = tf.reduce_sum(tf.pow(tf.subtract(reconstruct_auto, y), 2.0)) + l2_weight* tf.reduce_sum(tf.pow(embedded_auto, 2.0))optimizer = tf.train.AdamOptimizer(learning_rate = rate)training_op = optimizer.minimize(loss_op) Run SessionAdam optimizer 跑了 100 個 epochs Show some reconstructed images隨機選幾個 MNSIT 的重建圖: 隨機選幾個 notMNSIT 的重建圖: 可以看到 notMNIST 果然難多了。 Plot EmbeddingsMNIST 針對所有 training data 求得的 2-d embeddings 如下: notMNIST 針對所有 training data 求得的 2-d embeddings 如下: 如果只要做到 unsupervised dimension reduction 的話，使用 t-SNE 求得的 embedding 會比上圖都好看很多。但 t-SNE 沒有 Decoder，無法給定一個 embedding 去求回原先的 image。而這種 Encoder - Decoder 結構就相對彈性很多。 t-SNE 的 MNIST 圖如下: Do Image Generation by Decoder我們針對 Embedding Space 的一個區域去等距取出很多點，然後使用 Decoder 去 decode 出 image 來。 MNIST 的範圍選擇為， x 軸和 y 軸 [-1~1] 間隔 0.2，共 100 個點。(可參考上面 embedding space 了解選擇的範圍) notMNIST 的範圍選擇為， x 軸和 y 軸 [-2~2] 間隔 0.2，共 400 個點。(可參考上面 embedding space 了解選擇的範圍) 可以發現 embedding space 的兩個維度具有某些意義在! Reference 李宏毅 Deep AutoEncoder Distill t-SNE Reducing the Dimensionality of Data with Neural Networks (Hinton 2006) 本文之 [github]","tags":[{"name":"ML","slug":"ML","permalink":"https://bobondemon.github.io/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://bobondemon.github.io/tags/Deep-Learning/"},{"name":"auto-encoder","slug":"auto-encoder","permalink":"https://bobondemon.github.io/tags/auto-encoder/"}]},{"title":"Notes for Model Predictive Control","date":"2017-06-28T12:01:19.000Z","path":"2017/06/28/ModelPredictiveControl/","text":"從一開始決定上課後，經過了半年終於來到 Udacity Term2 最後一個 Project 了。只能說盡量讓自己把每一個做的 project 都寫一篇 blog 記錄，但這陣子時間真的不夠用，所以這篇就從 high level 的角度瀏覽一下內容。 目的我們用上圖來說明目的，MPC 要做的事情，就是給定一個指定的 reference trajectory (黃色的曲線，通常用一個 3rd polynomail 表示)，我們經由 motion model 來計算出最佳的控制 ($(\\delta,a)$分別表示車子輪子的角度跟加速度)，最佳的意思就是這樣的控制會產生出一個 predicted trajectory (綠色的曲線) 使得跟 reference trajectory cost 最小。這就等於將問題轉換成一個 nonlinear constraint optimization problem 了。另外剛才提到的控制項 $(\\delta,a)$，其中的 $\\delta$ 為下圖的 Wheel Orientation 角度:而 $a$ 表示加速度，正值是踩油門，負值是踩煞車。這邊我們當然假設油門和煞車同時只會有一個存在啦，開車技術沒這麼好。 Motion Model 這 6 個 states $(x,y,\\psi,v,cte,e\\psi)$ 分別表示 (車子x座標, 車子x座標, 車子heading角度, 車子速度, Cross Track Error, Error of 車子角度)CTE 或稱 XTE 是 reference position 跟 actual position 之間的誤差 同理 $e\\psi$ 就是 reference 的角度跟實際角度的差值了，注意到，由於 reference trajectory 可能是一個 3rd polynomail，我們可以算切線來求得 reference 的角度。 Tools of Nonlinear Constraint Opt兩個主要的 tool: IpoptInterior Point OPTimization，用來解 nonlinear constraint opt 問題。 CppAD在使用 Ipopt 的時候，需要計算 function 的 gradients，而 CppAD 可以幫我們自動計算。 一個很棒的使用兩個 tools 解 opt 問題的範例: link Consider with Latency通常下了一道 actuator 命令 (例如加速度要多少、輪子角度要多少)，到實際上車子運作會有一個 delay，而 Udacity simulator 設定這個 latency 是 0.1 second。 這個 latency 在車子速度較快的時候，影響會很大，導致車子無法正確開完。一個簡單的解法就是我們利用 motion model 去預測經過 latency 後的車子 states，然後後面所有流程都一模一樣即可。 Results [Video] With considering latency: [Video] Without considering latency: Referencemy github 目前解鎖成就","tags":[{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"},{"name":"Model Predictive Control","slug":"Model-Predictive-Control","permalink":"https://bobondemon.github.io/tags/Model-Predictive-Control/"},{"name":"Nonlinear Constraint Optimization","slug":"Nonlinear-Constraint-Optimization","permalink":"https://bobondemon.github.io/tags/Nonlinear-Constraint-Optimization/"}]},{"title":"Structure Perceptron and Structure SVM","date":"2017-05-20T01:41:27.000Z","path":"2017/05/20/Structure-Perceptron-and-Structure-SVM/","text":"記得當年念博的時候，對於SVM頗有愛，也覺得掌握度很高惹，就是 kernel method + convex optimization 的完美合體。直到某天看到 structureSVM，看了老半天實在不得要領，當時就放下沒再管了。多年後 (2015)，剛好台大李宏毅教授教的課程最後一堂 Project demo，有請我們部門介紹做的一些內容給學生，才看到了強大的李老師的課程內容。他所教的 structure learning/svm 實在有夠清楚，又非常 general，真的是強到爆! 本人又年輕，又謙虛，我的新偶像阿!附上一張我與新偶像的合照… XD 以下內容為筆記用，方便日後回想，來源 大都是李老師的內容。 A General Framework (Energy-based Model)一般來說 ML 要學習的是 $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$ 這樣的一個 mapping function，使得在學習到之後，能夠對於新的 input $x$ 求得預測的 $y=f(x)$。簡單的情況是沒問題，例如 binary classification、multi-class classification 或 regression。但是萬一要預測的是複雜得多的 output，譬如 $\\mathcal{Y}$ 是一個 tree、bounding box、或 sequence，原來的架構就很難定義了。 所以將要學的問題改成如下的架構。 Training: $F: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ Inference: Given an object $x$, $\\tilde{y}=argmax_y F(x,y)$ $F$ 可以想成用來計算 $(x,y)$ 的匹配度。而這樣的 function 也稱為 Energy-based model。好了，定義成這樣看起來沒什麼不同，該有的問題還是在，還是沒解決。沒關係，我們繼續看下去，先把三個重要的問題列出來。 Problem 1: 怎麼定義 $F(x,y)$ ? Problem 2: 怎麼解決 $argmax_y$ ? Problem 3: 怎麼訓練 $F(x,y)$ ? 這些問題在某種情況會變得很好解，什麼情況呢? 若我們將 $F(x,y)$ 定義成 Linear Model (Problem 1 用 linear 定義)，我們發現訓練變得很容易 (Problem 3 好解) !! 疑?! Problem 2呢? 先當作已解吧，ㄎㄎ。 Linear Model of $F(x,y)=w\\cdot \\phi(x,y)$我們先假裝 Problem 2 已解 (Problem 2 要能解 depends on 問題的domain，和feature的定義)，我們來看一下要怎麼訓練這樣的 linear model。首先用李老師課程的範例 (辨識初音) 的例子舉例，其中 $y$ 是一個 bounding box:這個例子有兩個 training pairs: $(x^1,\\hat{y}^1)$ 和 $(x^2,\\hat{y}^2)$，我們希望求得一個 $w$ 使得紅色的圓圈投影到 $w$ 上後要大於所有藍色的圓圈。同理，紅色的星星要大於所有藍色的星星。其實我們仔細想想，這問題跟 perceptron learning 非常類似，perceptron learning 在做的是 binary classification，而如果把每一筆 training data $(x^i,\\hat{y}^i)$ 和 $(x^i,y:y\\neq\\hat{y}^i)$ 當作是 positive and negative classes，剛好就是一個 bineary classification problem (雖然不同筆 training data 會有各自的 positive and negative 資料，但不影響整個問題)所以如果有解 (linear separable)，則我們可以使用 Structure Perceptron 在有限步驟內求解。 Structure Perceptron 證明的概念跟 perceptron 一樣，就是假設有解，解為 $\\hat{w}$，要求每一次的 update $w^k$，會跟 $\\hat{w}$ 愈來愈接近，也就是 $$\\begin{align} cos\\rho_k = \\frac{\\hat{w}\\cdot w^k}{\\Vert{\\hat{w}}\\Vert\\cdot\\Vert{w^k}\\Vert} \\end{align}$$ 要愈大愈好!但我們也知道 $cos$ 最大就 1，因此就有 upper bound，所以會在有限步驟搞定。詳細推倒步驟可參考李老師講義，或看 perceptron 的收斂證明。 Cost Function用 cost function 的角度來說，其實 perceptron 處理的 cost 是計算錯誤的次數，如果將 cost 的 function 畫出來的話，會是 step function，而無法做微分求解。因此通常會將 cost function 改成可微分的方式，例如 linear or quadratic or what ever continuous function。改成可微分就有很多好處了，可以針對 cost function 做很多需要的修改，這些修改包括 1. 對不同的錯誤有不同的懲罰 2. 加入 regularization term … 等等，我們等下會談到。 Picture is from Duda Pattern Classification 左圖就是原來的 perceptron cost，而右圖就是將 cost 改成 linear cost。Linear cost 可定義如下:$$\\begin{align} C=\\sum_{n=1}^N C^n \\\\ C^n=(max_y[w\\cdot\\phi(x^n,y)])-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$所以，就gradient descent下去吧，跟原來的 perceptron learning 改 cost function 一樣。 讓錯誤的程度能表現出來這是什麼意思呢? 原來的 cost function 對於每一個錯誤的情形都一視同仁，也就是在找那個 $w$ 的時候，只要錯誤的例子投影在 $w$ 上比正確的還要小就好，不在忽小多少，但事實上錯誤會有好壞之分。下面是一個李老師的例子，例如右邊黃色的框框雖然跟正確答案紅框框不同 (所以被當成錯誤的例子)，但有大致上都抓到初音的臉了，因此我們可以允許他跟正確答案較接近。因此 cost function 可以修改一下: $$\\begin{align} C=\\sum_{n=1}^N C^n \\\\ C^n=max_y[w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)]-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ $\\triangle(\\hat{y}^n,y)$ 定義了這個錯誤的例子額外的 cost (需&gt;=0)，以 bounding box 而言，舉例來說兩個 set A and B，$\\triangle(A,B)$ 可定義為 $1-/frac{A \\cap B}{A \\cup B}$。不過需要特別一提的是，多增加這個額外的定義，有可能使得原來容易解的 $argmax_y$ (Problem 2) 變得無法解，所以要注意。 Minimize the upper bound很有趣的一點是，在我們引入了 $\\triangle(\\hat{y}^n,y)$ (稱為 margin, 在後面講到 structure SVM 可以看得出來) 後，可以用另一個觀點來看這個問題。 假設我們希望能將 $C’$ 最小化: $$\\begin{align} \\tilde{y}^n=argmax_y{w\\cdot \\phi(x^n,y)} \\\\ C&apos;=\\sum_{n=1}^N{\\triangle(\\hat{y}^n,\\tilde{y}^n)} \\end{align}$$ 結果我們發現其實 $\\triangle(\\hat{y}^n,\\tilde{y}^n)\\leq C^n$，因而變成 而我們上面都是在最小化 $C$，所以其實我們在做的事情就是在最小化 $C’$ 的 upper bound。上界的證明如下:這種藉由最佳化 upper bound 的方式，在 adaboost 也見過。普遍來說，原來的式子不容易最佳化的時候，我們藉由定義一個容易最佳化的upper bound，然後最小化它。另外，EM 演算法也有類似的概念。 Regularization直接加入 norm-2 regularization:$$\\begin{align} C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N C^n \\\\ C^n=max_y[w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)]-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ Structure SVM先講結論: 上面 Linear Model 最後的 cost function (包含marginal and regularization terms) 就是等價於 SVM。原先問題 P1: Find $w$ that minimize $C$$$\\begin{align} C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N C^n \\\\ C^n=max_y[w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)]-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ 改寫後的問題 P2: Find $w$ that minimize $C$$$\\begin{align} C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N C^n \\\\ For \\forall{y}: C^n\\geq w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ 觀察 P2，我們注意到給定一個 $w$ 時，它最小的 $C^n$ 應該會是什麼呢? (找最小是因為我們要 minimize $C$) 譬如我要求 $x\\leq{ 5,1,2,10 }$ 這個式子的 $x$ 最小是多少，很明顯就是 $x=max{ 5,1,2,10 }$。因此式 P2 的式 (13) 可以寫成 P1 的式 (11)。寫成 P2 有什麼好處? 首先將 $C^n$ 改成 $\\epsilon^n$，然後再稍微改寫一下得到如下的問題: 問題 P3: Find $w,\\epsilon^n,\\epsilon^2,…,\\epsilon^N$ that minimize $C$$$C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N \\epsilon^n \\\\ For \\forall{y}\\neq{\\hat{y}^n}: w\\cdot (\\phi(x^n,\\hat{y}^n)-\\phi(x^n,y))\\leq \\triangle (\\hat{y}^n,y)-\\epsilon^n,\\epsilon^n\\leq 0$$ 注意到，對於一個 n-th training pair $(x^n,\\hat{y}^n)$ 和給定一個 $y\\neq\\hat{y}^n$ 來說，我們都會得到一個 linear constraint。可以將上面式子的 constant 用 a, b來表示變成:$w\\cdot a \\leq b - \\epsilon^n \\\\$ 發現了嗎? 對於變數 $w$ 和 $\\epsilon^n$ 來說，這就是一個 linear constraint。 眼尖的讀者，可能就會覺得 P3 很眼熟。沒錯!它跟 SVM 長很像! 讓我們來跟 SVM 的 Primal form (不是 dual form) 做個比較吧。可以發現有兩點不同，原 SVM from wiki 列出如下: Margin term 的不同，P3 的 margin 比較 general，可以根據每個 negative case 都有自己的 margin，而原來 binary SVM 的 margin 是定為 1。 Constraint 個數的不同，原 SVM 個數為 training data 的個數，但是 P3 的個數為無窮多個。 呼! 所以 P3 這個問題，就是 SVM 的 general 版本，我們也稱之為 Structure SVM，這裡終於跟 SVM 連結上了! Cutting Plane Algorithm原先 SVM 有限的 constraint 下，我們直接用一個 QP solver可以很快處理掉。但在 Structure SVM 有無窮多的 constraints 究竟要怎麼解? 是個問題。首先觀察到，其實很多 constraints 都是無效的。例如:所以這個演算法策略就是從一個空的 working set $\\mathbb{A}^n$ 出發，每次 iteration 都找一個最 violate 的 constraint 加進去，直到無法再加入任何的 constraint 為止。這裡其實有兩個問題要討論，第一個是什麼是最 violate 的 constraint? 第二個是，這演算法會收斂嗎? 難道不會永遠都找得到 violate 的 constraint 一直加入嗎?我們先把演算法列出來，再來討論上面這兩個問題。 Most Violated Constraint直接秀李老師的投影片注意到在 Degree of Violation 的推導中，所有與變數 $y$ 無關的部分可以去掉。因此我們最後可以得到求 Most Violated Constraint 就是在求 Problem 2 ($argmax_y$)。注意到其實我們一直 “先假裝 Problem 2 已解” Convergence?論文中證明如果讓 violate 的條件是必須超過一個 threshold 才算 violated，則演算法會在有限步驟內收斂。嚴謹的數學證明要參考 paper。 最後的麻煩: Problem 2 argmax這篇實在打太長了，以至於我想省略這個地方了 (淚)，事實上解 Problem 2 必須看問題本身是什麼，以 POS (Part-Of-Speech) tagging 來說，Problem 2 可用 Viterbi 求解。而這也就是李教授下一個課程 Sequence Labeling Problem。POS 如何對應到 Structure Learning 實在非常精彩! 真的不得不佩服這些人的智慧! 有興趣的讀者請一定要看李教授的投影片內容! 簡單筆記一下: POS 使用 HMM 方式來 model，例如一句話 x = “John saw the saw” 對應到詞性 y = “PN V D N”。然後把詞性當作 state, word 當作 observation，就是一個典型的 HMM 結構。接著使用 Conditional Random Field (CRF) 將 $log P(x,y)$ 對應到 $w\\cdot\\phi(x,y)$ 的形式，在 $P(x,y)$ 是由 HMM 定義的情形下，我們可以寫出相對應的 $\\phi(x,y)$ 該如何定義。因此就轉成一個 structure learning 的格式了。詳細請參考李老師課程講義。 彩蛋 我心愛的晏寶貝三歲生日快樂! 這幾天會有一個很重大的決定發生! Reference Hung-yi Lee ML courses Perceptron Learning Convergence Proof Duda Pattern Classification structureSVM 原始論文","tags":[{"name":"Structure SVM","slug":"Structure-SVM","permalink":"https://bobondemon.github.io/tags/Structure-SVM/"},{"name":"Structure Perceptron","slug":"Structure-Perceptron","permalink":"https://bobondemon.github.io/tags/Structure-Perceptron/"},{"name":"Hung-yi Lee","slug":"Hung-yi-Lee","permalink":"https://bobondemon.github.io/tags/Hung-yi-Lee/"}]},{"title":"統一的框架 Bayes Filter","date":"2017-05-10T14:15:16.000Z","path":"2017/05/10/Bayes-Filter-for-Localization/","text":"Bayes Filter Introduction前幾篇討論了很多 Kalman Filter 以及它相關的變形，如: EKF and UKF。這些方法我們都可以放在 Bayes Filter 的框架下來看，這麼做的話，KF 就只是其中一個特例了 (都是高斯分布的情形)。而如果我們只考慮幾個離散點的機率，並用蒙地卡羅法來模擬取樣的話，這種實作方式就會是 Particle Filter 。所以掌握了 Bayes Filter 背後的運作方式對於理解這些方法是很有幫助的。一些變數的意義仍然跟前幾篇一樣: z: measurement，也就是我們實際上經由 sensor 得到的測量值 (會有noise) x: state，我們希望估計出來的值，在 Localization 一般就是座標值 發現了嗎? 在上圖右 KF 的兩個步驟: Measurement Update 和 State Prediction 實際上就是上圖左邊的兩個數學式關係。搭配下圖文字一起看，Measurement Update 理解為得到一個觀察值 $z$ 後，我們用 Bayes Rule 可以估測出 state $x$ 的事後機率 $P(x|z)$，而該事後機率經由 motion model (eg. CTRV) 可以估測出下一個時間點的 x 機率分佈 $P(x’)$ (此步驟為 State Prediction)。得到新的 $P(x’)$ 就可以當成下一個時間點的事前機率，所以 Bayes rule 就可以接著下去重複此 loop。 與 Maximum a Posteriori (MAP) Adaptation 的關係事實上，這樣的框架也跟 MAP Adaptation 息息相關! 例如當事前機率是某些特別的機率分佈 (exponential family)，經由 Bayes rule 得到的事後機率，它的機率分佈會跟事前機率是同一類型的，(例如都是 Gaussian)。而這樣的選擇我們稱為 conjugate prior。由於 “事後” 與 “事前” 機率是同一種類型的機率分佈，因此把 “事後機率” 在當成下一次資料來臨時的 “事前機率” 也就很自然了! 這就是 MAP Adaptation 的核心概念，與 Bayes filter 一模一樣阿! Localization 詳細定義好的，我們來針對 Localization 詳細解釋吧，名詞定義如下: 觀測值 (time 1~t)、控制 (time 1~t)、和地圖 $m$ 都是假設已知，我們所不知的(要估測的)是目前 time t 的狀態值 $x$。舉例來說，一個一維的地圖如下:而觀測值 $z_{1:t}$ 如下:可以知道每一個時間點的觀測值是一個 dimension 為 k 的向量。 整個 Localization 的目的就是要計算對於位置 $x$ 我們有多少信心度，嚴謹地說，我們就是要計算如下:$$\\begin{align} bel(x_t)=p(x_t|z_{1:t},u_{1:t},m) \\end{align}$$ 意思是在已知目前所有的觀測值、控制、和地圖的情況下，位置 $x_t$ 的機率是多少，看數學式子的話，這不就正好就是 事後機率 嗎? 所以上面的 Bayes filter 架構就有發揮的空間了。另外一提的是，如果將地圖 $m$ 也當成未知的話，就是 SLAM 演算法了。(還沒有機會去讀這個演算法)下圖是一個一維的示意圖:但是要計算這樣的事後機率，必須要考慮從一開始到目前時間點的所有觀測值和控制，這樣的資料量實在太大，計算會非常沒有效率。因此，如果能只考慮目前的觀測值和控制，並用上一個時間的的事後機率就能推算出來的話，勢必會非常有效率。簡單來講，我們希望用遞迴的方式: 考慮 $bel(x_{t-1})$ 和目前的觀測值 $z_t$ 和控制 $u_t$ 就能推算 $bel(x)$。這就必須要簡化上面 $bel(x_t)$ 原始的定義了，要如何達到呢? 需借助 First-order Markov Assumption 。 First-order Markov Assumption 簡化 believe 假設目前的時間點為 $t$，我們知道要計算的 believe $bel(x_t)$ 代表事後機率，再套用 Bayes rule 之後，可以得到上面的表示。 事後機率 (Believe): 特別把時間點 t 的觀測值從原先定義拉出來，這是要強調我們在得到最新的觀測值 $z_t$ 後，希望去計算最新的 believe 事前機率 (Motion Model): 稱為 Motion Model 是因為假設我們目前在時間點 $t-1$，接著拿到下一次的控制 $u_t$ 後，我們希望估測出下一次的狀態值 $x_t$ 是什麼。有看過前幾篇的讀者應該馬上就能想到，可以利用 CTRV 之類的 motion model 去計算。 觀測值機率 (Observation Model): 這個是要計算當下的觀測值的機率分佈，這部分通常就是經由 sensor data 得到後，我們假設是高斯分布來計算。 Motion Model 遞迴 我們發現到，最後一行的結果，對照本文第一張圖的 State Prediction 式子是一樣的意思，差別只在一個是連續一個是離散。另一個差別是，此式子明顯寫出可以用上一次的事後機率做遞迴，所以第一張圖的 Measurement Update 藍色箭頭就這麼來的。 Observation Model 簡化 Bayes Filter Summary重新整理一下經由 “Motion Model 遞迴” 和 “Observation Model 簡化” 過後的事後機率 $bel(x_t)$，結果如下圖左。 (下圖右只是列出本文最開始的 Bayes Filter 式子來做對照)。結論是我們花了那麼大的力氣，用上了 1st Markov Assumption 去處理 Localization 的遞迴式子和簡化，結果不意外地就如同開始的 Bayes Filter 一樣。 另外，實作上如果所有的 pdf 都是高斯分布的話，結果就是 Kalman Filter。而如果透過 sampling 離散的狀態位置的話，結果就會是 Particle Filter。這部分就先不多說明了。(附上課程一張截圖) 有關 Particle Filter 的實作，在 Udacity Term2 Project3 中我們實作一個二維地圖的 localization。相關 Codes 可在筆者 github 中找到。 Reference Udacity 上課內容 MAP Adaptaion 部分詳細可參考: Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains","tags":[{"name":"Bayes Filter","slug":"Bayes-Filter","permalink":"https://bobondemon.github.io/tags/Bayes-Filter/"},{"name":"Localization","slug":"Localization","permalink":"https://bobondemon.github.io/tags/Localization/"},{"name":"Markov Localization","slug":"Markov-Localization","permalink":"https://bobondemon.github.io/tags/Markov-Localization/"},{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"}]},{"title":"Notes for Unscented Kalman Filter","date":"2017-04-12T12:50:16.000Z","path":"2017/04/12/Unscented-Kalman-Filter-Notes/","text":"資料為 Udacity 課程內容。事實上 UKF 挺囉嗦的，單純看本文應該無法理解，必須搭配前兩篇 KF and EKF 和 CTRV。主要是筆記用，讓自己可以根據文章完整實做出來。 一切的一切都來自於 Kalman Filter 的 State-Space model 假設，我們來稍微回顧一下。 $$\\begin{align} x_k = F_kx_{k-1}+\\nu_k \\\\ z_k = H_kx_k+\\omega_k&Tab;\\\\ \\end{align}$$ 式(1)表示狀態值 $x$ 滿足線性的遞迴關係式，而式(2)表示觀測值 $z$ 是當下狀態值的線性關係式。這個線性的關係式是為了使得我們的高斯分布在轉換後仍然滿足高斯分布所做的假設。但實際上常常不滿足線性的關係，例如假設我們的 $x$ 包含了 Cartesian coordinate 的座標位置和速度的資訊，但是 RADAR 的觀測值 $z$ 卻是用 Polar coordinate 來表示，就會有一個非線性的座標轉換。另一個會造成非線性的情況是發生在式(1)，也就是我們如果使用更精確的 motion model，如 CTRV。EKF 解決的方法是用 Jacobian 做線性的逼近，但是非線性的關係式如果一複雜，算 Jacobian 就會太複雜且造成運算速度變慢。因此，本篇要介紹的 Unscented KF 有相對簡單的辦法，並且運算速度快，且實際效果好。UKF 概念上怎麼做呢? 我們看上圖就可了解，首先原始的高斯分布(上面的紅色橢圓)，經由非線性轉換 $f$ 後得到的 “實際分佈” 為下面的黃色曲線，而該實際分布的 mean 和 covariance matrix 所形成的的高斯分布為下面的紅色橢圓，但是我們不容易得到! 那麼怎麼逼近下面的紅色橢圓呢? UKF 做法就是在上圖選擇一些代表的點，稱為 Sigma Points，經過 $f$ 轉換後，可以得到下面的星星，然後就可以根據這些轉換後的星星去計算他們的 mean 和 covariance matrix，而得到藍色的橢圓。那麼我們馬上開始說明如何設定 Sigma Points 吧。 Sigma Points 選擇假設 state dimension 為 $n_x$，Sigma Points 就選擇 $2n_x+1$ 個點。我們以 $n_x=2$ 來舉例說明會比較清楚，而擴展到更高的維度也就非常 trivial 了。 可以知道我們需選擇5個點($2n_x+1$)，第一個點是 mean vector，接著針對每一個 dimension 都根據 mean vector 向該 dimension 去做正負方向的 perturb，而 $\\lambda$ 表示要 perturb 多遠(使用者給定的值)。但是要特別注意的是，這裡的 perturb dimension 必須是正規化後的方向 (Whitening)，否則若原來的高斯分布某一個方向特別大(想像一個很扁的橢圓)，使用原來的 covariance matrix 就會被該方向 dominate。上例的 sigma points 如下: CTRV Sigma Points我們來看 CTRV model 下的 sigma points 選擇，其中 state vector and noise term 分別定義如下 $$\\begin{align} x= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v \\\\ \\psi \\\\ \\dot{\\psi} \\end{array} \\right) \\end{align}$$ $$\\begin{align} v_k= \\left[ \\begin{array}{center} v_{a,k} \\\\ v_{\\ddot{\\psi},k} \\end{array} \\right]\\\\ v_{a,k}\\sim N(0,\\sigma_a^2),v_{a,k}\\sim N(0,\\sigma_{\\ddot{\\psi}}^2) \\\\ Q=E[v_k,v_k^T]= \\left[ \\begin{array}{clr} \\sigma_a^2 &amp; 0 \\\\ 0 &amp; \\sigma_{\\ddot{\\psi}}^2 \\\\ \\end{array} \\right] \\end{align}$$ $v_k$ 的第一個 term 是加速度的 noise，而第二個表示 yaw rate 的變化率。由於原始的 state recursion 還參雜了 $Stochastic_k$ 這樣的 vector (參考式(7)and(8))，因此要計算他們的 covariance matrix 會太難搞! (因為我們需要知道 covariance matrix 才能對每個 whitening 後的維度去 perturb 取點) $$\\begin{align} x_{k+1}=x_k+Deterministic_k+ \\left[ \\begin{array}{center} \\frac{1}{2}(\\Delta{t})^2cos(\\psi_k)\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2sin(\\psi_k)\\cdot v_{a,k} \\\\ \\Delta{t}\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2\\cdot v_{\\ddot{\\psi},k} \\\\ \\Delta{t}\\cdot v_{\\ddot{\\psi},k} \\end{array} \\right]\\\\ x_{k+1}=x_k+Deterministic_k+Stochastic_k \\end{align}$$ 比較簡單的作法是將 noise term (式(4)) 當成 state vector 的另外的維度，主要的好處是 covariance matrix 就變得很容易計算了。然後一樣用上述的方式產生 Sigma Points。因此整個流程如下圖: 可以看到原本維度從5變成7，因此要產生15點的 sigma points，而 augmentated state vector 的 covariance matrix 變得很容易定義。 Sigma Points Prediction產生了這些 sigma points 之後，我們就可以透過式(7)，做 nonlinear recursion 到下一個時間點的 state vector (注意到 noise term 也被 sigma points 取樣了，所以可以帶入式(7)中)! Mean and Covariance of Sigma Points還記得嗎? 將 sigma point transform 後，我們下一步就是要估計出 mean 和 covariance，忘記的同鞋們可以看一下本文最開始的圖 (藍色的高斯分布)。基本上根據一些 data points 算它們的高斯分布非常簡單，但是由於我們當初取的 sigma points 它們之間本來的機率就不同，因此在計算轉換後的高斯分布必須要考慮每個點的權重。權重的設定有不同方法，課程直接建議下面的設定，所以沒特別要說明的，就照公式計算而已: Measurement Prediction對於 RADAR 來說式(2)也是一個非線性的關係，因此也可以用 sigma points 的方法來逼近。假設我們在時間點 $k$ 取的 sigma points 為 $x$，經過非線性 state recursion 後得到時間點 $k+1$ 的 sigma points 為 $x’$，我們可以直接將 $x’$ 當作新取的 sigma points，拿來做 measurement 非線性轉換 $z’=h(x’)+w$，然後一樣用上面的公式算一下 measurement space 的高斯分布即可。RADAR 的 $h()$ 定義如下: $$\\begin{align} z=h(x)= \\left( \\begin{array}{clr} \\rho \\\\ \\phi \\\\ \\dot{\\rho} \\end{array} \\right) = \\left( \\begin{array}{clr} \\sqrt{p_x^2+p_y^2} \\\\ \\arctan(p_y/p_x) \\\\ \\frac{p_xcos(\\psi)v+p_ysin(\\psi)v}{\\sqrt{p_x^2+p_y^2}} \\end{array} \\right) \\end{align}$$ 稍微要注意的是，計算 covariance 時須考慮 noise 的 covariance (下圖紅色框起來的地方)，這跟計算 state space 中的高斯分布不同。這是因為在 measurement space 是兩個 independent 的高斯分布相加 (一個是 sigma point 估出來的，另一個是 noise 的高斯)，covariance 就是相加而已。 另外對於 LIDAR 來說 measurement 的轉換是線性關係，所以不使用 sigma point 的方法，因此在處理兩種 sensor data 時，記得區分一下 case。 Measurement Update終於來到最後的步驟了。我們費盡千辛萬苦根據時間點 $k$ 的 state vector 估計出了時間點 $k+1$ 的 measurement 值，而此時我們在時間點 $k+1$ 也收到了真正的 sensor data measurement。因此同樣可以使用 KF 的流程去計算所有的 update! 原因是我們其實全部都高斯化了 (透過 sigma points 方法)。 紅色框起來處為跟以前不同的地方，變成要計算 cross-correlation of “Measurement Prediction 那個 section 的第二張圖那兩排的 vectors” 心得其實概念並不困難，但是頗多計算流程和符號，同時也必須先了解 Kalman Filter 和 CTRV motion model，下一步就實作 Project 吧! 附上 predict 的結果:","tags":[{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"},{"name":"Unscented Kalman Filter","slug":"Unscented-Kalman-Filter","permalink":"https://bobondemon.github.io/tags/Unscented-Kalman-Filter/"}]},{"title":"CTRV Motion Model","date":"2017-04-11T14:15:41.000Z","path":"2017/04/11/CTRV-Motion-Model/","text":"Motion Models 資料為 Udacity 課程內容 在上一篇 EKF 中，我們其實假設的是 constant velocity model (CV)，也就是如下的關係式$$\\begin{align} x_k = Fx_{k-1}+\\nu_k \\\\ x_k= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v_x \\\\ v_y \\end{array} \\right), F= \\left( \\begin{array}{clr} 1 &amp; 0 &amp; \\Delta{t} &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\Delta{t} \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right) \\end{align}$$正好滿足 Kalman Filter 中 State-space model 的假設，但這樣的 motion model 很明顯太單純了，因為車子總是在變速且轉彎。因此真實在使用的時候不會用 CV model，那會用什麼呢? 以下為幾種可用的: constant turn rate and velocity magnitude model (CTRV) constant turn rate and acceleration (CTRA) constant steering angle and velocity (CSAV) constant curvature and acceleration (CCA) Udacity 在這次的 project 中讓我們使用了 CTRV，而此 model 的 state vector $x$ 定義如下:$$\\begin{align} x= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v \\\\ \\psi \\\\ \\dot{\\psi} \\end{array} \\right) \\end{align}$$其中 $p_x,p_y$ 是 $x,y$ 座標位置，$v$ 是速度的 magnitude，$\\psi$ 是速度的向量與水平軸的夾角稱 yaw angel，最後的 $\\dot{\\psi}$ 則是該夾角的變化率稱 yaw rate。而 CTRV 假設的是 $v$ 和 $\\dot{\\psi}$ 是 constant。而此 model 已不是一個線性系統了，也就是無法用 matrix 來表達，所以我們將式(1)改為如下的表達方式:$$\\begin{align} x_{k+1} = f(x_k,\\nu_k) \\end{align}$$如何將 function $f$ 寫成遞迴式子呢? 請看下一段 CTRV State Vector Recursion我們先忽略 noise $\\nu_k$ 這項，晚點再加回來。State vector 隨時間變化的式子如下:$$\\begin{align} x_{k+1}=x_k+\\int_{t_k}^{t_{k+1}}{ \\left[ \\begin{array} \\\\ \\dot{p}_x(t) \\\\ \\dot{p}_y(t) \\\\ \\dot{v}(t) \\\\ \\dot{\\psi}(t) \\\\ \\ddot{\\psi}(t) \\end{array} \\right] }dt\\\\ x_{k+1}=x_k+ \\left[ \\begin{array}{center} \\int_{t_k}^{t_{k+1}}{v(t)\\cdot cos(\\psi(t))}dt \\\\ \\int_{t_k}^{t_{k+1}}{v(t)\\cdot sin(\\psi(t))}dt \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] \\end{align}$$注意到 CTRV 的假設 $v$ 和 $\\dot{\\psi}$ 是 constant，也就會造成式(5)中 $\\dot{v}(t)=\\ddot{\\psi(t)}=0$，且從時間 $k$ 到 $k+1$ 的 $\\dot{\\psi}(t)$ 都等於 $\\dot{\\psi}_k$，也因此得到式(6)。但是我們仍然要處理式(6)前兩項的積分，首先一樣基於CTRV假設 $v(t)=v_k$ 對於時間 $k$ 到 $k+1$ 都是一樣，所以提到積分外面。然後由於 yaw rate 是 constant，因此 $\\psi(t)$ 可以明確表示出來，總之改寫如下:$$\\begin{align} x_{k+1}=x_k+ \\left[ \\begin{array}{center} v_k\\int_{t_k}^{t_{k+1}}{cos(\\psi_k+\\dot{\\psi}_k\\cdot(t-t_k))}dt \\\\ v_k\\int_{t_k}^{t_{k+1}}{sin(\\psi_k+\\dot{\\psi}_k\\cdot(t-t_k))}dt \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] \\end{align}$$然後沒什麼好說的，就積它吧:$$\\begin{align} x_{k+1}=x_k+ \\left[ \\begin{array}{center} \\frac{v_k}{\\dot{\\psi}_k}(sin(\\psi_k+\\dot{\\psi}_k\\Delta{t})-sin(\\psi_k)) \\\\ \\frac{v_k}{\\dot{\\psi}_k}(-cos(\\psi_k+\\dot{\\psi}_k\\Delta{t})+cos(\\psi_k)) \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] =x_k+Deterministic_k \\end{align}$$這邊有一個實作上需要避免的地方，就是當 $\\dot{\\psi}_k=0$ 時，上式的第1,2項會除0。不過我們知道當 $\\dot{\\psi}_k=0$ 表示車子是直直往前開，yaw angle不會改變，因此實際上可以用如下來計算:$$\\begin{align} x_{k+1}=x_k+ \\left[ \\begin{array}{center} v_kcos(\\psi_k)\\Delta{t} \\\\ v_ksin(\\psi_k)\\Delta{t} \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] =x_k+Deterministic&apos;_k \\end{align}$$ Recursion With Noise TermNoise term $v_k$ 這裡是假設如下:$$\\begin{align} v_k= \\left[ \\begin{array}{center} v_{a,k} \\\\ v_{\\ddot{\\psi},k} \\end{array} \\right] \\end{align}$$ 第一個 term 是加速度的 noise，而第二個表示 yaw rate 的變化率。考慮如果有這兩項 noises 的話，並且假設時間 $k$ 到 $k+1$ 這兩個 noises 的值是固定的，那麼 state vector 會變成如下:$$\\begin{align} x_{k+1}=x_k+Deterministic_k+ \\left[ \\begin{array}{center} \\frac{1}{2}(\\Delta{t})^2cos(\\psi_k)\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2sin(\\psi_k)\\cdot v_{a,k} \\\\ \\Delta{t}\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2\\cdot v_{\\ddot{\\psi},k} \\\\ \\Delta{t}\\cdot v_{\\ddot{\\psi},k} \\end{array} \\right]\\\\ x_{k+1}=x_k+Deterministic_k+Stochastic_k \\end{align}$$第三項是速度 $v$ 會被加速度 $v_{a,k}$ 這種 noise 怎麼影響，所以很明顯是線性增加，同理第四和第五項也很容易得到。第一和第二項，$x$ and $y$ 的位置這裡就比較麻煩，因此採用的是一個近似而已。這邊假設 yaw rate 沒有太高的情況下，下圖的兩個紅色圈圈位置應該是很接近，因此我們可以考慮走直線的紅色圈圈位置，也就得到了(11)第一二項的近似值。 Summary All CTRV省略解釋，寫出 state recursion 的計算。$$x= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v \\\\ \\psi \\\\ \\dot{\\psi} \\end{array} \\right)$$ if $\\dot{\\psi}_k\\neq0$, then$x_{k+1}=x_k+Deterministic_k+Stochastic_k$where$$Deterministic_k= \\left[ \\begin{array}{center} \\frac{v_k}{\\dot{\\psi}_k}(sin(\\psi_k+\\dot{\\psi}_k\\Delta{t})-sin(\\psi_k)) \\\\ \\frac{v_k}{\\dot{\\psi}_k}(-cos(\\psi_k+\\dot{\\psi}_k\\Delta{t})+cos(\\psi_k)) \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right]$$and$$Stochastic_k= \\left[ \\begin{array}{center} \\frac{1}{2}(\\Delta{t})^2cos(\\psi_k)\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2sin(\\psi_k)\\cdot v_{a,k} \\\\ \\Delta{t}\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2\\cdot v_{\\ddot{\\psi},k} \\\\ \\Delta{t}\\cdot v_{\\ddot{\\psi},k} \\end{array} \\right]$$otherwise $\\dot{\\psi}_k=0$, then$x_{k+1}=x_k+Deterministic&apos;_k+Stochastic_k$where$$Deterministic&apos;_k= \\left[ \\begin{array}{center} v_kcos(\\psi_k)\\Delta{t} \\\\ v_ksin(\\psi_k)\\Delta{t} \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right]$$ Unscented Kalman Filter 簡介由於 CTRV 是非線性的，會破壞 State-space model 的線性假設，例如下圖中原先紅色的高斯分布經過非線性轉換後分布為黃色。不過我們知道 EKF 可以利用 Jaccobian matrix 做線性逼近計算，所以我們同樣可以計算。但要計算上述非線性系統的 Jaccobian matrix 實在顯得有點複雜，好在 Unscented KF 可以完全避開這個麻煩。它利用選擇幾個代表的 candidates vectors，叫做 Sigma Points，去計算經過非線性轉換後的值，然後就可以得到 output domain 的 mean 和 covariance matrix，也就是上圖的綠色高斯分布。這邊要注意的是，output domain 的真實分佈不是高斯分布(黃色)，但我們仍然將它當成是高斯分布(綠色)去計算 mean 和 covariance matrix，因為這樣才能繼續套用 Kalman filter 的方法。說到這可知道 UKF 仍然只是逼近，不過根據 Udacity 的說法，實際應用上 UKF 是很快 (不用計算 Jaccobian) 且實際上效果很好!下回預告，UKF完整介紹。","tags":[{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"},{"name":"Motion Model","slug":"Motion-Model","permalink":"https://bobondemon.github.io/tags/Motion-Model/"},{"name":"Unscented Kalman Filter","slug":"Unscented-Kalman-Filter","permalink":"https://bobondemon.github.io/tags/Unscented-Kalman-Filter/"}]},{"title":"Notes for Kalman Filter and Extended KF","date":"2017-04-03T08:56:13.000Z","path":"2017/04/03/Kalman-Filter-and-Extended-KF-Notes/","text":"Udacity term2 (Sensor Fusion, Localization, and Control) 的第一個 Project 就是用 KF and EKF 將 Lidar and Radar 的資訊做 fusion 並且可以 tracking。由於 KF/EKF 的數學符號很多，因此想筆記一下方便日後回想，所以主要以我自己看的角度，可能有些地方會沒有明確說明。本篇的筆記來源是 這裡，這篇真的講的超棒的，清楚易懂! 非常建議直接去看! Udacity 課程內容 若要實作所有的計算流程不管理論的話，可直接跳到 “7. 總結 Lidar and Radar Fusion”。 State Space Model這是整個 KF/EKF 的模型假設，寫出來如下: $$\\begin{align} x_k = F_kx_{k-1}+\\nu_k \\\\ z_k = H_kx_k+\\omega_k \\end{align}$$ \\(x_k\\) 是在時間點 \\(t\\) 的 states，也是我們希望能夠估計出來的(但是無法直接觀察到)。而 states 滿足 線性的一次遞迴 關係，也就是式子(1)。 \\(\\nu_k\\sim\\mathcal{N}(0,Q_k)\\) 是 state nose。\\(z_k\\) 是在時間點 \\(t\\) 的 observations，透過 \\(H_k\\) 將 states 轉換到 observations。 \\(\\omega_k\\sim\\mathcal{N}(0,R_k)\\) 是 sensor noise，而 \\(R_k\\) 基本上會由製造廠商提供。基本上兩個 noises 都跟所有人都 independent。 Prediction Stage整個 KF/EKF 都是基於 Gaussian distribution。因此假設我們有 \\(k-1\\) 時間點的 state 估計，所以我們知道 \\(x_{k}\\sim\\mathcal{N}(\\hat{x}_k,P_k)\\) 會變成如下的一個 Gaussian: $$\\begin{align} \\hat{x}_{k}=F_k\\hat{x}_{k-1} \\\\ P_k = F_kP_{k-1}F_k^T+Q_k \\end{align}$$ 式(3)and(4)即為 Prediction Stage。又因為我們知道 observation 跟 state 之間的關係為透過 \\(H_k\\) 轉換，在完全沒有 sensor noise 情況下，所以可以得知 prediction 的觀察值為: $$\\begin{align} z_{expected}\\sim\\mathcal{N}(\\mu_{expected},\\Sigma_{expected}) \\\\ \\mathcal{N}\\left( \\begin{array}{c} \\vec{\\mu}_{expected}=H_k\\hat{x}_{k}, &amp; \\Sigma_{expected} = H_kP_kH_k^T \\end{array} \\right) \\end{align}$$ Update Stage我們令實際上的觀察值為 \\(z_k\\sim\\mathcal{N}(\\vec{z}_k,R_k)\\)，將觀察值的 Gaussian 和 predict 的 Gaussian 畫出如下: 而將兩個 Gaussian pdfs 相乘的話:$$\\begin{align} \\mathcal{N}(x,\\mu_0,\\Sigma_0)\\cdot\\mathcal{N}(x,\\mu_1,\\Sigma_1)=\\mathcal{N}(x,\\mu&apos;,\\Sigma&apos;) \\\\ \\end{align}$$仍然會得到另一個 Gaussian:$$\\begin{align} K=\\Sigma_0(\\Sigma_0+\\Sigma_1)^{-1} \\\\ \\mu&apos;=\\mu_0+K(\\mu_1-\\mu_0) \\\\ \\Sigma&apos;=\\Sigma_0-K\\Sigma_0 \\end{align}$$ \\(K\\) 稱為 Kalman Gain。由式(10)可知，update 後的 covariance matrix 會愈來愈小，表示我們對於 prediction 的觀察值會愈來愈確定。另外由(9)可知，Kalman Gain 控制著要相信哪邊多一點。把估測的觀察值 pdf 和實際觀察值的 pdf，即 \\(z_k\\sim\\mathcal{N}(\\vec{z}_k,R_k)\\) 和式(5)兩個 pdfs 代入到式 (8)~(10) 得到如下: $$\\begin{align} H_k\\hat{x}_k&apos;=H_k\\hat{x}_k+K(\\vec{z}_k-H_k\\hat{x}_k) \\\\ H_kP_k&apos;H_k^T=H_kP_kH_k^T-KH_kP_kH_k^T \\\\ K=H_kP_kH_k^T(H_kP_kH_k^T+R_k)^{-1} \\end{align}$$ 把 (11)~(13) 開頭的 \\(H_k\\) 去掉，並且把 (12) and (13) 結尾的 \\(H_k^T\\) 去掉變成$$\\begin{align} \\hat{x}_k&apos;=\\hat{x}_k+K(\\vec{z}_k-H_k\\hat{x}_k) \\\\ P_k&apos;=P_k-KH_kP_k \\\\ K=P_kH_k^T(H_kP_kH_k^T+R_k)^{-1} \\end{align}$$ (14)~(16)就是 KF 的 Update Stage! 新的 states 估計值就被我們得到，然後這個值就可以被當成下一次 loop 的初始值。 KF Flow擷取網站上的圖片: Lidar/Radar 的一些設定state 定義為 \\(x=(p_x,p_y,v_x,v_y)\\) 分別是 (x 的位置, y 的位置, x 的速度, y 的速度)。 \\(F_k\\) 會根據兩次 sensor data 之間的時間間隔 \\(\\vartriangle t\\) 來表示: 另外我們將 加速度考慮為一個 mean = 0, covariance matrix = Q 的一個 random noise 的話，式 (1) and (4) 必須做修改。其中 \\(Q_v\\) 使用者自己設定調整，所以 state noise 的 covariance matrix 為 Lidar 只會觀察到位置，因此 Lidar 的 \\(H\\) 為:$$H_{lidar}= \\left( \\begin{array}{clr} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\end{array} \\right)$$ Radar 就比較特別了，它觀察到的是以 polar coordinate 來表示。所以它的 states 和 observation 之間的關係無法用一個 matrix \\(H\\) 來代表，是如下的 non-linear 式子:$$\\begin{align} h(x)= \\left( \\begin{array}{clr} \\rho \\\\ \\phi \\\\ \\dot{\\rho} \\end{array} \\right) = \\left( \\begin{array}{clr} \\sqrt{p_x^2+p_y^2} \\\\ \\arctan(p_y/p_x) \\\\ \\frac{p_xv_x+p_yv_y}{\\sqrt{p_x^2+p_y^2}} \\end{array} \\right) \\end{align}$$ 為了讓它符合 state-space model 的線性式子，只好使用 Taylor 展開式，只使用 Jaccobian matrix 針對 \\(h\\) 去展開，而這個就是 Extended KF。 EKF稍微改寫一下 Update Stage:$$\\begin{align} y=(\\vec{z}_k-H_k\\hat{x}_k) \\\\ S=H_kP_kH_k^T+R_k \\\\ \\hat{x}_k&apos;=\\hat{x}_k+Ky \\\\ P_k&apos;=P_k-KH_kP_k \\\\ K=P_kH_k^TS^{-1} \\end{align}$$在 EKF 中，由於我們使用 Taylor 展開式去逼近 \\(h\\)，因此上述的 \\(H_k\\) 必須使用如下式子計算:但是，這邊還有一個 tricky 的地方! 就是 式(18)直接使用式(17) \\(h\\) 的 non-linear function 計算!回想一下我們將 \\(h\\) 做 linearlization 的目的: 就是式(5),(6)下的那張圖的轉換。如果 Gaussian pdf 經過 nonlinear 轉換後會變成 “非Gaussian”，因此只好做線性逼近。既然線性轉換的 pdf 都已經是逼近了，不如就將 mean 使用最精確的值，因此 \\(y\\) 就直接使用式(17)計算。所以式(18)要改成:$$\\begin{align} y=(\\vec{z}_k-h(\\hat{x}_k)) \\end{align}$$ 總結 Lidar and Radar Fusion [Predict]$$\\hat{x}_{k}=F_k\\hat{x}_{k-1} \\\\ P_k = F_kP_{k-1}F_k^T+Q$$ where [Lidar Update]$$y=(\\vec{z}_k-H_{lidar}\\hat{x}_k) \\\\ S=H_{lidar}P_kH_{lidar}^T+R_k \\\\ \\hat{x}_k&apos;=\\hat{x}_k+Ky \\\\ P_k&apos;=P_k-KH_{lidar}P_k \\\\ K=P_kH_{lidar}^TS^{-1}$$ where \\(R_k\\) sensor noise covariance matrix 由廠商提供, and $$H_{lidar}= \\left( \\begin{array}{clr} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\end{array} \\right)$$ [Radar Update]$$y=(\\vec{z}_k-h(\\hat{x}_k)) \\\\ S=H_kP_kH_k^T+R_k \\\\ \\hat{x}_k&apos;=\\hat{x}_k+Ky \\\\ P_k&apos;=P_k-KH_kP_k \\\\ K=P_kH_k^TS^{-1}$$ where \\(R_k\\) sensor noise covariance matrix 由廠商提供, and $$h(x)= \\left( \\begin{array}{clr} \\rho \\\\ \\phi \\\\ \\dot{\\rho} \\end{array} \\right) = \\left( \\begin{array}{clr} \\sqrt{p_x^2+p_y^2} \\\\ \\arctan(p_y/p_x) \\\\ \\frac{p_xv_x+p_yv_y}{\\sqrt{p_x^2+p_y^2}} \\end{array} \\right)$$ Reference How a Kalman filter works, in pictures Udacity Term2 Lecture","tags":[{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"},{"name":"Kalman Filter","slug":"Kalman-Filter","permalink":"https://bobondemon.github.io/tags/Kalman-Filter/"},{"name":"Extended Kalman Filter","slug":"Extended-Kalman-Filter","permalink":"https://bobondemon.github.io/tags/Extended-Kalman-Filter/"}]},{"title":"WGAN Part 2: 主角 W 登場","date":"2017-03-17T13:25:12.000Z","path":"2017/03/17/WGAN-Part-2/","text":"前情提要GAN 作者設計出一個 Minimax game，讓兩個 players: 生成器 G 和 鑑別器 D 去彼此競爭，並且達到平衡點時，此問題達到最佳解且生成器 G 鍊成。大致上訓練流程為先 optimize 鑑別器 D for some iterations，然後換 optimize 生成器 G (在 optimize G 時，此問題等價於最佳化 JSD 距離)，重複上述 loop 直到達到最佳解。但是仔細看看原來的最佳化問題之設計，我們知道在最佳化 G 的時候，等價於最佳化一個 JSD 距離，而 JSD 在遇到真實資料的時會很悲劇。怎麼悲劇呢? 原因是真實資料都存在 local manifold 中，造成 training data 的 p.d.f. 和 生成器的 p.d.f. 彼此之間無交集 (或交集的測度為0)，在這種狀況 JSD = log2 (constant) almost every where。也因此造成 gradients = 0。這是 GAN 很難訓練的一個主因。 也因此 WGAN 的主要治本方式就是換掉 JSD，改用 Wasserstein (Earth-Mover) distance，而修改過後的演算法也是簡單得驚人! Wasserstein (Earth-Mover) distance我們先給定義後，再用作者論文上的範例解釋定義如下:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_g)=\\inf_{\\gamma\\in\\prod(\\mathbb{P}_r,\\mathbb{P}_g)}E_{(x,y)\\sim \\gamma}[\\Vert x-y \\Vert] \\end{align}$$\\(\\gamma\\)指的是 real data and fake data 的 joint distribution，其中 marginal 為各自兩個 distributions。先別被這些符號嚇到，直觀的解釋為: EM 距離可以理解為將某個機率分佈搬到另一個機率分佈，所要花的最小力氣。 我們用下面這個例子明確舉例，假設我們有兩個機率分佈 f1 and f2:$$\\begin{align*} f_1(a)=f_1(b)=f_1(c)=1/3 \\\\\\\\ f_1(A)=f_1(B)=f_1(C)=1/3 \\end{align*}$$這兩個機率分佈在一個 2 維平面，如下:而兩個 \\(\\gamma\\) 對應到兩種 搬運配對法$$\\begin{align*} \\gamma_1(a,A)=\\gamma_1(b,B)=\\gamma_1(c,C)=1/3 \\\\\\\\ \\gamma_2(a,B)=\\gamma_2(b,C)=\\gamma_2(c,A)=1/3 \\end{align*}$$可以很容易知道它們的 marginal distributions 正好符合 f1 and f2 的機率分佈。則這兩種搬運法造成的 EM distance 分別如下:$$\\begin{align*} EM_{\\gamma_1}=\\gamma_1(a,A)*\\Vert a-A \\Vert + \\gamma_1(b,B)*\\Vert b-B \\Vert + \\gamma_1(c,C)*\\Vert c-C \\Vert \\\\\\\\ EM_{\\gamma_2}=\\gamma_2(a,B)*\\Vert a-B \\Vert + \\gamma_2(b,C)*\\Vert b-C \\Vert + \\gamma_2(c,A)*\\Vert c-A \\Vert \\end{align*}$$明顯知道 $\\theta=EM_{\\gamma_1}&lt;EM_{\\gamma_2}$而 EM distance 就是在算所有搬運法中，最小的那個，並將那最小的 cost 定義為此兩機率分佈的距離。這個距離如果是兩條平行 1 維的直線 pdf (上面的例子是直線上只有三個離散資料點)，會有如下的 cost: 對比此圖和上一篇的 JSD 的結果，EM 能夠正確估算兩個沒有交集的機率分佈的距離，直接的結果就是 gradient 連續且可微 ! 使得 WGAN 訓練上穩定非常多。 一個關鍵的好性質: Wasserstein (Earth-Mover) distance 處處連續可微原始 EM distance 的定義 (式(1)) 是 intractable一個神奇的數學公式 (Kantorovich-Rubinstein duality) 將 EM distance 轉換如下:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_\\theta)=\\sup_{\\Vert f \\Vert _L \\leq 1}{ E_{x \\sim \\mathbb{P}_r}[f(x)] - E_{x \\sim \\mathbb{P}_\\theta}[f(x)] } \\end{align}$$注意到 sup 是針對所有滿足 1-Lipschitz 的 functions f，如果改成滿足 K-Lipschitz 的 functions，則值會相差一個 scale K。但是在實作上我們都使用一個 family of functions，例如使用所有二次式的 functions，或是 Mixture of Gaussians，等等。而經過近幾年深度學習的發展後，我們可以相信，使用 DNN 當作 family of functions 是很洽當的選擇，因此假定我們的 NN 所有參數為 \\(W\\)，則上式可以表達成:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\approx\\max_{w\\in W}{ E_{x \\sim \\mathbb{P}_r}[f_w(x)] - E_{z \\sim p(z)}[f_w(g_{\\theta}(z))] } \\end{align}$$這裡不再是等式，而是逼近，不過 Deep Learning 優異的 Regression 能力是可以很好地逼近的。 我們還是需要保證整個 EM distance 保持處處連續可微分，這樣可以確保我們做 gradient-based 最佳化可以順利，針對這點，WGAN 作者很強大地證明完了，得到結論如下: 針對生成器 \\(g_\\theta\\)任何 feed-forward NN 皆可 針對鑑別器 \\(f_w\\)當 \\(W\\) 是 compact set 時，該 family of functions \\(\\{f_w\\}\\) 滿足 K-Lipschitz for some K。具體實現很容易，因為在 \\(R^d\\) space，compact set 等價於 closed and bounded，因此只需要針對所有的參數取 bounding box即可!論文裡使用了 [-0.01,0.01] 這個範圍做 clipping。 與 GAN 第一個不同點為: 鑑別器參數取 clipping。 EM distance 為目標函式所造成的不同我們將兩者的目標函式列出來做個比較$$\\begin{align} GAN: E_{x \\sim \\mathbb{P}_r} [\\log f_w(x)] + E_{z \\sim p(z)}[\\log (1-f_w(g_{\\theta}(z)))] \\\\ WGAN: E_{x \\sim \\mathbb{P}_r}[f_w(x)] - E_{z \\sim p(z)}[f_w(g_{\\theta}(z))] \\end{align}$$發現到 WGAN 不取 log，同時對生成器的目標函式也做了修改 與 GAN 第二個不同點為: WGAN 的目標函式不取 log，同時對生成器的目標函式也做了修改。 第三個不同點是作者實驗的發現 與 GAN 第三個不同點為: 使用 Momentum 類的演算法，如 Adam，會不穩定，因此使用 SGD or RMSProp。 WGAN 演算法總結一下與 GAN 的修改處 A. 鑑別器參數取 clipping。B. WGAN 的目標函式不取 log，同時對生成器的目標函式也做了修改。C. 使用 SGD or RMSProp。 WGAN 的優點一: 目標函式與訓練品質高度相關原始的 GAN 沒有這樣的評量指標，因此會在訓練中途用人眼去檢查訓練是否整個壞掉了。 WGAN 解決了這個麻煩。作者的範例如下，可以發現WGAN的目標函式 Loss 愈低，sampling出來的品質愈高。 二: 鑑別器可以直接訓練到最好原始的 GAN 需要小心訓練，不能一下子把鑑別器訓練太強導致導函數壞掉 三: 不需要特別設計 NN 的架構GNN 使用 MLP (Fully connected layers) 難以訓練，較成功的都是 CNN 架構，並搭配 batch normalization。而在 WGAN 演算法下， MLP架構可能穩定訓練 (雖然品質有下降) 四: 沒有 collapse mode (保持生成多樣性)作者自己說在多次實驗的過程都沒有發現這種現象 My Questions 原先 GAN 會有 collapse mode 看到有人討論是因為 KL divergence 不對稱的關係導致對於 “生成器生出錯誤的 sample” 比 “生成器沒生出所有該對的sample” 逞罰要大很多，不過這邊自己還是有疑問，因為 JSD 已經是對稱的 KL 了，還會有逞罰不同導致 collapse mode 的問題嗎? 需要再多看一下 paper 了解。 如何控制 sample 出來的 output，譬如 mnist 要 sampling 出某個 class。前提是希望不能對 data 有任何標記過，不然就沒有 unsupervised 的條件了。 Conditional GAN? 有空再研究一下這個課題 Tensorflow 範例測試主要參考此 github，用自己的寫法寫一次，並做些測試 用 MNIST dataset 做測試，原始 input 為 28x28，將它 padding 成 32x32，因此 input domain 為 32x32x1 生成器幾個重點，第一個是生成器用的是 conv2d_transpose (doc)，這是由於原先的 conv2d 無法將 image 的 size 變大，頂多一樣。因此要用 conv2d_transpose，以 第 15 行舉例。argument wc2 的 shape 為 [3, 3, 256, 512] 分別表示 [filter_h, filter_w, output_depth, input_depth]。argument [batch_size, 8, 8, 256] 表示 output layer 的 shape。後面兩個 argument 就很明顯了，分別是 strides [batch_stride, h_stride, w_stride, channel_stride] 和 padding。第二個重點是最後一層 out_sample = tf.nn.tanh(conv5)，由於我們會將 data 先 normalize 到 [-1,1]，因此使用 tanh 讓 domain 一致。 123456789101112131415161718192021222324252627282930313233z_dim = 128def generator_net(z): with tf.variable_scope('generator'): # Layer 1 - 128 to 4*4*512 wd1 = tf.get_variable(\"wd1\",[z_dim, 4*4*512]) bd1 = tf.get_variable(\"bd1\",[4*4*512]) dense1 = tf.add(tf.matmul(z, wd1), bd1) dense1 = tf.nn.relu(dense1) # reshape to 4*4*512 conv1 = tf.reshape(dense1, (batch_size, 4, 4, 512)) # Layer 2 - 4*4*512 to 8*8*256 wc2 = tf.get_variable(\"wc2\",[3, 3, 256, 512]) conv2 = tf.nn.conv2d_transpose(conv1, wc2, [batch_size, 8, 8, 256], [1,2,2,1], padding='SAME') conv2 = tf.nn.relu(conv2) # Layer 3 - 8*8*256 to 16*16*128 wc3 = tf.get_variable(\"wc3\",[3, 3, 128, 256]) conv3 = tf.nn.conv2d_transpose(conv2, wc3, [batch_size, 16, 16, 128], [1,2,2,1], padding='SAME') conv3 = tf.nn.relu(conv3) # Layer 4 - 16*16*128 to 32*32*64 wc4 = tf.get_variable(\"wc4\",[3, 3, 64, 128]) conv4 = tf.nn.conv2d_transpose(conv3, wc4, [batch_size, 32, 32, 64], [1,2,2,1], padding='SAME') conv4 = tf.nn.relu(conv4) # Layer 5 - 32*32*64 to 32*32*1 wc5 = tf.get_variable(\"wc5\",[3, 3, 1, 64]) conv5 = tf.nn.conv2d_transpose(conv4, wc5, [batch_size, 32, 32, 1], [1,1,1,1], padding='SAME') out_sample = tf.nn.tanh(conv5) return out_sample 鑑別器這個就是最一般的 CNN，output 最後是一個沒有過 log 的 scaler 且也沒有經過 activation function。比較重要的是變數都是使用 get_variable 和 scope.reuse_variables() (請參考 Sharing Variables)。具體的原因是因為我們會對 real data 呼叫一次鑑別器，而對於 fake data 也會在呼叫一次。若沒有 share variables，就會導致產生兩組各自的 weights。tf.get_variable() 跟 tf.Variable() 差別在於如果已經有名稱一樣的變數時 get_variable() 不會再產生另一個變數，而會 share，但是要真的 share 還必須多一個動作 reuse_variables 確保不是不小心 share 到的。 12345678910111213141516171819202122232425262728293031323334353637# Construct CriticNetdef conv2d(x, W, b, strides=1): x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME') x = tf.nn.bias_add(x, b) return tf.nn.relu(x)def critic_net(x, reuse=False): with tf.variable_scope('critic') as scope: size = 64 if reuse: scope.reuse_variables() # Layer 1 - 32*32*1 to 16*16*size wc1 = tf.get_variable(\"wc1\",[3, 3, 1, size]) bc1 = tf.get_variable(\"bc1\",[size]) conv1 = conv2d(x, wc1, bc1, strides=2) # Layer 2 - 16*16*size to 8*8*size*2 wc2 = tf.get_variable(\"wc2\",[3, 3, size, size*2]) bc2 = tf.get_variable(\"bc2\",[size*2]) conv2 = conv2d(conv1, wc2, bc2, strides=2) # Layer 3 - 8*8*size*2 to 4*4*size*4 wc3 = tf.get_variable(\"wc3\",[3, 3, size*2, size*4]) bc3 = tf.get_variable(\"bc3\",[size*4]) conv3 = conv2d(conv2, wc3, bc3, strides=2) # Layer 4 - 4*4*size*4 to 2*2*size*8 wc4 = tf.get_variable(\"wc4\",[3, 3, size*4, size*8]) bc4 = tf.get_variable(\"bc4\",[size*8]) conv4 = conv2d(conv3, wc4, bc4, strides=2) # Fully connected layer - 2*2*size*8 to 1 wd5 = tf.get_variable(\"wd5\",[2*2*size*8, 1]) bd5 = tf.get_variable(\"bd5\",[1]) fc5 = tf.reshape(conv4, [-1, wd5.get_shape().as_list()[0]]) logit = tf.add(tf.matmul(fc5, wd5), bd5) return logit Graph這裡有幾個重點，第一個是由於我們在最佳化過程中，會 fix 住一邊的參數，然後最佳化另一邊，接著反過來。此作法參考 link第二個重點是使用 tf.clip_by_value，可以看到我們對於所有透過 tf.get_collection 蒐集到的變數都增加一個 clip op。第三個重點是使用 tf.control_dependencies([opt_c]) link，這個定義了 op 之間的關聯性，它會等到 argument 內執行完畢後，才會接著執行下去。所以我們可以確保先做完 RMSPropOptimizer 才接著做 clip_by_value。另外 tf.tuple link 會等所有的 input arguments 都做完才會真的 return 出去，以確保每個 tensors 都做完 clipping 了。 1234567891011121314151617181920212223242526272829# build graphdef build_graph(): z = tf.placeholder(tf.float32, shape=(batch_size, z_dim)) fake_data = generator_net(z) real_data = tf.placeholder(tf.float32, shape=(batch_size, 32, 32, 1)) # Define loss and optimizer real_logit = critic_net(real_data) fake_logit = critic_net(fake_data, reuse=True) c_loss = tf.reduce_mean(fake_logit - real_logit) g_loss = tf.reduce_mean(-fake_logit) # get the trainable variables list theta_g = tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator') theta_c = tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic') # freezing or only update designated variables opt_g = tf.train.RMSPropOptimizer(learning_rate=lr_generator).minimize(g_loss, var_list=theta_g) opt_c = tf.train.RMSPropOptimizer(learning_rate=lr_critic).minimize(c_loss, var_list=theta_c) # then pass those trainable variables to clip function clipped_var_c = [tf.assign(var, tf.clip_by_value(var, clip_lower, clip_upper)) for var in theta_c] # wait until RMSPropOptimizer is done with tf.control_dependencies([opt_c]): # fetch the clipped variables and output as op opt_c = tf.tuple(clipped_var_c) return opt_g, opt_c, z, real_data WGAN Algorithm Flow照 paper 上的演算法 flow 123456789101112131415161718192021222324252627282930def wgan_train(): dataset = input_data.read_data_sets(\".\", one_hot=True) opt_g, opt_c, z, real_data = build_graph() saver = tf.train.Saver() config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True) config.gpu_options.allow_growth = True config.gpu_options.per_process_gpu_memory_fraction = 0.8 def next_feed_dict(): train_img = dataset.train.next_batch(batch_size)[0] train_img = 2*train_img-1 train_img = np.reshape(train_img, (-1, 28, 28)) npad = ((0, 0), (2, 2), (2, 2)) train_img = np.pad(train_img, pad_width=npad, mode='constant', constant_values=-1) train_img = np.expand_dims(train_img, -1) batch_z = np.random.normal(0, 1, [batch_size, z_dim]).astype(np.float32) feed_dict = &#123;real_data: train_img, z: batch_z&#125; return feed_dict with tf.Session(config=config) as sess: sess.run(tf.global_variables_initializer()) summary_writer = tf.summary.FileWriter(log_dir, sess.graph) for i in range(max_iter_step): print(\"itr = \",i) for j in range(c_iter): feed_dict = next_feed_dict() sess.run(opt_c, feed_dict=feed_dict) feed_dict = next_feed_dict() sess.run(opt_g, feed_dict=feed_dict) if i % 1000 == 999: saver.save(sess, os.path.join(ckpt_dir, \"model.ckpt\"), global_step=i) 一點小結論5.1. 上述架構沒有用 batch normalization，有用的話效果會好很多，生成器和鑑別器都可用。5.2. 鑑別器換成其他 CNN 架構也可以。5.3. MLP 架構也可以。 整體來說，對於熟悉 tensorflow 的人來說不難實作 (剛好我不是很熟)，尤其 WGAN 從根本上做的改進，讓整個 training 很容易!讓我們期待接下來的發展吧~ Reference GAN Wasserstein GAN，作者的 github 令人拍案叫绝的Wasserstein GAN A Tensorflow Implementation of WGAN: 使用 tf.contrib.layers，一個 higher level 的 API，比我現在的實作可以簡潔很多。 A GENTLE GUIDE TO USING BATCH NORMALIZATION IN TENSORFLOW: Batch Normalization, MLP, and CNN examples using tf.contrib.layers","tags":[{"name":"ML","slug":"ML","permalink":"https://bobondemon.github.io/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://bobondemon.github.io/tags/Deep-Learning/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://bobondemon.github.io/tags/Generative-Model/"}]},{"title":"WGAN Part 1: 先用 GAN 鋪梗","date":"2017-03-16T13:25:12.000Z","path":"2017/03/16/WGAN-Part-1/","text":"Open.ai 這張表達 generative modeling 的意思很清楚，忍不住就借用了。 筆者才疏學淺，如有錯誤，還請指正 Generative Adversarial Nets 提出了一個 NN 的 generative modeling 方法，在這之前，NN 要成為 p.d.f. 必須依賴於 sigmoid activation 的 Restricted Boltzmann Machines (RBM) 結構。例如 Deep Belief Net，整個 network 才會是一個 p.d.f.。然而學習這樣的一個 p.d.f. 必須使用 Contrastive Divergence 的 MCMC 方法， model 訓練完後要產生 sample 時也還是必須依賴 MCMC。加上在實用上，偏偏 sigmoid 很多時候效果不如 ReLu, maxout 等，例如 sigmoid 有嚴重的 gradient vanish problem。這使得 NN 在 generative modeling 又或是 unsupervised learning 上一直困難重重。 GAN 一出立即打破這個難堪的限制 ! 怎麼說呢? GAN 捨棄能夠明確表達出 p.d.f.的作法，寫不出明確的 p.d.f. 一點也沒關係，只要能生成 夠真的sample點，並且sample的機率跟training data一樣就好 然而 GAN 在實作上卻會遇上一些困難，例如生成的 samples 多樣性不足，訓練流程/架構 和 hyper-parameters 需要小心選擇，無法明確知道訓練的收斂狀況，這些問題等下會說明。 本篇的主角 (事實上下一篇才會登場) Wasserstein GAN (WGAN)，從本質上探討 GAN 目標函式中使用的 distance measure，進而根本地解決上述三個問題，這大大降低了 generative modeling 訓練難度 ! 我們還是來談談 GAN 怎麼一回事先吧。 Generative Adversarial NetsGAN 使用一個 two-player minimax gaming 策略。先用直觀說，我們有一個 生成器 \\(G\\)，用來生成夠真的 sample，另外還有一個 鑑別器 \\(D\\)，用來分辨 sample 究竟是真實資料 (training data) 來的呢，還是假的 (\\(G\\)產生的)。當這兩個模型互相競爭到一個平衡點的時候，也就是 \\(G\\) 能夠產生到 \\(D\\) 分辨不出真假的 sample，我們的生成器 \\(G\\) 就鍊成了。而 GAN 作者厲害的地方就在於 一: 將這兩個model的競爭規則轉換成一個最佳化問題二: 並且證明，當達到賽局的平衡點時(達到最佳解)，生成器就鍊成 (可以完美表示 training data 的 pdf，並且可sampling) 我們還是必須把上述策略嚴謹的表達出來 (寫成最佳化問題)，並證明當達到最佳化問題的最佳解時，就剛好完成生成器的鍊成。 Two-player Minimax Game原則上我們希望鑑別器 \\(D\\) 能分辨出真假 sample，因此 \\(D(x)\\) 很自然地可以表示為 sample \\(x\\) 為真的機率另外生成器 \\(G\\) 則是負責產生假 sample，也可以很自然地表達為 \\(G(z)\\)，其中 \\(z\\) 為 latent variables，且我們可以假設該 latent variables \\(z\\) follow 一個 prior distribution \\(p_z(z)\\)。 我們希望 \\(D(x)\\) 對來自於真實資料的 samples 能夠盡量大，而對來自於 \\(G\\) 產生的要盡量小，因此對於鑑別器來說，它的目標函式可定義為如下: $$\\begin{align} Maximize: E_{x \\sim p_{data}(x)} [\\log D(x)] + E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ 另一方面，我們希望 \\(G\\) 能夠強到讓 \\(D\\) 無法分辨真偽，因此生成器的目標函式為: $$\\begin{align} Minimize: E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ 結合上述兩個目標函式就是如下的 minmax problem了 $$\\begin{align} \\min_G{ \\max_D{V(D,G)} } = E_{x \\sim p_{data}(x)} [\\log D(x)] + E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ 這邊作者很漂亮地給出了上述問題的理論證明。證明了兩件事情: 上述最佳化問題 (式(3)) 達到 global optimum 時, \\( p_g = p_d \\)。 (生成器產生出來的 pdf 會等於真實資料的 pdf，因此生成器鍊成!) 使用如下的演算法可以找到 global optimum 接下來我們只討論第一個事情的證明，因為這關係到 GAN 的弱點，也就是 WGAN 要解決的問題根源! 證明 Global optimum 發生時，鍊成生成器大方向是這樣的 A. 假如給定 \\(G\\)，我們都可以找到一個相對應的 \\(D_G^*\\) 最佳化鑑別器的目標函式 (1)。B. 改寫原來的目標函式 \\(V(G,D)\\)，改寫後只跟 \\(G\\) 有關，我們定義為 \\(C(G)\\)，這是因為對於每一個 \\(G\\) 我們已經配給它相對應的 \\(D_G^*\\) 了，接著證明最佳解只發生在 \\( p_g = p_d \\) 的情況。 步驟 A: $$V(G,D)=\\int_{x}{p_d(x)\\log(D(x))dx}+\\int_{z}{p_z(z)\\log(1-D(g(z)))dz} \\\\ =\\int_x[p_d(x)\\log(D(x))+p_g(x)\\log(1-D(x))]dx$$ 而一個 function \\(f(x)=a\\log (y)+b\\log (1-y)\\) 的最佳解為 \\(y=\\frac{a}{a+b}\\)因此我們得到 \\( D_G^*(x) = \\frac{p_d(x)}{p_d(x)+p_g(x)} \\) 步驟 B: $$\\begin{align*} &amp; C(G)=\\max_{D}V(G,D) \\\\ &amp; =E_{x \\sim p_d}[\\log D_G^*(x)]+E_{z \\sim p_z}[\\log(1-D_G^*(G(z)))] \\\\ &amp; =E_{x \\sim p_d}[\\log D_G^*(x)]+E_{x \\sim p_g}[\\log(1-D_G^*(x))] \\\\ &amp; =E_{x \\sim p_d}[\\log{\\frac{p_d(x)}{p_d(x)+p_g(x)}}]+E_{x \\sim p_g}[\\log{\\frac{p_g(x)}{p_d(x)+p_g(x)}}] \\end{align*}$$ 然後我們特別觀察如果 \\(p_g = p_d\\)，上式會 $$\\begin{align} =E_{x \\sim p_d}[-\\log 2]+E_{x \\sim p_g}[-\\log 2]=-\\log4 \\end{align}$$ 重新改寫一下 \\(C(G)\\) 如下 $$\\begin{align} C(G)=-\\log4+KL(p_d\\vert\\frac{p_d+p_g}{2})+KL(p_g\\vert\\frac{p_d+p_g}{2}) \\\\ =-\\log4+2JSD(p_d \\vert p_g) \\end{align}$$ 馬上觀察到 \\(JSD\\geq0\\) 和 \\(JSD=0 \\Leftrightarrow p_g = p_d \\)這表示 \\(C(G)\\) 最佳值為 \\(-\\log4\\)，且我們已知當 \\(p_g = p_d\\) 時達到最佳值 (式(4))，因此為最佳解 結論整個 GAN 的流程:我們基於一個生成器 \\(G\\) 去最佳化 \\(D\\) 得到 \\(D_G^*\\)，接著要繼續最佳化生成器的時候，問題從目標函式 (3) 變成等價於要最佳化一個 JSD 的問題 (式(5))。藉由最佳化 JSD 問題，得到新的 \\(G\\)，然後重複上面步驟，最後達到式(3)的最佳解，而我們可以保證此時生成器鍊成， \\(p_g = p_d\\)。 問題出在哪? 問題就出在最佳化一個 JSD 的問題上面 ! JSD 有什麼問題?我們通過最佳化 JSD，而將 \\(p_g\\) 逐漸拉向 \\(p_d\\)。但是 JSD 有兩個主要的問題: A. 在 實際狀況 下，無法給初連續的距離值，導致 gradient 大部分都是 0，因而非常難以訓練B. 產生的樣本多樣性不足，collapse mode。 這邊要解釋一下 實際狀況 是什麼意思。一般來說，真實資料我們都會用非常高的維度去表示，然而資料的變化通常只被少數幾種變因所控制，也就是只存在高維空間中的 local manifold。例如一個 swiss roll 雖然是在 3 維空間中，但它是在一個 2 維的 manifold 空間裡。 這樣會造成一個問題就是， \\(p_d\\) 和 \\(p_g\\)，不會有交集，又或者交集處的集合測度為0!這樣的情況在JSD衡量兩個機率分布的時候會悲劇。作者給出了下面一個簡單易懂的例子: 兩個機率分布都是在一個 1 維的 manifold 直線上，x 軸的距離維 \\(\\theta\\)，此時的 JSD 值為右圖所示，全部都是 \\(\\log2\\)，除了在 \\(\\theta\\) 那點的值是 0 (pdf完全重疊)。這樣計算出的 Gradients 幾乎都是 0，這也就是為什麼 GAN 很難訓練的原因。 這問題在 WGAN 之前還是有人提出解決的方法，不過就很偏工程思考: 加入 noise 使得兩個機率分部有不可忽略的重疊。因此讓 GAN 先動起來，動起來之後，再慢慢地把 noise 程度下降。這是聰明工程師的厲害辦法! 但終歸來說還是治標。真正的治本方法，必須要替換掉 JSD 這樣的量測函式才可以。 本篇鋪梗結束 (這梗也太長了)。下篇終於輪到主角登場， WGAN 的 W !","tags":[{"name":"ML","slug":"ML","permalink":"https://bobondemon.github.io/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://bobondemon.github.io/tags/Deep-Learning/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://bobondemon.github.io/tags/Generative-Model/"}]},{"title":"Why-Aggregation-Work","date":"2017-03-13T13:29:47.000Z","path":"2017/03/13/Why-Aggregation-Work/","text":"為何三個臭皮匠會勝過一個諸葛亮?在 ML 中有一類的演算法稱為 Aggregation Methods，這方法的運作方式其實我們可能從小就接觸到了。有沒有遇過一種情況就是，當一群人遇到一個不知道最好答案的時候，最直接的方式就是大家的答案取平均。聽起來很直覺，但心裡老覺得怪怪的，因為根本不知道到底可不可靠。Aggregation methods 就是這樣的運作模式，這邊就給個結論，它很可靠! 以下的推導出自於林軒田教授的講義，這裡用自己的理解方式重新表達，主要作筆記用 開頭還是給先定義清楚一些 terms，對於理解式子才不會混淆 定義在先 Input: \\(x \\in X\\) 正確答案: \\(f(x)\\) 臭皮匠: \\(g_t(x),t=1,2,…\\) 臭皮匠們的決策結果: \\(G(x)=avg_t(g_t(x))\\) 衡量方法 \\(g\\) 的錯誤率: \\( Error(g)=E_x[(g(x)-f(x))^2]\\) 這邊要特別說的是衡量一個方法 \\(g\\) 的錯誤率，是針對所有的 input \\(x\\)，也就是針對 \\(X\\) domain 來算期望平方誤差 運算簡單但有點神奇的推導 我們先針對 一個固定的 x，來看看臭皮匠們統合的意見是否真的會得到較好的結果，由於input已經固定，所以下面會忽略 x 的 term首先是 “臭皮匠們各自的平方錯誤率” 的平均值$$avg_t((g_t-f)^2)$$將平方拆開後得$$=avg_t(g_t^2-2g_tf+f^2)$$將 avg 移入並用 G=avg(gt) 定義得到$$=avg_t(g_t^2)-2Gf+f^2$$再做如下的簡單代數運算$$=avg_t(g_t^2)-G^2+(G-f)^2 \\\\=avg_t(g_t^2)-2G^2+G^2+(G-f)^2 \\\\=avg_t(g_t^2-2g_tG+G^2)+(G-f)^2 \\\\=avg_t((g_t-G)^2)+(G-f)^2$$ 目前為止是針對 一個特定的輸入 x，而我們需要知道的是對 整個 domain X 的錯誤率因此真正要計算的是這個目標錯誤率$$avg_t(Error(g_t))=avg_t(E_x[(g_t(x)-f(x))^2])$$將 Expection for all x 代入進去剛剛上面針對一個 x 的結果，得到如下式子\\begin{eqnarray}=avg_t(E_x[(g_t(x)-G(x))^2])+E_x[(G(x)-f(x))^2] \\\\=avg_t(E_x[(g_t(x)-G(x))^2])+Error(G) \\\\\\geq Error(G) \\end{eqnarray} 怎麼解釋?重複一下最後的重要式子: $$avg_t(Error(g_t)) = avg_t(E_x[(g_t(x)-G(x))^2])+Error(G) \\\\\\geq Error(G)$$ 最直接的結論就是: “統合出來的結果”的錯誤率 會比 “各自決定”的平均錯誤率 還要低 可以看到針對 一組固定 的臭皮匠們 \\({g_t}\\)，不等式左邊 \\(avg_t(Error(g_t))\\) 是固定值，因此若要找一個統合大家意見的方法 \\(G\\)，而該方法有最小的錯誤率 (最小化 \\(Error(G)\\) )，很明顯就是要最大化 \\(avg_t(E_x(g_t-G)^2)\\)，而此最大化的結果 就是 \\(G\\) 是 \\({g_t}\\) 的平均值(uniform blending)，符合我們一開始說的最直覺的策略! 另一方面，如果我們選到兩組 set \\({g_t}\\) and \\({h_t}\\) 他們的 Error 相同: \\(avg_t(Error(g_t))= avg_t(Error(h_t))\\) ，那我們當然是要選擇意見最不同的那一組臭皮匠們，這是因為意見愈不同代表 \\(avg_t(E_x(g_t-G)^2)\\) 愈大，因而導致 \\(Error(G)\\) 會愈小。 小結 剛剛上面這個結論就很有趣，意見遇不同的話，統合起來的效果愈好，也就是你我之間的意見有很大的分歧時，這代表是好事! 事實上 Adaboost 就是採取這麼一個策略，每一次的 iteration 會選擇跟上次統合完的結果意見差最多那一位臭皮匠進來，有機會再補上 Adaboost，這是我很喜歡的一種 ML 演算法。 而這邊還可以引出一個方法, Bootstrap. Bootstrap aggregation方法很簡單。對我們的dataset每一次重新resampling (e.g. 取N’筆，每次取的data都再放回去，因此data可以重複。可重複這點造成dataset的point具有weight的性質，這在adaboost每一次iteration的re-weighting有同樣意思) 這個叫做bootstrap，針對該次的data算出我們的weak learner gt，iterate很多次後，把每一次的gt做uniform blending。 我認為 aggregation methods 就算放到現在的 Deep Learning 火熱的時代還是相當有用的，除了本身這些方法如 adaboost 好用之外，其概念也相當有用，例如 Deep Learning 的 dropout 事實上可以用 bootstrap 來解釋 (有機會再補上資料)","tags":[{"name":"ML","slug":"ML","permalink":"https://bobondemon.github.io/tags/ML/"},{"name":"uniform blending","slug":"uniform-blending","permalink":"https://bobondemon.github.io/tags/uniform-blending/"},{"name":"aggregation","slug":"aggregation","permalink":"https://bobondemon.github.io/tags/aggregation/"},{"name":"adaboost","slug":"adaboost","permalink":"https://bobondemon.github.io/tags/adaboost/"},{"name":"bootstrap","slug":"bootstrap","permalink":"https://bobondemon.github.io/tags/bootstrap/"}]},{"title":"Vehicle-Tracking","date":"2017-03-12T14:27:13.000Z","path":"2017/03/12/Vehicle-Tracking/","text":"這個 Porject 目的是要偵測畫面中所有的車子, 大致上的流程是先訓練好 car/non-car 的 classifer, 然後用 sliding window 搭配不同的 window size 去偵測, 最後再把 bounding boxes 做一些後處理, 例如 merge boxes, 和對時間序列的處理以下為 git hub 的 REAMDE.md The goals / steps of this project are the following: Perform a Histogram of Oriented Gradients (HOG) feature extraction I implement HOG feature extraction and using a subset of training data to search a good settings of parameters. Images are stored in output_images/HOG_with_YCrCb.jpg and output_images/grid_search.jpg Train Classifier I trained a Linear SVM classifier with HOG + color_hist + bin_spatial which achieved 98% accuracy on test set. Sliding Window Search I implemented a sliding window search method with two scales of window. HOG features are extracted once for an given image. Showing Examples so far I showed 4 examples with the pipeline so far. Image is stored in output_images/example_before_post_processing.jpg Video Implementation I showed the results with a short video clip (test_video.mp4) as well as the final result that adopted post-processing below. Further Post-processing A buffer for heat-maps is used for keeping a 6 consecutive heat-maps in frames. This will filtered out some false accepts. Discussion A short discussion is made. – Rubric Points 1. Histogram of Oriented Gradients (HOG) Explain how (and identify where in your code) you extracted HOG features from the training images. Explain how you settled on your final choice of HOG parameters. I randomly selected examples of car and notcar and showed their HOG results in each channel of HLS space: In order to get a good enough setting for those parameters (orientations, pixels_per_cell and cells_per_block), I applied a grid searching method with a linear SVM on a small subset of training data. Grid searching space is defined as follows (24 combinations): 123orient_set = range(9,19,3)pix_per_cell_set = [4,8,16]cell_per_block_set = [1,2] The purpose of this stage is not finding the optimal, but rather, a good enough setting. So I choose orient=15, pix_per_cell=8, cell_per_block=2, cspace=&#39;RGB2YCrCb&#39; 2. Train Classifier Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them). Before training the classifier, dataset should be processed first.Since the vehicles/GTI*/*.png contains time-series data, I manually selected images to avoid train and test sets having identical images. In addition, 20% images in each training folder are treated as test images. The same partition method applied to non-vehicles images too. Then I trianed a Linear SVM model with HOG + color_hist + bin_spatial features which has performance: 1inside-acc=1.0, outside-acc=0.9802036199095022 3. Sliding Window Search Describe how (and identify where in your code) you implemented a sliding window search. How did you decide what scales to search and how much to overlap windows? The course provided a very useful code snippet that can extract HOG features once no matter how much windows are. So I reuse it as the feature extraction function!I used two types of scales, 1.5 and 1.2, which deal with large and small window respectively (car with near and far positions from camera). Also, I found that the overlaping of cells_per_step = 1 (more dense windows) has better results in my implementation. Before going through, it is worth checking the image values. Since feature extraction pipeline processed .png files with mpimg.imread, it reads images with values [0,1]. However, mpimg.imread reads the .jpg file with values within [0,255]. So it is necessary to divide 255 before calling the feature extraction pipeline while reading .jpg images with mpimg.imread. Make sure your images are scaled correctly The training dataset provided for this project ( vehicle and non-vehicle images) are in the .png format. Somewhat confusingly, matplotlib image will read these in on a scale of 0 to 1, but cv2.imread() will scale them from 0 to 255. Be sure if you are switching between cv2.imread() and matplotlib image for reading images that you scale them appropriately! Otherwise your feature vectors can get screwed up. To add to the confusion, matplotlib image will read .jpg images in on a scale of 0 to 255 so if you are testing your pipeline on .jpg images remember to scale them accordingly. And if you take an image that is scaled from 0 to 1 and change color spaces using cv2.cvtColor() you’ll get back an image scaled from 0 to 255. So just be sure to be consistent between your training data features and inference features! 4. Showing Examples Show some examples of test images to demonstrate how your pipeline is working. How did you optimize the performance of your classifier? The followings are some examples. As you can see in the example 2, there exists a false accept. This will be filtered out in the post-processing part. 5. Video Implementation Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.) Following is the final result (combined with post-processing as described below) 6. Further Post-processing Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes. A heat-map to further filtered out some false positives. Moreover, I used a buffer to keep the 6 consecutive frames of heat-maps, and then accumulated those heat-maps in buffer. The accumulated heat-map then thresholded and produced the final results. 7. Discussion Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust? There still have too much parameters that effect the robustness, like ystart, ystop, scale factors, thresholds for heat-maps, and etc. Moreover, with more challanging conditions, those settings might work in one condition but fail in others. I think the most important part in those pipelines is the classifier itself. The linear SVM I used in this project is not good enough as you can see in the video that still has few false accepts. So a deep-learning based classifier might achieve better results and actually helpful to the following pipelines. This would be my future work.","tags":[{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"},{"name":"CV","slug":"CV","permalink":"https://bobondemon.github.io/tags/CV/"}]},{"title":"Lane-Finding","date":"2017-02-27T02:12:28.000Z","path":"2017/02/27/Lane-Finding/","text":"以下是 github 上的 README, 全英文. 此 Project 主要都是在做 Computer Vision 相關的東西. 學到了許多使用 Python and CV 相關的技巧. 整理來說是個滿有趣的 project! The goals / steps of this project are the following: Compute the camera calibration matrix and distortion coefficients given a set of chessboard images. Apply a distortion correction to raw images. Use color transforms, gradients, etc., to create a thresholded binary image. Apply a perspective transform to rectify binary image (“birds-eye view”). Detect lane pixels and fit to find the lane boundary. Determine the curvature of the lane and vehicle position with respect to center. Warp the detected lane boundaries back onto the original image. Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position. Rubric Points1. Camera calibrationThe images for calculating the distortion and 3-D to 2-D mapping matrix are stored in ./camera_cal/calibration*.jpg.Firstly, I used cv2.findChessboardCorners to find out all those corner points (corners) in the images.Then I used cv2.calibrateCamera to calculate the distortion (dist) and mapping matrix (mtx) given the corners pts and their corresponding predifined 3-D pts objp 2. Provide an example of a distortion-corrected imageHere is an example of distortion-corrected image: 3. Create a thresholded binary image and provide exampleI used magnitude of gradients, direction of gradients, and L and S in HLS color space.A combined rule is used: 12combined[((mag_binary == 1) &amp; (dir_binary == 1)) |\\ ((hls_binary == 1) &amp; (dir_binary == 1) &amp; (bright_binary == 1))] = 1 Example masking image is showed: Moreover, I used widgets to help tunning the parameters of those masking functions. It can provide instantaneous binary result that really help for accelarating this step. The widgets codes are list here: 123456789def interactive_mask(ksize, mag_low, mag_high, dir_low, dir_high, hls_low, hls_high, bright_low, bright_high): combined = combined_binary_mask(image,ksize, mag_low, mag_high, dir_low, dir_high,\\ hls_low, hls_high, bright_low, bright_high) plt.figure(figsize=(10,10)) plt.imshow(combined,cmap='gray') interact(interactive_mask, ksize=(1,31,2), mag_low=(0,255), mag_high=(0,255),\\ dir_low=(0, np.pi/2), dir_high=(0, np.pi/2), hls_low=(0,255),\\ hls_high=(0,255), bright_low=(0,255), bright_high=(0,255)) 4. Perspective transformFirst, I defined the source and destination of perspective points as follows: Source Destination 585, 460 320, 0 203, 720 320, 720 1127, 720 960, 720 695, 460 960, 0 Then the perspective_warper function is defined which returns perspective image and the matrix warpM as well.warM is needed for the later step which does the inverse perspective back to the original image. 1perspective_img, warpM = perspective_warper(undist,src,dst) An example is showed here: 5. Lane line pixel and polynomial fittingI applied a windowing approach to identify the lane pixels In this example, I used 9 windows for both lane lines. The window is processed in an order from the buttom to the top. Pixels are detected by the following function 1def identify_lane_pixel(img, lcenter_in, rcenter_in, win_num=9, win_half_width=150, start_from_button=False): lcenter_in and rcenter_inare the centers (in horizontal coordinate) of windows. win_num defines how many windows are used. In this example, 9. win_half_width refers to the half length of window width start_from_button indicates how the initial centers of windows are set. Specifically, Let the current window as j and current frame index as i. If start_from_button=True, the center of window j will be initally set as window j-1. Otherwise, it will be initally set as window j in frame i-1. Then, by using the initial position just set, the lane pixels are identified if the histogram of that window is high enough. Finally, based on those identified pixels, update the center position of current widnow j. Next, a simple second order polynomial fitting is applied to both identified pixels 123# Fit a second order polynomial to eachleft_fit = np.polyfit(lpixely, lpixelx, 2)right_fit = np.polyfit(rpixely, rpixelx, 2) But wait! Since we are assuming “birds-eye view”, both lanes should be parallel! So I first tried a method that ties the polynomial coefficients except the shifting ones! this method results in the following example As can be seen in the figure, curves are indeed parallel. However, when I applied this method to the final video, I found that it wobbling a lot! (see “8. Video” below) After some investigation, I wonder that this problem is caused by the fixed source points of perspective. Since the pre-defined source points are always at the center of the camera while the lane curves are usually not, the result perspective curves is intrinsically not parellel! Hence, I applied a dynamic source point correction. Idea of method is showed in the follows: mapping inversely from coordinates in perspective images to original images can use the following formula: and results in the following example It works great! Unfortunately, if the lane curves are not stable, the resulting new source points may fail. This is the major difficulty of this method! (see “8. Video” below) 6. Radius of curvature of the lane and the position of the vehicleThe curvature is calculated based on the following formula. Udacity provides a very good tutorial here ! 1234a1, b1, c1 = left_fit_coefficientsa2, b2, c2 = right_fit_coefficientsr1 = ((1+(2*a1*height*ym_per_pix+b1)**2)**1.5)/(2*np.abs(a1))r2 = ((1+(2*a2*height*ym_per_pix+b2)**2)**1.5)/(2*np.abs(a2)) There’s no need to worry about absolute accuracy in this case, but your results should be “order of magnitude” correct. So I divide my result by 10 to make it seems more reasonable. And of course, the “order of magnitude” remains intact. 7. Warp the detected lane boundaries back onto the original imageIn order to warp back onto the original image, we need to calculate the inverse of perspective transform matrix warpMjust apply Minv = inv(warpM) which is from numpy.linalg import inv Then, simply apply cv2.warpPerspective with Minv as input. Note: use cv2.putText to print the curvature and position onto images 8. Video Simple poly-fit (Most stable! Simple is better ?!) Shared coefficients of poly-fit (Wobbling problem) Dynamic source points of perspective (Unstable, crash sometimes. If the lane curves are not stable, the resulting new source points may fail) DiscussionBasically, I applied those techniques suggested by Udacity. I did some efforts trying to parallize both curves in the perspective “bird eye view”. Two methods are applied Shared coefficients of polynomial fitting Dynamic source points of perspetive Each has its own issue. For (1.), wobbling, and for (2.) unstable. Future works will focus on solving the (2.) unstable issue. Maybe a smoothing method is a good idea. Moreover, for more difficult videos, pixels may not be detected which makes the pipeline crash. One way to overcome this problem is when this issue happens, the lane curve is set to be the same as previous frame. Generelizing this idea, a confidence measure of lane pixels is worth to apply. If the confidence is low, then set the lane curve as the same as previous frame might be a good way to better estimate result. Finally, finding a robust combination of masking rule and tweaking those parameters precisely might help too. 附上中文其他討論: Reviewer 給了很多有用的 article links! 這邊附上做未來參考 Perspective bird eye view:http://www.ijser.org/researchpaper%5CA-Simple-Birds-Eye-View-Transformation-Technique.pdfhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3355419/https://pdfs.semanticscholar.org/4964/9006f2d643c0fb613db4167f9e49462546dc.pdfhttps://pdfs.semanticscholar.org/4074/183ce3b303ac4bb879af8d400a71e27e4f0b.pdf Lane line pixel identification:https://www.researchgate.net/publication/257291768_A_Much_Advanced_and_Efficient_Lane_Detection_Algorithm_for_Intelligent_Highway_Safetyhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5017478/https://chatbotslife.com/robust-lane-finding-using-advanced-computer-vision-techniques-46875bb3c8aa#.l2uxq26sn lane detection with deep learning:http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w3/papers/Gurghian_DeepLanes_End-To-End_Lane_CVPR_2016_paper.pdfhttp://lmb.informatik.uni-freiburg.de/Publications/2016/OB16b/oliveira16iros.pdfhttp://link.springer.com/chapter/10.1007/978-3-319-12637-1_57 (chapter in the book Neural Information Processing)http://ocean.kisti.re.kr/downfile/volume/ieek1/OBDDBE/2016/v11n3/OBDDBE_2016_v11n3_163.pdf (in Korean, but some interesting insights can be found from illustrations)https://github.com/kjw0612/awesome-deep-vision (can be useful in project 5 - vehicle detection)噁心到吐血的真實挑戰: 還是老話一句, 真的要成為可用的產品, 難道超級無敵高阿!!","tags":[{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"},{"name":"CV","slug":"CV","permalink":"https://bobondemon.github.io/tags/CV/"}]},{"title":"Neural Art","date":"2017-02-13T14:04:36.000Z","path":"2017/02/13/Neural-Art/","text":"Art with Neural Network風格, 創作這種能力在現在Alpha Go已經稱霸的時代, 目前覺得還是人類獨有的不過有趣的是, 對於那些已經在 ImageNet 訓練得非常好的模型, 如: VGG-19, 我們通常已經同意模型可以辨別一些較抽象的概念那麼是否模型裡, 也有具備類似風格和創作的元素呢? 又或者風格在模型裡該怎麼表達? 本篇文章主要是介紹這篇 A Neural Algorithm of Artistic Style 的概念和實作, 另外一個很好的投影片 by Mark Chang 也很值得參考 先給出範例結果, 結果 = 原始的內容 + 希望的風格 Content Image Style Image Result Image 說在前頭的最佳化在講下去之前, 我們先講 NN 的事情, 一般情況, 我們是給定 input image x, 而參數 w 則是要求的變數, 同時對 loss (objective function) 做 optimize, 實作上就是 backprob.上面講到的三種東西列出來: x: input image (given, constant) w: NN parameters (variables) loss: objective function which is correlated to some desired measure 事實上, backprob 的計算 x and w 角色可以互換. 也就是將 w 固定為 constant, 而 x 變成 variables, 如此一來, 我們一樣可以用 backprob 去計算出最佳的 image x.因此, 如果我們能將 loss 定義得與風格和內容高度相關, 那麼求得的最佳 image x 就會有原始的內容和希望的風格了!那麼再來就很明確了, 我們要定義出什麼是 Content Loss 和 Style Loss 了 Content Loss針對一個已經訓練好的 model, 我們常常將它拿來做 feature extraction. 例如一個 DNN 把它最後一層辨識的 softmax 層拿掉, 而它的前一層的 response (做forward的結果), 就會是對於原始 input 的一種 encoding. 理論上也會有很好的鑑別力 (因最只差最後一層的softmax). Udacity 的 traffic-sign detection 也有拿 VGG-19, ResNet, 和 gooLeNet 做 feature extraction, 然後只訓練重新加上的 softmax layer 來得到很高的辨識率. 因此, 我們可以將 forward 的 response image 當作是一種 measure content 的指標!知道這個理由後, 原文公式就很好理解, 引用如下: So let p and x be the original image and the image that is generated and Pl and Fl their respective feature representation in layer l. We then define the squared-error loss between the two feature representations 簡單來說 Pl 是 content image P 在 l 層的 response, 而 Fl 是 input image x (記得嗎? 它是變數喔) 在 l 層的 response.這兩個 responses 的 squared-error 定義為 content loss, 要愈小愈好. 由於 response 為 input 的某種 encoded feature, 所以它們如果愈接近, input 就會愈接近了 (content就愈接近).引用 Mark Chang 的投影片: Style Loss個人覺得最神奇的地方就在這裡了! 當時自己怎麼猜測都沒猜到可以這麼 formulate.我個人的理解是基於 CNN 來解釋假設對於某一層 ConvNet 的 kernel 為 w*h*k (width, hieght, depth), ConvNet 的 k 通常代表了有幾種 feature maps說白一點, 有 k 種 filter responses 的結果, 例如第一種是線條類的response, 第二種是弧形類的responses … 等等而風格就是這些 responses 的 correlation matrix! (實際上用 Gram matrix, 但意義類似)基於我們對於 CNN 的理解, 愈後面的 layers 能處理愈抽象的概念, 因此愈後面的 Gram matrix 也就愈能代表抽象的 style 概念.原文公式引用如下: 總之就是計算在 l 層上, sytle image a 和 input image x 它們的 Gram matrix 的 L2-norm 值 一樣再一次引用 Mark Chang 的投影片:也可以去看看他的投影片, 有不同角度的解釋 實戰主要參考此 gitHub一開始 load VGG-19 model 就不說了, 主要的兩個 loss, codes 如下:123456789101112131415161718def content_loss_func(sess, model): \"\"\" Content loss function as defined in the paper. \"\"\" def _content_loss(p, x): # N is the number of filters (at layer l). N = p.shape[3] # M is the height times the width of the feature map (at layer l). M = p.shape[1] * p.shape[2] # Interestingly, the paper uses this form instead: # # 0.5 * tf.reduce_sum(tf.pow(x - p, 2)) # # But this form is very slow in \"painting\" and thus could be missing # out some constants (from what I see in other source code), so I'll # replicate the same normalization constant as used in style loss. return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(x - p, 2)) return _content_loss(sess.run(model['conv4_2']), model['conv4_2']) 12345678910111213141516171819202122232425262728293031323334353637383940414243# Layers to use. We will use these layers as advised in the paper.# To have softer features, increase the weight of the higher layers# (conv5_1) and decrease the weight of the lower layers (conv1_1).# To have harder features, decrease the weight of the higher layers# (conv5_1) and increase the weight of the lower layers (conv1_1).STYLE_LAYERS = [ ('conv1_1', 0.5), ('conv2_1', 1.0), ('conv3_1', 1.5), ('conv4_1', 3.0), ('conv5_1', 4.0),]def style_loss_func(sess, model): \"\"\" Style loss function as defined in the paper. \"\"\" def _gram_matrix(F, N, M): \"\"\" The gram matrix G. \"\"\" Ft = tf.reshape(F, (M, N)) return tf.matmul(tf.transpose(Ft), Ft) def _style_loss(a, x): \"\"\" The style loss calculation. \"\"\" # N is the number of filters (at layer l). N = a.shape[3] # M is the height times the width of the feature map (at layer l). M = a.shape[1] * a.shape[2] # A is the style representation of the original image (at layer l). A = _gram_matrix(a, N, M) # G is the style representation of the generated image (at layer l). G = _gram_matrix(x, N, M) result = (1 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2)) return result E = [_style_loss(sess.run(model[layer_name]), model[layer_name]) for layer_name, _ in STYLE_LAYERS] W = [w for _, w in STYLE_LAYERS] loss = sum([W[l] * E[l] for l in range(len(STYLE_LAYERS))]) return loss 一開始給定 random input image:style image 選定如下:隨著 iteration 增加會像這樣: 第一次的 backprob: 1000 iteration: 2000 iteration: 3000 iteration: 4000 iteration: 5000 iteration: 短節這之間很多參數可以調整去玩, 有興趣可以自己下載 gitHub 去測 上一篇的 “GTX 1070 參見” 有提到, 原來用 CPU 去計算, 1000 iteration 花了六個小時! 但是強大的 GTX 1070 只需要 6 分鐘! 不過, 就算是給手機用上GTX1070好了 (哈哈當然不可能), 6分鐘的一個結果也是無法接受!PRISMA 可以在一分鐘內處理完! 這必定不是這種要算 optimization 的方法可以達到的.事實上, 李飛飛的團隊發表了一篇論文 “Perceptual Losses for Real-Time Style Transfer and Super-Resolution“訓練過後, 只需要做 forward propagation 即可! Standford University 的 JC Johnson 的 gitHub 有完整的 source code!找時間再來寫這篇心得文囉!","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://bobondemon.github.io/tags/Deep-Learning/"},{"name":"Art","slug":"Art","permalink":"https://bobondemon.github.io/tags/Art/"}]},{"title":"GTX 1070","date":"2017-02-12T13:44:40.000Z","path":"2017/02/12/GTX-1070/","text":"NVIDIA GTX 1070 參見經過兩次的Udacity DNN Projects後, 我受不了用CPU訓練了! 這實在是太慢了!考量應該會長期使用GPU, AWS實在不怎麼便宜 (1hr=1USD @ Tokyo site), 加上local端訓練也比較方便, 就殺下去了!! 安裝 CUDA and cuDNN大致上的安裝流程如下, 並不複雜, 更詳細可參考 link 安裝 CUDA Drivers上述聯結中有下載路徑, 然後照頁面一步步選擇 (Operating System, Version, Installer Type)Installer Type如果網路不好建議選擇 exe local, 然後下載後執行安裝就對了Windows 環境變量中 CUDA_PATH 是 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0, 但是仍須加上 bin\\ 和 lib\\x64\\, 記得加上. 安裝 cuDNN要下載這個還要填一些登入資料, 需要再Accelerated Computing Developer Program註冊, 總之註冊後就可下載解壓後會有一個資料夾 cuda, 裡面三個子資料夾 bin, include, lib將上述的檔案放到相對應的 CUDA Driver 的安裝路徑內, 預設是在 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0 安裝 tensorflow-gpu最簡單的一步pip install tensorflow-gpu然後即可測試, 如果有成功會有以下畫面, 注意 successfully 字有無出現 1234567&gt;&gt;&gt; import tensorflow as tfI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally&gt;&gt;&gt; 測試 GTX 1070 強大能力使用兩個極端例子分別測試有無使用GPU速度上的差異 Neural Art 的例子: A Neural Algorithm of Artistic Style 這個例子是所有的東西都可以 load 進 memory 中, 因此沒有任何 I/O, 直接比拚運算能力! 因此可以直接看出 GPU 和 CPU 的計算能力差異 結果: 時間沒有很嚴格計算, 是看產生結果的時間稍微計算的, 但這效能已經很誇裝了, 60倍, 60倍, 60倍!跑出來的圖: Content Image Style Image Result Image 不是所有的情況都能把 training data 和 model 都 load 進 memory 中, 所以勢必會有其他拖慢速度的環節, 其中最慢的就是 I/O 剛好 Udacity 的 project 3 就是每筆 training data 都需要去 load image 並且 on-the-fly 運算一堆 preprocessing. 這個情況剛好是另一種可能的極端 結果跑一個epoch所花的時間為這種case看來只能加速到約 2倍. 沒辦法, 其他拖油瓶的動作佔太多比例了 短結大部分的情況下, 提升的速度範圍會落在 2~60 倍 之間, 總之是值得的! 就算不玩DNN, 電動也要把它打到物超所值!","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://bobondemon.github.io/tags/Deep-Learning/"},{"name":"NVIDIA","slug":"NVIDIA","permalink":"https://bobondemon.github.io/tags/NVIDIA/"},{"name":"cuDNN","slug":"cuDNN","permalink":"https://bobondemon.github.io/tags/cuDNN/"},{"name":"CUDA","slug":"CUDA","permalink":"https://bobondemon.github.io/tags/CUDA/"}]},{"title":"Driving by Learning Your Style","date":"2017-02-05T13:58:07.000Z","path":"2017/02/05/Driving-by-Learning-Your-Style/","text":"Udacity Self Driving Project 3: behavioral cloningA great simulator is provided that can log your driving data (speed, throttle, brake, steering, and images) and test the driving algorithm.Two modes are provided, Training mode and Atuonomous mode. By using Training mode, you can collect training data to train the model. Then test the model with the Atuonomous mode. For those driving log data, steering and images are the most important features that we are going to use in this project. The goal is, given an image, find out the corresponding steering angle. Some might wonder that speed, throttle, and brake are features that are useful too.Also, driving images are time correlated, not just a given static image.With ignoring so much useful information, does the goal still reasonable? Nvidia just showed it works! and works pretty well!So our first step is to collect the data, and fortunately, Udacity provides data for us and I used it for training. Training Data Analysis8036 data are provided. Each data has 3 positions of images (left, center, right) with 1 corresponding steering angle.Most of angles are 0, and I found that randomly ignoring half of 0-angle data is fine and can speed up. Moreover, I duplicated some samples that has angles within the range +-[0.2, 1] in order to balance the data.Histograms of before/after data selection are shown below: Data AugmentationData augmentation is a practical way to avoid overfit and generalized the model. I used 5 types of augmentations: Flipping – Flipping is a useful way to balance both turns of data. For each data, a 1/2 probability is used to decide wheter to flip. Also, steering angle is multiplied by -1. Horizontal shift – [-20,+20] pixels are randomly selected as the shift value. By doing so, it can help to recover the vehicle when it goes outside the lane.By referencing this article, I added 0.004 steering angle units per pixel shift to the right, and subtracted 0.004 steering angle units per pixel shift to the left.Results in [-0.8~+0.8] steering values adjustment which corresponding to [-2~+2] degrees (steering value * 25 = degree) Brightness – Brightness is done in the “HSV” domain. I found that with a ratio of [0.5~1.1] for “V” domain works fine. Blurring – A Gaussian blur with kernel size 3 is applied. Not sure how useful of this method helps for robustness. Left/Right camera images – These left/right images are very useful for data augmentation and also help for recovering off-lane driving. Udacity: You also might wonder why there are three cameras on the car: center, left, and right. That’s because of the issue of recovering from being off-center.In the simulator, you can weave all over the road and turn recording on and off. In a real car, however, that’s not really possible. At least not legally.So in a real car, we’ll have multiple cameras on the vehicle, and we’ll map recovery paths from each camera. I adjusted the steering angles for left/right images with a naive method. Following figure shows how I correct the angle of right image: I found that setting offset = 6 or 5 is good enough. For large value, the car starts zig-zagging. An example of correction shows below, where the steering angles are indicated by red lines: Data Normalization Normalization – Images are normalized with (x-128)/128. Cropping – Images are trimmed with 40, 20, 20, and 20 pixels from top, bottom, left, and right respectively. This will cut most of the car hood and sky. Resizing – resized to 66 x 200, same as NVIDIA CNN. Model ArchitectureI adopted NVIDIA CNN with dropout layers: Generator and Training Generator: It is very useful to use a python generator to feed the training data batch-by-batch rather than loading all the data in memory at once.A useful link to learn python iterator/generator list here ( for those who doesn’t familiar with python just like me :) ). In order to further speed up. I tried pre-loading a chunck of data, e.g. 5000 images, into memory, and loaded another chunck if the batch data (required by generator) is outside the chunck in memory. However, it does not speed up! Somewhat weired. For each input images, a position is randomly chosen (left,center,right).Then flipping and shadowing are applied with a random fair coin. Finally, brighteness and horizonal shift are adopted with the corresponding angle adjustment. Training: Some hyper-parameters are listed: epoch–50 samples for each epoch – 8896 optimizer – Adam with 1e-4 batch-size – 64 Although Keras did shuffle, it only applies in the batched data. So I shuffled the entire training set for each epoch to get more de-correlated data. Driving PolicyI found that instead of giving a constant throttle, controlling to a constant speed is more stable to drive.So I used a simple policy that tries to keep speed near 20. 123456789speed = float(speed) if speed &gt; 25: throttle = 0.05 elif speed &gt; 20: throttle = 0.2 elif speed &gt; 10: throttle = 0.35 else: throttle = 0.5 ResultsSee below for the track1 drive. However, I failed on track2. Hit a wall during a right turn and still working on it.Hope some tweaks on data selection and model architecture might work~","tags":[{"name":"ML","slug":"ML","permalink":"https://bobondemon.github.io/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://bobondemon.github.io/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"},{"name":"CNN","slug":"CNN","permalink":"https://bobondemon.github.io/tags/CNN/"}]},{"title":"traffic-sign-detection","date":"2017-01-18T14:35:21.000Z","path":"2017/01/18/traffic-sign-detection/","text":"前言終於來到 project 2 了, 這次的主要目的是練習使用 tensorflow 做交通號誌識別Dataset 為 German Traffic Sign Dataset有43種交通號誌, 是一種43選1的概念, 因為沒有考慮都不是這個選項, 理論上這類問題較簡單, 有researcher達到99.81%的辨識率 共 51839 張 training data, 而 testing 有 12630 張, 分佈如下, 可以看的出來資料分佈不均每種類別 random 挑一張出來如下圖Udacity 很好心的幫忙把所有的 image 幫你打包成只剩下 traffic sign Download, 且 cv2.resize(image,(32,32)) 了, 只需要 pickle.load 下來就搞定而原始的 data 是給你一大張image, 然後再告訴你那些traffic signs在image中的rectangular window座標, 還要再多處理較麻煩 要注意的一點是, dataset 是經由一秒鐘的 video 擷取下來, 因此鄰近的 data 會很相近 [1], 如果使用 train_test_split 會 random 選擇, 導致 train 和 validation 會相近而看不出差異 Input Data PreprocessingUdacity 建議我們可以處理幾個方向 將 data 數量弄得較 balance NN 算 loss 的時候不會根據每個類別數量的多寡作權重, 因此最單純的方法是就想辦法產生出一樣多的數量, 如第2點 可以增加 fake data 我的 image processing 實在很弱, 只單純的使用 rotation, 而且只敢稍微讓angle為正負5度, 怕那種有方向箭頭的號誌轉壞 1cv2.getRotationMatrix2D(image_center, angle, scale) 這樣的方式我實驗起來其實沒啥幫助, XD 我看到有人還使用 cv2.WarpPerspective, 果然專業多了! 我相信產生種類夠多的 fake data 一定會有幫助, 例如加 noise, blur 等等 將 data 做 normalization 做語音習慣了, 直覺就用 guassian normalization, mean=0, var=1, 結果整個大失敗! 只有不到1%辨識率, why?? 後來用 mean substraction, 然後除 abs 的最大值, 我只選擇使用 YUV 的 Y channel 當 input CNN 架構要設計和調整架構有點花時間, 加上我時間不多(懶), 所以我直接就用LeNet架構1234567layer_depth = &#123; 'layer_1': 6, 'layer_2': 16, 'fully_connected_1': 120, 'fully_connected_2': 84, 'out': n_classes,&#125; 自己多加了 dropout 和 l2 regularization, 原因是每次跑 training 的 accuracy 都要標到98 99, 但是 validation set 始終很難突破 93, 一直有 overfit 的感覺tensorflow 的 dropout 是設定要保留多少比例 (keep_prob), 在 training 的時候設定在最後的兩層 fully connected layers, keep_prob 愈小基本上愈難訓練也需要愈多 epoch另外記得在做 evaluation 的時候要把 keep_prob 設定成回 1 [1] 的架構想法不錯, 將較低層的 conv. layer 和較上層的 conv. layer 一併當作 fully connected layer 的 input, 這樣同時能夠有 low-level feature, higher-resolution 和 high-level feature, lower-resolution 兩種資訊一起當決策 其他 Hyper-parameters Optimizer: 說實話, 要不停的調整出最好的參數實在沒那個心力, 所以與其用SGD, 我就直接用 Adam 了 (Adagrad也是一種懶人選擇) pooling: 沒啥特別選, 因此用 max-pooling batch-size: 原先設定128, 有一次改成256就實在train不好, 就退回128了 learning rate: 0.001 l2 weight: 0.01 Learning Performancetest set accuracy = 0.893 自選測試圖片Udacity希望能學員自己找圖片來測試, 因此我就在德國的 google map 上找圖, (看著看著心都飄過去了)20張圖辨識結果如下:剛好錯10個, 只有 50% 正確率, 這實在有點悲劇其中有兩個錯誤值得注意右圖是top5辨識到的類別及機率, 可以發現除了正確答案的 traffic signal 在第二名外, 第一名的 general causion 其實跟 traffic signal 超像的 (只看灰階)看來必須把 input 的色彩資訊也加進去才能進一步改善了另一個是如下這個錯誤自己分析的原因是因為 training data 的 speed limit 都是圓的外框, 而此case剛好是一個長方形牌子, 裡面才是退色很嚴重的圓形, 所以導致辨識失敗或許真的 train 得很好的 CNN 有能力找出重要的判斷資訊, 因此會去忽略外面的方框, 而選擇去”看”外面退色的圓形和裡面的數字結論就是, 應該是我自己沒train好吧 ?! 短結小小做過一輪交通號誌辨識, 才比較有感覺真實狀況會有多困難阿~找時間來 visualize 一下每層的 hidden units 對什麼樣的 image 會有較高的 activation! This paper by Zeiler and Fergus with toolbox 要能 train 出好 model 除了參考文獻培養對 model 架構的好直覺外, engineering 的苦工也會是很大的關鍵! 後續嘗試對於目前的辨識率很不滿意. 不死心下就實作[1]的架構, 然後將 NN 的 model size 擴大, 並且將顏色資訊 YUV 的 U 加進去訓練 (結果上述因顏色錯誤的traffic signal就分對了)12345678910111213# Hyper-parametersEPOCHS = 30BATCH_SIZE = 128rate = 0.001drop_out_keep_prob = 0.5layer_depth = &#123; 'layer_1': 16, 'layer_2': 32, 'fully_connected_1': 256, 'fully_connected_2': 128, 'out': n_classes,&#125; 得到了 Test Accuracy = 0.953 ! 但是自選圖雖有進步仍很低 65%另外, 上述的參數設定下, 如果加了 l2_weight = 0.01 的話, validation 只能到 0.91x, 實在不大好訓練, 後來只好放棄第一次的 submission, reviewer 給了一些不錯的 reference 如下: Extra Important MaterialLately on slack few students asked for a good Deep Learning book.So after lot of research found a book which is also recommended by Elon Musk Deep Learning (Adaptive Computation and Machine Learning series) Github and on Amazon Fast.ai A Guide to Deep LearningFew Articles Traffic sign classification using brightness augmentation Dealing with unbalanced dataExtra Materials I noted a linkage here to discuss about how should we choose the batch_size of Stochastic Gradient Decent Since you might be interested into “Adam Optimizer”, here is a website that talks about it. You might like to learn the whole idea of Dropout It’s gives a brief analysis of the technique. reviewer 很用心阿!棒棒! Reference[1.] Traffic Sign Recognition with Multi-Scale Convolutional Networks","tags":[{"name":"ML","slug":"ML","permalink":"https://bobondemon.github.io/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://bobondemon.github.io/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"https://bobondemon.github.io/tags/Udacity/"},{"name":"CNN","slug":"CNN","permalink":"https://bobondemon.github.io/tags/CNN/"}]},{"title":"使用AWS訓練DNN步驟","date":"2017-01-16T13:47:43.000Z","path":"2017/01/16/aws-procedure/","text":"AWS Instance注意事項及連線建立AWS instance的時候, 由於我們使用jupyter需要port 8888, 需要 Configure the Security Group Running and accessing a Jupyter notebook from AWS requires special configurations. Most of these configurations are already set up on the udacity-carnd AMI. However, you must also configure the security group correctly when you launch the instance. By default, AWS restricts access to most ports on an EC2 instance. In order to access the Jupyter notebook, you must configure the AWS Security Group to allow access to port 8888.Click on “Edit security groups”.On the “Configure Security Group” page:Select “Create a new security group”Set the “Security group name” (i.e. “Jupyter”)Click “Add Rule”Set a “Custom TCP Rule”Set the “Port Range” to “8888”Select “Anywhere” as the “Source”Click “Review and Launch” (again) 成功建立AWS instance之後, 開啟git bashssh -i ‘C:\\Users\\bobon\\.ssh\\MyKeyPair.pem’ carnd@54.65.11.64其中54.65.11.64是instance的ip AWS上開啟jupyter notebook kernel首先先把project clone下來, 並設定好conda env1234git clone https://github.com/udacity/CarND-Traffic-Sign-Classifier-Projectcd CarND-Traffic-Sign-Classifier-Projectconda env create -f environment.ymlsource activate CarND-Traffic-Sign-Classifier-Project 接著安裝tensorflow-gpu1pip install tensorflow-gpu opencv 安裝1conda install -c https://conda.binstar.org/menpo opencv 剛剛已經建立conda的環境, 且activate CarND-Traffic-Sign-Classifier-Project, 所以可以直接開啟kernel1jupyter notebook 在local瀏覽器上輸入http://[all ip addresses on your system]:8888/例如aws ip為54.65.11.641http://54.65.11.64:8888/ 抓取AWS上的資料下來local端在自己local的terminal上12scp -i &apos;C:\\Users\\bobon\\.ssh\\MyKeyPair.pem&apos; carnd@54.65.11.64:/home/carnd/Traffic-sign/cnn-traffic-sign* ./models/scp -i &apos;C:\\Users\\bobon\\.ssh\\MyKeyPair.pem&apos; carnd@54.65.11.64:/home/carnd/Traffic-sign/checkpoint ./models/ 接著輸入密碼即可 (carnd)","tags":[{"name":"aws","slug":"aws","permalink":"https://bobondemon.github.io/tags/aws/"}]},{"title":"Hexo 中文顯示 and Markdown 測試","date":"2017-01-08T13:47:43.000Z","path":"2017/01/08/chinese-encoding/","text":"除了將Hexo的_config.yml 設定成 language: zh-tw 之外文章如果用UltraEdit編輯的話的話, 要使用轉換編碼, 將ASCII轉UTF-8(Unicode編輯), 中文才能正常顯示 引言測試同一個區塊的引言 內容文字, 強調 引言測試二同一個引言測試二的區塊 無序清單, item1 仍然是item1的內容 item2 item3 有序item1 item2 仍然是item2的內容 item3 1234567891011121314151617181920212223bool text(const string inPath, const string outPath)&#123; ifstream ifs(inPath.c_str()); if (!ifs) return false; ofstream ofs(outPath.c_str()); if (!ofs) return false; string line; while (getline(ifs,line)) &#123; istringstream iss(line); string token; while (iss&gt;&gt;token) &#123; cout &lt;&lt; \"&lt;Token&gt;: \" &lt;&lt; token &lt;&lt; endl; ofs &lt;&lt; \"&lt;Token&gt;: \" &lt;&lt; token &lt;&lt; endl; &#125; &#125; ofs.close(); ifs.close(); return true;&#125; 新的item? Here is an example of AppleScript: tell application &quot;Foo&quot; beep end tell Normal paragrah 以下為分隔線 上線使用三個*中間有空格 上線使用三個*中間無空格 上線使用5個*中間有空格 上線使用三個-中間有空格 數學公式測試 $$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$\\(x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\ 方法:在文章要有 &lt;script type=”text/javascript” src=”http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default“ &gt;&lt;/script&gt; 這行指令然後安装插件 Hexo-math, 安装方法如下, 依次为1$ npm install hexo-math --save 在 Hexo 文件夹中执行：1$ hexo math install 在 _config.yml 文件中添加：1plugins: hexo-math","tags":[{"name":"markdown","slug":"markdown","permalink":"https://bobondemon.github.io/tags/markdown/"},{"name":"hexo","slug":"hexo","permalink":"https://bobondemon.github.io/tags/hexo/"}]}]