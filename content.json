[{"title":"Stochastic Processes Week 2 Poisson Processes","date":"2021-12-12T00:42:36.000Z","path":"2021/12/12/Stochastic-Processes-Week-2-Poisson-Processes/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes (本文) Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes Week 2.1-4: Definition of a Poisson process as a special example of renewal process. Exact forms of the distributions of the renewal process and the counting processJust a recap令我們有如上面所述的 renewal process[Poisson Processes Def1]:&emsp;A Poisson process 是一個 renewal process, 且 $\\xi_i\\sim p(x)=\\lambda e^{-\\lambda x}\\cdot \\mathbf{1}(x&gt;0)$ which is exponential density function&emsp;此時 $\\mathbf{1}(\\cdot)$ 為 indicator function, 而 $\\lambda$ 稱為 intensity parameter, 或 rate of Poisson process 簡單說就是 exponential random variables 的 renewal process此時的 renewal process 的 $S_n$ (arrival time process) and $N_t$ (counting process) 有 closed form [Thm] Arrival time process $S_n$ 的 distribution and density functions:&emsp;$$F_{S_n}(x)=\\left\\{ \\begin{array} {rl} 1-e^{-\\lambda x} \\sum_{k=0}^{n-1}\\frac{(\\lambda x)^k}{k!}, &amp; x&gt;0 \\\\ 0, &amp; x&lt;0 \\end{array} \\right. \\\\ \\mathcal{P}_{S_n}(x)=\\lambda\\frac{(\\lambda x)^{n-1}}{(n-1)!}e^{-\\lambda x}\\cdot \\mathbf{1}(x&gt;0)$$ [Proof]:&emsp;我們證明 p.d.f., 使用數學歸納法&emsp;考慮 $n=1$ 時 $\\mathcal{P}_{S_1}=?$&emsp;$S_1=\\xi_1\\sim \\lambda e^{-\\lambda x}\\cdot \\mathbf{1}(x&gt;0)$&emsp;假設 $n$ 成立, 考慮 $n+1$&emsp;$$\\mathcal{P}_{S_{n+1}}(x) = \\int_{y=0}^x \\mathcal{P}_{S_n}(x-y) \\cdot \\mathcal{P}_{\\xi_{n+1}}(y) dy \\\\ = \\int_{y=0}^x \\frac{\\lambda^n(x-y)^{n-1}}{(n-1)!}e^{-\\lambda (x-y)} \\lambda e^{-\\lambda y} dy \\\\ = \\int_{y=0}^x \\frac{\\lambda^{n+1}(x-y)^{n-1}}{(n-1)!}e^{-\\lambda x} dy \\\\ = \\frac{\\lambda^{n+1}}{(n-1)!}e^{-\\lambda x} \\int_{y=0}^x (x-y)^{n-1}dy \\\\ = \\frac{\\lambda^{n+1}}{(n-1)!} e^{-\\lambda x} \\cdot \\frac{x^n}{n} = \\lambda\\frac{(\\lambda x)^n}{n!} e^{-\\lambda x}$$ [Thm] Counting process $N_t$ 是 Poisson distribution, $\\mathcal{P}{N_t=n}\\sim Pois(\\lambda t)$:&emsp;$${\\color{orange} { \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\lambda t)=e^{-\\lambda t}\\frac{(\\lambda t)^n}{n!} } }$$[Proof]:&emsp;我們有 $\\mathcal{P}\\{N_t=n\\}=\\mathcal{P}\\{ S_n\\leq t\\} - \\mathcal{P}\\{ S_{n+1}\\leq t\\}\\ldots(\\star)$&emsp;L.H.S. 意思是, 到時間 $t$ 為止正好發生 $n$ 次事件的機率&emsp;R.H.S. 以下兩種都可以解釋:&emsp;1. 到時間 $t$ 為止已發生至少 $n$ 次事件的機率, 扣掉, 到時間 $t$ 為止已發生至少 $n+1$ 次事件的機率&emsp;&emsp;—&gt; 很顯然只會剩下到時間 $t$ 為止正好發生 $n$ 次事件的機率&emsp;2. 第 $n$ 個事件發生的時間比 $t$ 小的機率, 扣掉, 第 $n+1$ 個事件發生的時間比 $t$ 小的機率.&emsp;&emsp;$\\{N_t=n\\}=\\{S_n\\leq t\\} \\cap \\{S_{n+1}&gt;t\\}$ 用 $A \\cap B$ 表示, 由於&emsp;$$\\left. \\begin{array}{r} A\\cap B=A\\backslash B^c \\\\ B^c\\subset A \\end{array} \\right\\} \\Rightarrow \\mathcal{P}(A\\cap B)=\\mathcal{P}(A)-\\mathcal{P}(B^c)$$&emsp;因此&emsp;$$(\\star) = \\left( 1-e^{-\\lambda t} \\sum_{k=0}^{n-1}\\frac{(\\lambda t)^k}{k!} \\right) - \\left( 1-e^{-\\lambda t} \\sum_{k=0}^{n}\\frac{(\\lambda t)^k}{k!} \\right) \\\\ = e^{-\\lambda t}\\frac{(\\lambda t)^n}{n!}$$ Week 2.5: Memoryless property[Def] Memoryless Property:&emsp;A random variable $X$ possesses the memoryless property, $iff$&emsp;$\\mathcal{P}\\{X&gt;u+v\\} = \\mathcal{P}\\{X&gt;u\\} \\mathcal{P}\\{X&gt;v\\}$ If $\\mathcal{P}\\{x&gt;v\\}&gt;0$, then$\\mathcal{P}\\{ X&gt;u+v \\vert X&gt;v \\} = \\mathcal{P}\\{ X&gt;u \\}\\ldots(\\square)$這是因為$$\\mathcal{P}\\{X&gt;u+v\\} = \\mathcal{P}\\{X&gt;u+v , X&gt;v\\}=\\mathcal{P}\\{X&gt;u+v \\vert X&gt;v\\}\\mathcal{P}\\{X&gt;v\\} \\\\ \\text{mem.less prop.}\\Rightarrow \\mathcal{P}\\{X&gt;u\\} \\mathcal{P}\\{X&gt;v\\} = \\mathcal{P}\\{X&gt;u+v \\vert X&gt;v\\}\\mathcal{P}\\{X&gt;v\\} \\\\ \\Rightarrow \\mathcal{P}\\{X&gt;u\\} = \\mathcal{P}\\{X&gt;u+v \\vert X&gt;v\\}$$ 從這就可以看出為何稱 memoryless. 這是因為 已經等了 $v$ 的時間, 要再多等 $u$ 的時間, 跟一開始就等 $u$ 的時間的機率是一樣的 再來有一個定理可以看出 Poisson process 適不適合用來 model 一個問題 [Thm] Memoryless is exactly exponential distribution:&emsp;Let $X$ be a random variable with density $p(x)$, 則&emsp;$X\\text{ memoryless } \\Longleftrightarrow p(x)=\\lambda e^{-\\lambda x}\\text{, }X\\sim\\text{ exponential p.d.f.}$ 範例: 公車每 $20 \\pm 2$ 分鐘來一班, 也就是說 $X$ 是公車到的時間的 r.v. 值在 $20 \\pm 2$. 這個問題是否可以用 Poisson process 來 model ? (檢察 r.v. $X$ 是否具有 memoryless property)令 $v=19$, $u=10$, 則考慮 $(\\square)$L.H.S. 為 $\\mathcal{P}\\{X&gt;29|X&gt;19\\}=0$R.H.S. 為 $\\mathcal{P}\\{ X&gt;10 \\}=1$兩者不相等, 所以不具有 memoryless property, 因此此 random variable 不能用在 renewal process 讓它成為 Poisson process [Quiz]: Week 2.6-7: Other definitions of Poisson processes之前定義 Poisson process 是 renewal process 的一個特例 (當 $\\xi$ 是 exponential distribution)現在給出另一種定義, 這種定義跟 Poisson process 與 Levy precess 的關聯有很大的關係. (之後才會知道) [Poisson Processes Def2] from Levy precess:&emsp;Poisson process is an integer-valued process, $N_t$, with $N_0=0$ a.s. (almost surely), such that&emsp;1. $N_t$ has independent increments&emsp;&emsp;$\\forall t_0&lt;t_1&lt;...&lt;t_n$, we have $N_{t_1}-N_{t_0}, ..., N_{t_n}-N_{t_{n-1}}$ are independent&emsp;2. $N_t$ has stationary increments&emsp;&emsp;$N_t-N_s$ 與 $N_{t-s}$ 具有相同的 distribution&emsp;3. $N_t-N_s\\sim Pois(\\lambda(t-s))$&emsp;&emsp;$${\\color{orange} { \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\lambda t)=e^{-\\lambda t}\\frac{(\\lambda t)^n}{n!} } }$$ 特性 3 可以推導出特性 2$X \\sim Pois(\\mu)$ has $\\mathbb{E}[X]=Var[X]=\\mu$ 我們再來推導一個定理, 該定理讓我們有第三種 Poisson process 的定義. 且跟 queueing theory 有關聯.當在一個極短的時間段之內, Poisson process 能分成三種情況:&emsp;1. 沒有事件發生&emsp;2. 事件發生 $1$ 次&emsp;3. 事件發生 $\\geq 2$ 次 定義 $f(h)=o(g(h))$ 為$\\lim_{h\\rightarrow0}\\frac{f(h)}{g(h)}=0$ 直觀理解為 $f(h)$ 比 $g(h)$ 小的更快. [Thm] Poisson process 在極小時間段的行為:&emsp;$$\\left\\{ \\begin{array}{rl} \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1 -\\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h), &amp; h\\rightarrow0 \\end{array} \\right.$$[Proof]:&emsp;我們先證明 $=0$ 的情況&emsp;根據之前的 定義2 我們知道 $\\mathcal{P}\\{N_{t+h}-N_t=0\\}=e^{-\\lambda h}$, 則&emsp;$$\\lim_{h\\rightarrow0}\\frac{1-\\mathcal{P}\\{N_{t+h}-N_t=0\\}}{h} = \\lim_{h\\rightarrow0}\\frac{1-e^{-\\lambda h}}{h}\\\\ \\text{by L&apos;Hospital&apos;s rule} = \\lim_{h\\rightarrow0}\\frac{\\lambda e^{-\\lambda h}}{1} = \\lambda$$&emsp;i.e., as $h\\rightarrow0$,&emsp;$$\\frac{1-\\mathcal{P}\\{N_{t+h}-N_t=0\\}}{h} = \\lambda + o(1) \\\\ \\Rightarrow \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1-\\lambda h + ho(1) \\\\ = 1-\\lambda h + o(h)$$&emsp;$o(h)$ 的正負號不重要, Q.E.D.&emsp;而 $\\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h), h\\rightarrow0$ 這一條可以藉由計算&emsp;$$\\lim_{h\\rightarrow0}\\frac{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}{h} = \\lim_{h\\rightarrow0}\\frac{e^{-\\lambda h}\\lambda h}{h}\\\\ \\text{by L&apos;Hospital&apos;s rule} = \\lim_{h\\rightarrow0}\\frac{-\\lambda e^{-\\lambda h}\\lambda h + \\lambda e^{-\\lambda h}}{1} = \\lambda$$&emsp;i.e., as $h\\rightarrow0$,&emsp;$$\\frac{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}{h} = \\lambda + o(1) \\\\ \\Rightarrow \\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h)$$&emsp;Q.E.D. &emsp;最後, $\\mathcal{P}\\{N_{t+h}-N_t\\geq2\\}=1-\\mathcal{P}\\{N_{t+h}-N_t=0\\} - \\mathcal{P}\\{N_{t+h}-N_t=1\\}$ 可以計算得到&emsp;注意到 $-o(h)=o(h)$, $2o(h)=o(h)$&emsp;Q.E.D. [Poisson Processes Def3] from queueing process:&emsp;Poisson process is an integer-valued process, $N_t$, with $N_0=0$ a.s. (almost surely), such that&emsp;1. $N_t$ has independent increments&emsp;&emsp; $\\forall t_0&lt;t_1&lt;...&lt;t_n$, we have $N_{t_1}-N_{t_0}, ..., N_{t_n}-N_{t_{n-1}}$ are independent&emsp;2. $N_t$ has stationary increments&emsp;&emsp;$N_t-N_s$ 與 $N_{t-s}$ 具有相同的 distribution&emsp;3. 滿足:&emsp;&emsp;$\\lim_{h\\rightarrow0} \\frac{\\mathcal{P}\\{N_{t+h} - N_t\\geq2\\}}{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}=0$ 定義 2 和定義 3 的差別只在第 3 點的條件. 然而兩種定義等價.我們其實已經證明了 (定義 2 $\\Rightarrow$ 定義 3), 但另一個方向還沒有 Week 2.8-9: Non-homogeneous Poisson processes也可以參考 https://www.randomservices.org/random/poisson/Nonhomogeneous.html, 但對我來說有點難懂 [Non-homogeneous Poisson Processes Def1]:&emsp;Let $\\Lambda(t)$ be a differentiable increasing function, and $\\Lambda(0)=0$,&emsp;$N_t$ is a N.H.P.P. (Non-Homogeneous Poisson Processes), if&emsp;1. $N_0=0$&emsp;2. $N_t$ has independent increments&emsp;3. $N_t-N_s \\sim Pois(\\Lambda(t)-\\Lambda(s))$&emsp;&emsp;$${\\color{orange} { \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\Lambda(t))=e^{-\\Lambda(t)}\\frac{\\Lambda(t)^n}{n!} } }$$&emsp;定義 $\\lambda(t) = \\Lambda’(t)$, 稱 intensity function 在 P.P. 的定義 2, 我們知道 $N_t-N_s\\sim Pois(\\lambda(t-s))$, 所以當 $\\Lambda(t)=\\lambda t$ 的話, non-homogeneous P.P. 等於 P.P. 我們可以發現 non-homogeneous 去掉 stationary increment 特性, 也就是 Poisson distribution 的 rate $\\lambda$ 在每個時間段落不一定會都一樣, depends on $\\Lambda(t)$ [Properties of N.H.P.P.]:&emsp;1. $\\mathbb{E}[N_t]=\\Lambda(t)$&emsp;&emsp;我們算一下 $\\mathbb{E}[N_t]$ for N.H.P.P.:&emsp;&emsp;$N_t = N_t - N_0 \\sim Pois(\\Lambda(t)-\\Lambda(0))=Pois(\\Lambda(t))$&emsp;&emsp;$\\therefore \\mathbb{E}[N_t]=\\Lambda(t)$&emsp;2. 如果 $\\lambda(t)=\\text{const} \\Rightarrow \\Lambda(t) = \\text{const}\\cdot t$, 回退到原來的 (homogeneous) P.P.&emsp;3. 因為 $\\Lambda(t)$ is differentialble and increasing, 所以 $\\Lambda^{-1}(t)$ 存在&emsp;&emsp;讓我們假設 $\\text{Image}(\\Lambda(t))=\\mathbb{R}^+$, 所以 $\\Lambda^{-1}(t)$ 對於 $t\\in\\mathbb{R}^+$ 都是 well defined&emsp;&emsp;對於一個 N.H.P.P. 的 $N_t$ 來說, 考慮 $N_{\\Lambda^{-1}(t)}$ 這些 r.v.s 的話, 會發現變成了 homogeneous P.P. 了! 第三點提供了一個 N.H.P.P. 與 P.P. 的對應方法但具體來說, 如果一個 N.H.P.P. 剛好是 P.P. 的話, iff 他也是個 renewal process, 下一節證明 Week 2.10-12: Relation between renewal theory and non-homogeneous Poisson processes我們試著從一個 $N_t$ 是 N.H.P.P. 去建構 renewal process 看看, 我們有如下的 N.H.P.P.Renewal process 的 arrival time $S_n$ 可以這麼構建$S_n=\\arg\\min_t\\{N_t=n\\}$ $\\{N_t=n\\}$ 表示發生 $n$ 次事件的時間的集合, 所以很顯然的取最小那個就是剛剛好第 $n$ 個事件發生的時間, i.e. $=S_n$有 $S_n$ 就可以得到 interarrival time $\\xi_n=S_n-S_{n-1}$ 這樣子的 arrival times $S_n$ and interarrival times $\\xi_n$ 是一個 renewal process 嗎?首先我們知道若要成為一個 renewal process, $\\xi_1, \\xi_2, …$ 必須是 i.i.d. 才行Note that:$${ \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\Lambda(t))=e^{-\\Lambda(t)}\\frac{\\Lambda(t)^n}{n!} }$$ 先證明以下兩個等式: $\\mathcal{P}_{\\xi_1}(x)=\\lambda(x)e^{-\\Lambda(x)}$:[Proof]:&emsp;$$\\mathcal{P}\\{\\xi_1\\leq x\\}=\\mathcal{P}\\{S_1\\leq x\\}=\\mathcal{P}\\{N_x\\geq 1\\} = 1 - \\mathcal{P}\\{N_x=0\\} \\\\ = 1- Pois(\\Lambda(x)) = 1-e^{-\\Lambda(x)}$$&emsp;用到 $\\{S_n\\leq t\\}=\\{N_t\\geq n\\}$, 然後左右等式微分得到:&emsp;$\\mathcal{P}_{\\xi_1}(x)=\\lambda(x)e^{-\\Lambda(x)}$&emsp;其中 $\\lambda=\\Lambda’$, Q.E.D. $\\mathcal{P}_{(\\xi_2|\\xi_1)}(t|s)=\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)}$:[Proof]:&emsp;先計算一下兩個 r.v.s $\\xi_1,\\xi_2$ 的 joint C.D.F. $\\mathcal{F}_{\\xi_1,\\xi_2}(s,t)$ &emsp;$$\\mathcal{F}_{(\\xi_1,\\xi_2)}(\\xi_1=s,\\xi_2=t)=\\mathcal{P}\\{\\xi_1\\leq s, \\xi_2\\leq t\\} \\\\ =\\int_{y=0}^s \\mathcal{P}\\{ \\xi_1\\leq s, \\xi_2\\leq t \\vert \\xi_1=y \\} \\mathcal{P}_{\\xi_1}(y)dy$$&emsp;$\\because y\\leq s$&emsp;$$=\\int_{y=0}^s \\mathcal{P}\\{ \\xi_2\\leq t \\vert \\xi_1=y \\} \\mathcal{P}_{\\xi_1}(y) dy \\\\ = \\int_{y=0}^s \\mathcal{P}\\{ N_{t+y} - N_y \\geq 1 \\vert \\xi_1 = y \\} \\mathcal{P}_{\\xi_1}(y) dy \\ldots(\\star)$$&emsp;由 N.H.P.P. 的 independent increments 特性知道 $(N_{t+y} - N_y),(N_y-N_0)$ 這兩個 r.v.s 為 independent. 因此&emsp;$$\\mathcal{P}\\{N_{t+y} - N_y \\geq 1 | \\xi_1=y\\} = \\mathcal{P}\\{ N_{t+y} - N_y \\geq 1 | N_y-N_0=1 \\} \\\\ = \\mathcal{P}\\{N_{t+y} - N_y \\geq 1 \\}$$&emsp;接續 $(\\star)$&emsp;$$(\\star) = \\int_{y=0}^s \\mathcal{P}\\{ N_{t+y}-N_y \\geq 1 \\} \\cdot \\mathcal{P}_{\\xi_1}(y) dy \\\\ = \\int_{y=0}^s (1 - \\mathcal{P}\\{ N_{t+y}-N_y = 0 \\}) \\cdot \\mathcal{P}_{\\xi_1}(y) dy \\\\ = \\int_{y=0}^s (1 - e^{-\\Lambda(t+y)+\\Lambda(y)}) \\cdot \\lambda(y)e^{-\\Lambda(y)} dy$$&emsp;所以兩個 r.v.s $\\xi_1,\\xi_2$ 的 joint C.D.F.&emsp;$\\mathcal{F}_{(\\xi_1,\\xi_2)}(s,t) = \\int_{y=0}^s (1 - e^{-\\Lambda(t+y)+\\Lambda(y)}) \\cdot \\lambda(y)e^{-\\Lambda(y)} dy$&emsp;微分可以計算 P.D.F.&emsp;$$\\mathcal{P}_{(\\xi_1,\\xi_2)}(s,t) = \\frac{\\partial}{\\partial t} \\left( \\frac{\\partial}{\\partial s} \\mathcal{F}_{(\\xi_1,\\xi_2)}(s,t) \\right) \\\\ \\text{replace y by s} = \\frac{\\partial}{\\partial t} \\left( (1 - e^{-\\Lambda(t+s)+\\Lambda(s)}) \\cdot \\lambda(s)e^{-\\Lambda(s)} \\right) \\\\ = \\lambda(t+s) e^{-\\Lambda(t+s)+\\Lambda(s)}\\cdot \\lambda(s) e^{-\\Lambda(s)} \\\\ = \\lambda(t+s) e^{-\\Lambda(t+s)+\\Lambda(s)} \\cdot \\mathcal{P}_{\\xi_1}(s)$$&emsp;所以&emsp;$$\\mathcal{P}_{(\\xi_2|\\xi_1)}(t|s) = \\frac{\\mathcal{P}_{(\\xi_1,\\xi_2)}(s,t)}{\\mathcal{P}_{\\xi_1}(s)} \\\\ =\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)}$$&emsp;Q.E.D. 回到考慮什麼情況下的 N.H.P.P. 的 $\\xi_1, \\xi_2, …$ 是 i.i.d. 這個問題我們先假設 $\\xi_1, \\xi_2, …$ 是 i.i.d., i.e. 假設 N.H.P.P. 是 renewal process, 則$$\\mathcal{P}_{\\xi_2 | \\xi_1}(t|s)=\\mathcal{P}_{\\xi_2}(t) \\\\ \\because\\text{i.i.d.}=\\mathcal{P}_{\\xi_1}(t), \\forall t,s&gt;0$$帶入我們花很多力氣推導的上述兩個結果得到:$\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)} = \\lambda(t)e^{-\\Lambda(t)}$然後對兩邊都做積分 $\\int_0^T\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)} dt = \\int_0^T \\lambda(t)e^{-\\Lambda(t)} dt \\ldots (\\square)$ $(\\square)$ R.H.S.:$$\\int_0^T \\lambda(t)e^{-\\Lambda(t)} dt = \\int_0^T e^{-\\Lambda(t)}d\\Lambda(t) \\left(= \\int_0^Te^{-y}dy\\right) \\\\ = -e^{-\\Lambda(T)}+e^{-\\Lambda(0)} = 1 -e^{-\\Lambda(T)}$$ $(\\square)$ L.H.S. 同理$$\\int_0^T\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)} dt = e^{\\Lambda(s)}\\int_0^T e^{-\\Lambda(t+s)} d\\Lambda(t+s) \\\\ \\left(\\text{this as: }e^{\\Lambda(s)}\\int e^{-y}dy\\right) \\\\ = e^{\\Lambda(s)} \\left[\\left. -e^{\\Lambda(t+s)}\\right|_{t=0}^T \\right] = e^{\\Lambda(s)}\\left[-e^{-\\Lambda(T+s)}+e^{-\\Lambda(s)}\\right] = -e^{-\\Lambda(T+s)+\\Lambda(s)}+e^0$$ 所以 L.H.S = R.H.S.$$\\Rightarrow e^0-e^{-\\Lambda(T+s)+\\Lambda(s)} = 1 - e^{-\\Lambda(T)} \\\\ \\Rightarrow \\Lambda(T+s) - \\Lambda(s) = \\Lambda(T), \\forall T,s&gt;0$$ 因為 $\\Lambda$ is increasing function, 上式知道是 linear function, 所以會得到 $\\Lambda(t)=\\text{const}\\cdot t$而這正好表明了 N.H.P.P. 變成了 homogeneous P.P. 了 結論是:N.H.P.P. is a renewal process $\\Longleftrightarrow$ $\\Lambda (t)=\\lambda t$ (i.e. 此時的 N.H.P.P. 也變成 homogenous 了) $(\\Longleftarrow)$ 證明很容易, 因為 homogeneous P.P. 是 renewal process 換句話說 N.H.P.P. is a renewal process $\\Longleftrightarrow$ it is homogenous P.P. Week 2.13: Elements of the queueing theory. M/G/k systems-1我們先前證明過 [Thm] Poisson process 在極小時間段:&emsp;$$\\left\\{ \\begin{array}{rl} \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1 -\\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h), &amp; h\\rightarrow0 \\end{array} \\right.$$ 對於 N.H.P.P. 也有類似的結果 [Thm] Non-homogeneous Poisson process 在極小時間段:&emsp;$$\\left\\{ \\begin{array}{rl} \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1 -{\\color{orange}{\\lambda(t)}} h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t=1\\} = {\\color{orange}\\lambda(t)} h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h), &amp; h\\rightarrow0 \\end{array} \\right.$$ 所以類似的對於 N.H.P.P. 我們也有另一種定義 (類似 [Poisson Processes Def3]) [Non-homogeneous Poisson Processes Def2]:&emsp;Let $\\Lambda(t)$ be a differentiable increasing function, and $\\Lambda(0)=0$,&emsp;$N_t$ is a N.H.P.P. (Non-Homogeneous Poisson Processes), if&emsp;1. $N_0=0$&emsp;2. $N_t$ has independent increments&emsp;3. 滿足:&emsp;&emsp;$\\lim_{h\\rightarrow0} \\frac{\\mathcal{P}\\{N_{t+h} - N_t\\geq2\\}}{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}=0$ (Non-homogeneous) Poisson processes 很適合用來 modeling queueing processes. 我們用 $M,D,G$ 來表示 distribution 種類: $M$: exponential distribution. 所以如果用來描述 arrival processes (memoryless) 就會變成 Poisson processes $D$: deterministic (constant distribution, 即不受 time $t$ 的影響? 還是連 value 都是 constant?) $G$: general, 表示可以是任何 distribution Queueing processes 包含了三個字母, e.g. $M/G/k$, 分別表示 Arrival process, Service time, 和 Number of servers Arrival process: $\\in\\{M,D,G\\}$ Service time: $\\in\\{M,D,G\\}$. 同時為 I.I.D. Number of servers: $\\in\\{1, 2, ..., \\infty\\}$ 我們以學生上機房用電腦來舉例, 若用 queueing process 為 $M/G/\\infty$ 來描述的話Arrival processes (by Poisson process) 描述了學生來的 $N(t)$Service time 表示學生會用多久電腦, 用 $G_Y(t)$ 這個 distribution 描述而電腦 (server) 的數量為 $\\infty$, 表示學生一到就馬上有一台電腦可以用, 無需等待 (更複雜的 queueing process 會考慮等待時間)所以 $N(t)$ 是一個 Poisson process, 考慮一個 fixed time $\\tau&gt;0$, 我們會有兩個 processes $N_1(t)$ and $N_2(t)$.$N_1(t)$ 表示有多少 還在處理 的事件 at time $\\tau$ (即在時間 $\\tau$ 還有多少學生在用電腦)$N_2(t)$ 表示有多少 已處理完 的事件 at time $\\tau$ (即在時間 $\\tau$ 已有多少學生用完電腦) $t$ and $\\tau$ 的關係在討論區有人這麼回答 考慮 $N_1(t+h) - N_1(t)$, 這個值表示在這一段時間內共來了多少事件並且還在處理 (即這段時間來了多少學生, 並且這些學生都還在用電腦), 所以:$$\\mathcal{P}\\{ N_1(t+h)-N_1(t)=1 \\} \\\\ = \\mathcal{P}\\{ N(t+h)-N(t)=1 \\} \\cdot \\mathcal{P}\\{ Y&gt;\\tau-t \\} + o(h) \\ldots(\\blacktriangle)$$ $Y$ 表示 service time 的 random variable, 其 distribution 為 $G_Y(t)$ “這段 $h$ 時間來了多少學生, 並且這些學生都還在用電腦” 可以近似於:“這段 $h$ 時間來了 $1$ 個學生, 並且這 $1$ 個學生還在用電腦” + $o(h)$注意到 $2$ 個學生以上的情形我們用 $o(h)$ 表示即可, 這是因為 Poisson process 的一個性質:$\\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h),h\\rightarrow0$ 所以用上開頭的 Theorem,$$(\\blacktriangle) = (\\lambda h + o(h)) \\cdot (1-G_Y(\\tau-t)) + o(h) \\\\ = \\lambda h(1-G_Y(\\tau-t)) + o(h)$$ 重述一遍, 我們有:$\\mathcal{P}\\{ N_1(t+h)-N_1(t)=1 \\} = \\lambda(1-G_Y(\\tau-t))h + o(\\delta)$令 $\\lambda(1-G_Y(\\tau-t))$ 等於某個 function $\\lambda_1(t)$, 則上式與 N.H.P.P. 的性質結果相同.同樣可以對 $\\mathcal{P}\\{ N_1(t+h)-N_1(t)=0 \\}$ 和 $\\mathcal{P}\\{ N_1(t+h)-N_1(t)\\geq2 \\}$ 推導出相同結果. 所以根據 [Non-homogeneous Poisson Processes Def2] 我們得到 $N_1$ 是 N.H.P.P. 的結論 $N_1$ 是 N.H.P.P. 且其 intensity function $\\lambda_1(t)=\\lambda(1-G_Y(\\tau-t))$ 我們對於 $N_1$ 的推論同樣也可以用在 $N_2$ 上, 結果也是一樣: $N_2$ 是 N.H.P.P. 且其 intensity function $\\lambda_2(t)=\\lambda \\cdot G_Y(\\tau-t)$ 下一節課會證明 $N_1$ and $N_2$ 為互相獨立的 r.v.s Week 2.14: Elements of the queueing theory. M/G/k systems-2欲證 $\\mathcal{P}\\{N_1(t)=n_1, N_2(t)=n_2\\} = \\mathcal{P}\\{N_1(t)=n_1\\}\\mathcal{P}\\{ N_2(t)=n_2\\}$$$\\mathcal{P}\\{N_1(t)=n_1, N_2(t)=n_2\\}\\\\ = \\mathcal{P}\\{ N_1(t)=n_1, N_2(t)=n_2 | N(t)=n_1+n_2 \\} \\cdot \\mathcal{P}\\{N(t)=n_1+n_2\\} \\\\ = \\mathcal{C}_{n_1}^{n_1+n_2}(1-G(\\tau-t))^{n_1}(G(\\tau-t))^{n_2} \\cdot e^{-\\lambda t}\\frac{(\\lambda t)^{n_1+n_2}}{(n_1+n_2)!} \\ldots(=)$$$(=)$ 為 Binomial term, 把”成功”的機率當成 “事件在時間 $\\tau$ 還在served的機率”, 這個機率由 service time 的 distribution probability 可以知道:$\\mathcal{P}\\{Y&gt;(\\tau-t)\\}=1-\\mathcal{P}\\{Y\\leq(\\tau-t)\\}=1-G(\\tau-t)$ 所以失敗的話就是 $G(\\tau-t)$共有 $n$ 個事件, 共有 $n_1$ 個事件 $\\in N_1(t)$, and $n_2$ 個事件 $\\in N_2(t)$, 所以 $\\mathcal{C}_{n_1}^{n_1+n_2}$ $$(=)= \\frac {(\\lambda t(1-G(\\tau-t)))^{n_1}}{n_1!} e^{-\\lambda t(1-G(\\tau-t))} \\cdot \\frac {(\\lambda tG(\\tau-t))^{n_2}}{n_2!} e^{-\\lambda tG(\\tau-t)} \\\\ =\\mathcal{P}\\{N_1(t)=n_1\\}\\mathcal{P}\\{N_2(t)=n_2\\}$$ 我不知道的是為何 $\\frac {(\\lambda t(1-G(\\tau-t)))^{n_1}}{n_1!} e^{-\\lambda t(1-G(\\tau-t))} = \\mathcal{P}\\{N_1(t)=n_1\\}$因為只知道 $N_1$ 是 N.H.P.P. 且其 intensity function $\\lambda_1(t)=\\lambda(1-G_Y(\\tau-t))$所以必須求得 $\\Lambda_1(t)$ 才能代入$\\mathcal{P}\\{N_t=n\\} \\sim Pois(\\Lambda(t))=e^{-\\Lambda(t)}\\frac{\\Lambda(t)^n}{n!}$所以看起來 $\\Lambda_1(t)=\\lambda t(1-G(\\tau-t))?$ 不懂… Q.E.D. Week 2.15-17: Compound Poisson processes可參考一個淺顯易懂的定義: https://gtribello.github.io/mathNET/COMPOUND_POISSON_PROCESS.html [Compound Poisson Processes (C.P.P.) Def]:&emsp;$X_t=\\sum_{k=1}^{N_t} \\xi_k$&emsp;其中&emsp;- $N_t$ 是 Poisson process with intensity $\\lambda$&emsp;- $\\xi_1,\\xi_2,…$ are i.i.d.&emsp;- $\\xi_1,\\xi_2,…$ and $N_t$ are independent $X_t$ 的 distribution 沒有 cloded form, 但若是某些特定的 $\\xi$ distribution 可以算出來. 注意到若 $\\xi_k=1$, 則 $X_t=N_t$, 可藉此想像一下 C.P.P. 的物理意義 如果 $\\xi$ (C.P.P.) 是 non-negative integer values, 我們使用 PGF 幫助計算[Probability Generating Function (PGF) Def]:&emsp;Let $\\xi$ 是一個 integer 的 random variable, with $\\geq 0$ values. 則 PGF 定義為:&emsp;$\\varphi_\\xi(u)=\\mathbb{E}[u^\\xi], \\text{ where }|u|&lt;1$ 根據定義我們可以得到 (expectation 寫出來), 如果 $\\xi_1 \\perp \\xi_2$, 則 $\\varphi_{\\xi_1+\\xi_2}(u)=\\varphi_{\\xi_1}(u)\\varphi_{\\xi_2}(u)$ 如果 $\\xi$ (C.P.P.) 是 non-negative (real) values, 我們使用 MGF 幫助計算[Moment Generating Function (MGF) Def]:&emsp;跟 Laplace transform 密切相關.&emsp;$\\mathcal{L}_\\xi(u)=\\mathbb{E}[e^{-u\\xi}], \\text{ where } \\xi\\geq0, u&gt;0$ 其實就是 $\\mathcal{L}_f(s)=\\int_{x=0}^\\infty e^{-sx}f(x)dx$, 將 $f$ 以 $\\xi$ 的 P.D.F. 代入 根據定義我們可以得到, 如果 $\\xi_1 \\perp \\xi_2$, 則 $\\mathcal{L}_{\\xi_1+\\xi_2}(u)=\\mathcal{L}_{\\xi_1}(u)\\mathcal{L}_{\\xi_2}(u)$. 注意到 ${\\xi_1+\\xi_2}$ 其 P.D.F. 是 $\\xi_1,\\xi_2$ 的 P.D.F.s 的 convolution. 之前我們也證過 Laplace transform 這個性質 對於 $\\xi$ (C.P.P.) 是 general case 的情況來說, 我們需借助 characteristic function 幫忙[Characteristic Function Def]:&emsp;For random variable $\\xi$, 定義 characteristic function $\\Phi:\\mathbb{R}\\rightarrow \\mathbb{C}$ 為&emsp;$\\Phi_\\xi(u) = \\mathbb{E}\\left[ e^{iu\\xi} \\right]$ 同樣根據定義我們可以得到, 如果 $\\xi_1 \\perp \\xi_2$, 則 $\\Phi_{\\xi_1+\\xi_2}(u)=\\Phi_{\\xi_1}(u)\\Phi_{\\xi_2}(u)$ [Characteristic Function of Increment of C.P.P.]:&emsp;For $t&gt;s\\geq0$, and $X_t$ is a C.P.P., we have&emsp;$\\Phi_{X_t-X_s}(u)=e^{\\lambda(t-s)(\\Phi_{\\xi_1}(u)-1)}$[Proof]:&emsp;$$\\Phi_{X_t-X_s}(u)=\\mathbb{E}\\left[ e^{iu(X_t-X_s)} \\right] \\\\ =\\sum_{k=0}^\\infty {\\color{orange} {\\mathbb{E}\\left[ \\left. e^{iu(X_t-X_s)} \\right| N_t-N_s=k\\right]} } \\cdot {\\color{green} {\\mathcal{P}\\{N_t-N_s=k\\}} }$$&emsp;注意到已知 $N_t-N_s=k$ 的情況下, $X_t-X_s$ 根據 C.P.P. 的定義就是 $\\xi_1+…+\\xi_k$, 再加上我們知道 $\\xi\\perp N_t$, 所以橘色部分的 condition 就可以拔掉. 同時已知綠色部分為 Poisson Processes.&emsp;因此:&emsp;$$= \\sum_{k=0}^\\infty \\mathbb{E}\\left[e^{iu(X_t-X_s)}\\right] \\cdot Pois(\\lambda(t-s)) \\\\ = \\sum_{k=0}^\\infty (\\Phi_{\\xi_1}(u))^k \\cdot e^{-\\lambda(t-s)}\\frac{(\\lambda(t-s))^k}{k!} \\\\ =e^{-\\lambda(t-s)}\\sum_{k=0}^\\infty\\frac{(\\Phi_{\\xi_1}(u)\\lambda(t-s))^k}{k!} \\\\ = e^{-\\lambda(t-s)}e^{\\Phi_{\\xi_1}(u)\\lambda(t-s)} =e^{\\lambda(t-s)(\\Phi_{\\xi_1}(u)-1)}$$&emsp;Q.E.D. 此定理描述了 increment of C.P.P. 的 characteristic function, 其具有 closed form solution所以 characteristic function of $X_t$ is$\\Phi_{X_t}(u)=e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)}$ 課程老師說這個 Theorem 很重要, 可以根據它推導出很多 Corollaries [Expectation and Variance of C.P.P.]:&emsp;對於此節定義的 C.P.P. 我們有&emsp;$$\\mathbb{E}[X_t]=\\lambda t\\mathbb{E}[\\xi_1] \\\\ Var[X_t]=\\lambda t\\mathbb{E}[\\xi_1^2]$$ 原來的 Poisson distribution $Pois(\\lambda t)$ 的 mean and variance 為 $\\lambda t$, 所以 C.P.P. 等於多乘上 $\\xi_1$ 的 moments [Proof]:&emsp;課程證 expectation, 而 variance 可用同樣流程證明&emsp;$\\mathbb{E}[\\xi^r]&lt;\\infty\\Rightarrow\\Phi_\\xi(u)$ is r-times 可微 at 0&emsp;因為 derivative and expectation 都是線性的, 所以我們有&emsp;$\\Phi^{(1)}_\\xi(u)=\\frac{d}{du}\\mathbb{E}[e^{iu\\xi}]=\\mathbb{E}[(i\\xi)e^{iu\\xi}]$&emsp;$\\Phi^{(2)}_\\xi(u)=\\frac{d}{du}\\frac{d}{du}\\mathbb{E}[e^{iu\\xi}]=\\mathbb{E}[(i\\xi)^2 e^{iu\\xi}]$&emsp;$…$&emsp;$\\Phi^{(r)}_\\xi(u)=\\mathbb{E}[(i\\xi)^r e^{iu\\xi}]$&emsp;$\\therefore \\Phi_\\xi^{(r)}(0)=i^r\\cdot\\mathbb{E}[\\xi^r]$&emsp;由前面的 Theorem 知道 $\\Phi_{X_t}(u)=e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)}$, 所以&emsp;$$\\Phi_{X_t}^{(1)}(u)=\\frac{d}{du}\\Phi_{X_t}(u) =\\frac{d}{du}e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)} \\\\ =\\lambda t \\Phi_{\\xi_1}^{(1)}(u)e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)} = \\lambda t \\Phi_{\\xi_1}^{(1)}(u)\\Phi_{X_t}(u)$$&emsp;計算 $\\mathbb{E}[X_t]$:&emsp;$$\\mathbb{E}[X_t]=\\frac{\\Phi_{X_t}^{(1)}(0)}{i} = \\frac{\\lambda t \\Phi_{ \\xi_1}^{(1)}(0) \\overbrace{\\Phi_{X_t}(0)}^{=1} } {i}$$&emsp;而根據 characteristic function 的特性&emsp;$\\frac{\\Phi_{\\xi_1}^{(1)}(0)}{i} = \\mathbb{E}[\\xi_1]$&emsp;所以 $\\mathbb{E}[X_t]=\\lambda t\\mathbb{E}[\\xi_1]$&emsp;Q.E.D. Applications of the Poisson Processes and Related ModelsApplications of the Poisson Processes and Related Models.pdf","tags":[{"name":"Coursera","slug":"Coursera","permalink":"http://yoursite.com/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"http://yoursite.com/tags/Stochastic-Processes/"},{"name":"Poisson Process","slug":"Poisson-Process","permalink":"http://yoursite.com/tags/Poisson-Process/"}]},{"title":"Stochastic Processes Week 1 Introduction & Renewal processes","date":"2021-12-11T12:56:53.000Z","path":"2021/12/11/Stochastic-Processes-Week-1-Introduction-Renewal-processes/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 Week 1: Introduction &amp; Renewal processes (本文) Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes Week 1.2: Difference between various fields of stochastics數學上的 stochastics 跟三個主要學科有關: Probability theory Mathematical statistics stochastic processes 用一個池子裡面的魚來當例子, 假設我們要分析某個時間點池子裡有多少魚, i.e. $N$ 條魚Probability theory 就是找出這個 $N$ 的機率分布, 然後可以分析其 mean, variance …而 mathematical statistics 是藉由一些統計實驗, 去估計出 $N$ 舉例來說, 抓 $M$ 條魚做記號後放回池子. 然後一次實驗為抓 $n$ 條魚, 若發現有 $m$ 條做過記號, 則這個機率我們可以算出來:$\\mathcal{P}\\{\\#\\text{marked}=m\\}=(C_M^m\\cdot C_{N-M}^{n-m})/C_N^n$ 我們重複這個實驗 $q$ 次, 得到的做過記號的魚的次數為 ${m_1,m_2,…,m_q}$, 因此可以算出 log-likelihood:$L(N)=\\sum_{k=1}^q \\log\\mathcal{P}\\{\\#\\text{marked}=m_k\\}$ 所以求解 $\\arg\\max_N L(N)$對於 stochastic processes 我們也可以問同樣的問題: $N$ 是多少? 不過此時會多考慮 $N$ 隨著時間變化 Week 1.3: Probability space建議先閱讀[測度論] Sigma Algebra 與 Measurable function 簡介: https://ch-hsieh.blogspot.com/2010/04/measurable-function.html[機率論] 淺談機率公理 與 基本性質: https://ch-hsieh.blogspot.com/2013/12/blog-post_7.html 課程使用兩個隨機實驗 (如同上面的參考連結): 從閉區間 $[0,1]$ 之中 任選一個數字 做 $n$ 次的丟銅板實驗 我們引用參考連結的定義$\\mathcal{F}$ 是一個 collection of subsets of $\\Omega$, 也就是說每一個 element 都是一個 $\\Omega$ 的 subset. 除此之外, 還必須是 $\\sigma$-algebra. $\\sigma$-algebra 和 topology 定義可參考 https://www.themathcitadel.com/topologies-and-sigma-algebras/ 所以一個 “事件” $A$ 是一個 subset of $\\Omega$ (i.e. $A\\subseteq\\Omega$), 也是一個 element of $\\mathcal{F}$, (i.e. $A\\in\\mathcal{F}$). 💡 注意 $A$ 是 subset of $\\Omega$, 還不夠. $A$ 還必須從是 $\\sigma$-algebra 的 $\\mathcal{F}$ 裡面挑. 最後 $\\mathcal{P}:\\mathcal{F} \\rightarrow [0,1]$ 此函數必須滿足以下公理對比 measure 的定義, 相當於多出了一條 $P(\\Omega)=1$, 所以 probability measure 是一個特殊的 measure再對比 measure space 的定義, 差別只在於 probability space 用的 measure 為 probability measure Week 1.4: Definition of a stochastic function. Types of stochastic functions.首先 random variables 其實是一個 measurable function $\\xi:\\Omega\\rightarrow\\mathbb{R}$, 要搞清楚什麼是 measurable function 之前, 我們先要了解 Borel $\\sigma$-algebra, Measurable Space 與 Measurable sets 一樣, 主要參考 [測度論] Sigma Algebra 與 Measurable function 簡介 $\\sigma(\\mathcal{E})$ 表示包含 $\\mathcal{E}$ 的最小 $\\sigma$-algebra, 且一定存在 (證明請參考該文章)令 $X:=\\mathbb{R}$, 則我們可以取所有 open sets 產生 Borel $\\sigma$-algebra, 記做 $\\mathcal{B}(\\mathbb{R})$ $\\mathcal{B}(\\mathbb{R})$ 可以想成實數軸上包含所有 open sets 的最小 $\\sigma$-algebra, 由 $\\sigma$-algebra 定義知也包含 closed sets $[a,b]$, ${a}$, $(a,b]$, $[a,b)$, 以及上述這些的 countable union (complement) 可稱 $\\sigma$-algebra 中的元素為 measurable set接著定義可測函數:對於 $f$ 來說, 其 domain ($X$) and image ($Y$) spaces 都配備了對應的 $\\sigma$-algebra $\\mathcal{A},\\mathcal{B}$回到 probability space, $(\\Omega, \\mathcal{F}, \\mathcal{P})$, 我們知道 $\\mathcal{F}$ 是定義在 sample space $\\Omega$ 的 $\\sigma$-algebra, i.e. $(\\Omega,\\mathcal{F})$ 是 measurable space.而 random variable $Z$ 其實是定義為由 $\\Omega$ 映射到 $\\mathbb{R}$ 的 measurable function. $\\mathbb{R}$ 配備 $\\mathcal{B}(\\mathbb{R})$.舉一個 Khan Academy 淺顯的例子, $X,Y$ 為兩個 r.v.s 分別把 outcomes 對應到 $\\mathbb{R}$ $Y$ 的 outcomes 是$\\{(a_1,...,a_7):a_i\\in \\{\\text{head,tail} \\} \\}$共 $2^7$ 種可能Mapping 到 $\\mathbb{R}$ 後我們就可以算對應的機率, 例如$\\mathcal{P}(Y\\leq30)$ or $\\mathcal{P}(Y\\text{ is even})$而需要 r.v. 是 measurable function 的原因可以從這看出來, 因為我們要知道 pre-image:$\\{\\omega:Y(\\omega)\\leq30\\}$也就是滿足這條件的 outcomes 集合, 必須要 $\\in\\mathcal{F}$. 所以它才會是個 “事件”這是因為我們的 probability measure $\\mathcal{P}:\\mathcal{F}\\rightarrow [0,1]$, 是定義在 $\\mathcal{F}$ (事件的 $\\sigma$-algebra) 上 Week 1.5: Trajectories and finite-dimensional distributions[Def]: Stochastic process &emsp;Stochastic process 是一個 mapping $X:T\\times\\Omega\\rightarrow\\mathbb{R}$, 而通常 $T=\\mathbb{R}^+$, 且滿足: &emsp;$\\forall t \\in T$, we have $X_t=X(t,\\cdot)$ is a random variable on probability space $(\\Omega,\\mathcal{F},\\mathcal{P})$ [Def]: Trajectory (sample path, or path) of a stochastic process &emsp;對一個 stochastic process $X$ 來說, fixed $\\omega\\in\\Omega$, 我們得到 $X(\\cdot,\\omega)$ 為 $T$ 的函數, 這就是 trajectory [Def]: Finite dimensional distributions &emsp;對一個 stochastic process $X$ 來說, 我們根據時間可以拿到 $n$ 個 random variables: &emsp;$(X_{t_1}, X_{t_2}, ..., X_{t_n})$, where $t_1, t_2,...,t_n \\in \\mathbb{R}$ 在 probability theory 裡面都是將這 $n$ 個 r.v.s 視為獨立, 但在 stochastic processes 裡面不能. 這是很大的不同. 所以就算是 finite dimensional distribution, 在 stochastic processes 也是很挑戰的. video 的試題: Week 1.6: Renewal process. Counting process可參考詳細解說: https://www.randomservices.org/random/renewal/Introduction.html第一個事件發生的時間為 $T_1$, 第二個事件發生的時間為 $T_2$, …每一個事件要隔多久發生都是從一個 random variable $X_i$ 決定的因此我們會有一個 sequence of interarrival times, $X=(X_1,X_2,…)$所以 sequence of arrival times 就會是 $T = (T_1, T_2, …)$, 其中$T_n=\\sum_{i=1}^nX_i$ , $n\\in\\mathbb{N}$它們的關係用圖來看如下:最後我們可以定義一個 random variable $N_t$ 表示到時間 $t$ 為止有多少個”事件”到達了:$N_t=\\sum_{n=1}^\\infty \\mathbf{1}(T_n\\leq t)$, $t\\in[0,\\infty)$其中 $\\mathbf{1}(\\cdot)$ 表示 indicator function.又或者可以用課程上的定義:$N_t=\\arg\\max_n\\{T_n\\leq t\\}$所以可以定義 counting process 為一個 random process $N=(N_t:t\\geq 0)$我們可以將 $N$ 這個 random processes 的 trajectory (path) 畫出來.這個意思就是 fixed 一個 sample space 的值, 例如固定 $X’=(X_1=0.5,X_2=0.11,…)$, 然後對每個時間點的 $N_t$ 的值隨著時間畫出來. 我們會發現是如下的 increasing step function:$T_n\\leq t$ 意思就是 $n$ 個事件發生的時間比 $t$ 小. 等同於到時間 $t$ 為止至少有 $n$ 個事件已經發生, i.e. $N_t\\geq n$ [Properties]: counting variables $N_t$ 與 arrival times $T_n$ 的關聯如下:&emsp;1. ${N_t\\geq n}={T_n\\leq t}$ or ${N_t\\leq n}={T_n\\geq t}$&emsp;2. $\\{N_t=n\\}=\\{T_n\\leq t\\} \\cap \\{T_{n+1}&gt;t\\}$ Week 1.7: Convolution我們有兩個互為獨立的 r.v.s $X\\bot Y$, 且已知 $X\\sim F_X$, $Y\\sim F_Y$ (in c.d.f.) 或是寫成 $X\\sim P_X$, $Y\\sim P_Y$ (in p.d.f.). $F_X$ and $F_Y$ 是 cumulated distribution function of $X$ and $Y$$P_X$ and $P_Y$ 是 probability density function of $X$ and $Y$$F_X(x)=P_X(X&lt;x)$ 則 convolution of two independent random variables 記做: $F_X\\ast F_Y$ (convolution in terms of distribution function): $F_{X+Y}(x)=\\int_\\mathbb{R} F_X(x-y)dF_Y(y)$ 或 $P_X \\ast P_Y$ (convolution in terms of density function): $P_{X+Y}(x)=\\int_\\mathbb{R} P_X(x-y)P_y(y)dy$ 💡 Convolution 同樣都是用 $\\ast$ 表示, 但根據是 c.d.f. or p.d.f. 會有不同定義, 然而兩者為等價 把 convolution 擴展到 $n$ 個 i.i.d. r.v.s $\\{X_1,X_2,...,X_n\\}$, 其中 $X_i \\sim F$, 則:$S_n=X_1+...+X_n\\sim {\\color{orange}{F^{n\\ast}=\\underbrace{F\\ast ...\\ast F}_\\text{n times}}}$ 注意到這裡的 convolution, $\\ast$, is in terms of distribution function 有幾個特性: $F^{n\\ast}(x)\\leq F^n(x)$, if $F(0)=0$這是因為$$\\{X_1 + ... + X_n \\leq x\\}\\subseteq\\{X_1\\leq x, ..., X_n\\leq x\\} \\\\ \\therefore P\\{X_1 + ... + X_n \\leq x\\}\\leq \\prod_{i=1}^n P\\{X_i\\leq x\\} \\\\ =F^{n\\ast}(x)\\leq \\prod_{i=1}^n F(x)=F^n(x)$$ $F^{n\\ast}(x)\\geq F^{(n+1)\\ast}(x)$這是因為$\\{X_1 + ... + X_n \\leq x\\}\\supseteq\\{X_1 + ... + X_n + X_{n+1} \\leq x\\}$ [Thm] Expectation of counting process equals to renewal function $\\mathcal{U}(t)$:&emsp;考慮 renewal process: $T_n=T_{n-1}+X_n$, 且 $X_1,X_2,…$ are i.i.d. r.v.s with distribution function $F$. $\\mathcal{U}(t):=\\sum_{n=1}^\\infty F^{n\\ast}(t) &lt; \\infty$ 課程略過證明. 此定理告訴我們該序列收斂. $F^{n\\ast}(t)$ 的意思是 $n$ 個 i.i.d. r.v.s 相加的 CDF, i.e. $P(X_1+…+X_n\\leq t)=P(T_n\\leq t)$, 而在 renewal process 指的就是 arrival time $P(T_n\\leq t)$, i.e. 到時間 $t$ 為止至少有 $n$ 個事件已經發生的機率. 然後 $\\sum_{n=1}^\\infty$ 有那種把所有可能的情況都考慮進去的意思, 因此 $\\mathcal{U}(t)$ 可能會跟到時間 $t$ 為止發生”事件”的次數的期望值 ($\\mathbb{E} N_t$) 有關. 而事實上就是. $\\mathcal{U}(t)$ 稱 renewal function $\\mathbb{E} N_t = \\mathcal{U}(t)$&emsp;$$\\mathbb{E}N_t = \\mathbb{E}\\left[ \\#\\{n:T_n\\leq t\\} \\right] =\\mathbb{E}\\left[ \\sum_{n=1}^\\infty \\mathbf{1}(T_n\\leq t) \\right] \\\\ =\\sum_{n=1}^\\infty \\mathbb{E}\\left[ \\mathbf{1}(T_n\\leq t) \\right] =\\sum_{n=1}^\\infty P(T_n\\leq t) =\\sum_{n=1}^\\infty F^{n\\ast}(t)$$ 雖然 $\\mathbb{E} N_t = \\mathcal{U}(t) =\\sum_{n=1}^\\infty F^{n\\ast}(t)$ 我們可以明確寫出來, 但對於 $F$ 是比較 general form 的話, 幾乎很難算出來. 因為要算 convolution, 又要求 sequence 的 limit. 課程試題: 附上 exponential distributino 定義 Week 1.8: Laplace transform. Calculation of an expectation of a counting process-1[Def]: Laplace transform&emsp;Given $f:\\mathbb{R}^+\\rightarrow\\mathbb{R}$, Laplace transform 定義為如下積分:&emsp;$\\mathcal{L}_f(s)=\\int_{x=0}^\\infty e^{-sx}f(x)dx$ 有幾個主要的 properties: 令 $f$ 是 p.d.f. of some random variable $\\xi$則 $\\mathcal{L}_f(s)=\\mathbb{E}\\left[ e^{-s\\xi} \\right]$ 給定任兩個 functions (不一定要是 p.d.f. or c.d.f.) $f_1$, $f_2$, 則 $\\mathcal{L}_{f_1\\ast f_2}(s)=\\mathcal{L}_{f_1}(s)\\cdot\\mathcal{L}_{f_2}(s)$其中 $\\ast$ 是 convolution in terms of density[Proof]:Let $f(x)=f_1(x)*f_2(x)$ 則 $$\\mathcal{L}_f(s)=\\int_{x=0}^\\infty e^{-sx}\\left(\\int_{y=0}^\\infty f_1(x-y)f_2(y)dy\\right)dx \\\\ =\\int_{y=0}^\\infty\\left(\\int_{x=0}^\\infty e^{-sx}f_1(x-y)dx\\right) \\\\ =\\int_{y=0}^\\infty\\left( e^{-sy}\\int_{x-y=y}^\\infty e^{-s(x-y)}f_1(x-y)d(x-y) \\right) f_2(y)dy \\\\ =\\mathcal{L}_{f_1}(s)\\cdot\\int_{y=0}^\\infty e^{-sy} f_2(y)dy = \\mathcal{L}_{f_1}(s)\\cdot\\mathcal{L}_{f_2}(s)$$ 令 $F$ 是 c.d.f. 且 $F(0)=0$, $p=F’$ is p.d.f., 則: $\\mathcal{L}_F(s)=\\frac{\\mathcal{L}_p(s)}{s}\\\\$ [Proof]: 使用分部積分, integration by part $$l.h.s.=-\\int_{\\mathbb{R}^+}F(x)\\frac{d(e^{-sx})}{s} = -\\left[F(x)e^{-sx}/s\\right]|_{x=0}^\\infty+\\frac{1}{s}\\int_{\\mathbb{R}^+}e^{-sx}dF(x) \\\\ = 0+\\frac{1}{s}\\int_{\\mathbb{R}^+}p(x)e^{-sx}dx = r.h.s.$$ [Example 1]: 令 $f(x) = x^n$, 求 $\\mathcal{L}_f(s)$&emsp;[sol]: 也是使用分部積分, integration by part$$\\mathcal{L}_{x^n}(s) = \\int_{\\mathbb{R}^+}x^n e^{-sx}dx =-\\int_{\\mathbb{R}^+}x^n\\frac{d(e^{-sx})}{s} \\\\ = \\frac{n}{s} \\int_{\\mathbb{R}^+}x^{n-1} e^{-sx}dx=...\\\\ =\\frac{n}{s}\\cdot\\frac{n-1}{s}\\cdot...\\cdot\\frac{1}{s}\\int_{\\mathbb{R}^+}e^{-sx}dx \\\\ =\\frac{n!}{s^n}\\cdot\\left[ -\\left.\\frac{1}{s}e^{-sx} \\right |_0^\\infty \\right] = \\frac{n!}{s^{n+1}}$$ [Example 2]: 令 $f(x) = e^{ax}$, 求 $\\mathcal{L}_f(s)$&emsp;[sol]: 答案為 $\\mathcal{L}_{e^{ax}}(s)=\\frac{1}{s-a}$, if $a&lt;s$&emsp;略… Week 1.9: Laplace transform. Calculation of an expectation of a counting process-2現在我們要來計算 $\\mathbb{E}N_t$, 回顧一下 $N_t$ 是一個 r.v. 表示到時間 $t$ 為止有多少個事件到達了而我們也證明了 $\\mathbb{E} N_t = \\mathcal{U}(t) =\\sum_{n=1}^\\infty F^{n\\ast}(t)$, 現在我們在仔細分析一下$$\\mathbb{E}N_t=\\mathcal{U}(t)=\\sum_{n=1}^\\infty F^{n\\ast}(t) \\\\ = F(t) + \\left( \\sum_{n=1}^\\infty F^{n\\ast} \\right) \\ast F(t) \\\\ = F(t) + \\mathcal{U}(t)\\ast F(t)$$因此我們得到: $\\mathcal{U}=F+\\mathcal{U}\\ast F$, 其中 convolution, $\\ast$, is in terms of distribution function然後我們對等號兩邊套用 Laplace transfrom, 但是我們注意到 Laplace transfrom 套用的 convolution 必須是 density function, 因此要轉換其實我們從定義可以知道 $\\mathcal{U}\\ast_{cdf}F=\\mathcal{U}\\ast_{pdf}p$ 已知 $p=F’$$\\int_{\\mathbb{R}}\\mathcal{U}(x-y)dF(y) = \\int_{\\mathbb{R}}\\mathcal{U}(x-y)p(y)dy$ 所以$\\mathcal{U}=F+\\mathcal{U}\\ast_{cdf} F = F+\\mathcal{U}\\ast_{pdf} p$, 然後就可以套用 Laplace transfrom, 並利用上面提到的 properties 2 &amp; 3$$\\mathcal{L}_\\mathcal{U}(s) = \\mathcal{L}_F(s) + \\mathcal{L}_\\mathcal{U}(s) \\cdot \\mathcal{L}_p(s) \\\\ =\\frac{\\mathcal{L}_p(s)}{s} + \\mathcal{L}_\\mathcal{U}(s) \\cdot \\mathcal{L}_p(s)$$因此$\\mathcal{L}_\\mathcal{U}(s) = \\frac{\\mathcal{L}_p(s)}{s(1-\\mathcal{L}_p(s))} \\ldots (\\star)$所以雖然無法直接計算出 $\\mathbb{E} N_t = \\mathcal{U}(t)$, 但我們可以迂迴地透過以下三個步驟來估計: 從 $F$ 算出 $\\mathcal{L}_p$ 利用 $(\\star)$ 從 $\\mathcal{L}_p$ 算出 $\\mathcal{L}_\\mathcal{U}$ 反推什麼樣的 $\\mathcal{U}$ 會得到 $\\mathcal{L}_\\mathcal{U}$, 這一步是最困難的 下一段課程將會給出一個如何使用上面三步驟的範例 Week 1.10: Laplace transform. Calculation of an expectation of a counting process-3[Example]: 假設我們有一個 renewal process, $S_n=S_{n-1}+\\xi_n$&emsp;其中 $\\xi_1, \\xi_2, ... \\sim p(x)=\\frac{e^{-x}}{2}+e^{-2x}$&emsp;我們要如何計算 $\\mathbb{E}N_t$?, 我們使用上面所述的三步驟:&emsp;&emsp;在最後一步的時候我要需要找出&emsp;什麼樣的 $\\mathcal{U}(t)$ 會有 $\\mathcal{L}_\\mathcal{U}(s)=\\frac{1}{s}$? Ans: $1$&emsp;什麼樣的 $\\mathcal{U}(t)$ 會有 $\\mathcal{L}_\\mathcal{U}(s)=\\frac{1}{s^2}$? Ans $t$&emsp;什麼樣的 $\\mathcal{U}(t)$ 會有 $\\mathcal{L}_\\mathcal{U}(s)=\\frac{1}{2s+3}$? Ans $\\exp\\{-\\frac{3}{2}t\\}$ Week 1.11: Limit theorems for renewal processes考慮一個 renewal process, $S_n=S_{n-1}+\\xi_n$, 其中 $\\xi_1,\\xi_2,...$ are i.i.d. &gt;0 almost surelySLLN 為 Strong Law of Large Number; CLT 為 Central Limit TheoremThm1 直觀上可以理解, 因為 $N_t$ 表示到時間 $t$ 為止共有多少個事件發生了. 當 $t$ 很大的時候, 每單位時間發生的事件次數, i.e. $\\frac{N_t}{t}$, 應該會十分接近頻率, i.e. $\\frac{1}{\\mu}$Thm2 就不好直接理解了, 其中 $\\xrightarrow[]{d}$ 表示 convergence in distribution. 注意到 $\\frac{1}{\\mu}$ 表示單位時間是間發生的次數, 所以乘上 $t$ 就是事件發生的次數, 注意到這是期望值, 而 $N_t$ 是發生次數的 random variable. 因此可以想像 mean 應該就是 $t/\\mu$. 困難的是 variance 是什麼, 以及這剛好會 follow normal distribution.而這兩個分別跟 SLLN (Strong Law of Large Number) and CLT (Central Limit Theorem) 相關. 以下給出證明 Thm1 的證明: Thm2 的證明: 開頭的第一行:$\\mathcal{P}\\left\\{ \\frac{\\xi_1 + \\xi_2 + ... + \\xi_n -n\\cdot\\mu}{\\sigma\\sqrt{n}} \\leq x \\right\\} = \\mathcal{P}\\left\\{ \\frac{S_n-n\\cdot\\mu}{\\sigma\\sqrt{n}} \\leq x \\right\\}$是從 CLT 出發第二行到第三行用到了, $\\mathcal{P}\\left\\{ S_n\\leq t \\right\\} = \\mathcal{P}\\left\\{ N_t \\geq n \\right\\}$然後由 $t=n\\mu+\\sigma\\sqrt{n}x\\Rightarrow n=\\frac{t}{\\mu}-\\frac{\\sigma\\sqrt{n}}{\\mu}x$ 並將 $n \\approx t/\\mu$ 帶入 r.h.s. 得到$n=\\frac{t}{\\mu}-\\frac{\\sigma\\sqrt{t}}{\\mu^{3/2}}x$, 然後代回到 $\\mathcal{P}\\left\\{ N_t \\geq n \\right\\}$, 再整理一下得到最後一行證明最後一行是 (被擋到):$\\mathcal{P}\\left\\{ Z_t \\leq x \\right\\} = \\mathcal{P}\\left\\{ Z_t &gt; -x \\right\\} \\rightarrow 1-\\Phi(-x) = \\Phi(x)$或可以參考 https://www.randomservices.org/random/renewal/LimitTheorems.html 的 The Central Limit Theorem Applications of the Renewal ProcessesApplications of the Renewal Processes.pdf","tags":[{"name":"Coursera","slug":"Coursera","permalink":"http://yoursite.com/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"http://yoursite.com/tags/Stochastic-Processes/"},{"name":"Probability Space","slug":"Probability-Space","permalink":"http://yoursite.com/tags/Probability-Space/"},{"name":"Renewal Process","slug":"Renewal-Process","permalink":"http://yoursite.com/tags/Renewal-Process/"},{"name":"Counting Process","slug":"Counting-Process","permalink":"http://yoursite.com/tags/Counting-Process/"}]},{"title":"Stochastic Processes Week 0 一些預備知識","date":"2021-12-11T12:01:14.000Z","path":"2021/12/11/Stochastic-Processes-Week-0-一些預備知識/","text":"Coursera Stochastic Processes 課程筆記, 共九篇: Week 0: 一些預備知識 (本文) Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; Itô formula Week 8: Lévy processes 本篇回顧一些基礎的機率複習, 這些在之後課程裡有用到.強烈建議閱讀以下文章: [測度論] Sigma Algebra 與 Measurable function 簡介 [機率論] 淺談機率公理 與 基本性質 A guide to the Lebesgue measure and integration Measure theory in probability 以下回顧開始: 回顧機率知識 $e^x=\\sum_{k=0}^\\infty\\frac{x^k}{k!}$. Proof link. Independent of Random Variables $X\\perp Y \\Longleftrightarrow \\mathcal{P}(XY)=\\mathcal{P}(X)\\mathcal{P}(Y)$ $Cov(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]$[Proof]: $$Cov(X,Y)=\\mathbb{E}[(X-\\mu_x)(Y-\\mu_y)] \\\\ =\\mathbb{E}[XY]-\\mu_x\\mathbb{E}[Y]-\\mu_y\\mathbb{E}[X]+\\mu_x\\mu_y \\\\ =\\mathbb{E}[XY]-\\mu_x\\mu_y$$ $Var(X)=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2$[Proof]: $$Var(X)=Cov(X,X)=\\mathbb{E}[XX]-\\mathbb{E}[X]\\mathbb{E}[X] \\\\ = \\mathbb{E}[X^2]-(\\mathbb{E}[X])^2$$ $X,Y$ uncorrelated $\\Longleftrightarrow Cov(X,Y)=0 \\Longleftrightarrow \\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ $X\\perp Y \\Rightarrow$ $X,Y$ uncorrelated $\\Rightarrow \\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ Covariance 是線性的: $Cov(aX+bY,cZ)=acCov(X,Z)+bcCov(Y,Z)$[Proof]: $$Cov(aX+bY,cZ)=\\mathbb{E}[(aX+bY)cZ]-\\mathbb{E}[aX+bY]\\mathbb{E}[cZ] \\\\ = ac\\mathbb{E}[XZ]+bc\\mathbb{E}[YZ]-ac\\mathbb{E}[X]\\mathbb{E}[Z]-bc\\mathbb{E}[Y]\\mathbb{E}[Z] \\\\ = ac(\\mathbb{E}[XZ]-\\mathbb{E}[X]\\mathbb{E}[Z])+bc(\\mathbb{E}[YZ]-\\mathbb{E}[Y]\\mathbb{E}[Z]) \\\\ = acCov(X,Z)+bcCov(Y,Z)$$ [Characteristic Function Def]: For random variable $\\xi$, 定義 characteristic function $\\Phi:\\mathbb{R}\\rightarrow \\mathbb{C}$ 為 $$\\Phi_\\xi(u) = \\mathbb{E}\\left[ e^{iu\\xi} \\right]$$ 如果 $\\xi_1\\perp\\xi_2$, 則 $\\Phi_{\\xi_1+\\xi_2}(u)=\\Phi_{\\xi_1}(u)\\Phi_{\\xi_2}(u)$ [Proof]: $$\\Phi_{\\xi_1+\\xi_2}(u)=\\mathbb{E}[e^{iu(\\xi_1+\\xi_2)}]=\\mathbb{E}[e^{iu\\xi_1}e^{iu\\xi_2}] \\\\ = \\int_{\\xi_1,\\xi_2} \\mathcal{P}(\\xi_1,\\xi_2)e^{iu\\xi_1}e^{iu\\xi_2} d\\xi_1 d\\xi_2 \\\\ = \\int_{\\xi_1,\\xi_2} \\left(\\mathcal{P}(\\xi_1)e^{iu\\xi_1}\\right)\\left(\\mathcal{P}(\\xi_2)e^{iu\\xi_2}\\right) d\\xi_1 d\\xi_2 \\\\ =\\mathbb{E}[e^{iu\\xi_1}]\\mathbb{E}[e^{iu\\xi_2}] \\\\ =\\Phi_{\\xi_1}(u)\\Phi_{\\xi_2}(u)$$ $\\mathbb{E}[X+Y]=\\mathbb{E}[X]+\\mathbb{E}[Y]$. 跟 $X,Y$ 是否獨立或不相關無關. $|Cov(X,Y)|\\leq\\sqrt{Var(X)}\\sqrt{Var(Y)}$[Proof]: $Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)$[Proof]: $$Var(X+Y)=Cov(X+Y,X+Y)\\\\ =Cov(X,X)+2Cov(X,Y)+Cov(Y,Y)\\\\ =Var(X)+Var(Y)+2Cov(X,Y)$$ 如果 $X,Y$ uncorrelated (所以 $X\\perp Y$ 也成立), 則 $Var(X+Y)=Var(X)+Var(Y)$ Normal distribution of one r.v. $$X\\sim\\mathcal{N}(\\mu,\\sigma^2), \\text{ for }\\sigma&gt;0,\\mu\\in\\mathbb{R} \\\\ p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$ The characteristic function of normal distribution is: $\\Phi(u)=e^{iu\\mu-\\frac{1}{2}u^2\\sigma^2}$ 獨立高斯分佈之和仍為高斯分佈, mean and variance 都相加[Proof]: $(X_1,...,X_n) \\text{ where } X_k\\sim\\mathcal{N}(\\mu_k,\\sigma_k^2),\\forall k=1,...,n$ 我們知道 $$X_k\\sim\\mathcal{N}(\\mu_k,\\sigma_k^2)\\longleftrightarrow \\Phi_k(u)=e^{iu\\mu_k-\\frac{1}{2}u^2\\sigma_k^2}$$ 則 $$\\sum_k X_k \\longleftrightarrow \\prod_k \\Phi_k(u) = e^{iu(\\sum_k \\mu_k)-\\frac{1}{2}u^2(\\sum_k \\sigma_k^2)}$$ 由特徵方程式與機率分佈一對一對應得知 $\\sum_k X_k \\sim \\mathcal{N}\\left(\\sum_k \\mu_k, \\sum_k \\sigma_k^2\\right)$ $K:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$ is symmetric positive semi-definite: 給定 $t_1&lt;t_2&lt;…&lt;t_n$ 一個很有用的技巧為:可以變成以下這些 disjoint 區段的線性組合 $(t_2-t_1),...,(t_n-t_{n-1})$ 具體如下: $$\\sum_{k=1}^n \\lambda_k B_{t_k} = \\lambda_n(B_{t_n}-B_{t_{n-1}}) + (\\lambda_n+\\lambda_{n-1})B_{t_{n-1}} + \\sum_{k=1}^{n-2} \\lambda_k B_{t_k} \\\\ = \\sum_{k=1}^n d_k(B_{t_n}-B_{t_{n-1}})$$ 會想要這樣轉換是因為課程會學到 independent increment 特性, 表明 disjoint 區段的 random variables 之間互相獨立, 因此可以變成互相獨立的 r.v.s 線性相加 Calculating Moments with Characteristic Functions (wiki)) Brownian Motion referenceBrownian Motion by Hartmann.pdf Lebesgue Measure and IntegrationA guide to the Lebesgue measure and integrationLebesgue integration from wiki[測度論] Sigma Algebra 與 Measurable function 簡介 Probability Theory[機率論] 淺談機率公理 與 基本性質Measure theory in probabilityDistribution function $F$ defined with (probability) measure $\\mu$ (ref)$F$ 又稱 cumulative distribution function (c.d.f.), 或稱 cumulative function其微分稱為 probability density function (p.d.f.), 或簡稱 density function. 注意到 density 不是 (probability) measure $\\mu$!而期望值可以用 Lebesgue measure or in probability measure 來看待:Given Probability space: $(\\Omega,\\Sigma,\\mathcal{P})$, 期望值定義為 所有 event $\\omega\\in\\Sigma$ 的 probability measure $\\mathcal{P}(\\omega)$, 乘上該 random variable 的值 $X(\\omega)$所有 outcome $\\omega\\in\\Omega$ 的 probability measure $\\mathcal{P}(d\\omega)$, 乘上該 random variable 的值 $X(\\omega)$ Lebesgue’s Dominated Convergence Theorem: Exchanging $\\lim$ and $\\int$Let $(f_n)$ be a sequence of measurable functions on a measure space $(S,\\Sigma,\\mu)$. Assume $(f_n)$ converges pointwise to $f$ and is dominated by some (Lebesgue) integrable function $g$, i.e. $|f_n(x)|\\leq g(x), \\qquad \\forall n,\\forall x\\in S$ Then $f$ is (Lebesgue) integrable, i.e. $\\int_S |f|d\\mu&lt;\\infty$and $$\\lim_{n\\rightarrow\\infty}\\int_S |f_n-f|d\\mu=0 \\\\ \\lim_{n\\rightarrow\\infty}\\int_S f_nd\\mu = \\int_S\\lim_{n\\rightarrow\\infty}f_nd\\mu = \\int_S f d\\mu$$","tags":[{"name":"Coursera","slug":"Coursera","permalink":"http://yoursite.com/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"http://yoursite.com/tags/Stochastic-Processes/"}]},{"title":"MCMC by Gibbs and Metropolis-Hasting Sampling","date":"2021-10-27T13:53:41.000Z","path":"2021/10/27/MCMC-by-Gibbs-and-Metropolis-Hasting-Sampling/","text":"PRML book sampling (chapter 11) 開頭把動機描述得很好, 也引用來當這篇文章的前言.在用 machine learning 很多時候會遇到需要計算某個 function $f(x)$ 的期望值, 當 $x$ follow 某個 distribution $p(x)$ 的情況, i.e. 需計算 $$\\begin{align} \\mu:=\\mathbb{E}_p[f]=\\int f(x)p(x)dx \\end{align}$$ 例如 EM algorithm 會需要計算 $\\mathbb{E}_{p(z|x)}[f(x,z)]$, 參考 ref 的式 (23), (28)又或者我們要做 Bayesian 的 prediction 時, 參考 ref 的式 (2) 這些情況大部分都無法有 analytical form. 不過如果我們能從給定的 distribution $p(x)$ 取 $L$ 個 sample 的話, 式 (1) 就能如下逼近 $$\\begin{align} \\mathbb{E}_p[f] \\approx \\hat f:= \\frac{1}{L}\\sum_{l=1}^L f(x_l) \\end{align}$$ 我們先來看一下 $\\hat f$ 這個估計的期望值是什麼: $$\\begin{align} \\mathbb{E}_p[\\hat f]=\\mathbb{E}_p\\left[ \\frac{1}{L}\\sum_{l=1}^L f(x_l) \\right] = \\frac{1}{L}\\sum_{l=1}^L\\mathbb{E}_p\\left[ f(x_l) \\right] = {E}_p [f] = \\mu \\end{align}$$ 得到一個好消息是我們只要估超多次的話, $\\hat f_1, \\hat f_2, …$ 這些估計的平均就是我們要的值 其實這等同於估一次就好, 但用超大的 $L$ 去估計. 問題是 $L$ 要多大才夠 ? 如果變數 $x$ 的維度增加, 需要的 $L$ 是否也要增加才會準確 ? i.e. 會不會有維度爆炸的問題 ? (參考 Curse of dimensionality [1]) 我們可以證明 (see Appendix): $$\\begin{align} var[\\hat f]=\\frac{1}{L}var(f) \\end{align}$$ 這告訴我們, 隨著 sample 數量 $L$ 愈大, 我們估出來的 $\\hat f$ 的”變化”會愈來愈小 (成反比). 更重要的是, 這跟 input dimension 無關! 所以不會有維度爆炸的問題. 課本說通常 $L$ 取個 10 個 20 個估出來的 $\\hat f$ 就很準了. (其實很好驗證) 所以剩下要解決的問題便是, 要怎麼從一個給定的 distribution 取 sample ? 本篇正文從這開始 先說明 1-d 情況下的 r.v. 怎麼 sampling 再來說明如何用 Markov chain sampling, 也就是大名鼎鼎的 MCMC (Markov Chain Monte Carlo) 最後介紹兩個實作方法 Gibbs and Metropolis-Hasting sampling. 以下文章內容絕大多數都是從 Coursera: Bayesian Methods for Machine Learning 課程來的非常推薦這門課程! 從 1-D 說起Discrete case先討論 discrete distribution 的情形, 我們總是可以取 samples from uniform distribution [0, 1], i.e. $\\text{sample} \\sim \\mathcal{U}[0,1]$所以若要從下圖例子的 discrete distribution 取 samples 其實很容易, 若落在 [0, 0.6) 就 sample $a_1$, 落在 [0.6, 0.7) 取 $a_2$, 落在 [0.7, 1) 取 $a_3$. Gaussian case如果是 continuous distribution 呢?考慮如下的 standard Gaussian distribution $\\mathcal{N}(0,1)$ 可以使用 Central Limit Theorem. 舉例來說我們可以從 $n$ 個 I.I.D. 的 $\\mathcal{U}[0,1]$ 取 samples, 然後平均起來. CLT 告訴我們當 $n$ 很大的時候, 結果分布會接近 $\\mathcal{N}(0,1)$ General continuous case那如果是 general case 呢? 方法是找一個已知會 sampling 的分布乘上 constant value 使它成為 upper bound例如利用 $2q(x)=2\\mathcal{N}(1,9)$ 可以變成 $p(x)$ 的 upper bound 因此我們可以 sample $\\tilde{x}$ from $2q(x)$, 舉例來說很有可能 $\\tilde{x}=0$ 因為在 $0$ 附近的機率最大. 但是對於我們真實想要 samping 的 $p(x)$ 來說, $0$ 反而機率比較小. 因此我們要有一些 rejection 機制. 所以流程就是, 首先先從已知的 $q(x)$ sample 出 $\\tilde{x}$, 由於 $2q(x)$ 是 $p(x)$ 的 upper bound, 因此我們可以根據比例來決定這一次的 $\\tilde{x}$ 是否接受. 上圖紅色為 rejection 而綠色為 acception. 因此 acception 機率為: $$\\begin{align} \\frac{p(x)}{2q(x)} \\end{align}$$ 我們解釋一下為何這方法可以運作, 首先注意到所有取出來的 $\\tilde{x}$ (還沒拒絕之前) 是均勻分布在 $2q(x)$ curve 下的 (見下圖). 而一旦引入我們 rejection 的方法, 取出來的點就是均勻分布在我們要的 $p(x)$ curve 下了. 從上面的說明可以看出, accept 的比例其實就是藍色的比例, 因此 upper bound 愈緊密效果愈好.所以如果 $p(x)\\leq Mq(x)$, 則平均 accept $1/M$ points. 這是因為 $p,q$ 都是機率分布, 所以 area under curve 都是 $1$. 因此比例就是 $1/M$.最後, 這個方法可以用在不知道 normalization term $Z$ 的情形. 例如我們只知道 $\\hat{p}(x)$, 但我們仍然可以找到一個 distribution $q(x)$ 乘上 constant $\\tilde{M}$ 後是 upper bound: $$\\hat{p}(x) \\leq \\tilde{M}q(x) \\\\ \\Longrightarrow p(x)=\\frac{\\hat{p}(x)}{Z} \\leq Mq(x)$$ 總解一下此法 結論就是雖然對大部分 distribution 都可以用, 但效率不好. 尤其在維度高的時候會大部分都 reject.那有什麼方法可以對付高維度呢? 下面要介紹的 MCMC with Gibbs/Metropolis-Hastings 就能處理. Markov Chains Monte Carlo這裡假設大家已經熟悉 Markov chain 了, 不多做介紹.使用 Markov chain 的策略為以下幾個步驟: 重點在如何設計一個 Markov chain (這裡等同於設計 transition probability $T$), 收斂的 stationary distribution 正好就是我們要的 $p(x)$首先不是每個 Markov chain 都會收斂, 但有一些充分條件如下圖 Theorem: 對照 Stochastic Processes 裡的筆記 (之後補 link), 這裡的 theorem 隱含了此 Markov chain 為 ergodic, i.e. 1-equivalence class, recurrent, and aperiodic. 而 ergodic Markov chain 必定存在 stationary distribution. Gibbs sampling上面提到使用 Markov chain 取 sample 的話, 怎麼樣的 $T$ 會讓它收斂到 desired $p(x)$Gibbs sampling 可以想成一種特殊的 $T$ 的設計方法, 可以確保收斂至 $p(x)$假設我們有一個 3-dim 的 P.D.F., 可以不知道 normalization term $Z$: $$\\begin{align} p(x_1,x_2,x_3)=\\frac{\\hat{p}(x_1,x_2,x_3)}{Z} \\end{align}$$ 從 $(x_1^0, x_2^0, x_3^0)$ 開始, e.g. $(0,0,0)$先對第一維取 sample: $$\\begin{align} x_1^1 \\sim p(x_1 | x_2=x_2^0, x_3=x_3^0) \\\\ = \\frac{\\hat{p}(x_1,x_2^0,x_3^0)}{Z_1} \\end{align}$$ 針對 1-d distribution 取 sample 是很容易的, 可以使用上一節的做法接著對第二維取 sample: $$\\begin{align} x_2^1 \\sim p(x_2 | x_1=x_1^{\\color{red}{1}}, x_3=x_3^0) \\end{align}$$ 最後對第三維取 sample: $$\\begin{align} x_3^1 \\sim p(x_3 | x_1=x_1^{\\color{red}{1}}, x_2=x_2^{\\color{red}{1}}) \\end{align}$$ 以上便是一次的 iteration, 所以: 顯而易見, 這個方法不能 parallel, 之後會說怎麼加速 (利用 Metropolis-Hastings) 證明收斂至 desired distribution現在要證明這樣的採樣方式定義了一個 Markov chain 且會收斂到 desired distribution $p(x)$, which is stationary!Markov chain 的 states 定義為 $p(x)$ 的 domain, 我們以 $n$-dim 來說就是 $(x_1,x_2,…,x_n)$Transition probabilities $p_T(x\\rightarrow x’)$ , i.e. 從 state $x$ 到 $x’$ 的機率, 使用 Gibbs sampling 來定義: $$\\begin{align} p_T(x\\rightarrow x&apos;)=p(x_1&apos;|x_2,x_3,...,x_n)p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) \\end{align}$$ 這裡我們做個假設, 令 $p_T(x\\rightarrow x’)&gt;0,\\forall x,x’$, 則由定理知道此 Markov chain 必 $\\exists !$ stationary distribution. 所以現在問題是該 stationary distribution 是我們要的 $p(x)$ 嗎?要證明 $p(x)$ 是 stationary, 我們只需證明: $$\\begin{align} p(x&apos;)=\\sum_x p(x\\rightarrow x&apos;)p(x) \\end{align}$$ 這表示 $p(x)$ 經過 1-step transition 後, 分布仍然是 $p(x)$所以再來就是用 $p_T(x\\rightarrow x’)$ 代入, 驗證看看對不對 $$\\begin{align} \\sum_x p_T(x\\rightarrow x&apos;)p(x) \\\\ = \\sum_x p(x_1&apos;|x_2,...,x_n)p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) p(x) \\\\ =p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_x p(x_1&apos;|x_2,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n)p(x) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_2,..,x_n} p(x_1&apos;|x_2,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\sum_{x_1}p(x) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_2,..,x_n} {\\color{orange}{p(x_1&apos;|x_2,...,x_n)}} ...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) {\\color{orange}{p(x_2,...,x_n)}} \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_2,..,x_n} {\\color{orange}{p(x_1&apos;,x_2,...,x_n)}} p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\ldots(\\star) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_3,..,x_n} {\\color{orange}{p(x_1&apos;,x_3,...x_n)}}p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_3,..,x_n} {\\color{orange}{p(x_1&apos;,x_2&apos;,x_3,...,x_n)}}p(x_3&apos;|x_1&apos;,x_2&apos;,x_4,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\ldots(\\square) \\end{align}$$ 觀察 $(\\star)$ 到 $(\\square)$, 是消耗掉 $x_2$ 的 summantion, 同時也消耗掉對 $x_2$ 的 gibbs sampling step. 因此我們可以對 $(\\square)$ 做一樣的事情, 去消耗掉 $x_3$ 的 summantion 以及對 $x_3$ 的 gibbs step.重複做會得到: $$\\begin{align} = p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;)\\sum_{x_n}p(x_1&apos;,...,x_{n-1} &apos;,x_n) \\\\ = p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) p(x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) \\\\ = p(x_1&apos;,x_2&apos;,...,x_n&apos;)=p(x&apos;) \\end{align}$$ Q.E.D. 總結大致上有兩個前提: 固定其他維度, 對某一維度取 samples 是很容易的 $p(x_i|x_1,...,x_{i-1}, x_{i+1}, ..., x_n)&gt;0$, 這保證了我們透過 Gibbs sampling 產生的 Markov chain 一定收斂到 desired $p(x)$ 優點為: 將 multi-dimensional sampling 化簡為 1-d sampling 容易實作 缺點為: Highly correlated samples, 這使得我們跑到 stationary distribution 後, 也不能連續的取 sample 點 Slow convergence (mixing) Not parallel (接下來介紹的 Metropolis Hastings 幫忙可以改善) Metropolis-HastingsGibbs sampling 缺點是 samples are too correlated, 且不能平行化. 注意到在 Gibbs sampling 方法裡, 已經定義好某一個特別的 Markov chain 了. Metropolis-Hastings 則可以定義出一個 famliy of Markov chain 都收斂到 desired distribution. 因此可以選擇某一個 Markov chain 可能收斂較快, 或是 less correlated.Metropolis-Hastings 中心想法就是 “apply rejection sampling to Markov chains” Algorithm 其中 $Q(x^k\\rightarrow x)$ 是任意事先給定的一個 transition probabilities (注意到需滿足 $&gt;0,\\forall x,x’$, 這樣才能保證唯一收斂)$A(x^k\\rightarrow x)$ 表示 given $x^k$ accept $x$ 的機率, 稱為 critic演算法流程為: 先從 $Q(x^k\\rightarrow x)$ 取樣出 $x’$, $x’$ 有 $A(x^k\\rightarrow x’)$ 的機率被接受, 一旦接受則 $x^{k+1}=x’$ 否則 $x^{k+1}=x^k$, 然後 iterate 下去使用這種方式的話, 我們其實可以算出 transition probability $T(x\\rightarrow x’)$, 如上圖所以關鍵就是, 怎麼選擇 $A(x^k\\rightarrow x)$ 使得這樣的 Markov chain 可以收斂到 desired probability $\\pi(x)$ 怎麼選擇 Critic $A$ 使得 Markov chain 收斂到 $\\pi$我們先介紹一個充分條件 (所以有可能 $\\pi(x)$ 是 stationary 但是不滿足 detailed balance equation) [Detailed Balance Equation]:若 $\\pi(x)T(x\\rightarrow x’)=\\pi(x’)T(x’\\rightarrow x), \\forall x,x’$, 則 $\\pi(x)$ 為 stationary distribution, i.e. $\\pi(x&apos;)=\\sum_x \\pi(x)T(x\\rightarrow x&apos;)$ [Proof]: $$\\begin{align} \\sum_x \\pi(x)T(x\\rightarrow x&apos;) \\\\ \\text{by assumption} = \\sum_x \\pi(x&apos;)T(x&apos;\\rightarrow x) \\\\ = \\pi(x&apos;)\\sum_x T(x&apos;\\rightarrow x) = \\pi(x&apos;) \\end{align}$$ 所以只要選擇的 $A(x\\rightarrow x’)$ 能夠讓 $T(x\\rightarrow x’)$ 針對 $\\pi(x)$ 滿足 detailed balance 特性就能保證 Markov chain 收斂到 $\\pi(x)$因此我們計算一下, 只需考慮 $x\\neq x’$ 的情形 (因為 $x=x’$ 一定滿足 detailed balance equation, 這不是廢話嗎) $$\\begin{align} \\pi(x)T(x\\rightarrow x&apos;)=\\pi(x&apos;)T(x&apos;\\rightarrow x) \\\\ \\Longleftrightarrow \\pi(x)Q(x\\rightarrow x&apos;)A(x\\rightarrow x&apos;) = \\pi(x&apos;)Q(x&apos;\\rightarrow x)A(x&apos;\\rightarrow x) \\\\ \\Longleftrightarrow \\frac{A(x\\rightarrow x&apos;)}{A(x&apos;\\rightarrow x)} = \\frac{\\pi(x&apos;)Q(x&apos;\\rightarrow x)}{\\pi(x)Q(x\\rightarrow x&apos;)} =: \\rho \\end{align}$$ 所以當 $\\rho&lt;1$ 我們設定 $$\\begin{align} \\left\\{ \\begin{array}{r} A(x\\rightarrow x&apos;)=\\rho \\\\ A(x&apos;\\rightarrow x)=1 \\end{array} \\right. \\end{align}$$ 而如果 $\\rho&gt;1$ 我們設定 $$\\begin{align} \\left\\{ \\begin{array}{r} A(x\\rightarrow x&apos;)=1 \\\\ A(x&apos;\\rightarrow x)=1/\\rho \\end{array} \\right. \\end{align}$$ 總結來說 $A$ 可以這麼設定 $$\\begin{align} A(x\\rightarrow x&apos;)=\\min\\left\\{ 1, \\frac{\\pi(x&apos;)Q(x&apos;\\rightarrow x)}{\\pi(x)Q(x\\rightarrow x&apos;)} \\right\\} \\end{align}$$ 注意到 $\\rho$ 是可以直接算出來的, 因為 $Q,\\pi$ 都是事先給定已知的, 因此我們就能設定出對應的 acceptance distribution $A$. 同時如果我們只有 unnormalized distribution, i.e. $\\hat\\pi(x)$, 由 $A$ 的設定可以看出不受影響 $$\\begin{align} A(x\\rightarrow x&apos;)=\\min\\left\\{ 1, \\frac{ {\\color{orange}{\\hat\\pi(x&apos;)}} Q(x&apos;\\rightarrow x)}{ {\\color{orange}{\\hat\\pi(x)}} Q(x\\rightarrow x&apos;)} \\right\\} \\end{align}$$ 怎麼選擇 $Q$首先需滿足 $Q(x\\rightarrow x’)&gt;0,\\forall x,x’$. 這樣才會有以上的推論.$Q$ 會希望能走”大步”一點, 也就是 transition 不要只圍繞在相鄰的點. 好處是產生的 sample 會比較無關.但如果走太大步, critic $A$ 就有可能一直 reject (why?) 導致效率太差 想像如果 $x$ 已經在機率很高的地方了, 例如 local maximum point. 如果 $Q$ 走太大步到 $x’$, 則容易 $\\pi(x’)&lt;&lt;\\pi(x)$, 造成 $A$ 太小容易 reject所以如果 $Q$ 走小步一點, $x’$ 還是圍繞在 $x$ 附近, 相對來說可能機率就不會那麼低 Example of Metropolis-Hastings1-d case toy example 告訴我們 proposal 的 distribution 選擇也是很重要的. 最後可以使用 Metropolis Hastings 來平行化 Gibbs sampling!我們使用如下圖 “錯誤的” Gibbs sampling 方法, 並將這方法視為 Metropolis Hastings 的 proposal $Q(x\\rightarrow x’)$因此可以平行對每個維度取 sample! (好聰明!) 結語MCMC 被譽為 20 世紀十個偉大的演算法發明之一 [3]. 找知乎的文章可以看到這個討論: 有什么理论复杂但是实现简单的算法？[4] 果然 MCMC 理論不是一般人能做的.後續對於 Metropolis-Hastings 的改進有一個算法是 Metropolis-adjusted Langevin algorithm [5] (MALA). 該方法提出使用 Langevin dynamics [6] 當作 proposal, 這會使得 random walk 會走向機率比較高的地方, 因此被拒絕機率較低. 但是 MALA 我實在看不懂, 只知道跟 Langevin dynamics sampling [7] 有關 在 Generative Modeling by Estimating Gradients of the Data Distribution [8] 的 Langevin dynamics 段落裡提到 MALA 可以只根據 score function ($\\nabla_x \\log p(x)$) 就從 P.D.F. $p(x)$ 取 samples! 會看到 MALA 是因為除了 GAN 之外最近很熱門的 generative models: DPM [9]), 其核心技術之一用到它.看來要全部融會貫通目前會先卡關在這了. MALA 你等著! 別跑啊, 不要以為我怕了你, 總有一天我 #$@^#@$Q (逃~) Appendix證明 $var[\\hat f]=\\frac{1}{L}Var(f)$ 如下: 首先兩個 independent r.v.s $X,Y$ 我們知道其 covariance 為 $0$: $$\\begin{align} 0 = Cov[XY] = \\mathbb{E}\\left[ (X-\\mu_x)(Y-\\mu_y) \\right] \\\\ = \\mathbb{E}[XY-X\\mu_y-\\mu_xY+\\mu_x\\mu_y] = \\mathbb{E}[XY] - \\mu_x\\mu_y \\\\ \\Rightarrow \\mathbb{E}[XY] = \\mu_x\\mu_y \\ldots(\\star) \\end{align}$$ 且有 variance 的性質: $Var(X)=\\mathbb{E}[X^2]-\\mu_x^2\\ldots(\\star\\star)$接著開始計算: $$\\begin{align} Var[\\hat f]=\\mathbb{E}[(\\hat f - \\mathbb{E}[\\hat f])^2] = \\mathbb{E}[(\\hat f - \\mu)^2] = \\mathbb{E}[\\hat f^2] - \\mu^2 \\\\ = \\mathbb{E}\\left[ \\frac{1}{L}\\sum_k f(x_k) \\frac{1}{L}\\sum_m f(x_m) \\right] - \\mu^2 \\\\ = \\frac{1}{L^2}\\sum_k\\sum_m\\left[ \\mathbb{E}[f(x_k)f(x_m)] - \\mu^2 \\right] \\\\ \\text{by }(\\star)= \\frac{1}{L^2}\\sum_k \\left[ (\\mathbb{E}[f(x_k)^2]-\\mu^2)+(L-1)(\\mu^2-\\mu^2) \\right] \\\\ \\text{by }(\\star\\star) = \\frac{1}{L^2}\\sum_k Var(f(x_k)) \\\\ = \\frac{1}{L} Var(f) \\end{align}$$ Reference Curse of Dimensionality — A “Curse” to Machine Learning Coursera: Bayesian Methods for Machine Learning The Best of the 20th Century: Editors Name Top 10 Algorithms 有什么理论复杂但是实现简单的算法？ Metropolis-adjusted Langevin algorithm: wiki Langevin dynamics: wiki 抽样理论中有哪些令人印象深刻(有趣)的结论? Generative Modeling by Estimating Gradients of the Data Distribution What are Diffusion Models?","tags":[{"name":"Coursera","slug":"Coursera","permalink":"http://yoursite.com/tags/Coursera/"},{"name":"MCMC","slug":"MCMC","permalink":"http://yoursite.com/tags/MCMC/"},{"name":"Markov Chain","slug":"Markov-Chain","permalink":"http://yoursite.com/tags/Markov-Chain/"},{"name":"Gibbs Sampling","slug":"Gibbs-Sampling","permalink":"http://yoursite.com/tags/Gibbs-Sampling/"},{"name":"Metropolis Hastings","slug":"Metropolis-Hastings","permalink":"http://yoursite.com/tags/Metropolis-Hastings/"}]},{"title":"Gumbel-Max Trick","date":"2021-08-07T10:41:01.000Z","path":"2021/08/07/Gumbel-Max-Trick/","text":"我們在介紹 VAE 的時候有說明到 re-parameterization trick, 大意是這樣的 $y$ 是 sampling from distribution $\\alpha$, i.e., $y=\\text{Sampling}(\\alpha)$, 其中 $\\alpha=\\text{NN}_1(a;\\theta)$由於我們有採樣, 因此 loss 採用期望值. Loss function 為: $$\\begin{align} L = \\mathbb{E}_{y\\sim\\alpha}[\\text{NN}_2(y;\\nu)] \\end{align}$$ Loss 對 $\\theta$ 偏微分的時候會失敗, 主要是因為: $$\\begin{align} \\nabla_\\theta L = \\nabla_\\theta \\mathbb{E}_{y\\sim\\alpha}[\\text{NN}_2(y;\\nu)] \\\\ \\neq \\mathbb{E}_{y\\sim\\alpha}[\\nabla_\\theta \\text{NN}_2(y;\\nu)] \\end{align}$$ 微分不能跟 Expectation 互換是因為 sampling 的 distribution $\\alpha$ 其實也是 depends on $\\theta$. 因此在 VAE 那邊的假設就是將 $\\alpha$ 定義為 Gaussian pdf. 因此可以變成: $$\\begin{align} \\nabla_\\theta L = \\nabla_\\theta \\mathbb{E}_{y\\sim\\alpha}\\left[ \\text{NN}_2(y;\\nu) \\right] \\\\ = \\nabla_\\theta \\mathbb{E}_{\\varepsilon\\sim N(0,I)}\\left[ \\text{NN}_2(\\mu+\\sigma\\varepsilon; \\nu) \\right] \\\\ = \\mathbb{E}_{\\varepsilon\\sim N(0,I)}\\left[ \\nabla_\\theta \\text{NN}_2(\\mu+\\sigma\\varepsilon; \\nu) \\right] \\end{align}$$ 採樣變成從一個 跟 $\\theta$ 無關的分布, 因此微分跟期望值就能互換, 所以可以做 backprop. 現在的情況是如果是 Gaussian 的情形很好做變換, 但如果是 categorical distribution 該怎麼辦呢? 什麼情況會遇到 categorical distribution? 在 reinforcement learning 時, $\\text{NN}_1$ predict 出例如 4 個 actions 的機率, 我們需要隨機採樣一種 action, 然後傳給後面的 NN 去計算 reward.(其實我不熟 RL, 看網路上的文章說的) Gumbel max trick 就提供了解法! Gumbel Distribution and Gumbel Max Sampling這一篇文章 The Humble Gumbel Distribution 提供了非常清晰的解釋, 十分推薦閱讀 假設我們經由一個 network 算出 logits $(x_k)_k$, 一般我們如果要 sampling 的話還必須過 softmax 讓它變成機率 $(\\alpha_k)_k$, 然後在用例如 np.random.choice 根據機率採樣出結果. 現在 sampling 流程改為: 先從標準 Gumbel 分佈 (先不管這分佈長什麼樣) 採樣出 $N$ 個值, 令為 $(G_k)_k$, 讓它跟 logits 相加: $z_k=x_k+G_k$, 然後 $\\text{argmax}_k (z_k)$ 就是我們這次的採樣結果 圖示為: 注意到我們唯一的一個採樣動作完全跟 network 的參數 $\\theta$ 無關! 因此 re-parameterization trick 就能用上. (先假設 $\\text{argmax}_k (z_k)$ 可微, 因此可以 backprop, 這等下會說)剩下唯一不確定的就是, 這樣的採樣行為出來的結果, 會跟使用 $(\\alpha_k)_k$ 的機率分佈採樣出來一樣嗎 ?換句話說, $\\text{argmax}_k (z_k)$ 出來的結果, 其結果的分佈是不是符合 $(\\alpha_k)_k$ ?程式驗證可參考 The Humble Gumbel Distribution, 將最主要的部分修短擷取後如下: 123456789101112131415161718192021222324252627282930313233343536373839# Modified from http://amid.fish/humble-gumbelimport numpy as npimport matplotlib.pyplot as plt# Assign categorical probabilities, for example:probs = [0.13114754, 0.01639344, 0.21311475, 0.24590164, 0.19672131, 0.06557377, 0.13114754]n_classes = len(probs)logits = np.log(probs) # logits is log probability (with constant offset)n_samples = 10000 # experimental number of samplingdef gumbel_sampling(logits): noise = np.random.gumbel(size=len(logits)) sample = np.argmax(logits + noise) return samplesamples_with_gumbel_max_trick = [gumbel_sampling(logits) for _ in range(n_samples)]samples_from_true_distribution = np.random.choice(np.arange(n_classes), size=n_samples , p=probs)# Plotting area, comparing `samples_with_gumbel_max_trick` and `samples_from_true_distribution`def plot_estimated_probs(samples, n_classes): estd_probs, _, _ = plt.hist(samples, bins=np.arange(n_classes + 1), align='left', edgecolor='white', density=True) plt.xlabel(\"Category\") plt.ylabel(\"Estimated probability\") return estd_probsplt.figure()plt.subplot(1, 2, 1)plot_estimated_probs(samples_from_true_distribution, n_classes)plt.title('Sampling from true pdf')plt.subplot(1, 2, 2)estd_probs = plot_estimated_probs(samples_with_gumbel_max_trick, n_classes)plt.title('Sampling with Gumbel-max trick')plt.tight_layout()plt.show() 可以看到用 Gumbel-max trick 採樣出來的 samples 其分佈跟真實的機率分佈十分接近.事實上可以證明會是一樣的, 在下一節我們將證明寫出來.再囉嗦一下, 不要忘記了, 使用 np.random.choice 對真實分佈採樣是沒有辦法做 backprop 的 (見 eq (2) (3))而透過 Gumbel-max trick 我們可以從一個與要 optimize 的參數 $\\theta$ 無關的分佈 (Gumbel distribution) 進行採樣, 才能利用 re-parameterization trick 做 backprop (例如 eq (4)~(6) 的概念) 其實我少講了一件事, np.argmax 不可微, 所以不能 backprop. 因此一個實際的做法是使用 softmax (with temperature) 近似: $$\\begin{align} \\text{softmax}(z_k,\\tau)=\\frac{\\exp(z_k/\\tau)}{\\sum_{i=1}^N\\exp(z_i/\\tau)} \\end{align}$$ 實作上會先讓 temperature $\\tau$ 從比較大的值開始 (比較不那麼凸顯值之間大小的差異), 之後慢慢變小接近 $0$ (等同於 argmax). 參考 paper 的圖: Proof of Gumbel-Max Trick for Discrete Distributions其實完全參考 The Gumbel-Max Trick for Discrete Distributions, 但最後一行的推導用看的實在沒看出來, 因此自己補齊完整一點 Math warning, 很枯燥 Gumbel PDF: $f(z;\\mu)=\\exp\\left[-(z-\\mu)-\\exp\\left[-(z-\\mu)\\right]\\right]$ $f(z;0)=\\exp\\left[-z-\\exp\\left[-z\\right]\\right]$ Gumbel CDF: $F(z;\\mu)=\\exp\\left[-\\exp\\left[-(z-\\mu)\\right]\\right]$ $F(z;0)=\\exp\\left[-\\exp\\left[-z\\right]\\right]$ Categorical distribution 例如分成 $N$ 類, NN 通常最後會輸出一個 logits vector, $(x_k)_k$, $k=1…N$ $z_k=x_k+G_k$, 其中 $G_k$ 是一個標準 Gumbel distribution (mean=0, scale=1) $$\\begin{align} \\Pr(k\\text{ is largest}|\\{x_i\\},z_k) = \\Pr(\\max_{i\\neq k}z_i&lt;z_k) \\\\ =\\prod_{i\\neq k}\\Pr(z_i&lt;z_k) = \\prod_{i\\neq k}\\Pr(x_i+G_i&lt;z_k) \\\\ =\\prod_{i\\neq k}\\Pr(G_i&lt;z_k-x_i) \\\\ =\\prod_{i\\neq k}F(z_k-x_i;0) \\\\ =\\prod_{i\\neq k}\\exp\\{-\\exp\\{-z_k+x_i\\}\\} \\end{align}$$ $$\\begin{align} \\therefore \\Pr(k\\text{ is largest}|\\{x_i\\})=\\int\\Pr(z_k)\\Pr(k\\text{ is largest}|\\{x_i\\},z_k)dz_k \\\\ = \\int f(z_k-x_k;0)\\prod_{i\\neq k}\\exp\\{-\\exp\\{-z_k+x_i\\}\\} \\\\ = \\int \\left(\\exp\\{-z_k+x_k-e^{-z_k+x_k}\\}\\right) \\prod_{i\\neq k}\\exp\\{-e^{-z_k+x_i}\\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k\\}\\prod_{i=1}^N{ \\exp\\{-e^{-z_k+x_i}\\} } dz_k \\\\ = \\int \\exp\\{-z_k+x_k\\} \\cdot \\exp\\{-\\sum_{i=1}^Ne^{-z_k+x_i}\\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k-\\sum_{i=1}^Ne^{-z_k+x_i} \\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k-e^{-z_k} {\\color{orange}{\\sum_{i=1}^Ne^{x_i}}} \\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k- {\\color{orange}A} e^{-z_k} \\} dz_k \\end{align}$$ 這裡我們為了方便定義 $A=\\sum_{i=1}^N e^{x_i}$ $$\\begin{align} =\\int \\exp\\{-z_k+x_k - {\\color{orange}{e^{\\ln A}}} e^{-z_k} \\} dz_k \\\\ = e^{x_k} \\int \\exp\\{-z_k-e^{-z_k + \\ln A}\\} dz_k \\\\ = e^{x_k} \\int \\exp\\{-z_k {\\color{orange}{+\\ln A-\\ln A}} -e^{-z_k + \\ln A}\\} dz_k \\\\ = e^{x_k}\\cdot e^{-\\ln A} \\int \\exp\\{-(z_k-\\ln A)-e^{-(z_k-\\ln A)}\\} dz_k \\\\ = \\frac{e^{x_k}}{A} \\int f(z_k;\\ln A) dz_k \\\\ = \\frac{e^{x_k}}{\\sum_{i=1}^N e^{x_i}} \\end{align}$$ Reference The Humble Gumbel Distribution The Gumbel-Max Trick for Discrete Distributions The Gumbel-Softmax Trick for Inference of Discrete Variables 【一文学会】Gumbel-Softmax的采样技巧 Categorical Reparameterization with Gumbel-Softmax","tags":[{"name":"Gumbel distribution","slug":"Gumbel-distribution","permalink":"http://yoursite.com/tags/Gumbel-distribution/"},{"name":"Gumbel max trick","slug":"Gumbel-max-trick","permalink":"http://yoursite.com/tags/Gumbel-max-trick/"},{"name":"Gumbel max sampling","slug":"Gumbel-max-sampling","permalink":"http://yoursite.com/tags/Gumbel-max-sampling/"},{"name":"Re-parameterization trick","slug":"Re-parameterization-trick","permalink":"http://yoursite.com/tags/Re-parameterization-trick/"}]},{"title":"Noise Contrastive Estimation (NCE) 筆記","date":"2021-06-05T02:15:04.000Z","path":"2021/06/05/Noise-Contrastive-Estimation-NCE-筆記/","text":"之前聽人介紹 wav2vec [3] 或是看其他人的文章大部分都只有介紹作法, 直到有一天自己去看論文才發現看不懂 CPC [2] (wav2vec 使用 CPC 方法). 因此才決定好好讀一下並記錄. 先將這些方法關係梳理一下, NCE –&gt; CPC (infoNCE) –&gt; wav2vec. 此篇筆記主要紀錄 NCE (Noise Contrastive Estimation) 在做 ML 時常常需要估計手上 training data 的 distribution $p_d(x)$. 而我們通常會使用參數 $\\theta$, 使得參數的模型跟 $p_d(x)$ 一樣. 在現在 DNN 統治的年代可能會說, 不然就用一個 NN 來訓練吧, 如下圖: 給 input $x$, 丟給 NN 希望直接吐出 $p_\\theta(x)$. 上圖的架構是 $x$ 先丟給參數為 $\\theta_f$ 的 NN, 該 NN 最後一層的 outputs 再丟給參數為 $w$ 的 linear layer 最後吐出一個 scalar 值, 該值就是我們要的機率.而訓練的話就使用 MLE (Maximum Likelihood Estimation) 來求參數 $\\theta$. 恩, 問題似乎很單純但真正實作起來卻困難重重. 一個問題是 NN outputs 若要保持 p.d.f. 則必須過 softmax, 確保 sum 起來是 1 (也就是要算 $Z_\\theta$). $$\\begin{align} p_\\theta(x)=\\frac{u_\\theta(x)}{Z_\\theta}=\\frac{e^{G(x;\\theta)}}{Z_\\theta} \\\\ \\text{where } Z_\\theta = \\sum_x u_\\theta(x) \\end{align}$$ 式 (1) 為 energy-based model, 在做 NN classification 時, NN 的 output 就是 $G(x;θ)$, 也就是常看到的 logit, 經過 softmax 就等同於式 (1) 在做的事 而做這件事情在 $x$ 是 discrete space 但數量很多, 例如 NLP 中 LM vocabulary 很大時, 計算資源會消耗過大.或是 $x$ 是 continuous space 但是算 $Z_\\theta$ 的積分沒有公式解的情形會做不下去. (不然就要用 sampling 方法, 如 MCMC) NCE 巧妙的將此 MLE 問題轉化成 binary classification 問題, 從而得到我們要的 MLE 解. 不過在此之前, 我們先來看看 MLE 的 gradient 長什麼樣. MLE 求解寫出 likelihood: $$\\begin{align} \\text{likilhood}=\\prod_{x\\sim p_d} p_\\theta(x) \\end{align}$$ Loss 就是 negative log-likelihood $$\\begin{align} -\\mathcal{L}_{mle}=\\mathbb{E}_{x\\sim p_d}\\log p_{\\theta}(x)= \\mathbb{E}_{x\\sim p_d}\\log \\frac{u_\\theta(x)}{Z_\\theta}\\\\ \\end{align}$$ 計算其 gradient: $$\\begin{align} -\\nabla_{\\theta}\\mathcal{L}_{mle}= \\mathbb{E}_{x\\sim p_d} \\left[ \\nabla_{\\theta}\\log{u_\\theta(x)} - \\color{orange}{\\nabla_{\\theta}\\log{Z_\\theta}} \\right] \\\\ \\color{orange}{\\nabla_{\\theta}\\log{Z_\\theta}} = \\frac{1}{Z_\\theta}\\nabla_{\\theta}Z_\\theta = \\frac{1}{Z_\\theta} \\sum_x \\nabla_{\\theta} e^{G(x;\\theta)} \\\\ =\\frac{1}{Z_\\theta} \\sum_x e^{G(x;\\theta)} \\nabla_{\\theta}G(x;\\theta) = \\sum_x \\left[ \\frac{1}{Z_\\theta}e^{G(x;\\theta)} \\right] \\nabla_{\\theta}G(x;\\theta) \\\\ =\\sum_x p_{\\theta}(x) \\nabla_{\\theta} \\log u_{\\theta}(x) = \\mathbb{E}_{x \\sim p_{\\theta}} \\nabla_{\\theta} \\log u_{\\theta}(x) \\\\ \\therefore \\text{ } -\\nabla_{\\theta}\\mathcal{L}_{mle} = \\mathbb{E}_{x\\sim p_d} \\left[ \\nabla_{\\theta} \\log u_{\\theta}(x) - \\color{orange}{\\mathbb{E}_{x \\sim p_{\\theta}} \\nabla_{\\theta} \\log u_{\\theta}(x)} \\right] \\\\ = \\mathbb{E}_{x\\sim p_d} \\nabla_{\\theta} \\log u_{\\theta}(x) - \\mathbb{E}_{x \\sim p_{\\theta}} \\nabla_{\\theta} \\log u_{\\theta}(x)\\\\ = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\nabla_{\\theta} \\log u_{\\theta}(x) \\\\ \\end{align}$$ 從 (11) 式可以看到, 估計的 pdf 與 training data 的 pdf 差越大 gradient 愈大, 當兩者相同時 gradient 為 0 不 update. Sigmoid or Logistic Function在說明 NCE 之前先談一下 sigmoid function. 假設現在我們做二分類問題, 兩個類別 $C=1$ or $C=0$. 令 $p$ 是某個 input $x$ 屬於 class 1 的機率 (所以 $1-p$ 就是屬於 class 0 的機率)定義 log-odd 為 (其實也稱為 logit): $$\\begin{align} \\text{log-odd} = \\log \\frac{p}{1-p} \\end{align}$$ 我們知道 sigmoid function $\\sigma(x)=\\frac{1}{1+e^{-x}}$ 將實數 input mapping 到 0 ~ 1 區間的函式. 若我們將 log-odd 代入我們很容易得到: $$\\begin{align} \\sigma(\\text{log-odd})=...=p \\end{align}$$ 發現 sigmoid 回傳給我們的是 $x$ 屬於 class 1 的機率值, i.e. $\\sigma(\\text{log-odd})=p(C=1|x)$. 所以在二分類問題上, 我們就是訓練一個 NN 能 predict logit 值. NCE 的 Network 架構首先 NCE 引入了一個 Noise distribution $q(x)$. 論文提到該 $q$ 只要滿足當 $p_d(x)$ nonzero 則 $q(x)$ 也必須 nonzero 就可以. 二分類問題為, 假設要取一個正例 (class 1), 就從 training data pdf $p_d(x)$ 取得. 而若要取一個反例 (class 0) 則從 noise pdf $q(x)$ 取得.我們可以取 $N_p$ 個正例以及 $N_n$ 個反例, 代表 prior 為: $$\\begin{align} p(C=1)=\\frac{N_p}{N_p+N_n} \\\\ p(C=0)=1-p(C=1) \\\\ \\end{align}$$ 因此就可以得到一個 batch 共 $N_p+N_n$ 個 samples, 丟入下圖的 NN structure 做二分類問題: Network 前半段還是跟原來的 MLE 架構一樣, 只是我們期望 $NN_{\\theta}$ 吐出來的是 logit, 由上面一個 section 我們知道經過 sigmoid 得到的會是 $x$ 屬於 class 1 的機率. 因此很容易就用 xent loss 優化. 神奇的來了, NCE 告訴我們, optimize 這個二分類問題得到的 $\\theta$ 等於 MLE 要找的 $\\theta$! $$\\begin{align} \\theta_{nce} = \\theta_{mle} \\end{align}$$ 且 NN 計算的 logit 直接就變成 MLE 要算的 $p_{\\theta}(x)$. 同時藉由換成二分類問題, 也避開了很難計算的 $Z_{\\theta}$ 問題.為了不影響閱讀流暢度, 推導過程請參照 Appendix 所以我們可以透過引入一個 Noise pdf 來達到估計 training data 的 generative model 了. 這也是為什麼叫做 Noise Contrastive Estimation. Representation由於透過 NCE 訓練我們可以得到 $\\theta$, 此時只需要用 $\\theta_f$ 的 NN 來當作 feature extractor 就可以了. 總結最後流程可以總結成下面這張圖: 最後聊一下 CPC (Contrastive Predictive Coding) [2]. 我覺得跟 NCE 就兩點不同: 我們畫的 NCE 圖裡的 $w$, 改成論文裡的 $c_t$, 所以變成 network 是一個 conditioned 的 network 不是一個二分類問題, 改成 N 選 1 的分類問題 (batch size $N$, 指出哪一個是正例), 因此用 categorical cross-entorpy 當 loss 所以文章稱這樣的 loss 為 infoNCE loss 同時 CPC [2] 論文中很棒的一點是將這樣的訓練方式也跟 Mutual Information (MI) 連接起來.證明了最小化 infoNCE loss 其實就是在最大化 representation 與正例的 MI (的 lower bound). 這些背後數學撐起了整個利用 CPC 在 SSL (Self-Supervised Learning) 的基礎. 簡單講就是不需要昂貴的 label 全部都 unsupervised 就能學到很好的 representation.而近期 facebook 更利用 SSL 學到的好 representation 結合 GAN 在 ASR 達到了 19 年的 STOA WER. 論文: Unsupervised Speech Recognition or see [9] SSL 好東西, 不試試看嗎? AppendixPrior pdf:$$\\begin{align} p(C=1)=\\frac{N_p}{N_p+N_n} \\\\ p(C=0)=1-p(C=1) \\\\ \\end{align}$$ Generative pdf:$$\\begin{align} p(x|C=1)=p_{\\theta}(x) \\\\ p(x|C=0)=q(x) \\end{align}$$ 因此 Posterior pdf:$$\\begin{align} p(C=1|x)=\\frac{p(C=1)p(x|C=1)}{p(C=1)p(x|C=1)+p(C=0)p(x|C=0)}=\\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_r q(x)} \\\\ p(C=0|x)=\\frac{p(C=0)p(x|C=0)}{p(C=1)p(x|C=1)+p(C=0)p(x|C=0)}=\\frac{N_r q(x)}{p_{\\theta}(x)+N_r q(x)} \\\\ \\end{align}$$其中 $N_r=\\frac{N_n}{N_p}$ 因此 likelihood 為:$$\\begin{align} \\text{likilhood}=\\prod_{t=1}^{N_p} p(C_t=1|x_t) \\cdot \\prod_{t=1}^{N_n} p(C_t=0|x_t) \\end{align}$$ Loss 為 negative log-likelihood:$$\\begin{align} - \\mathcal{L}_{nce} = \\sum_{t=1}^{N_p} \\log p(C_t=1|x_t) + \\sum_{t=1}^{N_n} \\log p(C_t=0|x_t) \\\\ = N_p \\left[ \\frac{1}{N_p} \\sum_{t=1}^{N_p} \\log p(C_t=1|x_t) \\right] + N_n \\left[ \\frac{1}{N_n} \\sum_{t=0}^{N_n} \\log p(C_t=0|x_t) \\right] \\\\ \\propto \\left[ \\frac{1}{N_p} \\sum_{t=1}^{N_p} \\log p(C_t=1|x_t) \\right] + N_r \\left[ \\frac{1}{N_n} \\sum_{t=0}^{N_n} \\log p(C_t=0|x_t) \\right] \\end{align}$$ 當固定 $N_r$ 但是讓 $N_p\\rightarrow\\infty$ and $N_n\\rightarrow\\infty$. 意味著我們固定正負樣本比例, 但取無窮大的 batch. 重寫上式成:$$\\begin{align} - \\mathcal{L}_{nce} = \\mathbb{E}_{x\\sim p_d} \\log p(C=1|x) + N_r \\mathbb{E}_{x\\sim q} \\log p(C=0|x) \\\\ \\therefore \\text{} -\\nabla_{\\theta}\\mathcal{L}_{nce} = \\nabla_{\\theta}\\left[ \\mathbb{E}_{x\\sim p_d} \\log \\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_rq(x)} + N_r\\mathbb{E}_{x\\sim q} \\log \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\right] \\\\ = \\mathbb{E}_{x\\sim p_d} \\color{orange}{\\nabla_{\\theta} \\log \\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_rq(x)}} + N_r \\mathbb{E}_{x\\sim q} \\color{green}{\\nabla_{\\theta} \\log \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} } \\end{align}$$ 計算橘色和綠色兩項, 之後再代回來: $$\\begin{align} \\color{orange}{\\nabla_{\\theta} \\log \\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_rq(x)}} = \\nabla_{\\theta}\\log\\frac{1}{1+N_r\\frac{q(x)}{p_{\\theta}(x)}} = -\\nabla_{\\theta}\\log \\left( 1+\\frac{N_rq(x)}{p_{\\theta}(x)} \\right) \\\\ = -\\frac{1}{1+\\frac{N_rq(x)}{p_{\\theta}(x)}}\\nabla_{\\theta}\\frac{N_rq(x)}{p_{\\theta}(x)} = -\\frac{N_rq(x)}{1+\\frac{N_rq(x)}{p_{\\theta}(x)}}\\nabla_{\\theta}\\frac{1}{p_{\\theta}(x)} \\\\ = -\\frac{N_rq(x)}{1+\\frac{N_rq(x)}{p_{\\theta}(x)}} \\frac{-1}{p_{\\theta}^2(x)} \\nabla_{\\theta} p_{\\theta}(x) \\\\ = \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\left[ \\frac{1}{p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) \\right] \\\\ = \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta} \\log p_{\\theta}(x) \\end{align}$$ $$\\begin{align} \\color{green}{\\nabla_{\\theta} \\log \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)}} = -\\nabla_{\\theta} \\log\\left( 1+\\frac{p_{\\theta}(x)}{N_rq(x)} \\right) = -\\frac{1}{1+\\frac{p_{\\theta}(x)}{N_rq(x)}} \\nabla_{\\theta} \\frac{p_{\\theta}(x)}{N_rq(x)} \\\\ = -\\frac{1}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) \\\\ = -\\frac{p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\left[ \\frac{1}{p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) \\right] \\\\ = -\\frac{p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} \\log p_{\\theta}(x) \\end{align}$$ 將 (34), (38) 代回去 (29) 得到: $$\\begin{align} - \\nabla_{\\theta}\\mathcal{L}_{nce} = \\mathbb{E}_{x\\sim p_d} {\\color{orange}{\\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta} \\log p_{\\theta}(x)}} - N_r \\mathbb{E}_{x\\sim q} {\\color{green}{\\frac{p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} \\log p_{\\theta}(x)}} \\\\ = \\sum_x \\left[ p_d(x) \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta} \\log p_{\\theta}(x) \\right] - \\sum_x \\left[ q(x) \\frac{N_r p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} \\log p_{\\theta}(x)\\right] \\\\ = \\sum_x \\frac{(p_d(x)-p_{\\theta}(x))N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ = \\sum_x \\frac{(p_d(x)-p_{\\theta}(x))q(x)}{\\frac{p_{\\theta}(x)}{N_r}+q(x)} \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ \\end{align}$$ 當 $N_r\\rightarrow\\infty$ 意味著我們讓負樣本遠多於正樣本, 上式變成:$$\\begin{align} \\lim_{N_r\\rightarrow\\infty} - \\nabla_{\\theta}\\mathcal{L}_{nce} = \\sum_x \\frac{(p_d(x)-p_{\\theta}(x))q(x)}{0+q(x)} \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ = \\sum_x (p_d(x)-p_{\\theta}(x)) \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\left( \\nabla_{\\theta}\\log u_{\\theta}(x) -\\nabla_{\\theta}\\log Z_{\\theta} \\right) \\end{align}$$ 此時我們發現這 gradient 也與 Noise pdf $q(x)$ 無關了! 最後我們將 MLE and NCE 的 gradient 拉出來對比一下:$$\\begin{align} -\\nabla_{\\theta}\\mathcal{L}_{mle} = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\nabla_{\\theta} \\log u_{\\theta}(x) \\\\ -\\nabla_{\\theta}\\mathcal{L}_{nce} = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\left( \\nabla_{\\theta}\\log u_{\\theta}(x) -\\nabla_{\\theta}\\log Z_{\\theta} \\right) \\end{align}$$ 我們發現 MLE and NCE 只差在一個 normalization factor (or partition) $Z_{\\theta}$.最魔術的地方就在於 NCE 論文 [1] 證明最佳解本身的 logit 已經是 probability 型式, 因此也不需要 normalize factor. 論文裡說礙於篇幅沒給出證明, 主要是來自 Theorem 1 的結果: 所以我們不妨將 $Z_{\\theta}=1$, 結果有: $$\\begin{align} \\color{red} {\\nabla_{\\theta}\\mathcal{L}_{mle} = \\nabla_{\\theta}\\mathcal{L}_{nce}} \\\\ \\color{red} {\\Rightarrow \\theta_{mle} = \\theta_{nce}} \\\\ \\end{align}$$ Reference 2010: Noise-contrastive estimation: A new estimation principle for unnormalized statistical models 2019 DeepMind infoNCE/CPC: Representation learning with contrastive predictive coding 2019 FB: wav2vec: Unsupervised pre-training for speech recognition 2020 MIT &amp; Google: Contrastive Representation Distillation Noise Contrastive Estimation 前世今生——从 NCE 到 InfoNCE “噪声对比估计”杂谈：曲径通幽之妙 [译] Noise Contrastive Estimation The infoNCE loss in self-supervised learning High-performance speech recognition with no supervision at all","tags":[{"name":"Noise Contrastive Estimation","slug":"Noise-Contrastive-Estimation","permalink":"http://yoursite.com/tags/Noise-Contrastive-Estimation/"},{"name":"NCE","slug":"NCE","permalink":"http://yoursite.com/tags/NCE/"},{"name":"infoNCE","slug":"infoNCE","permalink":"http://yoursite.com/tags/infoNCE/"}]},{"title":"Distributed Data Parallel and Its Pytorch Example","date":"2020-12-20T04:19:38.000Z","path":"2020/12/20/Distributed-Data-Parallel-and-Its-Pytorch-Example/","text":"訓練時候的平行化可分為: Model Parallel: 所有 GPUs 跑同一個 batch 但是各自跑模型不同部分 Data Parallel: GPUs 跑不同的 batches, 但跑同一個完整的模型 由於 Data Parallel 跑同一個完整模型且各 GPU 都用自己複製的一份, 在 update 參數時要如何確保更新一致? 可分為 synchronous 和 asynchronous update. (文章後面會詳細討論) 本文討論 Data Parallel with asynchronous update. 既然要做 data parallel, 第一件事情便是如何對不同 GPU 分派不同的 batches, 接下來我們就使用 PyTorch 做這件事. 指派不同 Batch 給不同 GPU直接上一個 toy example (minimal_distributed_data_example.py) 123456789101112131415161718192021222324252627282930313233343536# file: minimal_distributed_data_example.pyimport ...class SimpleDataset(torch.utils.data.Dataset): def __init__(self, start, end): assert(start &lt; end) self.start, self.end, self.data_num = start, end, end - start def __len__(self): return self.data_num def __getitem__(self, idx): return idx + self.startif __name__ == '__main__': # ===== Distributed Settings world_size = int(os.environ.get('WORLD_SIZE', 1)) local_rank = 0 is_distributed = world_size &gt; 1 if is_distributed: torch.distributed.init_process_group(backend='nccl') local_rank = torch.distributed.get_rank() torch.cuda.set_device(local_rank) device = torch.device(\"cuda\", local_rank) # ===== Dataset/DataLoader Settings dataset = SimpleDataset(0, 4*6) sampler = DistributedSampler(range(4*6), shuffle=False, seed=1111) # Shuffle here (set True) if needed rather than in DataLoader print(f'========== device:&#123;device&#125;') data_parallel_dl = DataLoader(dataset, batch_size=4, num_workers=8, shuffle=False, sampler=sampler) # since we use sampler, so we set shuffle to False (default) in DataLoader # ===== Traverse All Data arr = [] for sample_batch in data_parallel_dl: arr += sample_batch.tolist() t = np.random.randint(100)/100.0 sample_batch.to(device) print('sleep &#123;:.2f&#125;; device:&#123;&#125;\\t&#123;&#125;'.format(t, device, sample_batch)) time.sleep(t) print(f'device:&#123;device&#125;\\n&#123;np.sort(np.array(arr))&#125;') [Line 23~27 有關 Dataset/DataLoader] Line 24 dataset 只是一個 0 到 23 的 int list. Line 27 DataLoader 在分配 batches 給不同 GPUs 時只需要將 sampler 使用 DistributedSampler 創建就可以. DistributedSampler 在分配一個 batch 除了會指定資料是那些 index 之外, 還會指定該筆 batch 是要分到哪個 gpu. [Line 14~22 有關 Distributed Settings]在執行這個檔案的時候, 我們會使用 torch.distributed.launch, 範例指令如下: 1CUDA_VISIBLE_DEVICES=2,3 python -m torch.distributed.launch --nproc_per_node=2 --use_env minimal_distributed_data_example.py 此時 PyTorch 會開啟兩個 processes 去執行你的 .py, 這裡注意不是 threads, 這是因為 python Global Interpreter Lock (GIL) 的原因, 使用 thread 效率會不高. 另外使用 --use_env 則會在各自的 process 裡設定環境變數: WORLD_SIZE (範例 = 2) LOCAL_RANK (範例 = 0 or 1) 因此 line 17 我們便可藉由 world_size 得知是否為 distributed 環境. 是的話 line 20 就可以拿到這個 process 的 local_rank (可以想成是 worker 的編號, 也就是第幾個平行的單位), 接著 line 21, 22 就可以根據 local_rank 設置 gpu. [Line 28~36 有關 go through all data] 在執行時, 各個 process 會拿到相對應個 batches. Line 35 模擬處理該筆資料所花的時間. Line 36 為確認自己這個 process 總共拿到那些 batches. 以範例來說, 兩個 gpus 應該要拿到 exclusive 的兩個 sets 其聯集是 {0,1, …, 23}. 結果如下: Good Job! 現在我們會把每個 GPU 都分配不同的 batches 了, 不過還有一個關鍵的問題: 該怎麼各自計算 gradients 然後 update? 這就開始討論 update 的兩種 case, synchronous and asynchronous update. Asynchronous Update Synchronous: 每一次模型 update 要等到所有 device 的 batch 都結束, 統合後 update Asynchronous: 每個 device 算完自己的 batch 後即可直接 update 可以想像非同步的化可以更新的比較有效率, 但可能效果會不如同步的方式.Asynchronous 會遇到的狀況是算完 gradient 後要 update parameters 時, parameters 已經被其他 process update 過了, 那為什麼還可以 work? Asynchronous 狀況 1範例假設兩個 GPU (1&amp;2) 其參數空間都在 $\\theta_a$. Step 1. 假設 GPU2 先算完 $\\Delta P_2(\\theta_a)$ 並且 update 到 $\\theta_b$: $$\\begin{align} \\theta_b = \\theta_a + \\Delta P_2(\\theta_a) \\end{align}$$ Step2. 這時候 GPU1 算完 gradient 了, 由於當時算 gradient 是基於 $\\theta_a$, 因此 gradient 為 $\\Delta P_1(\\theta_a)$, 但是要 update 的時候由於已經被 GPU2 更新到 $\\theta_b$ 了, 所以會更新到 $\\theta_c$: $$\\begin{align} \\theta_c = \\theta_b + \\Delta P_1(\\theta_a) \\end{align}$$ 這裡讀者可能會疑問, 計算 gradient 與 update 時根據的參數是不同, 這樣 update 會不會出問題? 以上面這個例子來說, 還剛好沒事. 原因是其實等同於 synchronous update: $$\\begin{align} \\theta_c = \\theta_a + \\left[ \\Delta P_2(\\theta_a) + \\Delta P_1(\\theta_a) \\right] \\end{align}$$ 那可能會繼續問, 這只是剛好, 如果一個 GPU 比另一個慢很多, 會怎樣? 我們看看 case 2 Asynchronous 狀況 2GPU2 太快了… 已經 update 好幾輪 好吧… 想成類似有 momentum 效果吧 實務上會在幾次的 update 過後強制 synchronize update 一次, 可以想像如果一些條件成立 (譬如 gradients 是 bounded), 應該能保證收斂 (這邊我沒做功課阿, 純粹猜測) Synchronous Update每個 gpu 都算完各自 batch 的 gradients 後, 統一整理 update parameters, 常見兩種方式: Parameter Server Ring Allreduce 接著介紹的這兩種方法圖片主要從 Baidu: Bringing HPC techniques to deep learning [Andrew Gibiansky] 筆記下來. Parameter Server 的 Synchronous Update一次 Update 分兩步驟 GPU 0 全部都拿到 GPU 1~4 的 Gradients 後, 更新 parameters GPU 0 把 model 發送給 GPU 1~4 假設有 $N$ 個 GPU, 通信一次花費時間 $K$, 則 PS 方法成本為: Gradients passing: $(N-1)K$ Model passing: $(N-1)K$ Total $2K(\\color{orange}{N}-1)$, 跟 GPU 數量正比 Ring Allreduce 比較多圖, 特別拉出一個 section 說明 Ring Allreduce 的 Synchronous Update每一個 GPU 都分別有一個傳送和接收的對象 GPU, 分配起來正好形成一個環. 假設每個 GPUs 都算好 gradients 了, 並且我們將 gradients 分成跟 GPU 數量一樣的 $N$ 個 chunks: 這方法分兩步驟: Scatter Reduce All Gather 1. Scatter Reduce 做完 $N-1$ 次 iteration 後可以發現每張 GPU 都會有一個是完整的 chunk. 2. All Gather 做完 $N-1$ 次 iteration 後可以發現每張 GPU 都拿到所有完整的 chunk. All Gather 流程跟 Scatter Reduce 是一樣, 只是將累加行為變成取代而已. 成本每個 GPUs 都得到統合後的 gradients, 因此 各個 GPU 上的 model 可以各自 update (gradients 相同, 所以 update 後的 models 也相同) 假設有 $N$ 個 GPU,則成本為: 通信一次花費時間 $K/N$ (因為我們分成 $N$ 個 chunks 同時傳輸) Scatter reduce: $(N-1)K/N$ All gather: $(N-1)K/N$ Total $2K(\\color{orange}{N}-1)/\\color{orange}{N}$, 跟 GPU 數量無關 PyTorch: Model with DDP還記得最開頭的範例嗎? 我們做到了把每個 GPU 都分配不同的 batches, 但還不會將各自計算 gradients 統合然後 update. 其實我們只需要針對上面範例的 minimal_distributed_data_example.py 做點修改就可以. 針對 model 作如下改動: 1model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank) 這樣就使得 model 的 backward() 成為 sync op. 也就是在呼叫 loss.backward() 會等到每張 GPU 的 gradient 都算完且 sync 了 (PS or All Gather 都可以) 才會接下去執行. 注意事項 由於每個 process 都有自己的 optimizer(scheduler), 而 momentum 會根據當前的 gradient update, 如何確保每個 optimizers 都相同?Ans: 由於 .backward() 是 sync op, 因此 opt.step() 時每個 processes 的 gradients 已經同步了, 所以 momentum 會根據相同的 gradient update Batch-norm 的 statistics 同步?Ans: See torch.nn.SyncBatchNorm Save checkpoint 時在一張卡上存就可以 (通常用 LOCAL_RANK=0 的那個 process) 怎麼確保每個 process 上的 model random initial 相同的 weights?Ans: DistributedDataParallel 在 init 時就會確保 parameters/buffers sync 過了, see here model 經過 DistributedDataParallel 包過後 name 會多一個前綴 module., 如果訓練和加載模型一個使用 DDP 一個沒有 load_state_dict 有可能會因此出錯, 需自行處理 一些 metrics 如 accuracy/loss 由於在各個 GPUs 計算, 可以利用 torch.distributed.all_reduce, torch.distributed.all_gather 等來 syncSee DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED 有一個不錯的 DDP 範例 [2] 如果可以的話, 推薦使用 PyTorch Lightning, 直接幫你把這些繁瑣的細節包好, 告訴它要用幾張 GPUs 就結束了. Reference[1] Bringing HPC Techniques to Deep Learning[2] A good example of DDP in PyTorch","tags":[{"name":"Distributed Data Parallel (DDP)","slug":"Distributed-Data-Parallel-DDP","permalink":"http://yoursite.com/tags/Distributed-Data-Parallel-DDP/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://yoursite.com/tags/PyTorch/"}]},{"title":"Quantization 的那些事","date":"2020-10-03T01:35:24.000Z","path":"2020/10/03/Quantization-的那些事/","text":"NN 在做 quantization 時採用的是非對稱的方式, real ($r$) 和 quantized ($q$) values 對應關係如下: 其中 zero point $Z$ 會跟 $q$ 相同 type, 例如 int8, 而 scaling value $S$ 則會跟 $r$ 相同, 例如 float. 以 uint3 (0~7) 做 quantization, 如下圖所示: 本篇討論以下兩點: 同一個 real 值如何在不同的 $Z$/$S$ 做轉換, e.g.: $q_1$ with ($Z_1$/$S_1$) 如何對應到 $q_2$ with ($Z_2$/$S_2$) PyTorch 的 Quantization Aware Training (QAT) 討論 在不同 $Z$/$S$ 轉換有兩個常見理由: 在做 NN 的 quantization 時候, 每個 layer 的 output domain 都不同, 這導致了使用不同的 $Z$/$S$. 又或者丟給 NN 做 inference 之前, mfcc/mfb 需要先轉換到 NN input 的 $Z$/$S$ quantized domain 上. 額外提一點 PyTorch 的 quantized Tensor 其實就只是比原本的 Tensor 多了 $Z$ and $S$. 例如給定 $Z$ and $S$, torch.quantize_per_tensor 會將一個正常的 tensor 從 $r$ 轉成 $q$, 官網範例: 12345&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)tensor([-1., 0., 1., 2.], size=(4,), dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()tensor([ 0, 10, 20, 30], dtype=torch.uint8) 以下我們都以 uint8 當作 quantized 的 type, real value 以 float (4 bytes) 為準. 而 int 為 4 bytes. 先使用 Float 轉換要將第一個 domain ($Z_1$/$S_1$) 的數值轉換到第二個 domain ($Z_2$/$S_2$) 最簡單的方法就是先把第一個 domain 的 $r_1$ 算出來, 再利用第二個 domain 的 $Z_2$/$S_2$ 求得 $q_2$ $$\\begin{align} \\color{orange}{r_1}=(float)\\left( \\left( (int32)q_1-Z_1 \\right)*S_1 \\right) \\\\ q_2=\\text{uint8_saturated_round}\\left( \\frac{\\color{orange}{r_2}}{S_2}+Z_2 \\right) \\end{align}$$ 由於 $r_2=r_1$ 因此 (2) 可計算出 $q_2$. 但這樣計算還是用到 float, 其實我們可以完全使用 integer 運算來達成. 純用 Integer 運算 其中 $M&gt;1.0$ 是沒有意義的, e.g. $S_1&gt;S_2$. 如下圖舉例來說, data domain 分布只會在 8 個點位置上, 使用更細的 resolution 去存沒意義. $M_0$ 很明顯可以用 Q0.31 的 int32 來保存, 所以 $M_0$ 與 $(q_1-Z_1)$ 相乘的時候使用 fractional multiplication, 最後 $2^{-n}$ 使用 shift 即可. 什麼是 fractional multiplication? 一張圖表示就知道: 最後我們要驗證的話其實可以跟上一段講的 Float 版本對比就可以. 矩陣運算的 Quantization 轉換其實 convolution 裡的矩陣運算只是原來的 $r_2=r_1$ 變成 $r_3=r_1r_2$ 的關係而已, 其餘都相同. 貼一張論文的內容即可. 更多內容可以參考論文 ref [1], 例如使用 ReLU6 替代 ReLU, 因為如果我們使用 uint8 的話由於 ReLU6 將 domain 限制在 [0,6] 之間, 這樣 8 bits 可以用 $Z=0$, $S=1.0/2^5=0.03125$ 來表示. 同時最後再轉換成 quantization model 時可以直接拿掉 ReLU6 (因為直接使用 quantization 就好) Symmetric Fixed Point傳統上常見的 fixed point 採用的是 symmetric quantization, 例如 Q4.3 這種 int8 的表示方式 (-8.0 ~ 7.875). 但它其實只是 asymmetric quantization 的特例. Q4.3 基本上就是 $Z=0$ 和 $S=1.0/2^3=0.125$ 的 asymmetric quantization. PyTorch 的 Quantization Aware Training (QAT) 筆記PyTorch 1.7.0 quantization doc 一開始要先對你的 NN Module 先作如下改動: 在自己定義的 NN Module 裡, 所有用到 torch.nn.functional 的 op 都轉換成 torch.nn.Module 在自己定義的 NN Module 裡, forward 時先將 input 過 QuantStub(), 然後最後 output 過 DeQuantStub(). QuantStub() 會將正常的 input tensor 變成 quantized tensor (裡面包含 $Z$/$S$), 然後 DeQuantStub() 會將 quantized tensor 轉換成正常的 tensor. 在自己定義的 NN Module 裡, 使用 torch.quantization.fuse_modules 定義你的 fuse_model function. 目前 PyTorch 只支援有限種 modules fusion (see function fuse_known_modules in fuse_modules.py). 接著 QAT 為以下幾個步驟: 將 NN 的 object (net) 設定為 net.train() (如果只是做 post-quantization 則用 net.eval()).這是因為 QAT 要在 training 時模擬 inference 的 quantization precision loss, 所以要插入很多 fake-quantization 的 op. 可以參考論文 ref [1] 的 Figure C.4 到 Figure C.8. 而如果只是 post-quantization 則在原來正常的 floating trianing 完後, 將 net.eval() 設定好直接就 fuse model 了 (torch.quantization.fuse_modules 對是 train or eval 有不同的 fuse 行為). 呼叫 net.fuse_model().例如假設我們要 fuse [&#39;conv1&#39;, &#39;bn1&#39;, &#39;relu1&#39;], PyTorch 會將第一個 Module 變成 fused Module, 剩下的兩個為 Identity() Module 將 net 設定 attribute qconfig.例如: net.qconfig= torch.quantization.get_default_qat_qconfig(&#39;fbgemm&#39;) 呼叫 torch.quantization.prepare_qat(net, inplace=True).此 function 主要幫你做兩件事情: a. propagate qconfig: 對所有子 Module 設定相對應的 qconfig (因為步驟3我們只針對 root Module 設定 qconfig) b. add observer/fake-quantization: observer 為簡單的 min/max 線性量化方式(或 histogram 方式等). 將圖需要 quantization 的地方安插好這些 observer/fake-quantization. 執行一般 training 流程.在 training 的過程中就會順便統計好對應的 min/max 等, 然後每個 tensor 的 $Z$/$S$ 也會對應得到 (通常用 moving average 方式做 smoothing). 最後轉換成 quantized model torch.quantization.convert(net, inplace=True) 以上一個最小範例如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import torchimport torch.nn as nnfrom torch.quantization import QuantStub, DeQuantStubimport torch.quantizationclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.bn1 = nn.BatchNorm2d(6) self.relu1 = nn.ReLU() self.quant = QuantStub() self.dequant = DeQuantStub() def forward(self, x): x = self.quant(x) x = self.relu1(self.bn1(self.conv1(x))) x = self.dequant(x) return x # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization # This operation does not change the numerics def fuse_model(self): torch.quantization.fuse_modules(self, ['conv1', 'bn1', 'relu1'], inplace=True)net = Net()print('===== Before fuse_model:')print(net)print('===== After fuse_model:')net.train()net.fuse_model()print(net)print('===== Setting qconfig:')# Specify quantization configuration# Start with simple min/max range estimation and per-tensor quantization of weightsnet.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')print(net.qconfig)print('===== After torch.quantization.prepare:')torch.quantization.prepare_qat(net, inplace=True)print(net)# Do your regular trainingtraining_loop(net)print('===== After torch.quantization.convert:')torch.quantization.convert(net, inplace=True)print(net) 最後附上一個很棒的 convolution and batchnorm fusion 解說 [連結], 作者是 Nenad Markuš Reference Paper: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference (BETA) STATIC QUANTIZATION WITH EAGER MODE IN PYTORCH Nenad Markuš: Fusing batch normalization and convolution in runtime","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://yoursite.com/tags/PyTorch/"},{"name":"Asymmetric Quantization","slug":"Asymmetric-Quantization","permalink":"http://yoursite.com/tags/Asymmetric-Quantization/"},{"name":"Symmetric Quantization","slug":"Symmetric-Quantization","permalink":"http://yoursite.com/tags/Symmetric-Quantization/"},{"name":"Quantization Aware Training (QAT)","slug":"Quantization-Aware-Training-QAT","permalink":"http://yoursite.com/tags/Quantization-Aware-Training-QAT/"}]},{"title":"TF Notes (7), Some TF2.x Eager Mode Practices","date":"2020-06-26T02:52:18.000Z","path":"2020/06/26/TF-Notes-some-TF2-x-eager-mode-practices/","text":"為了學習 TF2.x 只好把以前練習的一些 projects 重寫一次, 但後來時間斷斷續續的, 所以只做了一部分. 總之先記錄一下目前的練習進度吧. TFDataset 練習: jupyter notebook if map() has random ops: dataset.shuffle().batch().cache().map().prefetch() map() has NO random ops: dataset.shuffle().batch().map().cache().prefetch() NeuralStyleTransfer: jupyter notebook 練習 optimization 變數是 input x 而不是原來的 weights w TSLearning ToyExample: jupyter notebook 固定某一部分的 model 最原始的 distillation AutoEncoder jupyter notebook: decoder 部分使用 deconvolution jupyter notebook: 全部 FC 但 decoder 是 encoder 的 transpose (share weights) GAN jupyter notebook: MMGAN jupyter notebook: WGAN jupyter notebook: WGAN-div Adversarial Domain Adaptation jupyter notebook 和 介紹及實驗結果 還有很多沒練習到, VAE, seq2seq, transformer 等….只好再說了","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"}]},{"title":"CTC Model and Loss","date":"2020-05-31T10:07:24.000Z","path":"2020/05/31/CTC-Model-and-Loss/","text":"CTC model 是一個 decoder 部分為簡單的 (independent) linear classifer 的 seq2seq model. 因此 input frame 有 $T$ 個, 就會有 $T$ 個 output distribution vectors. 正常來說 (ex: ASR) output token 數量 $N&lt;T$, 所以會有 alignment 問題. 以往的 alignment (HMM) 強迫每個 frame index 都需對應到一個 phone’s state, 但 CTC 允許對應到 “空” 的 state (null or blank). 這讓 CTC 的 alignment 比 HMM 更有彈性. RNN-T 是另一種比 CTC 更有彈性的 alignment 表達方式. CTC 的 gradient 可以非常有效率的用 dynamic programming 求得 (forward/backward 演算法, 下圖). 因此採用 gradient-based optimization 方法就很合適. 本文會詳細介紹上面提到的幾點. Decoding 部分不介紹. CTC Model and Loss: PDF Slide 連結 Link PDF","tags":[{"name":"CTC","slug":"CTC","permalink":"http://yoursite.com/tags/CTC/"}]},{"title":"Exp of Adversarial Domain Adaptation","date":"2020-05-17T09:15:21.000Z","path":"2020/05/17/Exp-of-Adversarial-Domain-Adaptation/","text":"Domain Adaptation 是希望在 source domain 有 label 但是 target domain 無 label 的情況下, 能針對 target domain (或同時也能對 source domain) 進行分類任務. “Adversarial” 的意思是利用 GAN 的 “對抗” 想法: Label predictor 雖然只能保證 source domain 的分類. 但由於我們把 feature 用 GAN 消除了 domain 之間的差異, 因此我們才能期望這時候的 source domain classifier 也能作用在 target domain. 這篇文章 張文彥, 開頭的圖傳達的意思很精確, 請點進去參考. 接著嘗試複現了一次 Domain-Adversarial Training of Neural Networks 的 mnist(source) to mnist_m(target) 的實驗. 上一篇說明 GAN 的 framework: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} D(x) - E_{x\\sim P_G}f^*(D(x)) \\right] \\\\ G^*=\\arg\\min_G{Div\\left(P_d\\|P_G\\right)} + reg(G) \\\\ \\end{align}$$ 對於 Adversarial Domain Adaptation 來說只要在正常 GAN 的 training 流程中, update $G$ 時多加一個 regularization term $reg(G)$ 就可以了. 而 $reg(G)$ 就是 Label Predictor 的 loss, 作用就是 train $G$ 時除了要欺騙 $D$, 同時要能降低 prediction error. 實驗 source domain 為標準的 mnist, 而 target domain 是 modified mnist, 如何產生可以參考Daipuwei/DANN-MNIST. 下圖是 mnist_m 的一些範例: 我們先來看一下分佈, 藍色的點是 mnist, 紅色是 mnist_m, 用 tSNE 跑出來的結果明顯看到兩個 domain 分佈不同: 我們之前說過, 不用管 GRL (Gradient Reversal Layer), 就一般的 GAN 架構, 加上 regularization term 就可以. 聽起來很容易, 我就隨手自己用了幾個 CNN 在 generator, 幾層 fully connected layers 給 classifier 和 discriminator 就做了起來. 發現怎麼弄都訓練不起來! 產生下面兩種情形: GAN too weak:重新調整了一下 $reg(G)$ 的比重後…. GAN too strong:兩個 domain 的 features 幾乎完全 overlap, 然後 classifier 幾乎無作用 (也看不出有10個分群). 話說, 這圖很像腦的紋路? 貪食蛇? 迷宮? 肚子裡的蛔蟲? 後來在嘗試調了幾個參數後仍然訓練不起來. 這讓我感到很挫折. 實在受不了後, 參考了網路上的做法改成以下幾點: WGAN 改成用 MMGAN RMSProp(1e-4) 改成 Adam(1e-3) 使用網路上一個更簡單的架構 github 改成用 MMGAN 後, 去掉 BN layer 就能訓練起來 然後就可以訓練起來了(翻桌xN), 訓練後的結果如下: 可以看到在 mnist 辨識率 ~99% 的情形下, mnist_m 能夠有 83.6% 的辨識率 (沒做 adaptation 只有約50%) Feature 的分布如下圖 (藍色的點是 mnist, 紅色是 mnist_m): 雖然還有一些 feature 沒有完全 match 到, 但已經很重疊了. 同時我們也能明顯到到 10 群的分類. 結論雖然理論上的理解很容易, 但實作起來卻發現很難調整. GAN 就是那麼難搞阿…. Reference GAN framework Domain-Adversarial Training of Neural Networks 參考產生 mnist_m 的 codes Daipuwei/DANN-MNIST Domain-Adversarial Training of Neural Networks with TF2.0: lancerane/Adversarial-domain-adaptation 張文彥 Domain-adaptation-on-segmentation 自己實驗的 jupyter notebook","tags":[{"name":"GAN","slug":"GAN","permalink":"http://yoursite.com/tags/GAN/"},{"name":"ADDA","slug":"ADDA","permalink":"http://yoursite.com/tags/ADDA/"}]},{"title":"Framework of GAN","date":"2020-05-11T12:29:12.000Z","path":"2020/05/11/Note-for-Framework-of-GAN/","text":"說來汗顏, 自從17年三月筆記完 WGAN 後, 就沒再碰 GAN 相關的東西了. 惡補了一下 李宏毅GAN 的課程和其他相關資料, 因此筆記一下. MMGAN(最原始的GAN), NSGAN(跟MMGAN差別在 G 的 update 目標函式有點不同), f-GAN, WGAN, ADDA (Adversarial Discriminative Domain Adaptation), infoGAN, VAE-GAN 等… 這些全部都是 follow 下面這樣的 framework: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} D(x) - E_{x\\sim P_G}f^*(D(x)) \\right] \\\\ G^*=\\arg\\min_G{Div\\left(P_d\\|P_G\\right)} + reg(G) \\\\ \\end{align}$$ 其中 $P_d$ 為 real data pdf, $P_G$ 為 generator 產生的 data pdf. $f^*$ 帶入不同的定義會產生不同的 divergence, 這之後會再說明. 式 (1) 定義了 $P_G$ 與 $P_d$ 的 divergence, 其中這個 divergence 的值為藉由解這個最佳化問題求得的. 式 (2) 表示要找的 $G$ 就是 divergence 最小的那個. Divergence 最小 ($=0$) 同時也表示 $P_G=P_d$ (生成器鍊成). 如果同時考慮 regularization term, $reg(G)$, 則會有很多變化產生, 如 ADDA, infoGAN, VAE-GAN… 我們接著來看 MMGAN, NSGAN, f-GAN, WGAN, ADDA, infoGAN, VAE-GAN 這些怎麼 fit 進這個框架. MMGANMMGAN 是 MinMax GAN 的縮寫, 指的是最原始的 GAN. 將 (1) 中的 $D(x)$ 使用 $\\log D(x)$ 替換, 並且 $f^*(t)=-\\log(1-exp(t))$ 替換得到如下式子: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} \\log D(x) - E_{x\\sim P_G}[-\\log(1-D(x))] \\right] \\\\ \\end{align}$$ 稍微再整理一下: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} \\log D(x) + E_{x\\sim P_G}[\\log(1-D(G(z)))] \\right] \\\\ \\end{align}$$ 這就是 GAN discriminator 原始的式子. 而我們知道給定 $G$ 上述的最佳解為 \\( D_G^*(x) = \\frac{P_d(x)}{P_d(x)+P_G(x)} \\), 並帶入 (4) 我們得到: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = -\\log4+KL(p_d\\vert\\frac{p_d+p_g}{2})+KL(p_g\\vert\\frac{p_d+p_g}{2}) \\\\ =-\\log4+2JSD(p_d \\vert p_g) \\end{align}$$ 因此 discriminator 的最大化目的是計算出 JS divergence. 而 generator $G$ 求解沒什麼好說, 直接對 (3) 最小化: $$\\begin{align} G^*=\\arg\\min_G E_{x\\sim P_G}[\\log(1-D(x))] \\end{align}$$ 注意到與 (2) 對比, MMGAN 只是沒有 regularization term 而已. NSGANNSGAN 為 Non-Saturating GAN 縮寫, 與 MMGAN 只差在 generator $G$ 求解式子不同, 原本是希望在一開始 generator 比較差的情形下用 (7) 算的 gradient 會太小, 因此改成下式, 使得 gradient 能在一開始的時候比較大, 讓 update 動起來. NSGAN generator $G$ 為: $$\\begin{align} G^*=\\arg\\min_G E_{x\\sim P_G}[-\\log(D(x))] \\end{align}$$ 如果我們將 \\( D_G^*(x) = \\frac{P_d(x)}{P_d(x)+P_G(x)} \\) 帶入並整理, 我們會發現: $$\\begin{align} G^*=\\arg\\min_G E_{x\\sim P_G}[-\\log(D^*(x))] \\\\ =\\arg\\min_G \\left[ KL(P_G\\|P_d)-2JSD(P_d\\|P_G) \\right] \\end{align}$$ 產生了兩個互相 trade-off 的 objective funtion… 這造成了矛盾 詳細推導請參考 令人拍案叫绝的Wasserstein GAN 一文, 非常棒的文章. 引用文章內的說明: 一句话概括：最小化第二种生成器loss函数，会等价于最小化一个不合理的距离衡量，导致两个问题，一是梯度不稳定，二是collapse mode即多样性不足。 f-GAN我們在 MMGAN 時提到 “將 (1) 中的 $D(x)$ 使用 $\\log D(x)$ 替換, 並且 $f^*(t)=-\\log(1-exp(t))$ 替換” 則會得到 discriminator 就是在求解 JS divergence. 那麼有沒有其他設定會產生其他 divergence 呢? 有的, 藉由 f-GAN 的定義可以囊括各式各樣的 divergence. 使用李老師的說明流程筆記: 首先定義 f-divergence, 可以發現 JSD, KL, reverse-KL, Chi square 等等都屬於其中的特例. 接著說明 convex function 的 conjugate function. 最後才說明怎麼跟 GAN 產生關聯 (神奇的連結). f-divergence$$\\begin{align} Div_f(P\\|Q)=\\int_x q(x)f\\left( \\frac{p(x)}{q(x)} \\right) dx \\\\ \\text{where } f \\text{ is }\\color{orange}{convex} \\text{ and } f(1)=0 \\end{align}$$ 明顯知道 $p(x)=q(x)$ 時 $Div_f(P|Q)=0$, 同時可以證明 $Div_f(P|Q)\\geq 0$, 因此滿足 divergence 定義(search “Divergence (statistics) wiki” for definition): $$\\begin{align} Div_f(P\\|Q)=\\int_x q(x)f\\left( \\frac{p(x)}{q(x)} \\right) dx \\\\ \\geq f\\left( \\int_x q(x)\\frac{p(x)}{q(x)} dx \\right)=f(1)=0 \\\\ \\end{align}$$ $f$ 是 convex 這點很重要, 才能將 (13) 到 (14) 使用 Jensen’s inequality. 定義不同 $f$ 會產生不同 divergence, 常見的為(李老師slide): 由於 $f$ 是 convex, 而每一個 convex function 都會有一個 conjugate function $f^*$ (它也是 convex), 利用這個特性最後可以跟 GAN 連起來. 因此以下先說明 conjugate function. Fenchel ConjugateEvery convex function $f$ has a conjugate function $f^*$: $$\\begin{align} f^*(t)=\\max_{x\\in dom(f)}\\{xt-f(x)\\} \\end{align}$$ 老師的投影片非常形象的表示出 $f$ 與 $f^*$ 的關係L 還具體舉了個當 $f(x)=x\\log x$ 的例子: 與 GAN 的關聯這是我覺得非常厲害的地方. 首先 $f^*$ 的 conjugate 就變回 $f$ 了, 它們互為 conjugate. $$\\begin{align} f^*(t)=\\max_{x\\in dom(f)}\\{xt-f(x)\\}\\longleftrightarrow f(x)=\\max_{t\\in dom(f^*)}\\{xt-f^*(t)\\} \\end{align}$$ 將 (11) 利用 conjugate 的關係重新表示一下 $$\\begin{align} Div_f(P\\|Q)=\\int_x q(x)f\\left( \\frac{p(x)}{q(x)} \\right) dx \\\\ =\\int_x q(x) \\left( \\max_{t\\in dom(f^*)} \\left[ \\frac{p(x)}{q(x)}t - f^*(t) \\right] \\right) dx \\end{align}$$ 厲害的地方來了…. 假設我們有一個 function $D$ 可以直接幫我們解出 (18) 的那個 $t$ 是什麼, 也就是: $$\\begin{align} D(x)=\\hat{t}=\\arg\\max_{t\\in dom(f^*)} \\left[ \\frac{p(x)}{q(x)}t - f^*(t) \\right] \\end{align}$$ 那麼 $Div_f(P||Q)$ 直接就是 $$\\begin{align} Div_f(P||Q)=\\int_x q(x) \\left[ \\frac{p(x)}{q(x)}\\hat{t} - f^*(\\hat{t})) \\right] dx \\end{align}$$ 實作上 $D$ 的表達能力有限, 同時讓我們找到最準的那個叫做 $\\hat{D}$, 因此只能求得一個下界並整理一下得到: $$\\begin{align} Div_f(P||Q)\\geq \\int_x q(x) \\left[ \\frac{p(x)}{q(x)}\\hat{D}(x) - f^*(\\hat{D}(x))) \\right] dx \\\\ \\approx \\int_x q(x) \\left[ \\frac{p(x)}{q(x)}\\hat{D}(x) - f^*(\\hat{D}(x))) \\right] dx \\\\ = \\int_x {p(x)\\hat{D}(x)}dx - \\int_x{q(x)f^*(\\hat{D}(x))} dx \\\\ = E_{x\\sim P}\\left[ \\hat{D}(x) \\right] - E_{x\\sim Q}\\left[ f^*( \\hat{D}(x) ) \\right] \\\\ = \\max_D \\left[ E_{x\\sim P}\\left[ D(x) \\right] - E_{x\\sim Q}\\left[ f^*( D(x) ) \\right] \\right] \\\\ \\end{align}$$ 請把 (25) 跟 (1) 比較, 其實就一模一樣. 因此, 只要 $f$ 是 convex function , 且 $f(1)=0$, discriminator $D$ 的最佳化問題 ((1) 用 $f$ 的 conjugate, $f^*$, 帶入) 就是在計算兩個分布的 f-divergence. 論文直接給出各種 f-divergence 的 $f$ and $f^*$ 因此我們可以發現 MMGAN 和 LSGAN 都是 f-GAN 的一種特例. WGAN具體請參考之前自己筆記的文章 李老師的講義對於 Earth Mover’s Distance (或稱 Wasserstein distance) 講解得很清楚, 其中的一個參考連結更解釋了 Wasserstein distance 如何轉換成求解 $\\max_D$ 且 $D$ 必須限制在 Lipschitz 條件下. 總之這裡要說的是, Wasserstein distance 不屬於 f-divergence, 但也完全 follow 我們一開始說的 (1) &amp; (2) 的架構: 令 $f^*(x)=x$ 同時多一個限制是 $D\\in k-Lipschitz$ $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_{D\\in k-Lipschitz}\\left[ E_{x\\sim P_d} D(x) - E_{x\\sim P_G}D(x) \\right] \\\\ \\end{align}$$ 求解 discriminator 的最佳化問題其實就是在估算兩個分布的 divergence. 原始論文針對 $D\\in k-Lipschitz$ 的限制直接用很暴力的 weight clipping 方法解掉. 因此後面有一篇 WGAN-GP (Gradient Panelty) 的方式補強. 這裡不展開討論, 因為我也沒什麼研究, 簡單帶過一點待讀的論文. 另外有一篇 SN-GAN “Spectral Normalization for Generative Adversarial Networks“ 看起來是一種主流訓練 WGAN 的方式, 事先就將 gradient 都限制 norm&lt;=1. 這篇文章大致整理各種變體, 參考連結. 關於 regularization term, $reg(G)$Adversarial Domain Adaptation我們先說 Domain-Adversarial Training of Neural Networks 這篇經典的文章. Generator 現在做的是 feature extractor 的工作, 而我們希望 target domain 的 feature 能跟 source domain 的 feature 分佈一樣, 這樣在 source domain (有 label) 訓練好的 model, 就能直接在 target domain (無 label) 上作用. 要做到無法區分出這個 feature 是 source or target domain 這件事情….正好就可以用 GAN 的方式達到. 不看 Label Predictor 的部分的話, 就是一個典型的 GAN. 作用就是把 source and target 的 feature 投影到共同的空間, 並且分不開. 但缺少 Label Predictor 有可能造成 feature extractor 產生 trivial solution (例如全部 map 到 constant) 這樣也能使 discriminator 分不開. 因此加上 Label Predictor 除了避免這件事外, 也保證在 source domain 能夠很好的完成我們的分類任務. 注意, 因為 label 只有在 source domain, 因此 label predictor 只能保證 source domain 的分類. 但由於我們把 feature 用 GAN 消除了 domain 之間的差異, 因此我們才能期望這時候的 source domain classifier 也能作用在 target domain. 論文使用了一個叫做 Gradient Reversal Layer (GRL), 其實我們可以忽略這件事情, 因為這只是 discriminator and generator 一個 maximize 另一個 minimize, 而使得要 update generator 時當時算的 discriminator gradient 要取負號. 我們照正常的 GAN training 就可以了. Label Predictor 的 loss 具體就是 (2) 的 regularization term, $reg(G)$. 這是希望我們 train $G$ 的時候除了要欺騙 $D$, 同時要能降低 $reg(G)$ (prediction loss). 後續有一篇 Advesarial Discriminative Domain Adaptation 算是豐富了這種架構. 論文裡對 source and target 的 feature extractor 使用不同的 neural networks. 並且一開始的 source domain feature extractor 是事先訓練好的. 然後後面的 GAN 部分訓練的時候, target domain 的 feature extractor 要去匹配 source domain 的. 這樣做的好處是至少一邊的分佈是固定住的, 比較容易訓練. 同時也簡化了訓練流程, 見下圖: infoGAN詳細就不解釋了, 事實上推導較複雜但實作上卻異常容易, 之後有機會再記錄一下. 總之在原始 GAN 架構上多了一個 Decoder, 用來還原 generator input 中所指定的部分($c$). Decoder 希望能將 $c$ 無損的還原, 那麼什麼叫無損? 指的就是 Mutual Information of $c$ and $\\hat{c}$ 最大. 其中 $\\hat{c}$ 表示由 Decoder 還原出來的結果. 還原的 loss term 基本就是 $reg(G)$, 同樣的理解, $G$ 除了要騙過 $D$ 之外, 多了一個任務就是使得還原的 loss 愈小愈好. 附上李宏毅教授課程的兩張圖片: VAE-GAN直接上老師的 slides 以 GAN 的角度來看, $G$ 除了要欺騙 $D$ 之外, 還多了 VAE 的 loss ($reg(G)$) 用來 reconstruct 原本的 input image. 對 GAN 來說是有好處的, 因為 GAN 雖然能夠產生夠真的 image, 但是會自己”捏造”, 因此多了 VAE 的 $reg(G)$ 會讓捏造的情況降低. 以 VAE 的角度來看, GAN 的 loss 變成了 regularization term 了. 也就是說 VAE 除了要產生跟原本接近的 image (pixel-level), 還要能騙過 $D$. 這是為了補足 VAE 的缺點, 原始 VAE 的目標函式是 pixel-level 的 l2-norm, 這跟人類認為的真實不真實不一致, 因此 AVE 會產生模糊的 image. 用 GAN 的 loss 當成 regularization term 則補足了 VAE 這點. 因此 VAE-GAN 這是個互惠的結構, 很漂亮. 這個結構新的一篇 Adversarial Latent Autoencoders 粗略講也是 VAE-GAN 架構, 只是 reconstruction 不是再 image, 而是在 latent space. 論文結果十分驚艷, github. 結論本篇開頭說明的 framework 基本可以解釋了上述各種 GAN. 但由於本魯才疏學淺, 還有一大堆沒看的變種, EBGAN, BEGAN, CycleGAN, …etc. 只能說之後讀到的時候, 看看能否試著這麼解釋. GAN 實在太多了, 可以看看 GAN Zoo 有多少用 GAN 來命名的架構(似乎停止更新). Reference 李宏毅GAN f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization 令人拍案叫绝的Wasserstein GAN WGAN筆記 Wasserstein GAN and the Kantorovich-Rubinstein Duality Spectral Normalization for Generative Adversarial Networks GAN论文阅读笔记3：WGAN的各种变体 by 林小北 InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets Domain-Adversarial Training of Neural Networks Advesarial Discriminative Domain Adaptation Autoencoding beyond pixels using a learned similarity metric Adversarial Latent Autoencoders","tags":[{"name":"GAN","slug":"GAN","permalink":"http://yoursite.com/tags/GAN/"},{"name":"ADDA","slug":"ADDA","permalink":"http://yoursite.com/tags/ADDA/"},{"name":"fGAN","slug":"fGAN","permalink":"http://yoursite.com/tags/fGAN/"},{"name":"WGAN","slug":"WGAN","permalink":"http://yoursite.com/tags/WGAN/"},{"name":"infoGAN","slug":"infoGAN","permalink":"http://yoursite.com/tags/infoGAN/"},{"name":"VAE-GAN","slug":"VAE-GAN","permalink":"http://yoursite.com/tags/VAE-GAN/"}]},{"title":"Notes for (conditional/cross-)Entropy, Mutual-information, ...","date":"2020-05-02T03:37:12.000Z","path":"2020/05/02/Notes-for-conditional-cross-Entropy-Mutual-information/","text":"整理下 entropy 的一些東西, 不然久沒看老是忘記. Entropy of a r.v. $X$: $H(X)$ Conditional Entropy of $Y$ given $X$: $H(Y|X)$ Cross(Relative) Entropy of two pdf, $p$ and $q$: $D(p\\Vert q)$ Mutual Information of two r.v.s: $I(X;Y)$ 文章會明確定義每一項, 然後在推導它們之間關係的同時會解釋其物理意義. 最後其實就可以整理成類似集合關係的圖 (wiki) Entropy$$\\begin{align} H(X) = \\sum_{x\\in X}{p(x)\\log{\\frac{1}{p(x)}}} \\end{align}$$ 一個 r.v. $X$ 假設配給他的 pdf 為 $p$, 則可以算出相對應的 entropy $H(X)$, 所以我們其實可以看成是 $H(p)$.可以知道當 $p$ 是 uniform distribution 時 entropy 達到最大 (後面再證明). 同時該定義可以很容易看出 $H(X)\\geq 0$. 直觀解釋: 因為 uniform distribution 時最大 (最無法確定哪一個 outcome 最有可能), 另一個極端為 $X$ 為 constant (r.v. 只有一個 outcome) 所以我們可以理解為 entropy 為量測一個 r.v. $X$ 的不確定性 如果要對每一個 outcome 決定要用多少 bit 來代表, 我們希望平均花的 bits 數能最小, 直觀上機率愈大的 outcome 用愈小的 bits 來表達. 因此 outcome $x_i$ 我們用 $\\log{\\frac{1}{p(x_i)}}$ 這麼多 bits 表示, 則 entropy 代表了 encode 所有 outcome 所需要的平均 bits 數, 則這個數是最小的. (這可以從下面的 cross entropy 得到證明) 我們可以用 entropy 來表示該 r.v. 所包含的資訊量. 因為比 entropy 更多的資訊量都是 reduandant 的 (由上一點可看出) 為了方便我們會交叉使用 資訊量 或是 encode 的最小平均 bits 數 來表示 entropy 的物理意義. Cross(Relative) Entropy為量測兩個 pdfs, $p(x)$ and $q(x)$ 之間的 “divergence”, 也稱為 KL divergence. 注意 divergence 不需滿足三角不等式也不需滿足對稱性, 因此不是一個 “metric”. 但也足夠當成是某種”距離”來看待 (不是指數學上的 norm) $$\\begin{align} D(p\\Vert q) = \\sum_x{p(x)\\log{\\frac{p(x)}{q(x)}}} \\end{align}$$ 由 Jensen’s inequality 可以推得 $D(p\\Vert q) \\geq 0$, 另外 $D(p\\Vert q)=0$ iff $p=q$ $$\\begin{align} D(p\\Vert q) = - \\sum_x{p(x)\\log{\\frac{q(x)}{p(x)}}} \\\\ \\text{by Jensen&apos;s inequality: } \\geq \\log\\sum_x{p(x)\\frac{q(x)}{p(x)}} = 0 \\end{align}$$ 重寫一下 (2):$$\\begin{align} 0\\leq D(p\\Vert q) = \\sum_x{p(x)\\log{\\frac{1}{q(x)}}} - H(p) \\end{align}$$ 這說明了對於 outcome $x_i$ (true distribution 為 $p(x)$) 我們用 $\\log{\\frac{1}{\\color{red}{q}(x_i)}}$ 這麼多 bits 表示是浪費的. 所以證明了上面 Entropy 第二種解釋. 直觀解釋: $D(p\\Vert q)$ 表示使用錯誤的 distribution $q$ 來 encode 要多花的平均 bits 數量 Conditional Entropy$$\\begin{align} H(Y|X) = \\sum_{x\\in X}{p(x)H(Y|x=X)} \\\\ =\\sum_x{p(x)\\sum_y{p(y|x)\\log{\\frac{1}{p(y|x)}}}} \\end{align}$$ $H(Y|x=X)$ 解釋為 given $x$ encode $Y$ 的最小平均 bits 數 (就是 entropy 只是又重複說了一次), 但是每一個 $x$ 有自己的機率, 因此對 $x$ 再取一次平均. case 1:如果 $Y$ 完全可以由 $X$ 決定, 因此一旦給定 $x$, $Y$ 就是一個 constant, 所以 $H(Y|x=X)=0$. 再對 $X$ 算期望值仍然是 0. case 2:如果 $X$ and $Y$ independent. $$\\begin{align} (7)=\\sum_x{p(x)\\sum_y{p(y)\\log{\\frac{1}{p(y)}}}}=\\sum_x{p(x)H(Y)}=H(Y) \\end{align}$$ 得到結論 $H(Y|X)=H(Y)$ Case 1 and 2 說明了 $X$ and $Y$ 在完全依賴和完全無關時 $H(Y|X)$ 分別為 $0$ and $H(Y)$. 直覺上我們可以認為 $0\\leq H(Y|X) \\leq H(Y)$, 因為剛好是兩個極端情形. 但直覺是不夠的, 我們證明一下. 使用定義, 我們可以很容易推導出 $$\\begin{align} \\color{orange}{I(X;Y)\\equiv}H(Y)-H(Y|X)=\\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)p(y)}}} \\\\ =D(p(x,y)\\Vert p(x)p(y)) \\geq 0 \\end{align}$$ 我們這裡先偷跑出了 mutual information $I(X;Y)$ 的定義, 下面會再詳細講. 直觀解釋:$H(Y|X)$ 表示給了 $X$ 的資訊後, $Y$ 剩下的資訊量. 其中 $0\\leq H(Y|X) \\leq H(Y)$ Chain Rule for Entropy我們在把 (7) 做個運算: $$\\begin{align} H(Y|X)=(7)=-\\sum_x{p(x)\\sum_y{p(y|x)\\log{p(y|x)}}} \\\\ =-\\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)}}} \\\\ =-\\sum_{x,y}{p(x,y)\\log{p(x,y)}}+\\sum_{x,y}{p(x,y)\\log{p(x)}} \\\\ =H(X,Y) - H(X) \\end{align}$$ 直觀解釋:給定 $X$ 後 $Y$ 剩下的資訊量 ($H(Y|X)$) 就等於 $X,Y$ 整體的資訊量扣掉單獨 $X$ 部分的資訊量 Mutual Information(9) 已經先給出了定義, 我們這裡重複一次: $$\\begin{align} I(X;Y)\\equiv\\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)p(y)}}} =D(p(x,y)\\Vert p(x)p(y))\\\\ \\text{(用 entropy 定義得到) }=H(Y)-H(Y|X) \\\\ \\text{(用 entropy 定義得到) }=H(X)-H(X|Y) \\end{align}$$ 在用 Chain Rule for Entropy: $H(Y)=H(X,Y)-H(X|Y)$ 代進 (16) 得到 $$\\begin{align} I(X;Y)=H(X,Y)-H(Y|X)-H(X|Y) \\end{align}$$ 我知道這麼多式子一定眼花了….好在完全可以用集合的 Venn diagram 表示! 所有式子的直觀表達 Reference wiki mutual information Dr. Yao Xie, ECE587, Information Theory, Duke University 本篇內容只是 lecture 1 ~ 4 的範圍","tags":[{"name":"Entropy","slug":"Entropy","permalink":"http://yoursite.com/tags/Entropy/"},{"name":"Conditional Entropy","slug":"Conditional-Entropy","permalink":"http://yoursite.com/tags/Conditional-Entropy/"},{"name":"Cross Entropy","slug":"Cross-Entropy","permalink":"http://yoursite.com/tags/Cross-Entropy/"},{"name":"Mutual Information","slug":"Mutual-Information","permalink":"http://yoursite.com/tags/Mutual-Information/"}]},{"title":"Determinant of Covariance Matrix","date":"2019-07-15T13:41:13.000Z","path":"2019/07/15/Determinant-of-Covariance-Matrix/","text":"筆記 covariance matrix $R$ 的 determinant 意義以及他的 bound. 這是在讀 Time-delay estimation via linear interpolation and cross correlation 時的 appendix 證明. 覺得有用就筆記下來. 開門見山, $det(R)$ 可以想成 volumn (等於所有 eigenvalues 相乘), 然後 upper bound 就是所有對角項元素相乘. $$\\begin{align} det(R)=\\prod_i \\lambda_i \\leq \\prod_i r_{ii} \\end{align}$$ $\\lambda_i$ 是 i-th eingenvalue. 事實上只要 $R$ 是 square matrix, 則 $|det(R)|$ 等於用每個 row vector 做出來的 “平行六面體” 的體積 [ref] 以下筆記論文中證明 $det(R)$ 的 upper bound, 從這個 bound 我們也能看出物理意義. Upper bound of the determinant of positive definite matrix[Theorem]: 令 $H$ 為 $L$ by $L$ 正定矩陣, 則$$\\begin{align} det(H) \\leq \\prod_{i=1}^{L} h_{ii} \\end{align}$$ [Proof]:先將 $H$ 作如下拆解$$\\begin{align} H=\\left( \\begin{array}{cc} \\tilde{H} &amp; h \\\\ h^T &amp; h_{LL} \\end{array} \\right) \\end{align}$$ 其中 $\\tilde{H}$ 是 $L-1$ by $L-1$ 矩陣.從 Determinant of block matrices 我們知道:$$\\begin{align} det(H)=det(\\tilde{H})(h_{LL}-h^T \\tilde{H}^{-1}h) \\end{align}$$ 因為 $H$ 是正定, 所以 $\\tilde{H}$ 也是正定, 包含其 inverse Every principal submatrix of a positive definite matrix is positive definite. 正定的 $det&gt;0$, 以及正定的二次式 $&gt;0$, 帶入到 (4) 就不難發現$$\\begin{align} det(H)\\leq h_{LL}det(\\tilde{H}) \\end{align}$$ 重複此步驟就能推導出 (2) Determinant of covariance matrix我們知道 covariance matrix $R$ 是正定 (嚴格上為半正定, 如果沒有兩個完全 linear depedent 的維度的話, 就是正定), 因此符合 upper bound (2). 觀察當 coordinate 之間為 independent 時, 表示非對角項都是 $0$, 只剩下對角項 (每個維度的 variance). 這時 (2) 的不等式變成等式, 對角項相乘意義相當於算 volumn 可以看出兩點結論 covariance matrix 對角項的相乘總是會比 $det$ 大 coordinate 之間是 independent 則 covariance matrix 對角項的相乘會等於 $det$ Correlation matrix我們知道 correlation matrix 對角項都是 $1$, 且是正定根據以上的討論知道:$$\\begin{align} 0\\leq det(\\mbox{corr}(X))\\leq 1 \\end{align}$$ Take Home Messages令 $R$ 為 covariance matrix, $\\tilde{R}$ 是 correlation matrix $R$ 對角項的相乘總是會比 $det(R)$ 大 coordinate 之間是 independent 則 $R$ 對角項的相乘等於 $det(R)$ $det(\\tilde{R})$ 在 0 和 1 之間 (包含) 令 $A$ 為 square matrix, 則 $|det(A)|$ 等於以每個 row vector 做出來的 “平行六面體” 的體積 [ref] Reference Time-delay estimation via linear interpolation and cross correlation Determinants and Volumes","tags":[{"name":"Covariance matrix","slug":"Covariance-matrix","permalink":"http://yoursite.com/tags/Covariance-matrix/"},{"name":"Correlation matrix","slug":"Correlation-matrix","permalink":"http://yoursite.com/tags/Correlation-matrix/"},{"name":"determinant","slug":"determinant","permalink":"http://yoursite.com/tags/determinant/"}]},{"title":"TF Notes (6), Candidate Sampling, Sampled Softmax Loss","date":"2019-07-02T12:34:12.000Z","path":"2019/07/02/TF-Notes-Candidate-Sampling/","text":"NN 做分類最後一層通常使用 softmax loss, 但如果類別數量很大會導致計算 softmax 的 cost 太高, 這樣會讓訓練變得很慢. 假如總共的 class 數量是 10000 個, candidate sampling 的想法就是對於一個 input $x$ 採樣出一個 subset (當然需要包含正確的 label), 譬如只用 50 個 classes, 扣掉正確的那個 class, 剩下的 49 個 classes 從 9999 個採樣出來. 然後計算 softmax 只在那 50 個計算. 那麼問題來了, 這樣的採樣方式最終訓練出來的 logits 會是對的嗎? 它與未採樣前 (full set) 的 logtis 有何對應關係? 採用 candidate sampling 方式的 softmax loss 在 tensorflow 中已經直接有 op 了, 參考 tf.nn.sampled_softmax_loss. 文檔裡最終推導得到如下的一個式子: $$\\begin{align} \\log(P(y|x_i,C_i))=\\log(P(y|x_i))-\\log(Q(y|x_i))+K&apos;(x_i,C_i) \\end{align}$$ 推導過程自行看文檔就可以, 重要的是了解式子的物理意義.$C_i$ 是對 input $x_i$ 採樣出的 subset, 包含了 一個正確的類別標籤 和 其他採樣出的類別 $S_i$. $Q(y|x_i)$ 是基於 input $x_i$, label $y$ 被選中成為 $S_i$ 的機率. $K’$ 是跟 $y$ 無關的, 所以對於式子來說是 constant. 注意到式子的變數是 $y$ 代表了是 softmax 的哪一個 output node. 式 (1) 的解釋為: “在 candidate set $C_i$ 下的 logits 結果” 等於 “在 full set 下的 logtis 結果減去 $\\log Q(y|x_i)$”, $K’$ 會直接被 $\\log P(y|x_i)$ 吸收, 因為 logits 加上 constant 對於 softmax 來說會分子分母消掉, 所以不影響. 以下我們順便複習一下, 為什麼 logits 可以寫成 “$\\mbox{const}+\\log P(y|x)$” 這種形式. (包含複習 Entropy, cross-entropy, softmax loss) Entropy 定義$$\\begin{align} \\sum_i{q(x_i)\\log{\\frac{1}{q(x_i)}}} \\end{align}$$ 對於 input $x_i$, 其機率為 $q(x_i)$, 若我們使用 $\\log{\\frac{1}{q(x_i)}}$ 這麼多 bits 的數量來 encode 它的話, 則上面的 entropy 代表了 encode 所有 input 所需要的平均 bits 數, 而這個數是最小的. 用錯誤的 encoding 方式我們假設用 $\\log{\\frac{1}{p(x_i)}}$ 這麼多 bits 的數量來 encode 的話, 則平均 encode bits 數為: $$\\begin{align} \\sum_i{q(x_i)\\log{\\frac{1}{p(x_i)}}} \\end{align}$$ 這個數量一定會比 entropy 來的大, 而大出來的值就是我們使用錯誤的 encoding 造成的代價 (cross-entropoy). Cross-entropy如上面所說, 錯誤的 encoding 方式造成的代價如下: $$\\begin{align} \\mbox{Xent}(p,q)\\triangleq\\sum_i{q(x_i)\\log{\\frac{1}{p(x_i)}}} - \\sum_i{q(x_i)\\log{\\frac{1}{q(x_i)}}} \\\\ =\\sum_i{q(x_i)\\log{\\frac{q(x_i)}{p(x_i)}}} \\\\ \\end{align}$$ Sparse softmax loss最常見的情形為當只有 $q(x_j)=1$ 而其他 $x\\neq x_j$ 時 $q(x)=0$ 的話 ($q$ 變成 one-hot), 上面的 corss-entropy 變成: $$\\begin{align} \\mbox{SparseSoftmaxLoss}\\triangleq\\mbox{Xent}(p,q\\mbox{ is one-hot})=-\\log p(x_j) \\\\ =-\\log\\frac{e^{z_j}}{\\sum_i{e^{z_i}}}=-\\log e^{z_j} + \\log\\sum_i{e^{z_i}} \\\\ =-z_j + \\log\\sum_i{e^{z_i}} \\end{align}$$ 其中 $z_i$ 表示 i-th logtis, 參考 tf.nn.sparse_softmax_cross_entropy_with_logits Logits 的解釋j-th logtis $z_j$ 可解釋為 “const + class $j$ 的 log probability”. $$\\begin{align} z_j = \\mbox{cosnt} + \\log p(j) \\end{align}$$ 為什麼呢? 這是因為 logtis 經過 softmax 後會變成機率, 我們假設經過 softmax 後 node $j$ 的機率為 $p’(j)$, 計算一下這個值: $$\\begin{align} p&apos;(j)=\\frac{e^{z_j}}{\\sum_i e^{z_i}} \\\\ =\\frac{e^{\\log p(j)}e^{\\mbox{const}}}{e^{\\mbox{const}}\\sum_i e^{\\log p(i)}} \\\\ =\\frac{p(j)}{\\sum_i p(i)} \\\\ =p(j) \\end{align}$$ 這時候我們再回去對照開始的式 (1), 就能清楚的解釋 candidate sampling 的 logtis 和 full set 的 logits 之間的關係了. Sampled softmax loss由式 (1) 我們已經知道 candidate sampling 的 logtis 和 full set 的 logits 之間的關係. 因此在訓練的時候, 正常 forward propagation 到 logits 時, 這時候的 logits 是 full set 的. 但由於我們計算 softmax 只會在 candidate set 上. 因此要把 full set logits 減去 $\\log Q(y|x_i)$, 減完後才會是正確的 candiadtes logits. 對於 inference 部分, 則完全照舊, 因為原本 forward propagation 的結果就是 full set logits 了. 這也是 tf 官網範例這麼寫的原因: 123456789101112131415if mode == \"train\": loss = tf.nn.sampled_softmax_loss( weights=weights, biases=biases, labels=labels, inputs=inputs, ..., partition_strategy=\"div\")elif mode == \"eval\": logits = tf.matmul(inputs, tf.transpose(weights)) logits = tf.nn.bias_add(logits, biases) labels_one_hot = tf.one_hot(labels, n_classes) loss = tf.nn.softmax_cross_entropy_with_logits( labels=labels_one_hot, logits=logits) Reference tf.nn.sampled_softmax_loss Candidate Sampling tf.nn.sparse_softmax_cross_entropy_with_logits","tags":[{"name":"Entropy","slug":"Entropy","permalink":"http://yoursite.com/tags/Entropy/"},{"name":"Candidate sampling","slug":"Candidate-sampling","permalink":"http://yoursite.com/tags/Candidate-sampling/"},{"name":"Sampled softmax loss","slug":"Sampled-softmax-loss","permalink":"http://yoursite.com/tags/Sampled-softmax-loss/"}]},{"title":"SphereFace Paper Study and Implementation Notes","date":"2019-06-18T13:13:46.000Z","path":"2019/06/18/SphereFace-paper-study-and-implementation-notes/","text":"SphereFace: Deep Hypersphere Embedding for Face Recognition 使得訓練出來的 embeddings 可以很好的使用 cosine similarity 做 verification/identification. 可以先網路上搜尋一下其他人的筆記和討論, 當然直接看論文最好.一般來說我們對訓練集的每個人用 classification 的方式訓練出 embeddings, 然後在測試的時候可以對比兩個人的 embeddings 來判斷是否為同一個人. 使用 verification 當例子, 實用上測試的人不會出現在訓練集中, 此情形稱為 openset 設定. 注意到 embedding 是使用 classification 方式訓練出來, 也就是說, 如果訓練集有 1000 個人, 最後一層的 softmax 就有 1000 個 nodes. 然後 embedding 一般取 softmax 前一層 (前兩層也可).測試時常見的做法就是計算兩個 embeddings 的 cosine similarity, 直觀上相同的人他們的 embedding 會接近, 因此夾角小 (cosine 大), 而不同的人夾角大 (cosine 小).但問題來了, 當初訓練 embedding 時並沒有針對 classification 用夾角來分類, 也就不能保證 softmax loss 對於使用 cosine similarity 是最有效的. Modified softmax loss (M-softmax loss) 和 Angular softmax loss (A-softmax loss) 就能針對這種情形 (測試時使用 cosine similarity) 計算 loss. A-softmax loss 比 M-softmax loss 條件更嚴苛, 除了希望針對 angular 做分類外, 還希望同一類的夾角能聚再一起, 不同類的夾角能盡量分開. 下面就說明一下 softmax loss, M-softmax loss and A-softmax loss, 然後以 tensorflow 的實作來說明 Softmax Loss其實沒什麼好說明的, 公式如下 Decision boundary 以兩類來看如下: $$\\begin{align} (W_1 - W_2)x+b_1 - b_2=0 \\end{align}$$ M-Softmax Loss如果我們將 $W_j$ 的 norm 限制為 1, 且去掉 biases, $b_j=0$, 則原來的 softmax loss 變成如下: Decision boundary 以兩類來看如下: $$\\begin{align} \\parallel x \\parallel (\\cos \\theta_1 - \\cos \\theta_2)=0 \\Rightarrow \\cos \\theta_1 = \\cos \\theta_2 \\end{align}$$ 我們可以發現 decision boundary 完全由夾角來決定了! 論文使用 toy example 來說明 M-softmax loss 造成的現象: A-Softmax Loss以兩類來說明, M-softmax loss 將 $x$ 分類成 class 1 的條件為 $\\cos \\theta_1 &gt; \\cos \\theta_2$, 也就是 $\\theta_1 &lt; \\theta_2$. A-softmax loss 則讓這個條件更嚴格, 它希望 $m$ 倍的 $\\theta_1$ 都還小於 $\\theta_2$, 因此條件為 $\\cos m\\theta_1 &gt; \\cos \\theta_2$. 論文中以幾何的方式說明很清楚: 因此 A-softmax loss 如下: 論文使用 toy example 來說明 A-softmax loss 造成的現象: 可以看到相比於 M-softmax loss, A-softmax loss 會使得 margin 增大 這種 within class 靠近, between class 拉遠就如同 LDA 的概念. A-softmax 也能造成這種效果且是在 angular 的 measure 下. 而常見的情形都是針對 euclidean distance, 例如使用 triplet loss (推薦這篇 blog 說明具體且 tensorflow 實現非常厲害). 原則上我們希望與 class $i$ 的夾角 $\\theta_i$ 愈小, 所算出來的 logits 也就是 $\\cos\\theta_i$ 要愈大, 所以放大 $m$ 倍的夾角所算出來的 logits, $\\cos m\\theta_i$ 必須要變小.但由於 $\\cos$ 是 periodic function, 一旦 $m\\theta_i$ 超過 $2\\pi$ 就反而可能使得 logits 變大, 這就適得其反了. 精確來說 $\\cos m\\theta_i &lt; \\cos\\theta_i$ 只會在 $\\theta_i$ 屬於 $[0,\\pi/m]$ 區間範圍內成立. 因此我們必須對 A-softmax loss 作如下改動: 其中 $$\\begin{align} \\psi(\\theta)=(-1)^k \\cos(m\\theta)-2k\\\\ \\mbox{where }\\theta\\in[\\frac{k\\pi}{m},\\frac{(k+1)\\pi}{m}]\\mbox{ and }k\\in[0,m-1] \\end{align}$$ 我們將 $\\psi$ 畫出來: 兩個觀察: 首先 $\\psi$ 的確會隨著角度變大而變小, 這符合我們要的 logits 的行為. 再來要計算出正確的 $\\psi(\\theta)$ 必須要先知道 $k$, 也就是需要知道 $\\theta$ 落在哪個區間才行. 第二點可能比較棘手, 我們思考一下怎麼在 tensorflow 的 graph 中實現 …. hmm…. 好像有點麻煩 Tensorflow Implementation A-softmax Loss其實網路上就很多 tensorflow 的實現了, 不看還好, 一看才發現 A-softmax loss 的 $\\psi$ 實現步驟如下: 這什麼操作?! 怎麼跟原來理解的 (3) and (4) 長相差這麼多! 網路上幾乎大家都直接拿來用, 也沒什麼說明. 不過我們仔細分析一下, 還是能發現端倪.首先注意到這樣的實現是基於 $m=4$ 做的. (論文的實驗最後在這個設定有不錯的效果) 因此將 $m=4$ 套入 (3)(4) 得: $$\\begin{align} \\psi(\\theta)=(-1)^k \\cos(\\color{red}{4}\\theta)-2k\\\\ \\mbox{where }\\theta\\in[\\frac{k\\pi}{\\color{red}{4}},\\frac{(k+1)\\pi}{\\color{red}{4}}]\\mbox{ and }k\\in[0,\\color{red}{3}] \\end{align}$$ 接著我們作如下分析: 發現 $s3=(-1)^k$ 和 $s4=-2k$, 因此 $$\\begin{align} \\psi(\\theta)=\\color{green}{(-1)^k} \\cos(4\\theta)\\color{blue}{-2k} = \\color{green}{s3}[1-8\\cos^2\\theta +8\\cos^4\\theta]\\color{blue}{+s4} \\end{align}$$ 而 $\\cos\\theta$ 則因為 weights $W$ 的 norm 限制為 1, 所以只需要 $Wx$ 再除以 $x$ 的 norm 即可. 到這裡最麻煩的實作問題分析完畢, 依樣畫葫蘆也可以做出 $m=2$, $m=3$. SummaryTake home messages: M-softmax loss 算出來的 embeddings 在 test 階段可以直接用 cosine measure A-softmax loss 更進一步使得各類別之間的角度拉更開, 達到 large margin 效果 A-softmax loss 實作上不好訓練, 可以使用論文中提到的訓練方法, 一開始偏向原來的 softmax loss, 然後漸漸偏向 A-softmax loss M-softmax loss 簡單實用, 經過 weight norm = 1 的條件, 論文中說明能去掉 prior 分布 Reference SphereFace: Deep Hypersphere Embedding for Face Recognition Blog: Triplet loss","tags":[{"name":"SphereFace","slug":"SphereFace","permalink":"http://yoursite.com/tags/SphereFace/"},{"name":"Angular softmax loss","slug":"Angular-softmax-loss","permalink":"http://yoursite.com/tags/Angular-softmax-loss/"},{"name":"Modified softmax loss","slug":"Modified-softmax-loss","permalink":"http://yoursite.com/tags/Modified-softmax-loss/"}]},{"title":"Adaptive Filters 簡介 (2) Fast Convolution and Frequency Domain","date":"2019-06-08T15:35:35.000Z","path":"2019/06/08/Adaptive-Filters-Notes-2/","text":"上一篇說明了 time domain 的 adaptive filters, 由於是 sample-by-sample 處理, 因此太慢了不可用, 真正可用的都是基於 frequency domain. 不過在深入之前, 一定要先了解 convolution 在 input 為 block-by-block 的情況下如何加速. 本文內容主要參考 Partitioned convolution algorithms for real-time auralization by Frank Wefers (書的介紹十分詳盡). Convolution 分類如下: 我們就針對最常使用的情形介紹: Input (UP) and Filter (0). 這是因為實際應用 input 是 infinite length, 所以需要 block-by-block 給定, 而 filter 通常都是 finite length, 可以選擇不 partition, 或 uniformly partitioned 以便得到更低的延遲效果. 針對 block-based input 的 convolution, 我們有兩種架構: OverLap-and-Add (OLA) OverLap-and-Save (OLS) OLAOLA 相對來說很好理解的. 每一個新來的 data block $x_i$ (長度為 $M$), 都與 filter $h$ (長度為 $N$) 做 linear convolution, 產生的 output $y_i$ (長度為 $M+N-1$) 開頭的 $N-1$ 個結果與前一個output block 重疊的部分疊加 (“add”), 所以稱 overlap-and-ADD. 示意圖如下: OLSOLS 則從 output 角度來看. 根據現在的 output 來決定需要用到那些 input 做 linear convolution. 舉例 input block $x_i$ 長度為 $B=3$, filter $h$ 長度為 $N=4$, 則 output block $y_i$ 的結果可以從下圖來看出來: 注意到, 我們一開始先將 $h$ 右邊補上 $B-1=2$ 個 $0$, 而 input block $x_i$ 左邊補上 $N-1=3$ 個舊的 input data. 目的是把 $x_i$ 和 $h$ 都湊成 $B+N-1$ 這麼長.則我們可以發現, 針對增長後的 input and filter 做 lineaer convolution, 雖然會得到長度為 $2*(B+N-1)-1$ 的 output, 但這其中有 $B$ 個結果是我們要的! 因此我們只需要 “save” 需要的這 $B$ 個 output, 其他都丟較即可. 所以稱 overlap-and-SAVE. 如何有效率的做 linear convolution?不管是 OLA 或 OLS 都需要對兩個固定長度 (通常使用 padding $0$ 成等長) 的 signal 做 linear convolution. 怎麼有效率的做 linear convolution 就變得十分重要.我們都知道頻域的相乘相當於時域的 circular convolution. 因此如果能用 ciruclar convolution 來做出 linear convolution 的話, 我們就能轉到頻域上再相乘就可以了.Circular convolution 的定義如下[1], 其實概念也很容易: 我們只需要適當地 padding zeros, 就可以使得 padding 後的 signals 做 circular convolution 會等於原來的 singals 做 linear convolution. 如下圖[1] 因此使用 FFT-domain 的 circular convolution 來實現 fast linear convolution 流程如下 Fast Conv with OLA在 OLA 架構中使用 FFT-domain 的 circular convolution 如下: Padding zeros 不管在前還是在後都可以, 只要滿足 $K=\\geq M+B-1$ 避免 aliasing 即可. Fast Conv with OLS在 OLS 架構中使用 FFT-domain 的 circular convolution 如下: Input signal 不是 padding zeros, 而是在左邊 padding 之前的 input 訊號 (參考本篇上面的 OLS 段落), 用這樣的 padding 方式來看 circular convolution 的話, 每一次我們就 “save” output 的最後 $B$ 個結果即可. 在實作上通常會將 $B=N$, 並且設定 $K=2B=2N$, 這樣我們每一次只需要保留前一次的 input block, 並且 padding 給新來的 input block. Frequncy Domain Adaptive FilterFrequency Domain Adaptive Filter (FDAF) 請參考 [2], 整理的非常好, 所以這裡就不多描述, 完全可以照著實作出來! 我們會發現其實它採用的是我們上面說過的 Fast Convolution with OLS 架構, 只是 filter 必須 adaptive 更新. 以下是 python implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# In the frequency domain methods, notations are defined as:# x: reference signal, [-1, 1]# d: desired signal, [-1, 1]# step_size: step size# alpha: the alpha filter for tracking the energy for each bin# w: the retruned filter# e: the error signal, of size (itr_num,)# ========== FDAF (Frequency Domain Adaptive Filters)def FDAF(x,d,step_size,N=512,alpha=0.9): iter_num = len(d)//N-1 assert(iter_num&gt;0) # Init W = np.zeros(2*N,dtype=complex) pow_lambda = np.ones(2*N)*np.finfo(np.float32).eps rtn_e = np.zeros((iter_num-1)*N) # Main Iteration for itridx in range(1,iter_num): x_2blocks = x[(itridx-1)*N:(itridx+1)*N] # (2N) d_block = d[itridx*N:(itridx+1)*N] # (N) X = fft(x_2blocks) # (2N) Y = np.einsum('i,i-&gt;i',X,W) y = ifft(Y) # (2N) y = y[N:] # (N), discard first half block # print (y) # e = np.real(d_block - y) # (N) e = d_block - y # (N) # print(len(rtn_e)) rtn_e[(itridx-1)*N:itridx*N] = np.real(e) e = np.concatenate([np.zeros([N]),e]) # (2N) E = fft(e) # (2N) pow_lambda = alpha*pow_lambda + (1-alpha)*(np.abs(X)**2) # scale error signal, just like NLMS E = E/pow_lambda # Set the upper bound of E, to prevent divergence m_errThreshold = 0.2 Enorm = np.abs(E) # (2N) # print(E) for eidx in range(2*N): if Enorm[eidx]&gt;m_errThreshold: E[eidx] = m_errThreshold*E[eidx]/(Enorm[eidx]+1e-10) # Constraint Part gradient = np.einsum('i,i-&gt;i',X.conj(),E) # (2N) gradient = ifft(gradient) gradient[N:] = 0 gradient = fft(gradient) # (2N) # Update Part W = W + step_size*gradient return rtn_e Summary我們介紹了針對 input 是 block-by-block 給定時, 計算 linear convolution 的兩種架構: OLA, OLS. 而如何加速 linear convolution 我們則介紹了使用 circular convolution 來等價地完成 linear convolution. Circular convolution 可以利用頻域相乘來加快速度 (得益於 FFT 的效率). 除了對 input 切 block 之外, 我們也還可以對 filter $h$ 切 block, 這樣的好處是計算量可以在更低, 且 latency 也會降低. 這部分請參考書的 Ch5, 附上一張書本裡的架構圖: 這種方式其實很重要, 原因是 webrtc 中的 AEC 採用的是 Partitioned Block Frequency Domain Adaptive Filter (PBFDAF) [3], 就是 filter 也是 uniformly partitioned. 最後我們利用 OLA 和 fast convolution, 列出來 frequency domain AF 的架構圖. 同時如果想要進一步降低 latency 則需使用 PBFDAF[3] (filter 也 partition). Reference Partitioned convolution algorithms for real-time auralization by Frank Wefers Block Adaptive Filters and Frequency Domain Adaptive Filters by Prof. Ioan Tabus On the implementation of a partitioned block frequency domain adaptive filter (PBFDAF) for long acoustic echo cancellation","tags":[{"name":"Adaptive Filters","slug":"Adaptive-Filters","permalink":"http://yoursite.com/tags/Adaptive-Filters/"},{"name":"OLA","slug":"OLA","permalink":"http://yoursite.com/tags/OLA/"},{"name":"OLS","slug":"OLS","permalink":"http://yoursite.com/tags/OLS/"},{"name":"circular convolution","slug":"circular-convolution","permalink":"http://yoursite.com/tags/circular-convolution/"},{"name":"linear convolution","slug":"linear-convolution","permalink":"http://yoursite.com/tags/linear-convolution/"}]},{"title":"Adaptive Filters 簡介 (1) Time Domain","date":"2019-05-14T14:03:03.000Z","path":"2019/05/14/Adaptive-Filters-Notes/","text":"粗略筆記 time domain adaptive filters, frequency domain adaptive filters 會在下一篇筆記. 應用以 Acoustic Echo Cancellation (AEC) 來說明. Motivation直接使用 wiki. AEC 要解決的是如下的情形 各個訊號關聯如下: $$\\begin{align} y(n)=h(n)\\ast x(n) \\\\ d(n)=y(n)+v(n) \\\\ \\hat{y}(n)=\\hat{h}(n)\\ast x(n)\\\\ e(n)=d(n)-\\hat{y}(n) \\end{align}$$ 目的是找到 $\\hat{h}$ 滿足下式: $$\\begin{align} \\hat{y}(n)\\approx y(n)\\Rightarrow e(n)\\approx v(n) \\end{align}$$ 對於第 $n$ 個 sample 點來說, 我們通常使用過去(含自己) $p$ 個 samples. 下面小寫粗體表示 vector, 大寫粗體表示 matrix. Optimal Solution也就是 Wiener solution. 但在真實世界中, 不管 reference signal ($x(n)$) or desired signal ($d(n)$) 都是 non-stationary 的. 最直接且暴力的想法就是每隔一段時間重新算一次 Wiener solution. 不過想當然爾這是行不通的. 因此就必須採用 Stochastic update 方式. Stochastic Update 上面紅色的式子就是典型的 LMS algorithm. 另外我們知道 optimization 還可以使用 second-moment, 也就是使用二階導函數 (Hessian Matrix). 這就是 Newton’s method: 針對 $\\mathbf{R}_{xx} \\mbox{ , } \\mathbf{R}_{xd}$ 使用不同的 approximation 方式就會得到不同演算法, 例如: 上面紅色的式子就是典型的 NLMS algorithm. 用這樣的方式還可推出 e-NLMS, leaky-LMS, RLS 等等… 實作上 NLMS 在 reference signal $x(n)$ 很小的時候, 由於公式上分母會除以 $x^Hx$, 而分子只有一次方 $x$, 因此除下來會導致 gradient 容易變大, 所以發散. 這是 NLMS 實作上要考慮的情形. Misadjustment我們知道最佳解為 Wiener solution, 但由於我們採用 stochastic gradient 方式, 也就是說 update 的 gradient 本身存在誤差, 這些 gradient 的 variance 就直接影響了最終收斂的效果跟 Wiener solution 的收斂結果之間的差距. 此差距我們稱 misadjustment 或稱 Excess Meam Square Error 直接擷取 Ali Sayed, Adaptive Filters p230 的定義: 為什麼要說這個呢? 是因為實作上有兩個因素會直接影響最終收斂效果的好壞, 分別是 tap length 和 step size. 從理論分析和實作經驗來說, tap length 太小會無法有效模擬 RIR, 而太大會導致 EMSE 提高 (收斂效果反而變差), 因此選取的 tap length 必須要根據 sampling rate 和要消除的 echo path 來計算一下. LMS or NLMS 的 EMSE 可猜考 Ali Sayed, Adaptive Filters p249 and p253. (條件已簡化在 ref and desired signals 為 stationary 情況) 另外 step size 較小會有較好的收斂效果, 但是收斂速度會慢且 tracking 能力較差. 一個有效的方式為使用 Practical Variable Step Size (PVSS) 方法, 具體可參考 待補 好了, time domain 到這就差不多了, 缺點也很明顯, 慢!, 因為是 sample-by-sample 處理. 接著稍微梳理一下 frequency domain 方法. Reference wiki Least mean squares filter Ali Sayed, Adaptive Filters","tags":[{"name":"Adaptive Filters","slug":"Adaptive-Filters","permalink":"http://yoursite.com/tags/Adaptive-Filters/"},{"name":"AEC","slug":"AEC","permalink":"http://yoursite.com/tags/AEC/"},{"name":"LMS","slug":"LMS","permalink":"http://yoursite.com/tags/LMS/"},{"name":"NLMS","slug":"NLMS","permalink":"http://yoursite.com/tags/NLMS/"}]},{"title":"Far Field Notes (4) How Spatial Feature Clusters","date":"2019-04-12T13:36:17.000Z","path":"2019/04/12/Far-Field-Notes-4-How-Spatial-Feature-Clusters/","text":"這是 far field 筆記系列第四篇, 寫這篇是因為做 CGMM-MVDR 時, 很好奇為何 spatial features 聚類的結果可以對應不同方向的聲源. 因此記錄下自己的一點想法. 假設我們有 $M$ 個麥克風, 則在 stft (short-time fourier transform) 上來說, $\\mathbf{f}_{\\omega,t}$ 表示一個頻率 $\\omega$, 時間 $t$ 的 $M$ 維向量. 對於某一個 $\\theta$ 方向的 narrowband 訊號, ideally 我們可以這麼表示 $$\\begin{align} \\mathbf{f}_{\\omega,t}^{\\theta}=f(\\omega)\\mathbf{\\upsilon}(\\theta)=f(\\omega) \\left[ \\begin{array}{clr} e^{-j\\omega\\tau_0} \\\\ e^{-j\\omega\\tau_1} \\\\ \\vdots \\\\ e^{-j\\omega\\tau_{M-1}} \\end{array} \\right] \\end{align}$$ $\\tau_i$ 表示由 $\\theta$ 產生的第 $i$ 個 mic 的 time delay. 因此 spatial feature 每個維度之間的 phase offset 關係是固定的, 由 $\\mathbf{\\upsilon}(\\theta)$ 決定. 所有如果有兩個方向 $\\theta_1$ and $\\theta_2$ 的聲源, phase offset 關係各自是 $\\mathbf{\\upsilon}(\\theta_1)$ 和 $\\mathbf{\\upsilon}(\\theta_2)$. 問題是要用什麼樣的 cluster 能對相同 phase offset 關係的 complex vector 聚類在一起, 而對不同 phase offset 關係能分開呢? 關鍵的答案就是 Circularly Symmetric Gaussian Distribution Circularly Symmetric Gaussian Distribution直接引用 slides 裡的一段定義 A complex Gaussian random vector $Z$ is circularly symmetric if $e^{j\\phi}Z$ has the same distribution as $Z$ for all real $\\phi$. 意思就是如果我們乘上固定的 phase offset $\\phi$ (聲源有 time delay), 這相當於不改變維度之間的 phase offset 關係 (不改變聲源方向 $\\theta$), 這樣的話它們會是同一個機率分佈, 而這種特性完全符合我們的需求! 我們直接擷取 slide 中的 Circularly Symmetric Gaussian Distribution 的定義: 詳細請見 [1] 的 slides. Reference Circularly Symmetric Gaussian Random Vectors","tags":[{"name":"CGMM","slug":"CGMM","permalink":"http://yoursite.com/tags/CGMM/"},{"name":"Spatial","slug":"Spatial","permalink":"http://yoursite.com/tags/Spatial/"}]},{"title":"懷舊篇, 單通道降噪, MMSE-STSA, MMSE-LSA 方法","date":"2019-03-20T13:04:18.000Z","path":"2019/03/20/MMSE-STSA-and-LSA/","text":"記錄一下單通道降噪的一個經典方法, MMSE-STSA, MMSE-LSA, 已經是 1984 左右的文章了. 單通道降噪 OMLSA 也從這衍生出來的. 我們先從 MMSE-STSA 說起, 全名是 minimum mean-square error short time spectral amplitude.$y(t)=x(t)+d(t),0\\leq t\\leq T$$x$, $d$, $y$ 分別是 speech, noise, 和收到的 noisy signal, 其中 $x$, $d$ 相互獨立. 相對應的第 $k$ 個 frequency bin 如下:$$X_k=A_k\\exp(j\\alpha_k) \\\\ D_k \\\\ Y_k=R_k\\exp(j\\theta_k)$$ MMSE-STSA $^{[1]}$目標函式為$$\\begin{align} \\arg\\min_{\\hat{A}_k}{\\mathbb{E}\\left[\\left(A_k-\\hat{A}_k\\right)^2\\vert y(t),0\\leq t\\leq T\\right]} \\end{align}$$ 最佳解為$$\\begin{align} \\hat{A}_k=\\mathbb{E}\\left[A_k\\vert y(t),0\\leq t \\leq T\\right] \\end{align}$$ 但關鍵是我們不知道 clean speech 的 amplitude $A_k$, 那該怎麼估呢? 首先我們對每個 frequency bin 的分布假設為 Gaussian distribution (complex). 引用原文 “Since the Fourier coefficient is, after all, a weighted sum (or integral) of random variables resulting from the random process samples”, 在一個短時的 frame 中大致上是 stationary, 因此可以看作是一個 WSS 的 ramdom process, 再加上 cental limit theorem, 就當作高斯分布吧. 套用 Guassian distribution 假設, 做如下推導$$\\begin{align} \\hat{A}_k=\\mathbb{E}\\left[A_k\\vert y(t),0\\leq t \\leq T\\right]=\\mathbb{E}\\left[A_k\\vert Y_0,Y_1,...\\right] \\\\ =\\mathbb{E}\\left[A_k\\vert Y_k\\right] \\\\ =\\int_0^{\\infty}\\int_0^{2\\pi}a_k p(a_k,\\alpha_k\\vert Y_k)d\\alpha_k d a_k = \\int_0^{\\infty}\\int_0^{2\\pi}a_k \\frac{p(a_k,\\alpha_k,Y_k)}{p(Y_k)}d\\alpha_k d a_k \\\\ =\\frac{ \\int_0^{\\infty}\\int_0^{2\\pi}a_k p(Y_k\\vert a_k,\\alpha_k) p(a_k,\\alpha_k) d\\alpha_k d a_k }{ \\int_0^{\\infty}\\int_0^{2\\pi} p(Y_k\\vert a_k,\\alpha_k) p(a_k,\\alpha_k) d\\alpha_k d a_k } \\end{align}$$ 其中 (3) 到 (4) 我們假設每個 frequency bin 是獨立的由於我們假設每個 frequency bin 都是 complex Gaussian distribution, 因此 (6) 的機率分佈如下定義:$$\\begin{align} p(Y_k\\vert a_k,\\alpha_k)=\\frac{1}{\\pi\\lambda_d (k)}\\exp\\left[ -\\frac{1}{\\lambda_d (k)}\\vert Y_k - a_k e^{j\\alpha_k} \\vert^2 \\right] \\\\ p(a_k,\\alpha_k)=\\frac{1}{\\pi\\lambda_x (k)}\\exp\\left[-\\frac{a_k^2}{\\lambda_x (k)}\\right] \\end{align}$$ 注意到 (7) 能這麼寫是因為我們知道 $x$ and $d$ 互相獨立, 因此在給定 $x$ 的情形下, 只是改變 mean 的位置, 其 variance 仍由 $d$ 來決定. 另外:$$\\begin{align} \\lambda_x (k)=\\mathbb{E}\\left[\\vert X_k \\vert ^2\\right]=A_k^2 \\\\ \\lambda_d (k)=\\mathbb{E}\\left[\\vert D_k \\vert ^2\\right] \\end{align}$$ 表示第 $k$ 個 bin 的 speech and noise 的 variance將 (7) and (8) 帶入 (6) 並感謝偉大的作者推導得到:$$\\begin{align} \\hat{A}_k=\\Gamma(1.5)\\frac{\\sqrt{\\upsilon_k}}{\\gamma_k}M(-0.5;1;-\\upsilon_k)R_k \\\\ \\hat{A}_k=\\Gamma(1.5)\\frac{\\sqrt{\\upsilon_k}}{\\gamma_k}\\exp\\left(-\\frac{\\upsilon_k}{2}\\right)\\left[(1+\\upsilon_k)I_0(\\frac{\\upsilon_k}{2})+\\upsilon_k I_1(\\frac{\\upsilon_k}{2})\\right]R_k \\end{align}$$ 其中 $\\Gamma$ 表示 gamma function, $\\Gamma(1.5)=\\sqrt{\\pi}/2$; $M(a;c;x)$ 是 confluent hypergeometric function (這是外星符號吧), $I_0$ and $I_1$ 是 modified Bessel funciton of zero and first order. 總之就是能帶入計算的東西, 最重要, 也是需要我們估計的變數如下:$$\\begin{align} \\upsilon_k\\triangleq \\frac{\\xi_k}{1+\\xi_k}\\gamma_k \\\\ \\color{orange}{ \\xi_k\\triangleq\\frac{\\lambda_x (k)}{\\lambda_d (k)} } \\\\ \\color{orange}{ \\gamma_k\\triangleq\\frac{R_k^2}{\\lambda_d (k)} } \\\\ \\end{align}$$ $\\xi_k$ 和 $\\gamma_k$ 分別稱為 prior SNR 和 posterior SNR. 總之如能估出 $\\xi_k$ 和 $\\gamma_k$, 我們就能計算出 gain 值, 之後的方法如 LSA, OMLSA 也都如此. 文章後面會使用 MCRA 來估算這兩個 SNR. 現在就算傳統方法一般也很少使用 MMSE-STSA, 至少會使用 LSA 取代. LSA 有近似的計算方式, 因此我們也不糾結 (12) 到底怎麼算出來. MMSE-LSA $^{[2]}$大致想法跟流程跟上面一樣(只是我算不出來), 只是目標函數針對 log 值來計算$$\\begin{align} \\arg\\min_{\\hat{A}_k}{\\mathbb{E}\\left[\\left(\\log A_k-\\log\\hat{A}_k\\right)^2\\vert y(t),0\\leq t\\leq T\\right]} \\end{align}$$ 同樣經過不是人類的計算後得到:$$\\begin{align} \\hat{A}_k=\\frac{\\xi_k}{1+\\xi_k}\\exp\\left[\\frac{1}{2}\\int_{\\upsilon_k}^{\\infty}\\frac{e^{-t}}{t}dt\\right]R_k \\end{align}$$ [3] 給出了一個好算的近似結果$$\\begin{align} \\int_{\\upsilon_k}^{\\infty}\\frac{e^{-t}}{t}dt\\approx \\left\\{ \\begin{array}{rcl} -2.31\\log_{10}(\\upsilon_k)-0.6\\mbox{ for }\\upsilon_k&lt;0.1 \\\\ -1.544\\log_{10}(\\upsilon_k)+0.166\\mbox{ for }0.1\\leq\\upsilon_k\\leq 1 \\\\ 10^{-(0.52\\upsilon_k+0.26)}\\mbox{ for }\\upsilon_k&gt;1 \\\\ \\end{array}\\right. \\end{align}$$ 另外還有 optimally-modified log-spectral amplitude (OMLSA) [4] 方法, 作者有提供 MATLAB codes. 這算單通道降噪標配了, 但實驗結果對聽覺有幫助, 對 WER 不一定降低. 總之不管哪一種方法, 都必須很好的估出 prior and posterior SNR. MCRA Prior/Posterior SNR 估計針對 STFT 時間 $l$, frequency bin $k$ 來說, 假設我們已估出來 speech presence probability $p(k,l)$, 我們可以這麼 update noise 的 variance:$$\\begin{align} \\hat{\\lambda}_d(k,l+1)=\\hat{\\lambda}_d(k,l)p(k,l)+\\left[\\alpha_d\\hat{\\lambda}_d(k,l)+(1-\\alpha_d)|Y(k,l)|^2\\right](1-p(k,l)) \\end{align}$$ 這很好理解, 如果有 speech 的話, noise variance 就沿用原來舊的, 而如果沒有 speech, nosie vaiance 就要用當前 frame 透過 $\\alpha_d$ 平滑地更新一下 (就稱這樣的平滑為 $\\alpha$ 平滑). 估計 $p(k,l)$ 之前, 文章的做法是都先針對 time and frequency 做平滑. frequency 可選用一個 window (可用類似 Gaussian window), 而時間上的平滑可使用 $\\alpha$ 平滑. 令 $S(k,l)$ 為我們平滑後的 spectrum power, 然後對每個 bin 都 tracking 一小段時間的最小值, 令為 $S’(k,l)$. 則很明顯如果 $S(k,l)&gt;\\delta S’(k,l)$, 我們就可以認為有 speech, 機率為 1, 否則為 0. 這樣的 speech 機率過了 $\\alpha$ 平滑的結果就是 $p(k,l)$. 明確一點寫下為:$$\\begin{align} p(k,l)=\\alpha_p p(k,l-1)+(1-\\alpha_p)\\mathbf{I}[S(k,l)&gt;\\delta S&apos;(k,l)] \\end{align}$$ 其中 $\\mathbf{I}[.]$ 為 indicator function MCRA 有哪些調整的參數實際情形有一些需要調整的參數, 列在下面 $\\alpha_d$: noise variance smoothing $\\alpha_p$: speech probability smoothing STFT 的 time and frequency smoothing 參數 $\\delta$: 判斷當前 frame and bin 是否為 speech 的 threshold tracking minimal power $S’(k,l)$ 的參數, 譬如要用多少個 frame 來找 minimum 待做些實驗才會知道效果… Reference Speech Enhancement Using a Minimum Mean-Square Error Short-Time Spectral Amplitude Estimator by Yariv Ephraim and David Malah Speech Enhancement Using a Minimum Mean-Square Error Log-Spectral Amplitude Estimator by Yariv Ephraim and David Malah [A Noise Reduction Pre-processor for Mobile Voice Communication] by R. Martin … Speech enhancement for non-stationary noise environments by Israel Cohen and Baruch Berdugo","tags":[{"name":"MMSE-STSA","slug":"MMSE-STSA","permalink":"http://yoursite.com/tags/MMSE-STSA/"},{"name":"MMSE-LSA","slug":"MMSE-LSA","permalink":"http://yoursite.com/tags/MMSE-LSA/"},{"name":"OMLSA","slug":"OMLSA","permalink":"http://yoursite.com/tags/OMLSA/"},{"name":"MCRA","slug":"MCRA","permalink":"http://yoursite.com/tags/MCRA/"}]},{"title":"Far Field Notes (3) Equivalence of MWF, MaxSNR, and MVDR Filters","date":"2019-03-18T12:33:46.000Z","path":"2019/03/18/Far-Field-Notes-3-MWF-MaxSNR-MVDR-Filters/","text":"這是 far field 筆記系列第三篇, 主要為自己學習用, 如有錯誤還請指正. 主要參考 Microphone Array Signal Processing Ch6 和 Speech Processing in Modern Communication: Challenges and Perspectives Ch9.3.4 在 narrow-band 的情形下, Multi-channel Wiener Filter (MWF), maximum SNR (MSNR) 和 Minimum Variance Distortionless Response (MVDR) 三者求出來的 filter 解只差在 norm 大小不同. 但反應在最後的 full-bank 行為仍然不同. 這部分可看書. 本篇主要紀錄 narrow-bank 下三者為何 equivalent. 算是書本的摘要筆記吧. Signal Model在 frequency doamin 下, 我們有如下的關係 $$\\begin{align} Y_n(j\\omega)=G_n(j\\omega)S(j\\omega)+V_n(j\\omega) \\\\ =X_n(j\\omega)+V_n(j\\omega)\\mbox{, }n=1,2,...,N \\\\ \\end{align}$$ $N$ 是麥克風數量, $S(j\\omega)$ 是原始訊號, $G_n(j\\omega)$ 是聲源到 mic $n$ 的 impluse response, $V_n(j\\omega)$ 是 noise, 而 $X_n(j\\omega)$ 是 mic $n$ 的訊號. 我們希望還原的是 $X_1(j\\omega)$ 而不是 $S(j\\omega)$.排成 vector 形式如下$$\\begin{align} Z(j\\omega)=\\mathbf{h}^H(j\\omega)\\mathbf{y}(j\\omega) \\\\ =\\mathbf{h}^H(j\\omega)[\\mathbf{x}(j\\omega)+\\mathbf{v}(j\\omega)] \\\\ \\end{align}$$ 其中$$\\begin{align} \\mathbf{y}(j\\omega)=[Y_1(j\\omega),Y_2(j\\omega),...,Y_N(j\\omega)]^T \\\\ \\mathbf{x}(j\\omega)=S(j\\omega)[G_1(j\\omega),G_2(j\\omega),...,G_N(j\\omega)]^T=S(j\\omega)\\mathbf{g}(j\\omega) \\\\ \\mathbf{v}(j\\omega)=[V_1(j\\omega),V_2(j\\omega),...,V_N(j\\omega)]^T \\\\ \\mathbf{h}(j\\omega)=[H_1(j\\omega),H_2(j\\omega),...,H_N(j\\omega)]^T \\\\ \\end{align}$$ 注意到$$\\begin{align} \\Phi_{xx}(j\\omega)=\\mathbb{E}\\left[\\mathbf{x}(j\\omega)\\mathbf{x}^H(j\\omega)\\right]=\\phi_{ss}(j\\omega)\\mathbf{g}(j\\omega)\\mathbf{g}^H(j\\omega) \\end{align}$$ MWF將 error term 寫出來 $$\\begin{align} \\mathcal{E}(j\\omega)=Z(j\\omega)-X_1(j\\omega) \\\\ =\\mathbf{h}^H(j\\omega)\\mathbf{v}(j\\omega)+[\\mathbf{h}(j\\omega)-\\mathbf{u}]^H\\mathbf{x}(j\\omega)\\\\ =\\color{orange}{\\mathcal{E}_v(j\\omega)}+\\color{blue}{\\mathcal{E}_x(j\\omega)} \\\\ \\end{align}$$ 其中 $\\mathbf{u}$ 是一個 $N\\times 1$ 的 vector, 只有第一個是1, 其他是0. Error term 可以拆成兩項, 分別對應了 noise reduction 程度和 speech distortion 程度MWF 的目標函式如下:$$\\begin{align} J_{MWF}[\\mathbf{h}(j\\omega)]=\\mathbb{E}\\left[| \\mathcal{E}(j\\omega) |^2\\right]\\\\ = \\color{orange}{ \\mathbb{E}\\left[| \\mathcal{E}_v(j\\omega) |^2\\right] } + \\color{blue}{ \\mathbb{E}\\left[| \\mathcal{E}_x(j\\omega) |^2\\right] } \\end{align}$$ 可以看成 noise reduction 和 speech distortion 同等重要情況下去求解最好的 $\\mathbf{h}$微分等於零求解得到如下:$$\\begin{align} \\Phi_{yy}(j\\omega)\\mathbf{h}_W(j\\omega)=\\Phi_{yx}(j\\omega)\\mathbf{u}=\\Phi_{xx}(j\\omega)\\mathbf{u} \\end{align}$$上式最後推導是由於 $x$ and $v$ 是 independent. 因此最後的 MWF 解為:$$\\begin{align} \\mathbf{h}_W(j\\omega)=\\Phi_{yy}^{-1}(j\\omega)\\Phi_{xx}(j\\omega)\\mathbf{u} \\\\ =\\left[ \\mathbf{I}_{N\\times N} - \\Phi_{yy}^{-1}(j\\omega)\\Phi_{vv}(j\\omega) \\right]\\mathbf{u} \\end{align}$$ MVDRMVDR 要解的問題如下:$$\\min \\color{orange}{ \\mathbb{E}\\left[| \\mathcal{E}_v(j\\omega) |^2\\right] }\\\\ \\mbox{subject to } \\color{blue}{\\mathbb{E}\\left[| \\mathcal{E}_x(j\\omega) |^2\\right]}=0$$ 這也可以看出 MVDR 為什麼叫 MVDR.首先先將 constraint 改寫成 (改寫 speech distortion error term, 定義在 (11), (12)):$\\left[\\mathbf{u}-\\mathbf{h}(j\\omega)\\right]^H\\mathbf{x}(j\\omega)=0 \\\\$ 並利用 $\\mathbf{x}(j\\omega)=S(j\\omega)\\mathbf{g}(j\\omega)$ 可得到$\\mathbf{h}^H(j\\omega)\\mathbf{g}(j\\omega)=G_1(j\\omega)$因此 MVDR 問題的通常如下表達:$$\\min \\mathbf{h}^H(j\\omega) \\Phi_{vv}(j\\omega) \\mathbf{h}(j\\omega) \\\\ \\mbox{subject to } \\mathbf{h}^H(j\\omega)\\mathbf{g}(j\\omega)=G_1(j\\omega) \\\\$$ 這個最佳化問題正好就是上一篇的 LCMV, 所以說 MVDR 是 LCMV 的一種 case.用 Lagrange multipliers 求解得到$$\\begin{align} \\mathbf{h}_{MVDR}(j\\omega)=G_1^{\\ast}(j\\omega)\\frac{\\Phi_{vv}^{-1}(j\\omega)\\mathbf{g}(j\\omega)}{\\mathbf{g}^H(j\\omega)\\Phi_{vv}^{-1}(j\\omega)\\mathbf{g}(j\\omega)} \\end{align}$$ 對 (18) 進一步推導, 為了精簡以下 ${j\\omega}$ 省略不寫$$\\begin{align} \\mathbf{h}_{MVDR}=\\frac{\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}G_1^{\\ast}}{tr\\left[\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}\\mathbf{g}^H\\right]}=\\frac{\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}\\mathbf{g}^H\\mathbf{u}}{tr\\left[\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}\\mathbf{g}^H\\right]} \\\\ =\\frac{ \\Phi_{vv}^{-1}\\Phi_{xx}\\mathbf{u} }{ tr\\left[\\Phi_{vv}^{-1}\\Phi_{xx}\\right] } \\\\ =\\frac{ \\Phi_{vv}^{-1}(\\Phi_{yy}-\\Phi_{vv})\\mathbf{u} }{ tr\\left[ \\Phi_{vv}^{-1}(\\Phi_{yy}-\\Phi_{vv}) \\right] } \\\\ =\\frac{ (\\Phi_{vv}^{-1}\\Phi_{yy}-\\mathbf{I})\\mathbf{u} }{ tr\\left[\\Phi_{vv}^{-1}\\Phi_{yy}\\right]-N } \\end{align}$$ (19) 到 (20) 使用了 (9).(22) 的形式書本說很重要, 因為避免了很難估計的 $\\mathbf{g}$, 取而代之的是我們要估計出 $\\Phi_{vv}$ MWF 與 MVDR 等價情形同樣為了精簡以下 ${j\\omega}$ 省略不寫, 首先我們知道$$\\begin{align} \\Phi_{yy}=\\Phi_{vv} + \\phi_{ss}\\mathbf{g}\\mathbf{g}^H \\end{align}$$ 使用 Woodbury’s identity 可得:$$\\begin{align} \\Phi_{yy}^{-1}=\\Phi_{vv}^{-1}-\\frac{ \\Phi_{vv}^{-1}\\Phi_{xx}\\Phi_{vv}^{-1} }{ 1+tr\\left[\\Phi_{vv}^{-1}\\Phi_{xx}\\right] } \\end{align}$$ 將 (24) 帶入到 (17) 並經過一些代數替換我們得到$$\\begin{align} \\mathbf{h}_{W}=\\frac{ \\Phi_{vv}^{-1}\\Phi_{xx} }{ 1+tr\\left[\\Phi_{vv}^{-1}\\Phi_{xx}\\right] }\\mathbf{u} \\end{align}$$ 這個式子與 MVDR 的 (20) 比較一下我們發現$$\\begin{align} \\mathbf{h}_{W}(j\\omega)=c(\\omega)\\mathbf{h}_{MVDR}(j\\omega) \\end{align}$$ 其中 $c(\\omega)$ 是與 $\\omega$ 相關的一個 scalar. 因此 MWF 與 MVDR 解只差在一個 $\\omega$ 相關的常數項 Maximum SNR (MSNR)同樣為了精簡以下 ${j\\omega}$ 省略不寫, output SNR 定義為:$$\\begin{align} \\mbox{oSNR}\\left[\\mathbf{h}\\right]=\\frac{ \\mathbf{h}^H \\Phi_{xx} \\mathbf{h} }{ \\mathbf{h}^H \\Phi_{vv} \\mathbf{h} } \\end{align}$$ 這個等同於 generalized eigenvalue problem.$$\\begin{align} \\Phi_{xx}\\mathbf{h}=\\lambda\\Phi_{vv}\\mathbf{h} \\end{align}$$ 所以$\\mathbf{h}_{MSNR}\\mbox{ is eigenvector w.r.t max eigenvalue of matrix } \\Phi_{vv}^{-1}\\Phi_{xx}$eigenvector 乘上一個 scalar 仍然是 eigenvector, 因此通常都會將 $\\mathbf{h}_{MSNR}$ 的 norm 定為 1 接著我們說明 $\\mathbf{h}$ 的解具有以下形式:$$\\begin{align} \\mathbf{h}\\propto\\Phi_{vv}^{-1}\\mathbf{g} \\end{align}$$ 先假定 (29) 為等式:$$\\begin{align} \\mathbf{h}=\\Phi_{vv}^{-1}\\mathbf{g} \\end{align}$$ 利用 (9) 和 (30) 得到以下的推導$$\\begin{align} \\Phi_{vv}^{-1}\\Phi_{xx}\\mathbf{h}=\\Phi_{vv}^{-1} \\phi_{ss}\\mathbf{g}\\mathbf{g}^H \\mathbf{h}=\\left(\\Phi_{vv}^{-1}\\mathbf{g}\\right)\\left(\\phi_{ss}\\mathbf{g}^H\\mathbf{h}\\right)=\\mathbf{h}\\lambda \\end{align}$$ 我們發現 $\\mathbf{h}$ 如有 (30) 的形式, 則為 maximum SNR (27) 的解. 當然 eigenvector 乘上 scalar 仍然是 eigenvector, 所以 (29) 為 maximum SNR 的解.最後由於 $\\Phi_{xx}$ 為 rank 1 所以只會有一個 nonzero eigenvalue, 因此 maximum SNR 所有解的形式必然為 (29) 的形式. MVDR 與 MSNR 等價情形檢查下 (18) 的 MVDR 解, 很快就發現滿足 (29) MSNR 的解的形式, 因此$$\\begin{align} \\mathbf{h}_{MVDR}(j\\omega)=d(\\omega)\\mathbf{h}_{MSNR}(j\\omega) \\end{align}$$ 結論MWF, MVDR, MSNR 三個問題的解在 narrowband 上只差在 scalar. 但以 fullband 來說, 表現還是不同的. Reference Microphone Array Signal Processing by Jocab Benesty Speech Processing in Modern Communication: Challenges and Perspectives","tags":[{"name":"MWF","slug":"MWF","permalink":"http://yoursite.com/tags/MWF/"},{"name":"MSNR","slug":"MSNR","permalink":"http://yoursite.com/tags/MSNR/"},{"name":"MVDR","slug":"MVDR","permalink":"http://yoursite.com/tags/MVDR/"}]},{"title":"Far Field Notes (2) LCMV filter and Frost's algorithm","date":"2019-03-02T09:36:58.000Z","path":"2019/03/02/Far-Field-Notes-2-LCMV-and-Frost/","text":"這是 far field 筆記系列第二篇, 主要為自己學習用, 如有錯誤還請指正. 主要參考 Microphone Array Signal Processing Ch4 和 Frost’s algorithm 上一篇最後雖然使用 fixed beamformer 得到了 response-invariant beamformer, 但這個方法限制是 filter 一旦設計好就寫死了, 沒辦法自己 update (所以才叫 “fixed” beamformer). 這引入一個問題是, 如果剛好有一個 inteference noise 在衰減不那麼大的角度時, 就無法壓得很好. 而這篇要介紹的 LCMV (Linear Constrained minimum variance) filter 以及 Frost’s beamformer 能針對給定的方向抽取訊號, 並且對其他方向的 inteference nosie 壓抑的最好. 注意 sound source 方向必須給定, LCMV 求得的 weights 會想辦法對其他方向的 inteference 壓抑. 如同 LCMV 字面上的意思一樣. 會將整個問題轉換成 minimize variance subject to some linear constraints. 另外相當經典的 Frost’s beamformer (1972年呢!) 則將 filter 的 optimal 求解改成使用 stochastic gradient descent 方式, 所以非常適合實際的 real time 系統, 這些下文會詳細說明. 架構設定和 Signal Model架構如下圖 (圖片來源:ref), 第一步是一個 delay stage, 這相當於是針對一個 steering direction 補償每個 mic 之間的 time delay (訊號對齊好). 第二步才是 beamformer, 我們知道 time domain 使用 filter-and-sum 架構, 如果是 frequency domain 則使用拆頻的架構. 忘了可參考第一篇. 本文以 filter-and-sum 來筆記, 另外 signal model 以下推導將會使用 anechoic model, 第一篇有定義可回去查閱. 同時本文接下來的 notation 會與圖中的不同. 上圖只是用來顯示 filter-and-sum 架構. Notations一些 notations 我們先定義起來. $N$ 是麥克風數量, $L$ 是 filter tap 數量, 我們 aligned 好的 anechoic model 如下:$$\\begin{align} \\mathbf{y}(k)=s(k)\\mathbf{\\alpha}+\\mathbf{v}(k) \\end{align}$$其中$$\\begin{align} \\mathbf{y}(k)=[y_1(k),...,y_N(k)]^T \\\\ \\mathbf{v}(k)=[v_1(k),...,v_N(k)]^T \\\\ \\mathbf{\\alpha}=[\\alpha_1,\\alpha_2,...,\\alpha_N]^T \\end{align}$$ $s(k)$ 是時間 $k$ 的聲源訊號, $\\alpha$ 是 $N\\times 1$ 的 attenuation factors, $\\mathbf{v}(k)$ 是時間 $k$ 的 $N\\times 1$ noise 訊號向量, 因此 $\\mathbf{y}(k)$ 是時間 $k$ 的 $N\\times 1$ 麥克風收到的訊號向量. 注意到由於我們先 align 好 delay 了, 所以原先的 anechoic model 可以簡化成上面的表達. 考慮到 filter-and-sum 架構, 我們將整個 $N$ 個 mic 每個 mic 都有 $L$ 個值以下圖(圖片來源:ref)的順序串成一個 $NL$ vector因此我們得到這些向量$$\\begin{align} \\mathbf{y}_{NL}(k)=[\\mathbf{y}^T(k), \\mathbf{y}^T(k-1), ..., \\mathbf{y}^T(k-L+1)]^T \\\\ \\mathbf{x}_{NL}(k)=[s(k)\\mathbf{\\alpha}^T, s(k-1)\\mathbf{\\alpha}^T, ..., s(k-L+1)\\mathbf{\\alpha}^T]^T \\\\ \\mathbf{v}_{NL}(k)=[\\mathbf{v}^T(k), \\mathbf{v}^T(k-1), \\mathbf{v}^T(k-L+1)]^T \\end{align}$$所以整體的 signal model 改寫 (1) 後可得:$$\\begin{align} \\mathbf{y}_{NL}(k)=\\mathbf{x}_{NL}(k) + \\mathbf{v}_{NL}(k) \\end{align}$$ Filter-and-sum 的 filter $\\mathbf{h}$ 也用這個順序定義如下, 因此是一個長度為 $NL$ 的向量$$\\begin{align} \\mathbf{h}=[\\mathbf{h}_0^T, \\mathbf{h}_1^T, \\mathbf{h}_{L-1}^T]^T \\end{align}$$最後整個 beamformer 的輸出 $z(k)$ 就可以這麼寫$$\\begin{align} z(k)=\\mathbf{h}^T\\mathbf{y}_{NL}(k) = \\color{orange}{ \\mathbf{h}^T\\mathbf{x}_{NL}(k) } + \\color{blue}{ \\mathbf{h}^T\\mathbf{v}_{NL}(k) } \\end{align}$$ Problem DefinitionLCMV 的主要想法就圍繞在 (10) 的橘色和藍色兩個部分上面: 我們希望橘色部分能夠還原出原始訊號 $s(k)$ 且藍色部分能夠愈小愈好 (代表著 noise 愈小愈好). 首先我們將橘色部分作如下推導: $$\\begin{align} \\color{orange}{ \\mathbf{h}^T\\mathbf{x}_{NL}(k) } =\\mathbf{h}^T \\left[ \\begin{array}{clr} s(k)\\mathbf{\\alpha} \\\\ s(k-1)\\mathbf{\\alpha} \\\\ \\vdots \\\\ s(k-L+1)\\mathbf{\\alpha} \\end{array} \\right] = sum\\left( \\left[ \\begin{array}{clr} \\mathbf{h}_0^T\\mathbf{\\alpha}\\cdot s(k) \\\\ \\mathbf{h}_1^T\\mathbf{\\alpha}\\cdot s(k-1) \\\\ \\vdots \\\\ \\mathbf{h}_{L-1}^T\\mathbf{\\alpha}\\cdot s(k-L+1) \\end{array} \\right] \\right) \\\\ = sum\\left( \\color{red}{ \\left[ \\begin{array}{clr} u_0\\cdot s(k) \\\\ u_1\\cdot s(k-1) \\\\ \\vdots \\\\ u_{L-1}\\cdot s(k-L+1) \\end{array} \\right] } \\right) \\end{align}$$ (12) 為引入的條件, 藉由這樣的條件來還原原始訊號.$u$ ($L$長度的向量) 定義了我們希望在時間 $k$ 的還原結果, 是原始訊號的權重和定義一個 matrix (size of $NL\\times L$) 如下: $$\\begin{align} \\mathbf{C}_{\\mathbf{\\alpha}}= \\left[ \\begin{array}{clr} \\mathbf{\\alpha} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{\\alpha} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{\\alpha} \\\\ \\end{array} \\right] = \\left[ \\begin{array}{clr} \\mathbf{c}_{\\alpha,0} &amp; \\mathbf{c}_{\\alpha,1} &amp; \\cdots &amp; \\mathbf{c}_{\\alpha,L-1} \\\\ \\end{array} \\right] \\end{align}$$ 觀察 (11) and (12) 並利用 $\\mathbf{C_{\\alpha}}$ 可以將 constraint 明確寫出如下: $$\\begin{align} \\mathbf{C_{\\alpha}}^T\\mathbf{h}=\\mathbf{\\mathbf{u}} \\end{align}$$ 藍色部分代表最後的 noise 成分, 希望愈小愈好計算藍色部分的能量為 $$\\begin{align} \\mathbf{h}^T \\mathbb{E} \\left[ \\mathbf{v}_{NL}(k)\\mathbf{v}_{NL}^T(k) \\right] \\mathbf{h}=\\mathbf{h}^T\\mathbf{R}_{\\mathbf{v},\\mathbf{v}}\\mathbf{h} \\end{align}$$ 但關鍵是我們無法得知實際的 noise signal, 我們有的只有 observation $\\mathbf{y}_{NL}(k)$, 那該怎麼辦呢?LCMV 很厲害的一點是, 由於上面剛提到的 constraints, 導致橘色部分的能量是 constant, 因此以下兩個問題是等價的 $$\\begin{align} \\min_{\\mathbf{h}}{ \\mathbf{h}^T\\mathbf{R}_{\\mathbf{v},\\mathbf{v}}\\mathbf{h} } \\equiv \\min_{\\mathbf{h}}{ \\mathbf{h}^T\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} } \\end{align}$$ 到這裡我們可以寫出完整的最佳化問題$$\\begin{align} \\begin{array}{clr} \\color{blue}{ \\min_{\\mathbf{h}}{ \\mathbf{h}^T\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} } } \\\\ \\color{orange}{ \\mbox{subject to }\\mathbf{C_{\\alpha}}^T\\mathbf{h}=\\mathbf{\\mathbf{u}} } \\end{array} \\end{align}$$ Optimal Solution要解問題 (17), 基本上使用 Lagrange function 求解就可以, 解如下:$$\\begin{align} \\mathbf{h}=\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}^{-1}\\mathbf{C_{\\alpha}} \\left( \\mathbf{C_{\\alpha}}^T \\mathbf{R}_{\\mathbf{y},\\mathbf{y}}^{-1} \\mathbf{C_{\\alpha}} \\right)^{-1} \\mathbf{u} \\end{align}$$ 但是重點來了, 以上這些推導全部都假設是 stationary, 實際情況一定是 non-stationary 怎麼辦? 最直覺的想法就是, 我們每隔一段時間就用 (18) 重新算一下 $\\mathbf{h}$. 但很明顯這非常沒效率 (covariance估計, inverse運算) 根本不可行. 因此必須改成 iteratively update $\\mathbf{h}$ 的方式.Frost’s algorithm 的一個重要貢獻也就是在這, 使用 stochastic gradient descent 方式 update $\\mathbf{h}$! Frost’s Algorithm問題 (17) 的 Lagrange function 如下:$$\\begin{align} \\mathcal{L}(\\mathbf{h},\\mathbf{\\lambda}) = \\frac{1}{2} \\mathbf{h}^T\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} + \\mathbf{\\lambda}^T(\\mathbf{C_{\\alpha}}^T\\mathbf{h}-\\mathbf{\\mathbf{u}}) \\end{align}$$因此 gradient 如下:$$\\begin{align} \\nabla_{\\mathbf{h}}\\mathcal{L} = \\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} + \\mathbf{C_{\\alpha}}\\mathbf{\\lambda} \\end{align}$$gradient descent update 式子如下:$$\\begin{align} \\mathbf{h}_{t+1} = \\mathbf{h}_{t} - \\mu \\left( \\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h}_t + \\mathbf{C_{\\alpha}}\\mathbf{\\lambda}_t \\right) \\end{align}$$由於有 constraint, 必須滿足 update 後仍然滿足條件, 因此:$$\\begin{align} \\mathbf{u}=\\mathbf{C_{\\alpha}}^T\\mathbf{h}_{t+1} \\end{align}$$將(21)帶入(22)整理得到$\\lambda_t$, 接著再將$\\lambda_t$帶回(21)得到結果如下, 並不困難只是一些代數運算:$$\\begin{align} \\mathbf{h}_{t+1} = \\mathbf{h}_{t} - \\mu \\left[ \\mathbf{I} - \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1}\\mathbf{C}^T \\right] \\mathbf{R}_{\\mathbf{y},\\mathbf{y}} \\mathbf{h}_{t} + \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1} \\left[ \\mathbf{u}-\\mathbf{C}^T\\mathbf{h}_t \\right] \\end{align}$$定義兩個 matrix $\\mathbf{A}$, $\\mathbf{B}$ 如下 (注意到這兩個 matrix 是事先計算好的):$$\\begin{align} \\mathbf{A} \\triangleq \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1}\\mathbf{u} \\\\ \\mathbf{B} \\triangleq \\mathbf{I} - \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1}\\mathbf{C}^T \\end{align}$$因此可以改寫(23)如下:$$\\begin{align} \\mathbf{h}_{t+1} = \\mathbf{B}[\\mathbf{h}_t - \\mu \\mathbf{R}_{\\mathbf{y},\\mathbf{y}} \\mathbf{h}_t] + \\mathbf{A} \\end{align}$$由於使用 stochastic 方式, 因此 expectation 使用最新的一次 sample 即可:$$\\begin{align} \\mathbf{R}_{\\mathbf{y},\\mathbf{y}} = \\mathbb{E} \\left[ \\mathbf{y}_{NL}(t)\\mathbf{y}_{NL}^T(t) \\right] \\thickapprox \\color{green}{ \\mathbf{y}_{NL}(t)\\mathbf{y}_{NL}^T(t) } \\end{align}$$將(27)帶入(26)並用(10)替換一下, 我們得到最終的 update 式子:$$\\begin{align} \\color{red}{ \\mathbf{h}_{t+1} = \\mathbf{B}[\\mathbf{h}_t - \\mu z(t)\\mathbf{y}_{NL}(t)] + \\mathbf{A} } \\end{align}$$由於 $\\mathbf{A}$ 和 $\\mathbf{B}$ 是固定的, 跟原來的 optimal 解比較 (18), 可以明顯知道速度上會快非常多.另外 $\\mathbf{h}_0$ 只需要選擇一個 trivial 的 feasible point 即可:$$\\begin{align} \\mathbf{h}_{0} = \\mathbf{A} \\end{align}$$ 結論本篇記錄了 filter-and-sum 架構的 beamformer, LCMV 的問題和其最佳解. LCMV 可以針對給定的一個方向, 找出 filter $\\mathbf{h}$ 使得抽取看的方向的訊號同時壓抑其他方向的訊號.實作上直接套用最佳解太慢不可行, 而 Frost’s algorithm 提供了一個 stochastic gradeint update 方法更新 $\\mathbf{h}$, 這使得 real-time system 變得可行. Reference Microphone Array Signal Processing by Jocab Benesty Frost’s algorithm","tags":[{"name":"MVDR","slug":"MVDR","permalink":"http://yoursite.com/tags/MVDR/"},{"name":"LCMV","slug":"LCMV","permalink":"http://yoursite.com/tags/LCMV/"},{"name":"Frost","slug":"Frost","permalink":"http://yoursite.com/tags/Frost/"}]},{"title":"Far Field Notes (1), Beampattern","date":"2019-02-26T12:22:55.000Z","path":"2019/02/26/Far-Field-Notes-1-Beampattern/","text":"這是 far field 筆記系列第一篇, 主要為自己學習用, 如有錯誤還請指正. 主要參考 Optimum Array Processing Ch2 以及 Microphone Array Signal Processing Ch3. Beampattern 就是希望能得到如下圖 [ref Fig3.3] 的表示, 說明經過一個麥克風陣列的處理後, 每個角度所得到的增益情形. 因此可以看出主要保留哪些方向的訊號, 以及抑制哪些方向的訊號. Geometry Settings遠場一般假設 plan wave, 和 narrow band. 實際處理語音等 broadband 時我們會採取 fft 分頻. 我們定義如下的 geometry, 其中重要的兩個角度為 $\\theta$ 和 $\\phi$ (如圖紅圈). $\\mathbf{a}$ 表示聲源的入射單位向量. 以下符號如果是粗體表示為向量或矩陣, 否則就是 scalar. Signal Model我們定義 $f(t)$ 為聲源訊號, $v_n(t)$ 為 nth mic 的噪聲源 Anechoic Model 我們以下的介紹都是基於 Anechoic Model, 並且先做如下的簡化 Reverberant Model 相當於將 attenuation factor $\\alpha$ 改成 impulse response, 所以相乘改成 convolution. Time Delay為了方便推導麥克風之間的 time delay, 我們先將 Anechoic Model 做如下簡化 因此對於 plan wave 假設和聲源的入射單位向量 $a$ 來說, 我們很容易就得到 time delay 如下 Uniform Lineary Array (ULA) Circular Array以一個 6 mic 的 circular array 來說, 有如下的 time delay Array Manifold Vector由於 time delay $\\tau$ 在 freqeuncy $\\omega$ 只是乘上 $e^{-j\\omega\\tau}$, 因此我們可以得到一個 compact 的表示 重複一遍這裡得到的重要式子 $$\\begin{align} \\color{red}{ \\mathbf{F}(\\omega,\\mathbf{p})=F(\\omega)\\mathbf{\\upsilon}_k(\\mathbf{k}) } \\end{align}$$ 我們稱 $\\mathbf{\\upsilon}_k(\\mathbf{k})$ 為 Array Manifold Vector. 要注意的是, 其實也可以用 time delay $\\tau$ 來表示, 這時我們這麼寫 (如上圖灰色的部分)$$\\mathbf{\\upsilon}_\\tau (\\mathbf{\\tau})= \\left[ \\begin{array}{clr} e^{-j\\omega\\tau_0} \\\\ e^{-j\\omega\\tau_1} \\\\ \\vdots \\\\ e^{-j\\omega\\tau_{N-1}} \\end{array} \\right]$$或甚至入射角度 $\\theta$ 如果可以完全表達 $\\tau$ 的話, 我們也能這麼寫 $\\mathbf{\\upsilon}_\\theta(\\mathbf{\\theta})$. Array Signal Processing早期的 array processing (narrow band) 是對每個麥克風有各自的 weights, 然後再總合起來, 這種作法叫做 weight-and-sum. 而對於 broadband 訊號來說, 相當於拆頻乘很多 narrow band, 因此在每個頻帶上, 都有 N 個麥克風的 weights. 這在時域上等價於每個麥克風都有各自的 filters, 稱 filter-and-sum. 以下介紹 filter-and-sum 和頻域的架構. Filter-and-Sum 在實作上通常採用 FIR filter, 因此架構如下:符號有點不同, 這是因為圖是採用另一本書 Microphone Array Signal Processing Frequency Domain針對 filter-and-sum 做 frequency transform 得到如下: 實際架構圖如下:一樣符號有點不同, 這是因為圖是採用另一本書 Microphone Array Signal Processing Frequency-wavenumber Response Function針對 frequency domain 的 array processing, 我們可以帶入先前推得的 (1) 得到如下: 所以 $\\Upsilon(\\omega,\\mathbf{k})$ 物理意義就是針對 frequency $\\omega$ 和 wavenumber $\\mathbf{k}$ (控制了聲源入射角度 $\\theta$ 等等的物理量) 的 response. Beampatternwavenumber $\\mathbf{k}$ 比較抽象, 如果我們換成角度 $\\theta$, $\\phi$ 就會直觀很多, 而 beampattern 只是針對 $\\Upsilon(\\omega,\\mathbf{k})$ 換成用角度而已. 所以 $B(\\omega:\\theta,\\phi)$ 物理意義就是針對 frequency $\\omega$ 和入射角度 $\\theta$, $\\phi$ 的 response. Delay-and-sum BeampatternDelay-and-sum 想法很簡單, 就是補償每個 mic 的 time delay 而已. 因此所需要的 filter $H(\\omega)$ 就是 array manifold vector 的 conjugate 即可. 如下圖: 但這麼做有個缺點, 就是高頻時雖然針對聲源方向的 mainlobe 變窄了, 但同時 sidelobe 卻變多了. 也就是在高頻時, 某些方向的聲源消不掉. 如下圖: Fixed Beampattern為了修正上述 DS beamformer 的問題, 我們希望得到 response-invariant broadband beamformer. 希望能有下圖的結果: 中心思想很簡單, 針對某個頻率 $\\omega$ 來求出相對應的 $H$ 使得 beampattern 會與我們 desired beampattern 有 least-sqaure 差異. 以下 $H(\\omega)$ 會省略 $\\omega$ 不寫 結論到這裡我們討論了遠場的 signal model, 針對 anechoic model 我們最終導出了 beampattern. 做為例子我們使用簡單的 delay-and-sum (DS) beamformer 來看它的 beampattern 長什麼樣. 可以看到在高頻時很多方向還是無法壓抑, 因此使用 least-square 方法找出每個頻率需要的 spatial filter 來逼近我們需要的 beampattern. Reference Optimum Array Processing: Part IV of Detection, Estimation, and Modulation Theory by Harry L. Van Trees Microphone Array Signal Processing by Jocab Benesty Direction of Arrival Estimation Using the Parameterized Spatial Correlation Matrix","tags":[{"name":"array manifold vector","slug":"array-manifold-vector","permalink":"http://yoursite.com/tags/array-manifold-vector/"},{"name":"beampattern","slug":"beampattern","permalink":"http://yoursite.com/tags/beampattern/"},{"name":"anechoic model","slug":"anechoic-model","permalink":"http://yoursite.com/tags/anechoic-model/"},{"name":"wavenumber","slug":"wavenumber","permalink":"http://yoursite.com/tags/wavenumber/"}]},{"title":"Bayesian Learning Notes","date":"2018-12-20T14:39:42.000Z","path":"2018/12/20/Bayesian-Learning-Notes/","text":"枉費我學習 ML 這麼久, 最近才完整了解 Bayesian learning 大架構, 以及與 MLE, MAP, Variational Inference, Sampling 之間的關聯. 這才終於有了見樹又見林的港覺阿! 筆記整理如下 … 圖片來自 wiki, 我也好想要這個裝飾燈. 就這麼一個 Baye’s Rule, 撐起了統計機器學習的基石! Bayesian Learning給定訓練集 ($X,Y$) 和一個 probabilistic classifier $p(y|x,\\theta)$, 同時定義好 prior distribution $p(\\theta)$. 根據 Baye’s rule, Training stage 如下: $$\\begin{align} p(\\theta|X,Y)=\\frac{p(Y|X,\\theta)p(\\theta)}{\\color{red}{\\int p(Y|X,\\theta)p(\\theta)\\,d\\theta}} \\end{align}$$ Testing stage 如下: $$\\begin{align} p(y^*|x^*,X,Y)=\\color{red}{\\int p(y^*|x^*,\\theta)p(\\theta|X,Y)\\,d\\theta} \\end{align}$$ 注意到關鍵的兩個紅色積分通常都是不容易算, 或根本算不出來. 此時我們有兩種選擇: 使用 Variational Inference 找出一個 $q(\\theta)$ 來逼近 $p(\\theta|X,Y)$ 使用 sampling 方法. 理解一下這個積分的形式, 可以發現這是在算根據某個機率分佈$p(x)$計算$f(x)$的期望值. 因此, 如果我們直接根據 $p(x)$ sample 出 $M$ 個 $x$, 就可以用如下的平均算出近似值了. $$\\begin{align} \\int p(x)f(x) \\,dx \\simeq \\frac{1}{M}\\sum_{i=1}^M f(x_i)\\mbox{, where }x_i \\sim p(x) \\end{align}$$ 我們可能會想, 是不是可以將 Bayesian learning 做些簡化來避掉上述紅色積分? 是的, MLE 和 MAP 就是簡化了完整的 Bayesian learning 過程. 下面介紹. MLE and MAPBaye’s rule (式 (1)), 在 ML 中舉足輕重, 幾乎是所有的根本. 重新列出來並用不同顏色做強調 $$\\begin{align} \\color{orange}{p(\\theta|X,Y)}=\\frac{\\color{blue}{p(Y|X,\\theta)}\\color{green}{p(\\theta)}}{\\color{red}{\\int p(Y|X,\\theta)p(\\theta)\\,d\\theta}} \\end{align}$$ 橘色稱為 posterior distribution, 藍色為 likelihood, 而綠色為 prior distribution. 注意到紅色的期望值基本算不出來, 在這種情況下, 我們要怎麼得到 posterior? MLEMLE (Maximum Likelihood Estimation) 的想法是, 既然 posterior 算不出來, 那乾脆直接用一個 $\\theta^*$ 代表整個 $p(\\theta|X,Y)$ 分布算了. 至於要找哪一點呢, 就找對 likelihood 最大的那點吧! 數學這麼寫: $$\\begin{align} \\theta_{MLE}=\\arg\\max_\\theta p(Y|X,\\theta) \\end{align}$$ 既然已經用一個點來代表整個 posterior 了, 因此原來的 testing (2) 就不需要積分了, testing stage 直接就是: $$\\begin{align} p(y^*|x^*,\\theta_{MLE}) \\end{align}$$ MAPMAP (Maximum A Posterior) estimation 跟 MLE 相同, 也使用一個點來代表整個 posterior: $$\\begin{align} \\theta_{MP}=\\arg\\max_\\theta p(\\theta|X,Y) \\end{align}$$ 意思是 MAP 直接使用 mode 來代表整個 posterior. 因此 testing stage 也如同 MLE 情形: $$\\begin{align} p(y^*|x^*,\\theta_{MP}) \\end{align}$$ 不過聰明的讀者應該會覺得很疑惑, posterior 不是很難計算, 或根本算不出來, 這樣怎麼可能找的到 mode? 是的, 一般情形下是找不出來, 但有一個特殊情況叫做 conjugate prior. conjugate prior 指的是 prior 與 posterior 屬於同一個 distribution family, 等於是告訴我們 posterior 是什麼樣的 distribution, 因此算不出來的紅色期望值(式(4))也根本沒必要去計算, 只不過是個 normalization constant. 因此明確知道 posterior 是什麼樣的 distribution, 找 mode 就容易多了. 所以對於 MAP 來說有哪些 distribution 是互為 conjugate 變得很重要. 我們可以從 wiki 上查到明確資料. 基本上 exponential family 都是. 完全避掉紅色積分項了嗎?很多模型都具有 latent variable (一般都用 $z$ 表示) 的形式 稍微說明下, 一般說的 latent variable 會隨著 data 變大而變大, 而 parameter $\\theta$ 不會. 以 GMM 為例子, latent variable 指每一個 observation 是哪一個 Gaussian 產生出來的那個 index, 而 parameter 是 Gaussian components 的 mean, var, 和 mixture weights 集合. 可以使用 EM algorithm 來找出 MLE 或 MAP . 其中 E-step 為 “令 $q(z)$ 等於 $p(z|x,\\theta^{odd})$”, 這又回到如同式 (1) 求 posterior 會遇到分母積分項的問題. 如果我們的 $z$ 的值有限個的 (如 GMM, $z$ 的值就是 component 的 index), $p(z|x,\\theta^{odd})$ 可以直接算出來. 但複雜一點就不行了, 所以情況又變得跟原來的 Bayesian learning 一樣, 兩種選擇: 使用 Variational Inference, 這時稱為 Variational EM. 使用 sampling 方法. Sampling 通常採用 MCMC 方式, 這時稱為 MCMC EM. Summary擷取自 Coursera 的 Bayesian Methods for Machine Learning 課程投影片如下: (圖中的 $T$ 指的是 latent variable)","tags":[{"name":"Bayesian Learning","slug":"Bayesian-Learning","permalink":"http://yoursite.com/tags/Bayesian-Learning/"},{"name":"Conjugate Prior","slug":"Conjugate-Prior","permalink":"http://yoursite.com/tags/Conjugate-Prior/"},{"name":"MLE","slug":"MLE","permalink":"http://yoursite.com/tags/MLE/"},{"name":"MAP","slug":"MAP","permalink":"http://yoursite.com/tags/MAP/"}]},{"title":"Gaussian Process used in Bayesian Optimization","date":"2018-12-09T10:46:36.000Z","path":"2018/12/09/Gaussian-Process-used-in-Bayesian-Optimization/","text":"上了 Coursera 的 Bayesian Methods for Machine Learning, 其中最後一週的課程介紹了 Gaussian processes &amp; Bayesian optimization 覺得很有收穫, 因為做 ML 最痛苦的就是 hyper-parameter tuning, 常見的方法就是手動調, grid search or random search. 現在可以有一個較 “模型” 的作法: Bayesian optimization. 為了瞭解這個過程, 我們會介紹如下內容並同時使用 GPy and GPyOpt 做些 toy example: Random Process and Gaussian Process Stationary and Wide-Sense Stationary (WSS) GP for regression GP for bayesian optimization 讓我們進入 GP 的領域吧 Random Process (RP) and Gaussian Process (GP)Random process (RP) 或稱 stochastic process 定義為 [Def]: For any $x\\in\\mathbb{R}^d$ assign random variable $f(x)$ 例如 $d=1$ 且是離散的情形, ($x\\in\\mathbb{N}$) 定義說明對於每一個 $x$, $f[x]$ 都是一個 r.v. 所以 $f[1]$, $f[2]$, … 都是 r.v.s. 此 case 通常把 $x$ 當作時間 $t$ 來看. 而 Gaussian Process 定義為 [Def]: Random process $f$ is Gaussian, if for any finite number points, their joint distribution is normal. Stationary and Wide-Sense Stationary (WSS)Stationary一個 RP 是 stationary 定義如下: [Def]: Random process is stationary if its finite-dimensional distributions depend only on relative position of the points 簡單舉例: 取三個 r.v.s $(x_1,x_2,x_3)$ 他們的 joint pdf 會跟 $(x_1+t,x_2+t,x_3+t)$ 一模一樣$$\\begin{align} p(f(x_1),f(x_2),f(x_3))=p(f(x_1+t),f(x_2+t),f(x_3+t)) \\end{align}$$所以 pdf 只與相對位置有關, 白話講就是我們觀察 joint pdf 可以不用在意看的是哪個區段的信號, 因為都會一樣. 進一步地, 如果這個 RP 是 GP 的話, 我們知道 joint pdf 是 normal, 而 normal 只由 mean and variance totally 決定, 因此一個 GP 是 stationary 只要 mean and variance 只跟相對位置有關就會是 stationary. 基於這樣的條件我們可以寫出一個 stationary GP 的定義: [Def]: Covariance matrix or Kernel 只跟相對位置有關, 以下為三種常見的定義方式不管怎樣, 通常相對位置近的 r.v. 都會假設比較相關, (這也符合實際狀況, 譬如聲音訊號時間點相近的 sample 會較相關), 也因此 kernel 都會長類似下面的樣子 支線 Wide-Sense Stationary (WSS)本段可跳過, 主要是為了更深地理解 stationary 做的補充.WSS 定義為 [Def]: Random Process is WSS if its finite-dimensional distribution’s mean and variance depend only on relative position of the points 注意 WSS 與 Stationary 的定義差異. 準確來說 Stationary 要求所有的 moments 都只與相對位置有關, 但 WSS 只要求到 first and second order moments. 說明了 WSS 將 stationary 的條件放寬. 注意到 WSS 不一定是 stationary 的 GP, 這是因為 WSS 沒有要求 distribution 必須是 Normal.WSS, Stationary GP, Stationary RP 之間的關係可以這麼描述:$$\\begin{align} \\mbox{Stationary GP}\\subset\\mbox{Stationary RP}\\subset\\mbox{WSS} \\end{align}$$ 其實 WSS 與本篇主要討論的 Bayesian Optimization 沒有直接關係, 會想介紹是因為滿足 WSS 的話, 能使我們更直覺的 “看出” 一個訊號是否可能是 stationary. (另外 WSS 在 Adaptive Filtering 非常重要, 相當於基石的存在)首先使用課程的 stationary 範例: 中間的圖明顯不是 stationary 因為 mean 隨著位置改變不是 constant, 但左邊和右邊就真的不是那麼容易看出來了. 那麼究竟有什麼方法輔助我們判斷 stationary 呢?WSS 的 power-spectral density property 說明了一個 signal 如果是 WSS, 則它的 Covariance matrix or Kernel (訊號處理通常稱 auto-correlation) 的 DTFT 正好代表的物理意義就是 power spectral density, 而因為 kernel 不會因位置改變, 這導致了不管我們在哪個區段取出一個 window 的訊號, 它們的 power spectral density 都會長一樣. 這個性質可以讓我們很方便的 “看出” 是否是 stationary. (簡單講就是看 signal 的 frequency domain 是否因為隨著時間而變化, 變的話就一定不是 stationary) 好了, 接著回到主線去. GP for regression直接節錄課程 slides, 因為 stationary GP 的 mean 是 const, 因此我們扣掉 offset 讓其為 0, 之後再補回即可.做 Regression 的目的就是 given $x$ 如何預測 $f(x)$, 而我們有的 training data 為 $x_1, …, x_n$ 以及它們相對的 $f(x_1),…,f(x_n)$. GP 就可以很漂亮地利用 conditional probability 預測 $f(x)$ 由於是 GP, 導致上面藍色部分結果仍是 Gaussian, 因此我們得到Regression 公式: 有時候我們的 observation $f(x)$ 是 noisy 的, 此時簡單地對 $f(x)$ 加上一個 random Gaussain noise 會使得我們的 model robust 些. 上面公式都不用改, 只要針對 kernel 作如下更動即可 GPy toolkit example我們使用 Gaussian process regression tutorial 的範例, 使用上算是很直覺, 讓我們簡單實驗一下 12345678910111213import numpy as npimport GPyimport matplotlib.pyplot as plt# Generate data points, where Y is noisyX = np.random.uniform(-3.,3.,(20,1))Y = np.sin(X) + np.random.randn(20,1)*0.2# Define kernel, we use RBFkernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)# Define GP regression modelm = GPy.models.GPRegression(X,Y,kernel,noise_var=1.)# See plotfig = m.plot()plt.show() Codes 的結果如下 [Note]: 上圖的 Mean 和 Confidence 指的是 Regression 公式的 $\\mu$ and $\\sigma^2$ (前幾張有紅框的圖), 另外使用 GPy 算 regression 結果的話這麼使用1mu, sigma2 = m.predict(np.array([[1.0]])) 基本上 input 是一個 shape=(batch_size,in_dim) 的 array 可以看到就算是 data point 附近, 所顯示的 y 還是有非常大的 uncertainty, 這是因為 observation noise 的 variance 可能太大了, 我們從 1.0 改成 0.04 (正確答案) 看看 可以看到有 data point 的地方不確定性降低很多, 且不確定性看起來很合理 (當然, 因為我們用正確答案的 noise var)接著我們改 kernel 的 lengthscale (控制平滑程度) 從 1 縮小成 0.5 應該可以預期 regression 的 mean 會扭曲比較大, 結果如下 看看一個極端情況, 將 kernel 的 lengthscale 降到非常小, 這會導致 kernel 退化成 delta function, 也就是除了自己大家互不相關. 查看 regression 公式 (前幾張有紅框的圖), 由於 kernel 退化成 delta function, k 向量趨近於 0, 所以 regression 公式的 mean 趨近於 0, variance 趨近於 prior K(0). 我們將 lengthscale 調整成 0.02 得到如下的圖 可以看到 x 稍微離開 data point 的地方, 基本 mean 就回到 0, 且 variance 回到 prior K(0) 從這簡單的實驗我們發現, 這三個控制參數:-RBF variance-RBF lengthscale-GPRegression noise_var 設置不好, 基本很悲劇, 那怎麼才是對的? 下一段我們介紹使用最佳化方式找出最好的參數. Learning the kernel parameters由於都是 normal, 因此 MLE 目標函式可微, 可微就使用 gradient ascent 來求參數. 課程 slide 如下 GPy 的使用就這麼一行 m.optimize(messages=True) 結果如下 GP for bayesian optimization問題再描述一下, 就是說模型有一大堆參數 (稱 $x$) 要調整, 選擇一組參數可以得到一個 validation accuracy (稱 $f(x)$), 我們希望找到一組參數使得 $f(x)$ 最大. 這個麻煩之處就在於我們對 $f(x)$ 的表示一無所知, 只能透過採樣得到 $f(x)$, 而每次要得到一個採樣的 $f(x)$ 都要經過漫長的訓練才能得到. 在這種情形下, 怎麼透過最少採樣點得到較大的 $f(x)$ 值呢? 大絕就是用 GP 來 approximate $f(x)$ 合理嗎? 我們這麼想, 由於 kernel 一般的定義會使得相近的採樣點有較高的相關值, 也就是類似的參數會得到較相關的 validation accuracy. 這麼想的話多少有些合理. 另一個好處是使用 GP 可以帶給我們機率分布, 這使得我們可以使用各種考量來決定下一個採樣點. 例如: 我們可以考慮在那些不確定性較大的地方試試看, 因為說不定有更高的$f(x)$, 或是在已知目前估測的 GP model 下有較高的 mean 值那裏採樣. 這兩種方式稱為 “Exploration” and “Exploitation” 所以 Bayesian optimization 主要就兩個部分, Surrogate model (可以使用 GP) 和 Acquisition function: 演算法就很直覺了, 根據 Acquisition function 得到的採樣點 $x$ 計算出來 $f(x)$ 後, 重新更新 GP (使用 MLE 找出最好的 GP 參數更新), 更新後繼續計算下個採樣點. 我們還未說明 Acquisition function 怎麼定義, 常用的方法有下面三種: Maximum probability of improvement (MPI) Upper confidence bound (UCB) Expected improvement (EI), 網路上說最常被用 Expected improvement (EI) 可以參考這篇 blog 搭配這個 implementation, 這裡就不重複了. 值得一提的是 Acquisition function 通常有個參數 $\\xi$ 可以控制 Exploration 和 Exploitation 的 tradeoff. 接著我們使用 GPyOpt 練習一個 toy example. GPyOpt toy example類似上面的 GP for regression 的範例, $x$ and $f(x)$ 我們簡單定義如下: 1234sample_num = 20offset = 10def probeY(x): # i.e. f(x) return np.sin(x) + np.random.randn()*0.2 + offset 這次我們將 $f(x)$ 故意加了一個 offset, 雖然對於 GP regression 的假設是 mean=0, 不過 GPyOpt 預設會對 $f(x)$ 去掉 offset, 所以其實我們可以很安全的使用 API. 接著關鍵程式碼如下 12345678bounds = [&#123;'name': 'var_1', 'type': 'continuous', 'domain': (-3.0,3.0)&#125;] # domain definitionmyBopt = GPyOpt.methods.BayesianOptimization(f=probeY, # function to optimize domain=bounds, # box-constraints of the problem normalize_Y=True, # normalize Y to mean = 0 (default) acquisition_type='EI', # acquisition function type (default) maximize=False, # do maixmization? (default=False) exact_feval = False)myBopt.run_optimization(max_iter) bounds 描述了每一個 probeY 的 arguments. 特別要說一下 exact_feval = False, 這是說明 $f(x)$ 是 noisy 的, 所以會有 noise variance 可以 model (參考 3. GP for regression 的 regression 公式含noise的情形), 這在實際情況非常重要. 更多 BayesianOptimization 參數描述 參考這 使用如下指令看 acquisition 和 regression function 的結果1myBopt.plot_acquisition() 才花1x個採樣求出來的 regression function 就很逼近真實狀況了, 還不錯. 另外注意到這裡的 $f(x)$ 已經去掉 offset 了.再來使用如下指令看每一次採樣值之間的差異, 以及採樣點的 $f(x)$1myBopt.plot_convergence() 第9次採樣開始, 採樣點 $x$ 以及 $f(x)$ 之間基本沒什麼差異了.若要拿到採樣過程的 $x$ and $f(x)$, 可使用 myBopt.X 和 myBopt.Y XGBoost parameter tuning針對 XGBoost 的參數進行最佳化 (可參考這篇 blog), 關鍵就在於上面的 $probeY$ 需替換成計算某個採樣的 evaluation accuracy (因此還需要訓練). 關鍵程式碼如下: 123456789101112131415161718192021222324252627282930313233343536from xgboost import XGBRegressorfrom sklearn.model_selection import cross_val_score... Some codes here# Score. Optimizer will try to find minimum, so we will add a \"-\" sign.def probeY(parameters): parameters = parameters[0] score = -cross_val_score( XGBRegressor(learning_rate=parameters[0], max_depth=int(parameters[2]), n_estimators=int(parameters[3]), gamma=int(parameters[1]), min_child_weight = parameters[4]), X, y, scoring='neg_mean_squared_error').mean() score = np.array(score) return score# Bounds (NOTE: define continuous variables first, then discrete!)bounds = [ &#123;'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)&#125;, &#123;'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)&#125;, &#123;'name': 'max_depth', 'type': 'discrete', 'domain': (1, 50)&#125;, &#123;'name': 'n_estimators', 'type': 'discrete', 'domain': (1, 300)&#125;, &#123;'name': 'min_child_weight', 'type': 'discrete', 'domain': (1, 10)&#125; ]np.random.seed(777)optimizer = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds, acquisition_type ='MPI', acquisition_par = 0.1, exact_eval=True)max_iter = 50max_time = 60optimizer.run_optimization(max_iter, max_time)optimizer.plot_convergence()optimizer.X[np.argmin(optimizer.Y)] 課程讓我們測試了 sklearn.datasets.load_diabetes() dataset, 使用 GPyOpt 可以讓 XGBoost 比預設參數有 9% 的提升! 還是很不錯的. 再來就很期待是否能真的在工作上對 DNN 套用了. Reference GPy and GPyOpt GPyOpt’s documentation can find APIs GPyOpt tutorial 很好的 GPy and GPyOpt 數學和範例 blog Acquisition function implementation WSS power-spectral density property https://www.imft.fr/IMG/pdf/psdtheory.pdf","tags":[{"name":"Gaussian Process","slug":"Gaussian-Process","permalink":"http://yoursite.com/tags/Gaussian-Process/"},{"name":"Bayesian Optimization","slug":"Bayesian-Optimization","permalink":"http://yoursite.com/tags/Bayesian-Optimization/"},{"name":"Stationary","slug":"Stationary","permalink":"http://yoursite.com/tags/Stationary/"}]},{"title":"CTC Implementation Practice","date":"2018-10-16T12:25:10.000Z","path":"2018/10/16/CTC-Implementation-Practice/","text":"Credit 是此篇 DingKe ipynb 的, 他完整呈現了 CTC loss 以及 gradient 的計算, 非常棒!此筆記加入自己的說明, 並且最後使用 tensorflow 來驗證.這篇另一個主要目的為改成可以練習的格式 (#TODO tag). 因為我相信最好的學習方式是自己造一次輪子, 所以可以的話, 請試著把 #TODO tag 的部分做完吧.我們只專注在 CTC loss 的 forward, backwark and gradient. Decoding 部分請參考原作者的 ipynb. 最後使用 tf.nn.ctc_loss and tf.gradients 與我們的計算做對比 完成以下步驟 完成 CTC_Practice.ipynb #TODO tag 參考 CTC_Practice_Answer.ipynb Reference DingKe ipynb Sequence Modeling With CTC Graves CTC","tags":[{"name":"CTC","slug":"CTC","permalink":"http://yoursite.com/tags/CTC/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"}]},{"title":"Variational Inference and VAE Notes","date":"2018-09-18T14:21:05.000Z","path":"2018/09/18/Variational-Inference-Notes/","text":"前一陣子學習了 Variational Inference, 因為自己記性只有 LSTM 沒有 L, 所以趕快記下筆記. 學得還是很粗淺, 又是一個大坑阿.監督學習不外乎就是 training 和 testing (inference). 而 inference 在做的事情就是在計算後驗概率 $p(z|x)$. 在 PGM 中通常是 intractable, 或要找到 exact solution 的計算複雜度太高, 這時 VI 就派上用場了. VI 簡單講就是當 $p(z|x)$ 不容易得到時, 可以幫你找到一個很好的近似, $q(z)$. 放上一張 NIPS 2016 VI tutorial 的圖, 非常形象地表示 VI 做的事情: 將找 $p(z|x)$ 的問題轉化成一個最佳化問題. 怎麼看作最佳化問題?我們要找到一個 $q(z)$ 去逼近 $p(z|x)$, 因此需要計算兩個機率分佈的距離, 而 KL-divergence 是個很好的選擇 (雖然不滿足數學上的距離定義). 所以我們的目標就是希望 $KL(q(z)\\Vert p(z|x))$ 愈小愈好, 接著我們對 KL 定義重新做如下的表達: $$\\begin{align} KL\\left(q(z)\\Vert p(z|x)\\right)=-\\sum_z q(z)\\log\\frac{p(z|x)}{q(z)}\\\\ =-\\sum_z q(z)\\left[\\log\\frac{p(x,z)}{q(z)}-\\log p(x)\\right]\\\\ =-\\sum_z q(z)\\log\\frac{p(x,z)}{q(z)}+\\log p(x) \\end{align}$$ 得到這個非常重要的式子: $$\\begin{align} \\log p(x)=KL\\left(q(z)\\Vert p(z|x)\\right)+ \\color{red}{ \\sum_z q(z)\\log\\frac{p(x,z)}{q(z)} } \\\\ =KL\\left(q(z)\\Vert p(z|x)\\right)+ \\color{red}{ \\mathcal{L}(q) } \\\\ \\end{align}$$ 為什麼做這樣的轉換呢? 這是因為通常 $p(z|x)$ 很難得到, 但是 complete likelihood $p(z,x)$ 通常很好求.觀察 (5), 注意到在 VI 的設定中 $\\log p(x)$ 跟我們要找的 $q(z)$ 無關, 也就造成了 $\\log p(x)$ 是固定的. 由於 $KL\\geq 0$, 讓 $KL$ 愈小愈好等同於讓 $\\mathcal{L}(q)$ 愈大愈好. 因此 VI 的目標就是藉由最大化 $\\mathcal{L}(q)$ 來迫使 $q(z)$ 接近 $p(z|x)$. $\\mathcal{L}(q)$ 可以看出來是 marginal log likelihood $\\log p(x)$ 的 lower bound. 因此稱 variational lower bound 或 Evidence Lower BOund (ELBO). ELBO 的 gradient我們做最佳化都需要計算 objective function 的 gradient. 讓要找的 $q$ 由參數 $\\nu$ 控制, i.e. $q(z;\\nu)$, 所以我們要找 ELBO 的 gradient 就是對 $\\nu$ 微分. $$\\begin{align} \\mathcal{L}(\\nu)=\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\\\ \\Rightarrow \\nabla_{\\nu}\\mathcal{L}(\\nu)=\\nabla_{\\nu}\\left(\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\right)\\\\ \\mbox{Note }\\neq \\mathbb{E}_{z\\sim q}\\left(\\nabla_{\\nu}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\right)\\\\ \\end{align}$$ 注意 (8) 不能將 Expectation 與 derivative 交換的原因是因為要微分的 $\\nu$ 與要計算的 Expectation 分布 $q$ 有關. 下面會提到一個很重要的技巧, Reparameterization trick, 將 Expectation 與 derivative 交換, 而交換後有什麼好處呢? 下面提到的時候再說明. 回到 (7) 展開 Expectation 繼續計算 gradient, 直接用 NIPS slide 結果如下: 計算一個機率分佈的 Expectation 可用 Monte Carlo method 採樣, 例如採樣 $T$ 個 samples$$\\begin{align} \\mathbb{E}_{z\\sim q}f(z)\\approx\\frac{1}{T}\\sum_{t=1}^Tf(z)\\mbox{, where }z\\sim q \\end{align}$$ 因此 gradient 可以這麼大致找出來, 不過這方法找出來的 gradient 與真實的 gradient 存在很大的誤差, 換句話說, 這個近似的 gradient variance 太大了. 原因兩個 $q$ 本身就還在估計, 本身就不準確了 Monte Carlo method 採樣所造成的誤差 下一段的 reparameterization trick 就可以去除掉上面第一個誤差, 因此估出來的 gradient 就穩定很多. Reparameterization Trick我們用 Gaussian 舉例, 令 $q$ 是 Gaussian, $q(z;\\mu,\\sigma)=\\mathcal{N}(\\mu,\\sigma)$, 其中 $\\nu=${$\\mu,\\sigma$}, 而我們其實可以知道 $z=\\mu+\\sigma \\epsilon$, where $\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})$. 因此:$$\\begin{align} \\mathcal{L}(\\nu)=\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z)-\\log q(z;\\nu)\\right]\\\\ =\\mathbb{E}_{ \\color{red}{ \\epsilon\\sim \\mathcal{N}(0,\\mathbf{I}) } }\\left[\\log p(x, \\color{red}{ \\mu+\\sigma \\epsilon } )-\\log q( \\color{red}{ \\mu+\\sigma \\epsilon } ;\\nu)\\right] \\end{align}$$ 這時候我們計算 ELBO 的 gradient 時, 我們發現 $\\nu$ 與 Expectation 的分佈, $\\mathcal{N}(0,\\mathbf{I})$, 無關了! 因此 (7) 套用上面的 trick 就可以將 Expectation 與 derivative 交換. 結果如下: $$\\begin{align} \\nabla_{\\mu}\\mathcal{L}(\\nu)=\\mathbb{E}_{\\epsilon\\sim \\mathcal{N}(0,\\mathbf{I})}\\left[\\nabla_{\\mu}\\left(\\log p(x,\\mu+\\sigma \\epsilon) - \\log q(\\mu+\\sigma \\epsilon;\\nu)\\right)\\right]\\\\ \\approx\\frac{1}{T}\\sum_{t=1}^T \\nabla_{\\mu}\\left( \\log p(x,\\mu+\\sigma \\epsilon) - \\log q(\\mu+\\sigma \\epsilon;\\nu) \\right)\\mbox{, where }\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})\\\\ \\end{align}$$ 在上一段計算 ELBO gradient 所造成誤差的第一項原因就不存在了, 因此我們用 reparameterization 得到的 gradient 具有很小的 variance. 這個 github 做了實驗, 發現 reperameterization 的確大大降低了估計的 gradient 的 variance. $$\\begin{align} \\nabla_{\\mu}\\left(\\log p(x,\\mu+\\sigma \\epsilon) - \\log q(\\mu+\\sigma \\epsilon;\\nu)\\right) \\end{align}$$ 怎麼計算呢? 我們可以使用 Tensorflow 將要計算 gradient 的 function 寫出來, tf.gradients 就能算 VAEVariational Inference 怎麼跟 Neural Network 扯上關係的? 這實在很神奇.我們先來看看 ELBO 除了 (6) 的寫法, 還可以這麼表示: $$\\begin{align} \\mathcal{L}(\\nu)=\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\\\ =\\mathbb{E}_{z\\sim q}\\left[ \\log p(x|z) + \\log p(z) - log q(z;\\nu) \\right]\\\\ =\\mathbb{E}_{z\\sim q}\\left[ \\log p(x|z)\\right] + \\mathbb{E}_{z\\sim q}\\left[ \\log \\frac{p(z)}{q(z;\\nu)}\\right]\\\\ =\\mathbb{E}_{z\\sim q}\\left[ \\log p(x|z)\\right] - KL(q(z;\\nu)\\|p(z))\\\\ \\end{align}$$ 我們讓 $p(x|z)$ 被參數 $\\theta$ 所控制, 所以最後 ELBO 如下:$$\\begin{align} \\mathcal{L}(\\nu,\\theta)=\\mathbb{E}_{z\\sim q}\\left[ \\log \\color{orange}{ p(x|z,\\theta) } \\right] - KL( \\color{blue}{ q(z;\\nu) } \\|p(z))\\\\ \\end{align}$$ 讓我們用力看 (19) 一分鐘接著在用力看 (19) 一分鐘最後在用力看 (19) 一分鐘 有看出什麼嗎? … 如果沒有, 試著對照下面這張圖 Encoder 和 Decoder 都同時用 NN 來學習, 這裡 $\\nu$ 和 $\\theta$ 分別表示 NN 的參數, 而使用 Reparameterization trick 來計算 ELBO 的 gradient (14) 就相當於在做這兩個 NN 的 backprop. 但是上圖的 Encoder 產生的是一個 pdf, 而給 Decoder 的是一個 sample $z$, 這該怎麼串一起? VAE 的做法就是將 $q(z)$ 設定為 diagonal Gaussian, 然後在這個 diagonal Gaussian 採樣出 $T$ 個 $z$ 就可以丟給 Decoder. 使用 diagonal Gaussian 有兩個好處: 我們可以用 reparameterization trick, 因此採樣只在標準高斯上採樣, 自然地 Encoder 的 output 就是 $\\mu$ 和 $\\sigma$ 了. (19)的 KL 項直接就有 closed form solution, 免掉算 expectation (假設$p(z)$也是Gaussian的話) 根據1, 架構改動如下: 將原來的 ELBO (10) 轉成 (19) 來看的話, 還可以看出一些資訊.當最大化 (19) 的時候 RHS 第一項要愈大愈好 (likelihood 愈大愈好), 因此這一項代表 reconstruct error 愈小愈好. RHS 第二項, 也就是 $KL(q(z;\\nu)\\Vert p(z))$ 則要愈小愈好. 因此會傾向於讓 $q(z;\\nu)$ 愈接近 $p(z)$ 愈好. 這可以看做 regularization. 但是別忘了一開始說 VI 的做法就是藉由最大化 ELBO 來迫使 $q(z;\\nu)$ 接近 $p(z|x)$, 而上面才說最大化 ELBO 會傾向於讓 $q(z;\\nu)$ 接近 $p(z)$.這串起來就說 $q(z;\\nu)$ 接近 $p(z|x)$ 接近 $p(z)$. 在 VAE 論文裡就將 $p(z)$ 直接設定為 $\\mathcal{N}(0,\\mathbf{I})$. 因此整個 VAE 訓練完的 Encoder 的 $z$ 分布會有高斯分布的情形. Conditional VAE (CVAE)原來的 VAE 無法控制要生成某些類別的圖像, 也就是隨機產生 $z$ 不知道這會對應到哪個類別. CVAE 可以根據條件來產生圖像, 也就是除了給 $z$ 之外需要再給 $c$ (類別) 資訊來生成圖像. 怎麼辦到的呢? 方法簡單到我嚇一跳, 看原本論文有點迷迷糊糊, 但這篇文章解釋得很清楚! 簡單來說將原來的推倒全部加上 condition on $c$ 的條件. 從 (4) 出發修改如下: $$\\begin{align} \\log p(x \\color{red}{ | c } ) =KL\\left(q(z \\color{red}{ | c } )\\Vert p(z|x, \\color{red}{ c } )\\right)+ \\sum_z q(z \\color{red}{ | c } )\\log\\frac{p(x,z \\color{red}{ | c } )}{q(z \\color{red}{ | c } )} \\\\ \\end{align}$$ 用推導 VAE 一模一樣的流程, 其實什麼都沒做, 只是全部 conditioned on $c$ 得到 (19) 的 condition 版本 $$\\begin{align} \\mathcal{L}(\\nu,\\theta \\color{red}{ | c } )=\\mathbb{E}_{z\\sim q}\\left[ \\log \\color{orange}{ p(x|z,\\theta, \\color{red}{ c } ) } \\right] - KL( \\color{blue}{ q(z;\\nu \\color{red}{ | c } ) } \\|p(z))\\\\ \\end{align}$$ 這說明了我們在學 Encoder 和 Decoder 的 NN 時必須加入 conditioned on $c$ 這個條件! NN 怎麼做到這點呢? 很暴力, 直接將 class 的 one-hot 跟原來的 input concate 起來就當成是 condition 了. 因此 CVAE 的架構如下: 實作細節就不多說了, 直接參考 codes 由於我們的 condition 是 one-hot, 如果同時將兩個 label 設定為 1, 是不是就能 conditioned on two classes 呢? 實驗如下 conditioned on ‘0’ and ‘4’ conditioned on ‘1’ and ‘3’ 另外, 如果給的 condition 值比較小, 是不是就可以產生比較不是那麼確定的 image 呢? 我們嘗試 conditioned on ‘4’ 且值從 0.1 (weak) 到 1.0 (strong), 結果如下: 這個 condition 值大小還真有反應強度呢! Neural network 真的很神奇阿~ Mean Field VI讓我們拉回 VI. Mean Field 進一步限制了 $q$ 的範圍, 它假設所有控制 $q$ 的參數 {$\\nu_i$} 都是互相獨立的, 這樣所形成的函數空間稱為 mean-field family. 接著採取 coordinate ascent 方式, 針對每個 $\\nu_i$ 獨立 update. 這種 fatorized 的 $q$ 一個問題是 estimate 出來的分布會太 compact, 原因是我們使用的指標是 $KL(q|p)$, 詳細參考 PRML Fig 10.2. 放上 NIPS 2016 slides, 符號會跟本文有些不同, 不過總結得很好: 另外想了解更多 Mean Field VI 或是透過例子了解, 推薦以以下兩個資料: Variational Inference tutorial series by Chieh Wu Variational Coin Toss by Björn Smedman Reference Variational Inference tutorial series by Chieh Wu Variational Inference: Foundations and Modern Methods (NIPS 2016 tutorial) Reparameterization Trick Goker Erdogan 有很好的 VAE, VI 文章 Conditional VAE 原論文 Conditional VAE 好文章 Variational Coin Toss by Björn Smedman My CVAE TF Practice Appendix: EM 跟 VI 很像阿在一般 EM 的設定上, 我們是希望找到一組參數 $\\tilde{\\theta}$ 可以讓 marginal likelihood $\\log p(x|\\theta)$ 最大, formally speaking: $$\\begin{align} \\tilde{\\theta}=\\arg\\max_\\theta \\log p(x|\\theta) \\end{align}$$ 如同 (4) 和 (5), 此時要求的變數不再是 $q$, 而是 $\\theta$: $$\\begin{align} \\log p(x|\\theta)=KL\\left(q(z)\\Vert p(z|x,\\theta)\\right)+\\sum_z q(z)\\log\\frac{p(x,z|\\theta)}{q(z)}\\\\ =KL\\left(q(z)\\Vert p(z|x,\\theta)\\right)+ \\color{orange}{ \\mathcal{L}(q,\\theta) } \\\\ \\end{align}$$ 此時的 $\\log p(x|\\theta)$ 不再是固定的 (VI是), 而是我們希望愈大愈好. 而我們知道 $\\mathcal{L}(q,\\theta)$ 是它的 lower bound 這點不變, 因此如果 lower bound 愈大, 則我們的 $\\log p(x|\\theta)$ 就當然可能愈大. 首先注意到 (23) 和 (24) 針對任何的 $q$ 和 $\\theta$ 等式都成立, 我們先將 $\\theta$ 用 $\\theta^{old}$ 以及 $q(z)$ 用 $p(z|x,\\theta^{old})$ 代入得到: $$\\begin{align} \\log p(x|\\theta^{old})= KL\\left(p(z|x,\\theta^{old})\\Vert p(z|x,\\theta^{old})\\right)+\\mathcal{L}(p(z|x,\\theta^{old}),\\theta^{old})\\\\ =0+\\mathcal{L}(p(z|x,\\theta^{old}),\\theta^{old})\\\\ \\leq\\max_{\\theta}\\mathcal{L}(p(z|x,\\theta^{old}),\\theta)\\\\ \\end{align}$$ 接著求$$\\begin{align} \\theta^{new}=\\arg\\max_{\\theta} \\mathcal{L}(p(z|x,\\theta^{old}),\\theta) \\end{align}$$ 如此 lower bound 就被我們提高了.(28) 就是 EM 的 M-step, 詳細請看 PRML Ch9.4 或參考下圖理解 “$q(z)$ 用 $p(z|x,\\theta^{old})$ 代入” 這句話其實有問題, 因為關鍵不就是 $p(z|x,\\theta)$ 很難求嗎? 這似乎變成了一個雞生蛋蛋生雞的情況. (就我目前的理解) 所以通常 EM 處理的是 discrete 的 $z$, 然後利用 $\\sum_z p(x,z|\\theta)$ 算出 $p(x|\\theta)$, 接著得到我們要的 $p(z|x,\\theta)$. 等於是直接簡化了, 但 VI 無此限制.","tags":[{"name":"Variational Inference","slug":"Variational-Inference","permalink":"http://yoursite.com/tags/Variational-Inference/"},{"name":"ELBO","slug":"ELBO","permalink":"http://yoursite.com/tags/ELBO/"},{"name":"Variational Auto Encoder (VAE)","slug":"Variational-Auto-Encoder-VAE","permalink":"http://yoursite.com/tags/Variational-Auto-Encoder-VAE/"}]},{"title":"Ensemble Algorithm Summary Notes","date":"2018-09-03T13:45:08.000Z","path":"2018/09/03/Ensemble-Algorithm-Summary-Notes/","text":"這是用自己理解的方式整理了林軒田老師 ML 課程. 其中 Decision tree and Random Forest 沒紀錄. 以前第一次接觸到 Adaboost 的時候就被它深深著迷了, 當時 face detection 可商用算法無不採用經典的 Viola and Jones adaboost method. 在現在 DNN 成主流的時候, 雖然 adaboost 光環已退去, 但在 data mining, data science 領域 boosting 方法仍是最成功的算法之一. 基本上在 Kaggle 比賽可以看到主要兩大方法, 舉凡聲音影像文字等等的辨識就是 DNN, 其他凡是 data mining 相關的就屬 boosting (xgboost).有趣的是, 近年也有研究人員用 ensemble 的角度看待 DNN, 從這角度就能理解為何一路從 highway network –&gt; skip layer resent –&gt; resnext 的架構演變, 以及為何效果這麼好. 可以參考 “深度神经网络中深度究竟带来了什么？” 很精彩的解釋, 或是 MSR 2017 這篇論文 Deep Convolutional Neural Networks with Merge-and-Run Mappings 筆記內容如下: Bagging (or bootstrap) Adaboost 演算法2.1 Adaboost large margin 解釋2.2 Adaboost exponential error 解釋 Additive Model (a framework) Gradient Boosting Adaboost as an additive model Gradient Boost Decision Tree (GBDT) 待研究: XGBoost (Kaggle 比賽神器) Bagging (or bootstrap)還記得我們在 Why-Aggregation-Work 這篇提到, 當我們有很多 weak learner ${g_t}$ 時, 要得到一個 strong learner $G$ 最簡單的方法就是投票(或平均). 所以一個關鍵問題是要怎麼產生很多的 $g_t$?Bagging (or bootstrap) 提供了一個簡單的方法: 假設 dataset $D$ 有 $N$ 筆資料, bagging 就是從 $D$ 中重複採樣出 $N’$ 筆, 我們稱 $D’$, 然後 $g_t$ 就可以用 $D’$ 訓練出來.既然現在可以方便地產生很多 ${g_t}$, 然後就 $G$ 就採用平均方式, ensemble algorithm 就結束了?! 當然沒有, 別忘了有一個很關鍵的特性是, 當 ${g_t}$ 意見愈分歧時產生出來的 $G$ 效果愈好!那我們就問了, bagging 不就採樣嗎? 我怎麼知道這次採樣出來的 $D’$ 所訓練出來的 $g_t$ 會跟之前一次的意見分歧?我們就是能知道! (神奇吧) 要了解為什麼, 我們必須先將 bagging 擴展一下, 想成是對 weighted $D$ 採樣, 其中每一筆資料 $x_n$ 的 weight $u_n$ 代表抽中的機率. 如果 bagging 是對 weighted $D$ 採樣的話, 在第 t 輪的 $g_t$ 得到方式如下: $$\\begin{align} g_t=\\arg\\min_{h\\in \\mathcal{H}}\\left(\\sum_{n=1}^N u_n^{(t)} \\mathbb{I}[y_n\\neq h(x_n)] \\right) \\end{align}$$ 其中 $\\mathbb{I}[…]$ 表示 indicator function, 條件為 true 則 return 1, otherwise return 0.想法就是我們要設計一組新的權重, 讓新的權重對於 $g_t$ 來說相當於亂猜, 這樣用新權重找出的 $g_t+1$ 就會跟之前的意見分歧了. 具體來說, 新權重要有以下的效果: $$\\begin{align} \\frac{\\sum_{n=1}^N{u_n^{(t+1)} \\mathbb{I}[y_n\\neq g_t(x_n)]}}{\\sum_{n=1}^N{u_n^{(t+1)}}}=\\frac{1}{2} \\end{align}$$ 物理意義就是對於 $g_t$ 來說$$\\begin{align} \\mbox{for weak learner }g_t\\mbox{: }\\left(\\mbox{total }u_n^{(t+1)}\\mbox{ of incorrect}\\right)= \\left(\\mbox{total }u_n^{(t+1)}\\mbox{ of correct}\\right) \\end{align}$$ 所以新的權重調整方式其實很簡單, 用一個例子解釋. 假如 $u_n^t$ incorrect 合是 300, $u_n^t$ correct 合是 500. 我們只要把之前的 $u_n^t$ incorrect部分都乘 500, 而 correct 部分乘 300就可以了.或者我們這麼寫, 定義 $\\epsilon_t=300/(300+500)$, 則$$\\begin{align} u_n^{(t+1)}=u_n^{(t)}(1-\\epsilon_t) \\mbox{, if } y_n\\neq g_t(x_n)\\\\ u_n^{(t+1)}=u_n^{(t)}\\epsilon_t \\mbox{, if } y_n = g_t(x_n)\\\\ \\end{align}$$ 或通常也可以這麼計算 所以目前為止, 我們可以用 bagging 的方式 (對 weighted data) 產生出看似相當意見不同的 $g_t$, 那最後的 $G$ 用平均就可以了嗎? 可能不大好, 因為 $g_t$ 是針對某一種權重的 dataset 好, 不代表對原來沒有權重 (或uniform權重) 的 dataset 是好的.既然直接平均可能不夠好, 不如就用 linear combination 方式組合 $g_t$ 吧, 不過組合的 coefficients 是需要巧思設計的. 而 Adaboost 就設計出了一種組合方式, 能證明這種組合方式會使得 training error 收斂至0. (另一種用 additive model 的解釋方式為這樣的 coefficient 設計方式相當於用 steepest descent 並選擇最佳的步長). 這些會在文章下面說明. Adaboost 演算法 Adaboost large margin 解釋一般來說, model 愈複雜愈容易 overfit, 不過很特別的是 adaboost 隨著 iteration 結合愈多 weak learners 反而不會有容易 overfit 的現象. 其中一種解釋方式是 adaboost 具有類似 SVM 的 large margin 效果.我們首先分析一下第 t+1 次 iteration, dataset 的 weights$$\\begin{align} u_n^{(t+1)}=u_n^{(t)}\\diamond_t^{-y_n g_t(x_n)}\\\\ =u_n^{(t)}\\exp (-y_n \\alpha_t g_t(x_n)) \\end{align}$$ 我們這裡使用 binary classification 來說明, 其中 $y_n,g_t(x_n)\\in${-1,+1}, 式 (6) 可以從上一段 “Adaboost 演算法” 的圖中步驟2的 update 式子看出. 而式 (7) 從 $\\diamond_t$ 定義得到.上式可以一路展開到開頭 (iteration 1), 如下: $$\\begin{align} u_n^{(T+1)}=u_n^{(1)}\\prod_{t=1}^T \\exp (-y_n \\alpha_t g_t(x_n)) \\\\ =\\frac{1}{N}\\exp\\left(-y_n \\color{orange}{ \\sum_{t=1}^T \\alpha_t g_t(x_n) } \\right) \\end{align}$$ 有發現嗎? 橘色的部分其實就是我們的 $G$ $$\\begin{align} G(x_n)=sign\\left( \\color{orange}{ \\sum_{t=1}^T \\alpha_t g_t(x_n) } \\right) \\end{align}$$ 而如果將 $\\alpha_t$ 看成是 t-th coefficient, $g_t(x_n)$ 看成是 t-th 維度的特徵, 橘色部分就等同於 unnormalized margin. (除以 coefficients 的 norm 就是 margin了)Adaboost 可以證明 (with exponential decay) $$\\begin{align} \\sum_{n=1}^N u_n^{(t)}\\rightarrow 0\\mbox{, for }t\\rightarrow 0 \\end{align}$$ 這意味著什麼? 說明了隨著 iteration 增加, 橘色的值會愈大, 等同於我們的 $G$ 對於資料的 margin 會愈大.證明可參考 李航 統計學習方法 p142 Adaboost exponential error 解釋其實單看式 (9) 我們完全可以把它當成 error function. 重寫一下: $$\\begin{align} u_n^{(T+1)}=\\frac{1}{N}\\exp\\left(-y_n \\color{orange}{ \\sum_{t=1}^T \\alpha_t g_t(x_n) } \\right)\\\\ =\\frac{1}{N}\\exp\\left(-y_n \\color{orange}{ f_T(x_n) } \\right) \\end{align}$$ 怎麼說呢? 其實橘色部分我們可想成是該筆資料 $x_n$ 的分數, 記做 $f_T(x_n)$, 當 $y_n=+1$ 時, 如果 $f_T(x_n)$ 很小則會導致 $\\exp(-y_n f_T(x_n))$ 會很大, 同理當 $y_n=-1$ 時, 如果 $f_T(x_n)$ 很大則會導致 $\\exp(-y_n f_T(x_n))$ 會很大. 因此 $\\exp(-y_n f_T(x_n))$ 可以當成 error function 來 minimize.而它跟 0-1 error function 有如下的關係: 而我們知道 Adaboost 滿足式 (11), 等同於說明 exponential error 收斂. 由於 upper bound 的關係也導致了 0-1 error 收斂.聽到有個方法可以使 error 迅速收斂到 0, 這不是太完美了嗎? 別高興得太早, 因為這個 error 是 inside error. 有學過 ML 的童鞋就應該會警覺到當 inside error 為 0, 意味著非常容易 overfit! 好在實作上 Adaboost 卻不是那麼容易 (原因在上一段 large margin 的解釋), 這就帶來了一個好處, 就是在使用 Adaboost 的時候, 我們可以很放心的直接訓練多次 iteration, 甚至到 inside error 接近 0, 最後的 outside test 也不會壞掉. 這特性倒是挺方便的. AdaBoost 小結論 我們希望藉由融合很多 {$g_t$} 來得到強大的 $G$, 同時我們知道 {$g_t$} 之間意見愈分歧愈好.每一個 $g_t$ 都是根據當前 weighted dataset 得到的. 利用調整資料權重的方式來讓上一次的 $g_t$ 表現很差, 這樣新權重的 dataset 訓練出來的 $g$ 就會跟之前的看法分歧.Adaboost 再利用一種頗為巧思的線性組合方式來融合 {$g_t$}, 最終得到強大的 $G$ Additive Model (a framework)這是非常重要的一個框架, Adaboost 在這框架下可視為它的一個 special case, 同時著名的 Gradient Boost Decision Tree (GBDT) 也是基於此框架下的演算法. 通常 supervised learning 就是在學習 input and output 之間的 mapping function $f$, 簡單講, 直接學一個好的 $f$ 可能很困難, 所以不如使用 greedy 方式, 就是從目前的 $f_t$ 出發, 考慮怎麼修正現在的 $f_t$ 來使得 error 更小. 嚴謹一點數學描述如下: 考慮 additive model$$\\begin{align} f_T(x)=\\sum_{t=1}^T \\alpha_t g_t(x) \\end{align}$$ 其中, $g_t(x)$ 為第 t 次學到的 base learner, $\\alpha_t$ 為它的權重.定義 $L(y,f(x))$ 為 loss (or error) function, 所以我們要找的修正的 mapping function 如下: $$\\begin{align} (\\alpha_T,g_T)=\\arg\\min_{\\eta,h}\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta h(x_n)) \\end{align}$$ 用上式的方法找到要修正的 mapping function 因此 mapping function 更新如下: $$\\begin{align} f_T(x)=f_{T-1}(x)+\\alpha_T g_T(x) \\end{align}$$ 我們可以想成是在函數空間做 gradient descent. 每一次就是找一個 descent direction, 在這裡就是 $h$, 然後設定合適的步長 $\\eta$, 這麼想就是最佳化的 gradient descent 了. Gradient BoostingAdditive model framework 很簡單, 難的地方在那個 $\\arg\\min$ 式 (15). 而 Gradient Boosting 可以說是一種明確實現 Additive model 的方式, 我們可以將 $\\eta$ 和 $h$ 分開找, 例如先找 $h$: $$\\begin{align} &amp;\\min_h\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta h(x_n))\\\\ &amp;\\mbox{by Taylor: }\\simeq \\min_h\\sum_{n=1}^N\\left(L(y_n,f_{T-1}(x_n))+\\eta h(x_n) \\color{red}{ \\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right) _{f=f_{T-1}} } \\right)\\\\ \\end{align}$$ Taylor 展開式那邊可以這麼想 $$\\begin{align} &amp;\\mbox{將 }L(y_n, \\color{green}{ f_{T-1}(x_n) }+ \\color{blue}{ \\eta h(x_n) } )\\mbox{ 看作 }\\hat{L}( \\color{green}{ \\tilde{x} }+ \\color{blue}{ \\delta } )\\\\ &amp;\\mbox{因此 by Taylor } \\simeq \\hat{L}( \\color{green}{\\tilde{x}} )+ \\color{blue}{\\delta} \\left(\\frac{\\partial \\hat{L}(x) }{\\partial x}\\right)_{x= \\color{green}{\\tilde{x}} } \\end{align}$$ 上面紅色部分在計算的時候是一個固定值, 我們先令為 $$\\begin{align} \\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right) _{f=f_{T-1}}= \\color{red}{-\\tilde{y}_n} \\end{align}$$ 所以 (18) 變成 $$\\begin{align} &amp;= \\min_h\\sum_{n=1}^N\\left(L(y_n,f_{T-1}(x_n)) \\color{red}{-} \\eta h(x_n) \\color{red}{ \\tilde{y_n} } \\right)\\\\ &amp;\\mbox{去掉與}h\\mbox{無關項並補上}2=\\min_h \\sum_{n=1}^N \\left(-2h(x_n)\\tilde{y}_n\\right) \\end{align}$$ 很明顯, 如果 $h$ 無限制, 則解為 $h=\\infty$, 這顯然不是我們要的, 在 optimization 的時候, 我們需要的只是 gradient 的方向, 而不是大小, 大小可以由 stepsize 控制. 不過如果加上 $norm(h)=1$ 條件並使用 Lagrange Multipliers 會較複雜, 實作上我們就直接將 $norm(h)$ 當作一個 penality 加在 loss 裡就可以. 因此 (23) 修改如下: $$\\begin{align} =\\min_h \\sum_{n=1}^N \\left(-2h(x_n)\\tilde{y}_n+(h(x_n))^2\\right) \\end{align}$$ 湊齊平方項會變成 (之前加的2是為了這裡湊平方項) $$\\begin{align} =\\min_h \\sum_{n=1}^N \\left( \\mbox{const}+\\left(h(x_n)-\\tilde{y}_n\\right)^2 \\right) \\end{align}$$ OK! 到這裡我們發現了一個重要的解釋, $h$ 的找法就是對 $\\tilde{y}_n$ 做 sqaure error regression! 得到 $g_T=h$ 後, 那麼步長 $\\eta$ 呢? $$\\begin{align} \\alpha_T=\\min_{\\eta}\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta g_T(x_n))\\\\ \\end{align}$$ 這個解通常很好算, 令 $L$ 微分為 0 即可, 是個單變量求解. 到目前為止, 我們可以將整個 Gradient Boost 演算法列出來了: $$\\begin{align} &amp;\\mbox{1. Init }g_0(x)\\\\ &amp;\\mbox{2. For }t=1~T\\mbox{ do:}\\\\ &amp;\\mbox{3. }\\tilde{y}_n=-\\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right)_{f=f_{t-1}}\\mbox{, n=1~N}\\\\ &amp;\\mbox{4. }g_t=\\arg\\min_h\\left(h(x_n)-\\tilde{y}_n\\right)^2\\\\ &amp;\\mbox{5. }\\alpha_T=\\arg\\min_{\\eta}\\sum_{n=1}^N L\\left(y_n,f_{t-1}(x_n)+\\eta g_t(x_n)\\right)\\\\ &amp;\\mbox{6. }f_t(x)=f_{t-1}(x)+\\alpha_t g_t(x) \\end{align}$$ Adaboost as an additive model將 Adaboost 套用 additive model framework 時會是什麼情況?首先 loss 是 exponential loss, 然後一樣用 binary classification 來說明, 其中 $y_n,g_t(x_n)\\in${-1,+1}, 則我們要找的 $h$ 如下 (對照 (12) and (13) 並使用 additive model (14) 的架構): $$\\begin{align} g_T=\\min_h\\sum_{n=1}^N\\exp\\left(-y_n\\left(f_{T-1}(x_n)+\\eta h(x_n)\\right)\\right)\\\\ =\\min_h\\sum_{n=1}^N u_n^{(T)}\\exp(-y_n\\eta h(x_n))\\\\ \\simeq\\min_h\\sum_{n=1}^N u_n^{(T)}(1-y_n\\eta h(x_n))\\\\ =\\min_h\\sum_{n=1}^N u_n^{(T)}(-y_n h(x_n))\\\\ \\end{align}$$ (33) 到 (34) 使用 $u_n^{(T)}$ 的定義, 參考 (13). 而最後的 (36) 表明了實際上就是選擇讓 training data 在新的 weighted dataset 下表現最好的那個 $h$, 具體原因看下圖.這不正是 Adaboost 選擇 weak learner 的方式嗎? 最後別忘了 stepsize, 將 (34) 換一下變數, $h$ 變 $\\eta$: $$\\begin{align} \\alpha_T=\\arg\\min_{\\eta}\\sum_{n=1}^N u_n^{(T)}\\exp(-y_n \\eta g_t(x_n))\\\\ \\end{align}$$ 兩種情況:$$\\begin{align} \\mbox{1. }y_n=g_t(x_n)\\mbox{: }u_n^{(T)}\\exp(-\\eta)\\\\ \\mbox{2. }y_n\\neq g_t(x_n)\\mbox{: }u_n^{(T)}\\exp(+\\eta)\\\\ \\end{align}$$ 所以$$\\begin{align} \\alpha_T=\\arg\\min_{\\eta}\\left(\\sum_{n=1}^N u_n^{(T)}\\right) \\cdot \\left(\\left(1-\\epsilon_T\\right)\\exp\\left(-\\eta\\right)+\\epsilon_T\\exp\\left(+\\eta\\right)\\right) \\end{align}$$ 令微分為 0, 我們可以很容易得到 $$\\begin{align} \\alpha_T = \\ln\\sqrt{\\frac{1-\\epsilon_T}{\\epsilon_T}} \\end{align}$$ 這正好也就是 adaboost 所計算的方式! 總結一下, Adaboost 在 additive model 框架下, 相當於使用 steepest gradient descent 方式在函數空間找 weaker learner, 並且將 stepsize 指定為最佳步長. Gradient Boost Decision Tree (GBDT)Gradient Boost 很棒的一個特性是 error function 沒限定, 例如使用 exponential error 就是 adaboost, 而另一個常用的是 sqaure error.當使用 square error 時, $\\tilde{y}_n$ 就會變成 $(y_n-x_n)$ 也就是 residual. 對照 GradientBoost (27)~(32) 來看, 我們發現整個演算法變成對每一次 iteration 的 residual 做 regression.另外在實務上 base learner 常常使用 Decision Tree (因為 decision tree 有很多好處: 可解釋性、訓練快、可處理缺失資料…), 不過這就要特別注意了, 因為如果長成 fully growed tree 就直接把 residual regression 到 0 了. 因此, decision tree 需要 regularization, 而實務上採用 pruned tree. 整個 GBDT 節自課程 slide 如下: XGBoost這篇文章 XGBoost的原理 介紹得很好 幾個重點整理, XGBoost 基本上也是 gradient boost 的一種, 比較特別的是泰勒展展開 (18) 使用到二階導函數: $$\\begin{align} &amp;\\min_h\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta h(x_n))\\\\ &amp;\\simeq \\min_h\\sum_{n=1}^N\\left(L(y_n,f_{T-1}(x_n))+\\eta h(x_n) \\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right) _{f=f_{T-1}}\\\\ \\color{red} { +\\eta^2h^2(x_n)\\left(\\frac{\\partial^2 L(y_n,f)}{\\partial^2 f}\\right)_{f=f_{T-1}} } \\right)\\\\ &amp;=\\min_h\\sum_{n=1}^N \\left( L(y_n,f_{T-1}(x_n)) + \\eta h(x_n)\\mbox{Gradient}_n + \\frac{\\eta^2h^2(x_n)}{2}\\mbox{Hessian}_n \\right)\\\\ &amp;=\\min_h\\sum_{n=1}^N \\left( \\eta h(x_n)\\mbox{Gradient}_n + \\frac{\\eta^2h^2(x_n)}{2}\\mbox{Hessian}_n \\right)\\\\ \\end{align}$$ 最後再加上一個 regularization term $$\\begin{align} &amp;=\\min_h\\sum_{n=1}^N \\left( \\eta h(x_n)\\mbox{Gradient}_n + \\frac{\\eta^2h^2(x_n)}{2}\\mbox{Hessian}_n \\right) + \\Omega(h)\\\\ \\end{align}$$ 針對 (46) 要找到最好的 $h$, 如果使用 Decision Tree, $\\Omega(h)$ 可以使用樹的深度、葉子數量、葉子值的大小等等計算. 但關鍵是如何有效率地找到很好的 $h$, 而在 Decision Tree 此問題相當於如何有效率的對 Tree 做 splitting. XGBoost 文章使用非常有效率的近似方法, 並且該方法可以很好的並行加速. 對於 xgboost 就只粗淺的了解到這了, 也還沒有真的有什麼調整的經驗, 就把這個課題放在 todo list 吧. Reference 林軒田老師 ML 課程 李航 統計學習方法 Why-Aggregation-Work 以前 Adaboost and face detection paper survey 其中Rapid object detection using a boosted cascade of simple features, 2001, cited 17597 深度神经网络中深度究竟带来了什么？ Deep Convolutional Neural Networks with Merge-and-Run Mappings XGBoost的原理 XGBoost: A Scalable Tree Boosting System","tags":[{"name":"bagging","slug":"bagging","permalink":"http://yoursite.com/tags/bagging/"},{"name":"Adaboost","slug":"Adaboost","permalink":"http://yoursite.com/tags/Adaboost/"},{"name":"Gradient Boost","slug":"Gradient-Boost","permalink":"http://yoursite.com/tags/Gradient-Boost/"}]},{"title":"TF Notes (5), GRU in Tensorflow","date":"2018-07-30T15:29:01.000Z","path":"2018/07/30/TF-Notes-GRU-in-Tensorflow/","text":"小筆記. Tensorflow 裡實作的 GRU 跟 Colah’s blog 描述的 GRU 有些不太一樣. 所以做了一下 TF 的 GRU 結構. 圖比較醜, 我盡力了… XD TF 的 GRU 結構 u 可以想成是原來 LSTM 的 forget gate, 而 c 表示要在 memory cell 中需要記住的內容. 這個要記住的內容簡單講是用一個 gate (r) 來控制之前的 state 有多少比例保留, concate input 後做 activation transform 後得到. 可以對照下面 tf source codes. TF Source Codesrnn_cell_impl.py 12345678910111213141516171819def call(self, inputs, state): \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\" gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._gate_kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias) value = math_ops.sigmoid(gate_inputs) r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1) r_state = r * state candidate = math_ops.matmul( array_ops.concat([inputs, r_state], 1), self._candidate_kernel) candidate = nn_ops.bias_add(candidate, self._candidate_bias) c = self._activation(candidate) new_h = u * state + (1 - u) * c return new_h, new_h","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"GRU","slug":"GRU","permalink":"http://yoursite.com/tags/GRU/"}]},{"title":"(what is) Probabilistic Graphical Models","date":"2018-06-16T02:27:30.000Z","path":"2018/06/16/what-is-Probabilistic-Graphical-Model/","text":"本篇主要介紹什麼是 PGM, 以及一個很重要的應用 Part-of-Speech tagging. PGM 的部分主要圍繞在 “它是什麼?” 也就是 Koller 課程的 Representation. Inference 不討論, 因為自己也沒讀很深入 (汗), 而 Learning 就相當於 ML 裡的 training, 會在介紹 POS 時推導一下. 文章結構如下: What is Probabilistic Graphical Model (PGM)? What is Bayesian Network (BN)? What is Markov Network (MN)? (or Markov Random Field) What is Conditional Random Field (CRF)? Part-of-Speech (POS) Tagging References 文長… What is Probabilistic Graphical Model (PGM)?它是描述 pdf 的一種方式, 不同的描述方式如 directed/undirected graphical model, or Factor Graph 所能描述的 pdf 範圍是不同的. (ref: PRML) 其中 P 代表所有 distributions 的集合, U 和 D 分別表示 undirected 和 directed graphical models. 以有向無向圖來分如下: Directed Acyclic Graph (DAG): Bayesian Network Undirected Graph: Markov Network 注意 BN 除了 directed 之外, 還需要 acyclic. 有了圖之後, 怎麼跟 distribution 產生連結的? 下面我們分別介紹 BN and MN. What is Bayesian Network (BN)?對於一個任意的 distibution over variables $x_1,…,x_V$ 我們可以用基本的 chain rule 拆解如下: $$\\begin{align} p(x_{1:V})=p(x_1)p(x_2|x_1)p(x_3|x_{1:2})...p(x_V|x_{1:V-1}) \\end{align}$$ 變數的 order 可以任意排列, 舉例來說 $$\\begin{align} p(x,y,z)\\\\ =p(x)p(y|x)p(z|x,y)\\\\ =p(x)p(z|x)p(y|x,z) \\end{align}$$ 基於這種拆解我們可以很自然地想到, 不如把每個變數都當成 nodes, 再將 conditioning 的關係用 edges 連起來. 因此基本可以這麼表達: $$\\begin{align} p(x_{1:V}|G)=\\prod_{t=1}^{V}p(x_t|pa(x_t)) \\end{align}$$ 其中 $pa(x_t)$ 表示 node $x_t$ 的 parent nodes. 我們用式 (3) 和 (4) 當例子就可以畫出如下的圖: 可以發現同一個 pdf 可以畫出多個 BN, 因此表達方式不是唯一. Conditioning V.S. Independency接著我們可能會想, 如果我們希望對 pdf 加一些獨立條件呢, 譬如如果希望 $x \\perp y$, 是不是可以直接將圖中的 $x$ 和 $y$ 的 edge 拔掉就可以了呢? 先破題, 答案是不行. 同樣以上面的例子解釋, 如果我們用拔掉 edge 的話, 圖變成: 事實上這兩個圖已經各自表示不同的 distribution 了. 特別要注意在右圖中拔掉 $x$ and $y$ 的 edge 沒有造成 $x \\perp y$. 解釋如下: 那究竟該如何從一個圖直接看出變數之間是否獨立? 為了解答這個問題, 我們先從簡單的三個 nodes 開始 Flow of Influence三個 nodes 的 DAG 圖本質上就分以下三類, 其中 given 的變數我們通常以實心圓表示 我們就個別討論 需要特別注意的是 case 3 的 v-structure, 行為跟其他兩種相反. 一種好記的方式是, 我們假設 given 的變量是一個石頭, 而 edges 可以想成是水流, 所以 given 變量就把水流擋住, 因此會造成獨立. 唯一個例外就是 v-structure, 行為剛好相反. Active Trail in BN我們可以很容易將三個 nodes 的 trail 擴展成 $V$ 個 nodes 的 trail. 因此可以很方便的觀察某條 trail 起始的 node 能否影響到最後的 node. d-separation繼續擴展! 我們假設在 BN $G$ 上 node $x$ and $y$ 有 $N$ 條 trails. 我們則可以藉由檢查每條 trail 是否 active 最終就會知道 $x$ 能否影響到 $y$. 需要注意的是, 這些 d-separation 條件我們都可以直接從給定的 $G$ 上直接讀出來 (對於 distribution 沒有任何假設), 為了方便我們定義以下兩個 terms $$\\begin{align} CI(G)=\\{\\textbf{d-sep}(x,y|z)|x,y,z\\textbf{ in }G\\}\\\\ CI(p)=\\{(x \\perp y|z)|x,y,z\\textbf{ in }G\\}\\\\ \\end{align}$$ $CI(G)$ 所列出的 statements 是由 d-sep 所提供, 也就是說從 $G$ 直接讀出來的, 而 $CI(p)$ 才是真的對於 distribution $p$ 來說所有條件獨立的 statements. OK, 到目前為止, 給定一個 BN $G$, 和一個 distribution $p$ (注意 $p$ 不一定可以被 $G$ 所表示), 他們之間的關係到底是什麼? 下面就要引出非常重要的定理 Factorization and Independent 的關係來說明 Factorization and Independent 白話文: 假設 $p$ 剛好可以寫成 $G$ 的 factorization 型式 (式 (5)), 則所有 $G$ 指出需要 $\\perp$ 的 statements (根據 d-sep 所列), $p$ 都滿足 白話文: 假設所有 $G$ 指出需要 $\\perp$ 的 statements (根據 d-sep 所列), $p$ 都滿足, 則 $p$ 可以寫成 $G$ 的 factorization 型式 (式 (5)) 我們用 PRML book 裡一個具體的描述來說明 Thm1 and Thm2 之間的關係 給定一個 $G$, 就好像一個篩子一樣, 根據兩種方式篩選 distribution $p$ 剛好可以寫成 $G$ 的 factorization 型式 (式 (5)) $G$ 指出需要 $\\perp$ 的 statements (根據 d-sep 所列), 剛好 $p$ 都滿足 用上面兩種篩選方式最後篩出來的 distributions 分別稱為 $DF1$ and $DF2$ 兩個 sets. 定理告訴我們它們式同一個集合! Example把下圖的 joint pdf 寫出來: 使用式 (5) 的方式寫一下, 讀者很快就發現, 這不就是 HMM 嗎? What is Markov Network (MN)?Factorization在解釋 MN 之前, 先了解一下什麼是 (maximal) clique. 因此, 我們可以用 maximal cliques 來定義一個 MN. $$\\begin{align} p(x)=\\frac{1}{Z}\\prod_{c\\in\\mathcal{C}}\\psi_c(x_c) \\end{align}$$ $\\mathcal{C}$ 是 maximal cliques 的集合. 然後 $Z$ 是一個 normalization term, 目的為使之成為 distribution. $$\\begin{align} Z=\\sum_x\\prod_{c\\in\\mathcal{C}}\\psi_c(x_c) \\end{align}$$ 舉個例子: 用無向圖的方式來表達 distribution 有一個很大的好處就是判斷 Active Trail 和 separation 變得非常非常簡單! 直接看下圖的說明 如同在 BN 時的討論, 給定一個 MN $H$, 和一個 distribution $p$ (注意 $p$ 不一定可以被 $H$ 所表示), 他們之間的關係可以由 Factorization and Independent 的定理來說明 Factorization and Independent我們直接擷取 Kevin Murphy 書所列的定理, Hammersley-Clifford 定理 跟 BN 一樣, factorization iff independence, 但有一個重要的 assumption, 就是 distribution 必須 strictly positive (如上圖紅色框的部分). 我們一樣用 PRML 篩子的觀念來具體化: 描述就跳過了. Example由於有 $p(x)&gt;0$ 的假設在, 因此如果將 factor functions $\\psi(x_c)$ 都使用 $exp$ 來定義的話, 整個 product 相乘後的 distribution 必定滿足 strictly positive. 因此 $exp$ 就不失為一種方便的 modeling 方式了 喘口氣的結論到這裡, 我們可以 用 graph 簡單的表示出 joint pdf (用 factorization). 也可以從 graph 中看出 conditional independence (用 active tail, separation) 因此我們可以針對要 model 的問題利用 graph 來描述 joint pdf 了. 但是光描述好 model 沒用, 我們還需要 inference (test) and learning (train). Inference 非常推薦看 PRML ch8, 講如何對 tree graph 做 sum-product algorithm (belief propagation) 非常精彩. 接著如何推廣到一般 general graph 則可以使用 junction tree algorithm (推薦看這篇文章, 解釋非常棒!). 上述兩種方式都屬於 exact inference, 對於一些情形仍會需要 exponential time 計算, 因此我們需要 variational inference 或 sampling 的方式算 approximation. 最後有關 learning 我們使用接下來的 POS tagging 當範例推導一下. 但別急, 在講 POS 之前我們得先談一個重要的東西, Conditional Random Field. What is Conditional Random Field (CRF)? 如同上圖的說明, 基本上 CRF 仍舊是一個 MN, 最大的差別是 normalization term 如今不再是一個 constant, 而是 depends on conditioning 的變數 $x$. 一個在 sequence labeling 常用的 CRF 模型是 Linear-Chain CRF 有了這些概念後我們就可以說說 POS 了 Part-of-Speech (POS) Tagging擷取自李宏毅教授上課投影片 基本上就是給定一個 word sequence $x$, 我們希望找出哪一個詞性標註的 sequence $y$ 會使得機率最大. 機率最大的那個 $y$ 就是我們要的詞性標註序列. 使用現學現賣的 PGM modeling 知識, 我們可以使用 BN or MN 的方式描述模型 BN: Hidden Markov Model (HMM) MN: Linear chain CRF with log-linear model 有向圖 HMM 方法一樣擷取自李宏毅教授上課投影片 還記得本文前面講 BN 時的 HMM example 嗎? $y$ 就是詞性, $x$ 就是字. HMM 是在 model 給定詞性序列情形下的字序列 distribution. 了解語音辨識的童鞋門應該再熟悉不過了, 只不過這裡問題比較簡單, 在語音辨識裡, 我們不會針對每個 frame 去標註它是屬於哪一個發音的 state, 因此標註其實是 hidden 的. 但在這裡每個 word 都會有一個對應正確答案的詞性標註, 沒有 hidden 資訊, 因此也不需要 EM algorithm, 簡單的 counting 即可做完訓練. that all … 無向圖 CRF 方法精確說是 Linear chain CRF with log-linear model 我們把 log-linear model 的 factor 帶入 linear chain CRF 中, 注意其中 $\\phi$ 是需要定義的特徵函數, 我們這裡先假設可以抽取出 $K$ 維. 因此可以推導如下 實作上我們會針對時間 share weights, 這是因為句子都是長短不一的, 另一方面這樣做也可以大量減少參數量. 所以最後可以簡化成一個 weigth vector $w$ 和我們合併的特徵向量 $f(x,y)$ 的 log-linear model. Learning 目標函數就是在最大化 CRF 的 likelihood. 採用 gradient method. 而 gradient 的推導事實上也不困難, 只要花點耐心即可了解 但是其實我說不困難只說對了一半, 紅色的地方事實上需要跑 inference 才可以得到, 好在 linear-chain 架構下正好可以用 Viterbi 做前向後算計算, 這部分的式子可以跟 “李航 統計學習方法“ 這本書的 p201 式 (11.34) 銜接上, 該式寫出了前向後向計算. ToolCRF++ 做為語音辨識的後處理十分好用的工具, in c++. ReferencesPGM 博大精深, 這個框架很完整且嚴謹, 值得我後續花時間研讀, 有機會看能否將 Koller 的課程上過一次看看. 通常這麼說就表示 …. hmm…你懂得 Bishop PRML book Kevin Murphy book Junction Tree Algorithm 李航 統計學習方法 李宏毅老師 ML 課程","tags":[{"name":"Probabilistic Graphical Models","slug":"Probabilistic-Graphical-Models","permalink":"http://yoursite.com/tags/Probabilistic-Graphical-Models/"},{"name":"Bayesian Network","slug":"Bayesian-Network","permalink":"http://yoursite.com/tags/Bayesian-Network/"},{"name":"Markov Network","slug":"Markov-Network","permalink":"http://yoursite.com/tags/Markov-Network/"},{"name":"Conditional Random Field","slug":"Conditional-Random-Field","permalink":"http://yoursite.com/tags/Conditional-Random-Field/"},{"name":"POS tagging","slug":"POS-tagging","permalink":"http://yoursite.com/tags/POS-tagging/"}]},{"title":"Kaldi Notes (1), I/O in C++ Level","date":"2018-05-31T15:32:43.000Z","path":"2018/05/31/Kaldi-Notes-IO-in-C-Level/","text":"Kaldi I/O C++ Level 筆記, 主要介紹以下幾點, 以及它們在 Kaldi c++ 裡如何關聯: 標準 low-level I/O for Kaldi Object XXXHolder類別: 一個符合標準 low-level I/O 的類別 Kaldi Table Object: &lt;key,value&gt; pairs 組成的 Kaldi 格式檔案 (scp, ark), 其中 value 為 XXXHolder 類別 標準 low-level I/O for Kaldi ObjectKaldi Object 有自己的標準 I/O 介面:12345class SomeKaldiClass &#123; public: void Read(std::istream &amp;is, bool binary); void Write(std::ostream &amp;os, bool binary) const; &#125;; 因此定義了該 Kaldi Class 如何針對 istream 讀取 (ostream 寫入). 在 Kaldi 中, istream/ostream 一般是由 Input/Output(在 util/kaldi-io.h 裡定義) 這個 class 來開啟的. 那為何不用一般的 c++ iostream 開啟一個檔案呢? 這是因為 Kaldi 想要支援更多樣的檔案開啟方式, 稱為 “Extended filenames: rxfilenames and wxfilenames“. 例如可以從 stdin/stdout, pipe, file 和 file with offset 讀取寫入, 詳細請看文檔的 “Extended filenames: rxfilenames and wxfilenames” 部分. 所以 Input/Ouput Class 會自動解析 rxfilenames/wxfilenames 然後開啟 istream/ostream. 開啟後, Kaldi Object 就可以透過標準的 I/O 介面呼叫 Read/Write 方法了. 官網範例如下:123456789101112&#123; // input. bool binary_in; Input ki(some_rxfilename, &amp;binary_in); my_object.Read(ki.Stream(), binary_in); // you can have more than one object in a file: my_other_object.Read(ki.Stream(), binary_in);&#125;// output. note, \"binary\" is probably a command-line option.&#123; Output ko(some_wxfilename, binary); my_object.Write(ko.Stream(), binary);&#125; 有時候會看到更精簡的寫法如下12345678int main(int argc, char *argv[]) &#123; ... std::string rxfilenames = po.GetArg(1); std::string wxfilenames = po.GetArg(2); SomeKaldiClass my_object; ReadKaldiObject(rxfilenames, &amp;my_object); WriteKaldiObject(my_object, wxfilenames, binary);&#125; 其中 ReadKaldiObject and WriteKaldiObject (defined in util/kaldi-io.h) 的作用只是將 Input/Output 開啟 xfilenames 為 iostream, 並傳給 my_object 的標準 I/O 介面包裝起 來而已. 擷取 define 片段如下: 12345678910111213141516171819template &lt;class C&gt; void ReadKaldiObject(const std::string &amp;filename, C *c) &#123; bool binary_in; Input ki(filename, &amp;binary_in); c-&gt;Read(ki.Stream(), binary_in);&#125;// Specialize the template for reading matrices, because we want to be able to// support reading 'ranges' (row and column ranges), like foo.mat[10:20].// 上面的 class C 如果是 Matrix&lt;float&gt; or Matrix&lt;double&gt; 的話, 使用下面兩個定義// Note: 這種方式是 template 的 specialization, 同樣名稱的 template function or class 可以重複出現，只針對某些 type 客製化template &lt;&gt; void ReadKaldiObject(const std::string &amp;filename, Matrix&lt;float&gt; *m);template &lt;&gt; void ReadKaldiObject(const std::string &amp;filename, Matrix&lt;double&gt; *m);template &lt;class C&gt; inline void WriteKaldiObject(const C &amp;c, const std::string &amp;filename, bool binary) &#123; Output ko(filename, binary); c.Write(ko.Stream(), binary);&#125; Kaldi Table ObjectTable Object 不直接透過標準的 Read/Write 操作, 是因為 Table object 的構成是由 &lt;key,value&gt; pairs 組成的, 而 value 才會是一個符合標準 Read/Write 操作的 object. 這種 table 所需要的讀寫可能有很多方式, 譬如 sequential access, random access 等等, 因此單純的 Read/Write 比較不能滿足需求, 更需要的是要有 Next, Done, Key, Value 等等的操作方式. 例如以下範例: 12345678910111213141516std::string feature_rspecifier = \"scp:/tmp/my_orig_features.scp\", transform_rspecifier = \"ark:/tmp/transforms.ark\", feature_wspecifier = \"ark,t:/tmp/new_features.ark\";// there are actually more convenient typedefs for the types below,// e.g. BaseFloatMatrixWriter, SequentialBaseFloatMatrixReader, etc.TableWriter&lt;BaseFloatMatrixHolder&gt; feature_writer(feature_wspecifier);SequentialTableReader&lt;BaseFloatMatrixHolder&gt; feature_reader(feature_rspecifier);RandomAccessTableReader&lt;BaseFloatMatrixHolder&gt; transform_reader(transform_rspecifier);for(; !feature_reader.Done(); feature_reader.Next()) &#123; std::string utt = feature_reader.Key(); if(transform_reader.HasKey(utt)) &#123; Matrix&lt;BaseFloat&gt; new_feats(feature_reader.Value()); ApplyFmllrTransform(new_feats, transform_reader.Value(utt)); feature_writer.Write(utt, new_feats); &#125;&#125; 主要有幾種 table classes:TableWriter, SequentialTableReader, RandomAccessTableReader 等等, 都定義在 util/kaldi-table.h. 我們就以 SequentialTableReader 來舉例. 上面的範例 feature_reader 就是一個 SequentialTableReader, 他的 &lt;key,value&gt; pairs 中的 value 定義為 BaseFloatMatrixHolder 類別 (一個符合標準 low-level I/O 的 Kaldi Class, 等於是多一層包裝). XXXHolder (如 KaldiObjectHolder, BasicHolder, BasicVectorHolder, BasicVectorVectorHolder, …) 指的是符合標準 low-level I/O 的 Kaldi Object, 因此這些 XXXHolder 都可以統一透過 Read/Write 來呼叫. 這些 Holder 的定義在 util/kaldi-holder.h.另外 kaldi-holder.h 最後一行會 include kaldi-holder-inl.h. “-inl” 意思是 inline, 通常會放在相對應沒有 -inl 的 .h 最後面, 用來當作是 inline implementation 用. SequentialTableReader 的定義在 “util/kaldi-table.h”, 擷取要介紹的片段:1234567891011template&lt;class Holder&gt;class SequentialTableReader &#123; public: typedef typename Holder::T T; inline bool Done(); inline std::string Key(); T &amp;Value(); void Next(); private: SequentialTableReaderImplBase&lt;Holder&gt; *impl_;&#125; Done(), Next(), Key(), and Value() 都可以從 feature_reader 看到如何使用, 應該很直覺, 而 Holder 的解釋上面說了. 剩下要說明的是這行 SequentialTableReaderImplBase&lt;Holder&gt; *impl_;. 在呼叫 SequentialTableReader 的 Next() 時, 他實際上呼叫的是 impl_ 的 Next(). 定義在 util/kaldi-table-inl.h 片段: 12345template&lt;class Holder&gt;void SequentialTableReader&lt;Holder&gt;::Next() &#123; CheckImpl(); impl_-&gt;Next();&#125; impl_ 的 class 宣告是 “SequentialTableReaderImplBase”, 該類別的角色是提供一個父類別, 實際上會根據 impl_ 真正的類別呼叫其對應的 Next(), 就是多型的使用. 現在假設 impl_ 真正的類別是 SequentialTableReaderArchiveImpl. 我們可以在 util/kaldi-table-inl.h 看到他的 Next (line 531) 實作如下:123456789virtual void Next() &#123; ... if (holder_.Read(is)) &#123; state_ = kHaveObject; return; &#125; else &#123; ... &#125;&#125; 到這才真正看到透過 XXXHolder 使用 low-level I/O 的 Read()! Kaldi Codes 品質很高阿, 要花不少時間讀, 果然 c++ 底子還是太差了. References Kaldi Project","tags":[{"name":"Kaldi","slug":"Kaldi","permalink":"http://yoursite.com/tags/Kaldi/"}]},{"title":"TF Notes (4), Deconvolution","date":"2018-05-09T11:59:12.000Z","path":"2018/05/09/TF-Notes-deconvolution/","text":"這篇是個小練習, 就兩點: 了解什麼是 deconvolution, 並在 tensorflow 中怎麼用 實作一個 CNN AutoEncoder, Encoder 用 conv2d, Decoder 用 conv2d_transpose What is deconvolution?破題: Deconvolution 的操作就是 kernel tranpose 後的 convolution. 使用李宏毅老師的上課內容, 如下圖: 其實圖已經十分明確了, 因此不多解釋. 另外在 tensorflow 中, 假設我們的 kernel $W$ 為 W.shape = (img_h, img_w, dim1, dim2). 則 tf.nn.conv2d(in_tensor,W,stride,padding) 會將 (dim1,dim2) 看成 (in_dim, out_dim). 而 tf.nn.conv2d_transpose(in_tensor,W,output_shape,stride) 會將 (dim1,dim2) 看成 (out_dim, in_dim), 注意是反過來的. 有兩點多做說明: tf.nn.conv2d_transpose 會自動對 $W$ 做 transpose 之後再 convolution, 因此我們不需要自己做 transpose. tf.nn.conv2d_transpose 需要額外指定 output_shape. 更多 conv/transpose_conv/dilated_conv with stride/padding 有個 非常棒的可視化 結果參考此 github CNN AutoEncoder結構如下圖 直接將 embedding 壓到 2 維, 每個類別的分布情形如下: embedding 是 128 維, 並使用 tSNE 投影到 2 維畫圖如下: Encoder 如下: 1234567891011121314151617181920def Encoder(x): print('Input x got shape=',x.shape) # (None,28,28,1) # Layer 1 encode: Input = (batch_num, img_height, img_width, cNum). Output = (batch_num, img_height/2, img_width/2, layer_dim['conv1']) layer1_en = tf.nn.relu(tf.nn.conv2d(x, weights['conv1'], strides=[1, 1, 1, 1], padding='SAME')) # Avg Pooling layer1_en = tf.nn.avg_pool(layer1_en, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') print('After Layer 1, got shape=',layer1_en.shape) # (None,14,14,32) # Layer 2 encode: Input = (batch_num, img_height/2, img_width/2, layer_dim['conv1']). Output = (batch_num, img_height/4, img_width/4, layer_dim['conv2']) layer2_en = tf.nn.relu(tf.nn.conv2d(layer1_en, weights['conv2'], strides=[1, 1, 1, 1], padding='SAME')) # Avg Pooling layer2_en = tf.nn.avg_pool(layer2_en, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') print('After Layer 2, got shape=',layer2_en.shape) # (None,7,7,64) # Layer embedded: Input = (batch_num, img_height/4 * img_width/4 * layer_dim['conv2']). Output = (batch_num, layer_dim['embedded']) flatten_in = flatten(layer2_en) embedded = tf.matmul(flatten_in,weights['embedded']) print('embedded has shape=',embedded.shape) return embedded Decoder 如下: 12345678910111213141516171819202122def Decoder(embedded): # API: tf.nn.conv2d_transpose = (value, filter, output_shape, strides, padding='SAME', ...) bsize = tf.shape(embedded)[0] # Layer embedded decode: Input = (batch_num, layer_dim['embedded']). Output = (batch_num, in_dim_for_embedded) embedded_t = tf.matmul(embedded,weights['embedded'],transpose_b=True) embedded_t = tf.reshape(embedded_t,[-1, 7, 7, layer_dim['conv2']]) print('embedded_t has shape=',embedded_t.shape) # Layer 2 decode: Input = (batch_num, 7, 7, layer_dim['conv2']). Output = (batch_num, 14, 14, layer_dim['conv1']) layer2_t = tf.nn.relu(tf.nn.conv2d_transpose(embedded_t,weights['conv2t'],[bsize, 14, 14, layer_dim['conv1']], [1, 2, 2, 1])) print('layer2_t has shape=',layer2_t.shape) # Layer 1 decode: Input = (batch_num, 14, 14, layer_dim['conv1']). Output = (batch_num, 28, 28, cNum) layer1_t = tf.nn.relu(tf.nn.conv2d_transpose(layer2_t,weights['conv1t'],[bsize, 28, 28, cNum], [1, 2, 2, 1])) print('layer1_t has shape=',layer1_t.shape) # Layer reconstruct: Input = batch_num x layer_dim['layer1']. Output = batch_num x img_dim. reconstruct = tf.nn.relu(tf.nn.conv2d(layer1_t, weights['reconstruct'], strides=[1, 1, 1, 1], padding='SAME')) - 0.5 print('reconstruct has shape=',reconstruct.shape) return reconstruct AutoEncoder 串起來很容易: 12345def AutoEncoder(x): embedded = Encoder(x) reconstruct = Decoder(embedded) return [embedded, reconstruct] 完整 source codes 參考下面 reference Reference 李宏毅 deconvolution 解釋 tf.nn.conv2d_transpose 說明 conv/transpose_conv/dilated_conv with stride/padding 可視化: github 本篇完整 source codes","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"Computational Graph","slug":"Computational-Graph","permalink":"http://yoursite.com/tags/Computational-Graph/"}]},{"title":"ROS in Self-driving Car system","date":"2018-04-15T11:05:29.000Z","path":"2018/04/15/ROS-in-Self-driving-Car-system/","text":"這是經歷了漫長的時間, 最後的一哩路了….從2016年12月開始, 到2018年4月中, 花了整整一年五個月. 其實我原先打算半年前就畢業的, 但是中途有狀況, 所以只好 term2 完成後停了半年才開始 term3, 也因此到昨天才剛確定畢業! 而昨天剛好也參加了 Udacity 在中國兩周年的會, 見到了 David Sliver 本人, 算是畢業的一個小紀念! 最後的 project 比較有別於以往, 採用 team work 的方式. 我們的 team 共五人, team lead Franz Pucher 德國, Theodore King 美國, 和我. 疑? 另外兩個呢? 對於 project 完全沒貢獻…我不想說了….= = ROS 簡介關於機器人控制和自動車都會使用 ROS (Robot Operating System), ROS 一定要參考 ROS wiki. 本次作業的 ROS 系統擷取課程圖片如下: 看不懂沒關係, 了解 ROS 主要三個概念: Node, Topic, Msg 就清楚上面的圖在幹嘛了. Node 簡單講類似於 class, 可以訂閱某些 Topic, 和發送 Msg 到指定的 Topic. 舉例來說當有某個 Node A 發送一個 msg M 到一個 topic T 時, 如果 Node B 有訂閱 topic T, 則 Node B 會收到 msg M, 並且執行預先設定好的 call back function. 用以下的程式範例舉例: 1234567891011class TrafficLightDetector(object): def __init__(self): rospy.init_node('tl_detector') # 要在開頭就先 init 好這是 ros node ... # 訂閱了一個 topic '/current_pose', 並且如果有 msg 發送到此 topic, 此 node 會收到並且呼叫 call back function self.pose_cb sub = rospy.Subscriber('/current_pose', PoseStamped, self.pose_cb, queue_size=1) # 此 node 會發送 msg 到 topic '/traffic_waypoint' self.upcoming_red_light_pub = rospy.Publisher('/traffic_waypoint', Int32, queue_size=1) def pose_cb(self, msg): self.pose = msg.pose 要注意的是, 由於 topic 運作方式為一旦有其他 node 發送 msg 到此 topic, 有訂閱此 topic 的 node 的 call back function 都會被呼叫. 這就意謂著 topic 如果發送 msg 太頻繁, 導致訂閱的 node 無法及時消化, 則 msg 會掉包. 一種解決方式為使用 rospy.Rate 控制發送的頻率. 但是其實還有另一種傳送 msg 的方式: Service 簡單講 Service 的概念就是 request and response, 不同於 topic, service 會將兩個 node 直接連接起來, 一個發起 request 後, 會等另一個 node response 才會接著做下去. 一個簡單的舉例如下: 12345678910111213141516171819# 需注意 ServiceClassName 要先在 package 裡的 srv folder 定義好class NodeA(object):... # 拿到該 service service = rospy.ServiceProxy('service_name',ServiceClassName) # 拿到可以 request 的 msg instance msg = ServiceClassNameRequest() # 修改 msg 成需要的狀態 ... # 發起 request 並得到 response response = service(msg) class NodeB(object):... rospy.Service('service_name',ServiceClassName, self.handler_func) ... def handler_func(self, msg): # 收到 request 的 msg, 在此 handler function 負責處理如何 response ... 上面的範例使用了兩個 nodes, node A 負責發起 request, 而 node B 負責 response. 另外筆記一些 ros 常用的指令和功能12345678910111213141516171819202122232425262728293031323334353637383940&gt;&gt; roscore # start ROS master# rosrun 可以指定負責要跑哪個 node&gt;&gt; rosrun package_name node_name# node 一多, 可以使用 roslauch 一次執行多個 nodes, 但是要寫好 launch file&gt;&gt; roslaunch launch/launchfile# 列出 active 的 nodes&gt;&gt; rosnode list# 列出 active 的 topics&gt;&gt; rostopic list# 查看某個 topic&gt;&gt; rostopic info topic_name# 將 publish 到此 topic 的 msgs 都即時顯示在 terminal 上&gt;&gt; rostopic echo topic_name# 一般來說 rospy.loginfo('info msg') 會顯示在 /rosout 這個 topic, 因此適合 debug&gt;&gt; rostopic echo /rosout# 查看某個 msg&gt;&gt; rosmsg info msg_name# build 自定義的 ros package&gt;&gt; cd ~/catkin_ws; catkin_make# 檢查 package 的 dependency&gt;&gt; rosdep install -i package_name# 如果將某個 package 加入到自己的 catkin_ws 時, 需加到 catkin_ws/src 資料夾下, 並且重新 make&gt;&gt; cd ~/catkin_ws/src&gt;&gt; git clone 'some packages'&gt;&gt; cd ~/catkin_ws&gt;&gt; catkin_make# Build 完後, 需要 source 才可以將 catkin_ws/src 下的所有 packages 都加到 ros 中&gt;&gt; source ~/catkin_ws/devel/setup.bash Debug 的話 rospy.loginfo, rospy.logwarn, rospy.logerr, rospy.logfatal 很好用, 它們分別會被記錄在以下幾個地方: Self-Driving Car ROS Nodes因此這最後的 project 主要就分成三個部分 Perception:這部分負責收到 /image_color 這個 topic 的影像後, 來找出 traffic sign 在哪裡並且是哪種燈號. 相當於 term1 的 Vehicle Tracking, 我主要負責此部分, 但是沒有使用當時做 project 的 sliding window + svm 方法. 下面會詳細介紹. Planning:負責根據目前車子的位置以及如果有紅燈的話, 必須規劃好新的路徑, 並將期望的速度一併發送給 Control. 相當於 term3 的 Path Planning Control:根據規畫的路徑和速度, 找出可以實際操控的參數 (throttle, brake, steering). 相當於 term2 的 Model Predictive Control. 但我們團隊沒有用 MPC, 而是使用 PID control. Perception Traffic Light由於小弟我不是做 CV 的, 沒這麼多厲害的能力, 因此一開始我也沒打算訓個 YOLO 之類的方法. 重頭開始訓練的話我只能先想到不如用上次 project 的 semantic segmantation 方法, 將認為是 traffic sign 的部分找出來, 接著用簡單的顏色區分一下好了. training set 我使用 Bosch Traffic Light Dataset, 共有 5093 張 images. 很多張影像完全沒有 traffic sign, 因此我就忽略, 並且有些 traffic sign 實在太小, 那種情況也忽略, 最後篩選出 548 張有 traffic signs 的影像並且 resize 成 600x800, 舉個例如下: 注意到用的 semantic segmentation 方法是 pixel level 的, 也就是說每個 pixel 都會去判別 yes/no traffic sign. 而我們看到就算是都有 traffic sign 的影像了, 實際上 pixel 是 traffic sign 所占的比例還是偏低, 這讓我開始有點懷疑是否 DNN 有能力分辨出來. 但是….還真的可以! 現在有種感覺, 有時候針對資料不平均做了一些方式讓每個 class 平均一些, 但是 DNN 的效果其實都沒啥提升, 感覺 DNN 對資料不平均的問題較不敏感 不過由於模擬器的 traffic sign 跟 Bosch 的差太多, 因此效果不大好. 我只好加入了一些模器器下的影像去訓練, 結果就好很多了. 但還是遇到一個問題, 我的 macbook 沒有 GPU, 跑一張影像花了 120 secs, 而一秒鐘 camera 會傳來 8 張影像! 根本處理不了, 關鍵是也不知道 Udacity 它們用自己 GPU 跑起來會多快. 所以我就將影像長寬各縮小一半, 總體速度會降到原來的 1/4. 就算如此還是無法驗證是否夠快. 我們團隊卡在這個無法驗證的狀況很久, 導致可能需要用到延長四周的情形. 最後在 teammate Theodore King 的幫助下, 我們使用了 tf 的 object detection API, 使用 mobilenet 速度快到靠北飛起來. 連 CPU 處理一張影像都只需要不到1秒的時間! 何況使用 GPU. 最終總算有驚無險過關了. 我之前做那麼辛苦幹嘛 閒聊其實 Udacity 規劃相當棒了, 主要幾個部分都有分別的實作過, 最後來個大一統, 真的很有意思. 但我仍要吐槽的是, 搞環境太麻煩了! 模擬器跑在 virtualbox 上, 而我的 virtualbox window 沒法裝好, 只能裝在 macbook, 但 macbook 又沒有 GPU, 導致使用 deep learning 的方法完全不知夠不夠快! 另外, VM 的環境我還搞不定怎麼跟 host share data, 搞得我只好上傳雲端再下載, 最後衰事接踵而來, VM 也搞不定翻牆 (對, 我在網路長城的牆內)…..80%都在搞環境….真的很痛苦 恩, 終於畢業了… 結束了這漫長的旅程. 原以為我會興奮得不得了, 不過可能是因為最後 project 搞環境太痛苦, 加上這樣子的團隊合作其實沒有約束力 (有兩個完全的壟員), 反而解脫感壓過了高興. 但總結來說, 還是很感謝 Udacity 陪伴了我一年多, 並且有了這麼有趣的經驗! 有機會的話, 我還是會繼續上 Udacity 其他課程的. Reference Our github ROS wiki TrafficLight_Detection-TensorFlowAPI Semantic Segmantation Path Planning Model Predictive Control","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"ROS","slug":"ROS","permalink":"http://yoursite.com/tags/ROS/"}]},{"title":"Udacity-Semantic-Segmentation","date":"2018-03-06T11:59:13.000Z","path":"2018/03/06/Udacity-Semantic-Segmentation/","text":"Udacity SDC term 3 第二個 Project 做的是使用 Deep Learning 學習識別 pixel 等級的路面區域. 簡單講就是有如下的 ground truth data, 標示出哪邊是正確的路面, 然後用 Fully Convolutional Network 去對每個 pixel 做識別. Fully-Convolutional-Network (FCN)主要是實作這篇論文 “Fully Convolutional Networks for Semantic Segmentation“ 換言之, 全部都是 convolution layers, 包含使用 1x1 convolution 替換掉原來 Convnet 的 fully-connected-layer, 和使用 deconvolution 做 upsampling. 架構圖如下: 分成 Encoder 和 Decoder 部分. Encoder 使用 pre-trained 好的 VGG16 network, 負責做特徵抽取. 抽取出來的特徵後, 接上 deconvolution layers (需訓練) 搭建而成的 decoder part. 有一個特別之處是使用了 skip 方法. 這個方法是在 decoder 做 upsampling 時, 會加上當初相對應大小的 Encoder layer 資訊. 這樣做論文裡提到會增加整個識別效果. Results效果有點讓我小驚豔, 因為只使用少少的 289 張圖片去訓練而已. 跑出來的測試結果如下: 另外還有一點是縱使已經有 dropout 了, 如果沒有加上 l2 regularization 的話, 會 train 不好! (l2 regularization 真讓我第一次看到有這麼重要), 同樣設定下, 有和沒有 l2 regularization 的差別: Reference Fully Convolutional Networks for Semantic Segmentation Source code github","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"}]},{"title":"Mixtures of Factor Analyzers","date":"2018-02-11T15:23:24.000Z","path":"2018/02/11/Mixtures-of-Factor-Analyzers/","text":"這篇使用 Bishop PRML 的 notations, 同使參考 Zoubin Ghahramani and Geoffrey E. Hinton (沒錯, 就是那位 Hiton, 另外, 第一作者也是神人級別, 劍橋教授, Uber 首席科學家) 1997 年的論文 “The EM Algorithm for Mixtures of Factor Analyzers“, 實作了 Mixtures of Factor Analyzers, 臥槽! 都20年去了! My python implementation, github. 關於 EM 的部分會比較精簡, 想看更多描述推薦直接看 PRML book. 文章主要分三個部分 什麼是 Factor Analysis, 以及它的 EM 解 推廣到 mixtures models 語者識別中很關鍵的 ivector 究竟跟 FA 有什麼關聯? 直接進入正題吧~ Factor Analysis一言以蔽之, sub-space 降維. 假設我們都活在陰魂不散的 Gauss 世界中, 所有 model 都是高斯分布. 我們觀察的資料 $x$ 都是高斯分布, 且都是高維度. 但實際上 $x$ 通常只由少數幾個看不到的變數控制, 一般稱這些看不到的變數為 latent variable $z$. 如下圖舉例: 所以我們的主要問題就是, 怎麼對 Gaussian distribution 建立 sub-space 模型? 答案就是使用 Linear Gaussian Model. 一些 notations 定義: $x$ 表示我們的 observation, 維度是 $D$. $z$ 是我們的 latent variable, 維度是 $K$. 我們一般都期望 $K \\ll D$. $x$ and $z$ follow linear-Gaussian framework, 有如下的關係: $$\\begin{align} p(z)=N(z|0,I) \\\\ p(x|z)=N(x|Wz+\\mu,\\Psi) \\\\ \\end{align}$$ $W$ 是一個線性轉換, 將低維度的 latent space 轉換到高維度的 observation space, 另外 $\\Psi$ 必須是對角矩陣. 由於是對角的關係, 因此 $\\Psi$ 捕捉了 obaservation 維度的各自變異量, 因此稱為 uniquenesses, 而 $W$ 就是負責捕捉共同項, 稱為 factor loading. 書裡有一個簡單明瞭的圖解釋上述的模型, 我就不多說了, 自行看圖: 因為是 linear-Gaussian model, 所以 marginal distribution 也是 Gaussian: $$\\begin{align} p(x)=N(x|\\mu,C) \\\\ \\mbox{where } C=WW^T+\\Psi \\end{align}$$ 同時, 事後機率也是 Gaussian $$\\begin{align} p(z|x)=N(z|GW^T\\Psi^{-1}(x-\\bar{x}),G^{-1}) \\\\ \\mbox{where } G=(I+W^T\\Psi^{-1}W)^{-1} \\end{align}$$ 完整的 lineaer-Gaussian model 公式, from PRML book: 有了 $p(x)$ (式 3) 基本上我們就可以根據 training data 算出 likelihood, 然後找出什麼樣的參數可以最大化它. 但是這裡的問題是含有未知變數 $z$, 這個在 training data 看不到, 因為我們只看的到 $x$. 不過別擔心, EM 演算法可以處理含有未知變數情況下的 maximal likelihood estimation. 忘了什麼是 EM, 可以參考一下這. 很精簡的講一下就是, 找到一個輔助函數 $Q$, 該輔助函數一定小於原來的 likelihood 函數, 因此只要找到一組參數可以對輔助函數最大化, 那麼對於原來的 likelihood 函數也會有提升, 重複下去就可以持續提升, 直到 local maximum.另外輔助函數就是 “complete-data log likelihood and take its expectation with respect to the posterior distribution of the latent distribution evaluated using ‘old’ parameter values”, 我知道很粗略, 還請自行看筆記或是 PRML Ch9. E-StepE-Step 主要算出基於舊參數下的事後機率的一階二階統計量 首先將符號做簡化, 方便後面的式子更簡潔 ($n$ 是訓練資料的 index): $$\\mathbb{E}[z_n]\\equiv\\mathbb{E}_{z_n|x_n}[z_n] \\\\ \\mathbb{E}[z_nz_n^T]\\equiv\\mathbb{E}_{z_n|x_n}[z_nz_n^T] \\\\$$ 事後機率的一階二階統計量如下: $$\\begin{align} \\mathbb{E}[z_n] = GW^T\\Psi^{-1}(x_n-\\mu) \\\\ \\mathbb{E}[z_nz_n^T] = G + \\mathbb{E}[z_n] \\mathbb{E}[z_n]^T \\\\ \\mbox{where } G=(I+W^T\\Psi^{-1}W)^{-1} \\end{align}$$ 因為事後機率是 Gaussian, 所以由式 (5) 可以推得式 (7) 和 式 (8). M-Step這一步就是最大化輔助函數 $Q$, 其中 $\\mu$ 等於 sample mean, 可以直接寫死不需要 iteration. 另外兩個參數 update 如下: $$\\begin{align} W^{new}=\\left[\\sum_{n=1}^N (x_n-\\mu)\\mathbb{E}[z_n]^T\\right]\\left[\\sum_{n=1}^N \\mathbb{E}[z_nz_n^T]\\right]^{-1} \\\\ \\Psi^{new}=\\mbox{diag}\\left[S-W^{new}\\frac{1}{N}\\sum_{n=1}^N \\mathbb{E}[z_n](x_n-\\mu)^T\\right] \\end{align}$$ $S$ 是 sample covariance matrix (除 N 的那個 biased) Toy Example黑色那條線是真正產生資料時的 $W$, 可以當成正確答案. 紅色的是 FA 估計出來的 $W$ 和 $p(x)$. 可以發現 $W$ 沒有跟正確答案一樣, 這是因為我們在做 maximum likelihood 的時候, 只關心 $p(x)$, 因此可以有不同的 latent space 產生相同的 $p(x)$. 範例也一併把 probabilistic PCA 做出來了, 可以發現 PPCA 算的 $W$ 跟正確答案很接近, 這是因為此範例的資料其實是根據 PPCA 的模型產生的, 所以 PPCA 較接近是正常. 同時我們看到 PPCA 估計出來的 $p(x)$ 其實也跟 FA 一樣, 再度佐證 FA 其實也沒算錯, 只是不同的表達方式. Mixtures of Factor Analyzers將 FA 假設有多個 components 組成就變成 MFA 了, 其實就跟 GMM 一樣, 差別在於我們用了 latent space 去各別 model 每個 Gaussian Components 而已! 要注意的是, 這時候的 latent variables 不只有 $z$, 還有 $m$ (=1~M 表示有 $M$ 個 components), 我們用下標 $j$ 表示 component 的 index. 另外, 每一個 component, 會有各自的 latent space, 因此有各自的 $W_j$ 和 $\\mu_j$, 但是全部的 components 共用一個 uniquenesses $\\Psi$. $$\\begin{align} p(x|z,m=j)=N(x|W_j z+\\mu_j,\\Psi) \\end{align}$$ 和 GMM 一樣, 每一個 component 都有一個 weights, $\\pi_j$, 合起來機率是1 E-Step一階和二階統計量如下: $$\\begin{align} \\color{red}{\\mathbb{E}[z_n|m=j]} = G_j W_j^T \\Psi^{-1}(x_n-\\mu_j) \\\\ \\color{red}{\\mathbb{E}[z_nz_n^T|m=j]} = G_j + \\mathbb{E}[z_n|m=j] \\mathbb{E}[z_n|m=j]^T \\\\ \\mbox{where } G_j=(I+W_j^T\\Psi^{-1}W_j)^{-1} \\end{align}$$ 而真正的事後機率為: $$\\begin{align} \\mathbb{E}[m=j,z_n] = h_{nj}\\mathbb{E}[z_n|m=j] \\\\ \\mathbb{E}[m=j,z_nz_n^T] = h_{nj}\\mathbb{E}[z_nz_n^T|m=j] \\\\ \\mbox{where } \\color{red}{h_{nj}}=\\mathbb{E}[m=j|x_n]\\propto p(x_n,m=j) \\end{align}$$ 將 (18) 解釋清楚一下, 基本上就是計算給定一個 $x_n$, 它是由 component $j$ 所產生的機率是多少. 我們可以進一步推導如下: $$\\begin{align} p(x_n,m=j)=p(m=j)p(x_n)\\\\ =\\pi_j N(x_n|\\mu_j,C_j=W_jW_j^T+\\Psi) \\end{align}$$ (19) 到 (20) 的部分可以由 (3) 和 (4) 所知道的 marginal distribution $p(x)$ 得到 到這裡, 所有需要的統計量, 紅色部分, 我們都可以算得了. M-Step通通微分等於零, 通通微分等於零, 通通微分等於零 … 得到: $$\\begin{align} \\pi_j^{new}=\\frac{1}{N}\\sum_{n=1}^N h_{nj} \\\\ \\mu_j^{new}=\\frac{\\sum_{n=1}^N h_{nj}x_n}{\\sum_{n=1}^N h_{nj}} \\\\ W_j^{new}=\\left[\\sum_{n=1}^N h_{nj}(x_n-\\mu_j)\\mathbb{E}[z_n|m=j]^T\\right]\\left[\\sum_{n=1}^N h_{nj}\\mathbb{E}[z_nz_n^T|m=j]\\right]^{-1} \\\\ \\Psi^{new}=\\frac{1}{N}\\mbox{diag}\\left[ \\sum_{nj} h_{nj} \\left( (x_n-\\mu_j) - W_j^{new}\\mathbb{E}[z_n|m=j] \\right)(x_n-\\mu_j)^T \\right] \\end{align}$$ Toy Example 圖應該很清楚了, 有正確 model 到 data 這個 MFA 還真的不容易實作, 寫起來很多要注意的地方, 很燒腦阿! 不過做完了之後頗有成就感~ i-vector其實會想寫這篇主要是因為語者識別中的 ivector, 而 ivector 基本上就是一個 FA. 在計算 ivector 時, 我們會先估計 Universal Background Model (UBM), 其實就是所有語者的所有語音特徵算出來的 GMM. 以下圖為例, UBM 有三個 mixtures, 用淡藍色表示. 而針對某一位 speaker, 其 GMM 為橘色. 傳統上我們將所有 mixture 的 mean 串接成一個長的向量, 則該向量就可以當作是該 GMM 模型的一個代表, 並稱為 supervector 不一起串接 covariance matrix 嗎? weight 呢? 當然也可以全部都串成一個非常長的向量, 但研究表明 mean 向量就足夠了 supervector 維度為 mfcc-dim x mixture數, 很容易有 40x1024 這麼高維! 因此 ivector 就是利用 FA 的方法將 supervector 降維. 那具體怎麼做呢? 首先我們要先用一個小技巧將 “多個 Gaussians” (注意不是 GMM, 因為沒有mixture weight的概念, 每一個 Gaussian都同等重要) 轉換成一個 Gaussain. 見圖如下: 我們可以很容易驗證兩邊是等價的. 轉換成一個 Gaussian 好處就是我們可以直接使用 FA 降維, 而 ivector 就是該 FA 的 latent variable $z$. 如同 (2) 的定義: $$\\begin{align} p(x|z)=N(x|Wz+\\mu,\\Sigma) \\\\ \\end{align}$$ 這裡的 $\\mu$ 是 UBM 的 supervector, $\\Sigma$ 則如同上圖的定義, 是一個 block diagonal matrix, 每一個 block 對應一個 UBM mixture 的 covariance matrix. 因此 $\\mu$ 和 $\\Sigma$ 都是使用 UBM 的參數. 針對式 (25) 去更仔細了解其所代表的物理意義是很值得的, 所以我們多說一點. 由於我們已經知道這樣的一個 Gaussian 實際上代表了原來 mfcc space 的多個 Gaussians. 所以針對某一個特定的 ivector $z^*$ 由式 (25) 得知, 他有可能代表了下圖橘色的三個 Gaussians (也因此可能代表了某一個 speaker 的模型): 到目前為止所描述的 ivector 實際上是根據自己的理解將 2005 年 “Eigenvoice Modeling with Sparse Training Data“ 裡的 Proposition 1 (p348) 的設定描述出來. 如有錯誤還請來信指正. 該設定中, 每一個 mfcc vector 都會事先被歸類好屬於哪一個 mixture, 等於硬分類. 但是其實並不需要, 一個明顯的改進方法就是使用後驗概率來做軟分類. 直接看圖: 目前的 ivector 計算都使用這種方式, 例如 Microsoft Research 的 MSR Identity Toolbox. 該 toolbox 使用 “A Straightforward and Efficient Implementation of the Factor Analysis Model for Speaker Verification“ 的實作方式, 可以由論文的式 (2),(5) 看出使用後驗概率的設定. 最後多說一些語者識別的事情. ivector 主要是針對原高維空間 (mfcc-dim x component數量) 做降維, 而沒有去針對語者的訊息. 所以傳統流程會再經過 WCCN + LDA, 而 LDA 就會針對同一個語者盡量靠近, 而不同語者盡量拉開. 經過 LDA 後就可以用 $cos$ 計算相似度進行語者之間的打分. 但事實上, 更好的做法是用一個 PLDA 做更好的打分. 關於 PLDA 請參考這邊原始文章 “Probabilistic Linear Discriminant Analysis for Inferences About Identity“, 而 PLDA 更是與本篇的 FA 脫離不了關係! 總體來說 FA, MFA 對於目前的語者識別系統仍然十分關鍵, 縱使目前 Kaldi 使用了深度學習替換了 ivector, 但後端仍然接 PLDA. Reference 自己實作的 Python MFA (含 Toy examples) github Zoubin Ghahramani and Geoffrey E. Hinton, The EM Algorithm for Mixtures of Factor Analyzers Bishop PRML 以前的 EM 筆記 以前的 GMM EM 筆記 i-vector 原始論文 PLDA 原始論文 Eigenvoice Modeling with Sparse Training Data A Straightforward and Efficient Implementation of the Factor Analysis Model for Speaker Verification MSR Identity Toolbox","tags":[{"name":"Factor Analysis","slug":"Factor-Analysis","permalink":"http://yoursite.com/tags/Factor-Analysis/"},{"name":"Expectation Maximization","slug":"Expectation-Maximization","permalink":"http://yoursite.com/tags/Expectation-Maximization/"},{"name":"ivector","slug":"ivector","permalink":"http://yoursite.com/tags/ivector/"}]},{"title":"Path-Planning-Udacity-Term3-Project1","date":"2018-02-06T15:38:48.000Z","path":"2018/02/06/Path-Planning-Udacity-Term3-Project1/","text":"停了半年 的 Udacity Self Driving Car (SDC) Program, 終於又開始了. 做為 Term3 的第一個 Project, 我抱著高度的期待. 不過完成後, 有點小失望. 失望的原因是這個 project 跟課程上的連結感覺不是那麼明顯. 例如課程上有講到 A*, hybrid A* 的算法, 但 project 是模擬 highway drive, 因此 A* 比較不適合 (適合在 parking lot 場景). 另外也有提到怎麼降低 jerk (加速度的微分, 主要是不舒適的來源), 當參考內容是很好, 不過在寫 Project 時感覺也不大需要. 這篇就是個紀錄, 會很水. 方法很簡單, 一張圖解決: ego-car 根據自己所在的車道, 最多可以有三條路徑選擇, 路徑就用 spline curve 產生, 確保夠 smooth. 同時有 sensor-fusion 的資料可以知道其他車子的狀況, 然後利用 prediction model 去預測其他車的路徑 (我就單純使用 constant velocity 線性路徑). 如果有 collision 在未來的 1 or 1.5 秒, 該路徑就無效. 另外, 如果有車子太靠近, 就減速. ego-car 的每個路徑都會有各自的 cost, cost 是根據一些喜好, 譬如哪一條車道可以跑得比較快, 哪一條路徑比較不會跟其他車子太接近等等… 這 Project 最麻煩的就是在設計 cost function, 和調整. (還有熟悉 project 的程式碼…麻煩阿) 影片連結 here. Project github here 沒了, 文章水不水 ? 好水阿, 真心虛","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"}]},{"title":"Maximum Mutual Information in Speech Recognition","date":"2017-12-16T04:08:44.000Z","path":"2017/12/16/Maximum-Mutual-Information-in-Speech-Recognition/","text":"Maximum Mutual Information (MMI) 序列的鑑別性訓練方法從早期的 GMM-HMM, 到現今就算使用了深度學習同樣十分有用, 如 Kaldi chain model 在 DNN-HMM 的基礎上加上序列鑑別訓練, 性能會再進一步提升. 前一陣子讀了俞棟、鄧力的這本 語音識別實踐, 對我來說整理得滿好的, 就是數學部分的推導有點簡潔了些, 所以這篇就基於該書的推導, 補齊了較詳細的步驟, 並且嘗試使用 Computational graph 的方式理解 MMI 的訓練. 那麼就開始吧! 用自己畫的 MMI 的計算圖譜當封面吧 :) MMI 數學定義定義$o^m=o_1^m,...,o_t^m,...,o_{T_m}^m$是訓練樣本裡第 m 句話的 observation (MFCC,fbank,…) sequence, 該 sequence 有 $T_m$ 個 observation vector. 而$w^m=w_1^m,...,w_t^m,...,w_{N_m}^m$則是該句話的正確 transcription, 有 $N_m$ 個字. 通過 forced-alignment 可以得到相對應的 state sequence$s^m=s_1^m,...,s_t^m,...,s_{T_m}^m$MMI 目的就是希望模型算出的正確答案 sequence 機率愈大愈好, 同時非正確答案 (與之競爭的其他 sequences) 的機率要愈小愈好, 所以正確答案放分子, 非正確放分母, 整體要愈大愈好. 由於考慮了競爭 sequences 的最小化, 所以是鑑別性訓練. 又此種方始是基於整句的 sequence 考量, 因此是序列鑑別性訓練. 數學寫下來如下:$$J_{MMI}(\\theta;S)=\\sum_{m=1}^M J_{MMI}(\\theta\\|o^m,w^m) \\\\ =\\sum_{m=1}^M \\log \\frac{ p(o^m\\|s^m,\\theta)^KP(w^m) }{ \\sum_w p(o^m\\|s^w,\\theta)^K P(w) }$$ 為了簡單化, 我們假設只有一條訓練語音, 所以去掉 $m$ 的標記, 然後 $\\sum_m$ 省略: $$\\begin{align} J_{MMI}(\\theta\\|o,w) =\\log \\frac{ p(o\\|s,\\theta)^KP(w) }{ \\sum_w p(o\\|s^w,\\theta)^K P(w) } \\end{align}$$ 接著我們要算針對 $\\theta$ 的微分, 才可以使用梯度下降算法: $$\\begin{align} \\triangledown_\\theta J_{MMI}(\\theta\\|o,w) =\\sum_t \\triangledown_{z_t^L}J_{MMI}(\\theta\\|o,w)\\frac{\\partial z_t^L}{\\partial\\theta} \\\\ =\\sum_t e_t^L\\frac{\\partial z_t^L}{\\partial\\theta} \\end{align}$$ 其中定義$e_t^L=\\triangledown_{z_t^L}J_{MMI}(\\theta\\|o,w)$ 語音聲學模型 (AM) 傳統上使用 GMM 來 model, 而現在都是基於 DNN, 其中最後的 output layer 假設為第 $L$ 層: $z_t^L$, 過了 softmax 之後我們定義為 $v_t^L$, 而其 index $r$, $v_t^L(r)=P(r|o_t)$ 就是給定某一個時間 $t$ 的 observation $o_t$ 是 state $r$ 的機率. 讀者別緊張, 我們用 Computational graph 的方式將上式直接畫出來: MMI Computational Graph 表達 上圖用 computational graph 清楚的表達了式 (3) 的計算, 因為所有參數 $\\theta$ 在所有的時間 $t$ 上是共享的, 因此要 $\\sum_t$, 也就是要累加上圖所有紅色的 gradient path. 計算 $\\partial z_t^L / \\partial\\theta$ 很容易, 就是 DNN 的計算圖譜的 gradient, 因此重點就在如何計算 $e_t^L$, 而整個 MMI 最核心的地方就是在計算這個了! MMI 數學推導我們把 $e_t^L(i)$ (就是$e_t^L$這個向量的第$i$個element)計算如下: $$\\begin{align} e_t^L(i)=\\triangledown_{z_t^L(i)}J_{MMI}(\\theta\\|o,w) \\\\ =\\sum_r \\frac{\\partial J_{MMI}(\\theta\\|o,w)}{\\partial\\log p(o_t|r)}\\frac{\\partial\\log p(o_t|r)}{\\partial z_t^L(i)} \\end{align}$$ 先解釋一下 $\\log p(o_t|r)$ 這個 term, 可以重寫成$$\\begin{align} \\log p(o_t|r)=\\log \\color{red}{p(r|o_t)} + \\log p(o_t) - \\log p(r) = \\log \\color{red}{v_t^L(r)} + \\log p(o_t) - \\log p(r) \\end{align}$$所以這個 term 是跟 $v_t^L(r)$ 相關的, 而由於 $v_t^L$ 是 $z_t^L$ 經過 softmax 得到, 因此式(5)才會有 $\\sum_r$.根據式 (6), 我們可以很快算得式 (5) 的第二個分子分母項如下:$$\\begin{align} \\frac{\\partial\\left[\\log v_t^L(r) + \\log p(o_t) - \\log p(r)\\right]}{\\partial z_t^L(i)}=\\frac{\\partial \\log v_t^L(r)}{\\partial z_t^L(i)} \\end{align}$$很明顯因為 $\\log p(o_t)$ 和 $\\log p(r)$ 都跟 $z_t^L(i)$ 無關所以去掉.為了計算式 (5) 的第一個分子分母項, 我們把先把式 (1) 的 log 項拆開:$$\\begin{align} J_{MMI}(\\theta\\|o,w)= K\\color{green}{\\log p(o\\|s,\\theta)}+\\color{blue}{\\log p(w)} - \\color{orange}{\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]} \\end{align}$$所以$$\\begin{align} \\frac{\\partial J_{MMI}(\\theta\\|o,w)}{\\partial \\log p(o_t|r)}= K\\color{green}{ \\frac{\\partial\\log p(o\\|s,\\theta)}{\\partial \\log p(o_t|r)} } + \\color{blue}{ \\frac{\\partial\\log p(w)}{\\partial \\log p(o_t|r)} } - \\color{orange}{ \\frac{\\partial\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]}{\\partial \\log p(o_t|r)} } \\end{align}$$ 綠色部分注意到 $\\log p(o|s,\\theta)$ 在 HMM 的情況下, 是給定 state sequence 的觀測機率值, 因此只是每個 state 時間點的 emission probability, 所以$$\\begin{align} \\log p(o\\|s,\\theta)= \\sum_{t&apos;} \\log p(o_{t&apos;}\\|s_{t&apos;},\\theta) \\end{align}$$而只有 $t’=t$ 時與微分項有關, 因此變成$$\\begin{align} \\frac{\\partial\\log p(o\\|s,\\theta)}{\\partial \\log p(o_t\\|r)}= \\frac{\\partial \\log p(o_t\\|s_t,\\theta)}{\\partial \\log p(o_t\\|r)}=\\delta(r=s_t) \\end{align}$$ 藍色部分與微分項無關，因此$$\\begin{align} \\frac{\\partial\\log p(w)}{\\partial \\log p(o_t|r)}=0 \\end{align}$$ 橘色部分$$\\begin{align} \\frac{\\partial\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]}{\\partial \\log p(o_t|r)}= \\frac{1}{\\sum_w p(o\\|s^w,\\theta)^K p(w)}\\times\\frac{\\partial \\sum_w \\color{red}{p(o\\|s^w,\\theta)}^K p(w)}{\\partial \\log p(o_t|r)} \\end{align}$$ 紅色的部分如同上面綠色項的討論, 只有時間點 $t$ 才跟微分項有關, 不同的是這次沒有 $\\log$ 因此是連乘, 如果 $s_t\\neq r$ 整條 sequence 的機率與微分項無關, 因此只會保留 $s_t=r$ 的那些 $w$ sequences.另外,$\\frac{\\partial p(o_t\\|r)^K}{\\partial\\log p(o_t\\|r)} \\mbox{ 可想成 } \\frac{\\partial e^{Kx}}{\\partial x} = Ke^{Kx}$綜合以上討論橘色部分為$$\\begin{align} \\frac{\\partial\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]}{\\partial \\log p(o_t|r)}= K\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)} \\end{align}$$ 全部帶入並整理 $e_t^L(i)$將 (11),(12),(14) 代回到 (9) 我們得到$$\\begin{align} \\frac{\\partial J_{MMI}(\\theta\\|o,w)}{\\partial \\log p(o_t|r)}= K\\left(\\delta(r=s_t)-\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)}\\right) \\end{align}$$繼續將 (15),(7) 代回到 (5) 我們終於可以得到 $e_t^L(i)$ 的結果了!$$\\begin{align} e_t^L(i)=\\sum_r K\\left(\\delta(r=s_t)-\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)}\\right) \\times \\frac{\\partial \\log v_t^L(r)}{\\partial z_t^L(i)} \\\\ = \\sum_r K\\left(\\delta(r=s_t)-\\color{red}{\\gamma_t^{DEN}(r)}\\right) \\times \\frac{\\partial \\log v_t^L(r)}{\\partial z_t^L(i)} \\\\ =K\\left(\\delta(i=s_t)-\\gamma_t^{DEN}(i)\\right) \\end{align}$$其中一個很重要的定義$$\\begin{align} \\gamma_t^{DEN}(r)=\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)} \\end{align}$$物理意義就是時間$t$在狀態$r$的機率! 理論上來說我們要取遍所有可能的 word sequence $w$ 並求和計算, 但實際上只會在 decoding 時的 lattice 上計算, 以節省時間. 到目前為止我們算完了 MMI 最困難的部分了, 得到 $e_t^L(i)$ 後 (式(18))，剩下的就只是 follow 上圖的 MMI computational graph 去做. 有讀者來信詢問式 (17) 如何推導至 (18), 過程如下圖: (抱歉偷懶不打 Latex 了) 結論還有一些其他變種如 boost MMI (bMMI)、MPE、MCE等等, 差別只是在最小化不同的標註精細度, 最重要的還是要先了解 MMI 就可以容易推廣了. 這些都有一個統一的表達法如下:$$\\begin{align} e_t^L(i)=K\\left(\\gamma_t^{DEN}(i)-\\gamma_t^{NUM}(i)\\right) \\end{align}$$注意到正負號跟 (18) 相反, 因為只是一個最大化改成最小化表示而已. 並且多了一個分子的 lattice 計算. Reference 俞棟、鄧力: 語音識別實踐 Ch8 Kaldi chain model","tags":[{"name":"Speech Recognition","slug":"Speech-Recognition","permalink":"http://yoursite.com/tags/Speech-Recognition/"},{"name":"Maximum Mutual Information","slug":"Maximum-Mutual-Information","permalink":"http://yoursite.com/tags/Maximum-Mutual-Information/"},{"name":"Computational Graph","slug":"Computational-Graph","permalink":"http://yoursite.com/tags/Computational-Graph/"}]},{"title":"TF Notes (3), Computational Graph in Tensorflow","date":"2017-11-29T12:36:59.000Z","path":"2017/11/29/TF-Notes-Computational-Graph-in-Tensorflow/","text":"這篇介紹 computational graph (計算圖譜), 主要來源參考自李宏毅教授的課程內容. 並且我們使用 tensorflow 的求導函數 tf.gradients 來驗證 computational graph. 最後我們在 MNIST 上驗證整個 DNN/CNN 的 backpropagation 可以利用 computational graph 的計算方式訓練. Computational Graph主要就截圖李教授的投影片, 一個 computational graph 的 node 和 edge 可以定義如下 對於 chain rule 的話, 我們的計算圖譜可以這麼畫 其實就是 chain rule 用這樣的圖形表示. 比較要注意的就是 case 2 的情況, 由於 $x$ and $y$ 都會被 $s$ 影響, 因此計算 gradients 時要累加兩條路徑. 再來另一項要注意的是如果有 share variables 我們的計算圖譜該怎麼表示呢 ? 舉例來說如下的函式$y=x\\cdot e^{x^2}$ 計算圖譜畫出來長這樣 簡單來說把相同變數的 nodes 上所有的路徑都相加起來. 上面的範例就是計算 $\\frac{\\partial y}{\\partial x}$ 時, 有三條路徑是從 node $x$ 出發最終會影響到 node $y$ 的, 讀者應該可以很容易看出來. 另外如果分別計算這三條路徑, 其實很多 edges 的求導結果會重複, 因此從 $x$ 出發計算到 $y$ 會很沒有效率, 所以反過來 (Reverse mode) 從 root ($y$) 出發, 反向找出要求的 nodes ($x$) 就可以避免很多重複運算. Verify with Tensorflow考慮以下範例, 其中 $&lt;,&gt;$ 表示內積, $a$, $b$, 和 $1$ 都是屬於 $R^3$ 的向量. 它的計算圖譜如下: 在 Tensorflow 中, tf.gradients 可以幫助計算 gradients. 舉例來說如果我們要計算 $\\frac{\\partial e}{\\partial c}$, 我們只要這樣呼叫即可 ge_c=tf.gradients(ys=e,xs=c). 為了方便, 我們將 $\\frac{\\partial y}{\\partial x}$ 在程式裡命名為 gy_x. 下面這段 codes 計算出上圖 5 個 edges 的 gradients: 123456789101112131415161718import tensorflow as tfimport numpy as npa = tf.placeholder(tf.float32, shape=(1,3))b = tf.placeholder(tf.float32, shape=(1,3))c = a + bd = b + 1e = tf.matmul(c,d,transpose_b=True)ge_c, ge_d = tf.gradients(ys=e,xs=[c,d])gc_a, gc_b = tf.gradients(ys=c,xs=[a,b])gd_b = tf.gradients(ys=d,xs=b)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) gec, ged, gca, gcb, gdb = sess.run([ge_c, ge_d, gc_a, gc_b, gd_b],feed_dict=&#123;a:[[2,1,0]],b:[[1,2,3]]&#125;) print('ge_c=&#123;&#125;\\nge_d=&#123;&#125;\\ngc_a=&#123;&#125;\\ngc_b=&#123;&#125;\\ngd_b=&#123;&#125;'.format(gec, ged, gca, gcb, gdb)) 計算結果為 12345ge_c=[[ 2. 3. 4.]]ge_d=[[ 3. 3. 3.]]gc_a=[[ 1. 1. 1.]]gc_b=[[ 1. 1. 1.]]gd_b=[array([[ 1., 1., 1.]], dtype=float32)] 可以自己手算驗證一下, 結果當然是對的 (使用 Jacobian matrix 計算) 所以我們如果要得到 $\\frac{\\partial e}{\\partial b}$, 我們只要算 ge_c*gc_b + ge_d*gd_b 就可以了. 不過這樣自己把相同路徑做相乘，不同路徑做相加, 太麻煩了! 其實有更好的方法. 對於同一條路徑做相乘, tf.gradients 有一個 arguments 是 grad_ys 就可以很容易做到. tf.gradients(ys=,xs=,grad_ys=) 以下圖來說明 但事實上根本也不用這麼麻煩, 除非是遇到很特殊的狀況, 否則我們直接呼叫 tf.gradients(c,a), tensorflow 就會直接幫我們把同樣路徑的 gradients 做相乘, 不同路徑的 gradients 結果做相加了! 所以上面就直接呼叫 tf.gradients(c,a) 其實也就等於 gb_a2 了. 最後回到開始的 e=&lt;a+b,b+1&gt; 的範例, 如果要計算 $\\frac{\\partial e}{\\partial b}$, 照原本提供的 codes 需要計算三條路徑各自相乘後再相加, 其實只要直接呼叫 ge_b=tf.gradients(e,b) 就完成這件事了 (ge_c*gc_b + ge_d*gd_b) MNIST 用計算圖譜計算 back propagationDNN 的計算圖譜為了清楚了解 Neural network 的 backpropagation 如何用 computational graph 來計算 gradients 並進而 update 參數, 我們不使用 tf.optimizer 幫我們自動計算. 一個 3 layers fo MLP-DNN 的計算圖譜我們可以這樣表示: 圖裡的參數直接對應了程式碼裡的命名, 因此可以很方便對照. 其中 tensorflow 裡參數的名字跟數學上的對應如下: $$\\begin{align} gll=\\frac{\\partial \\mbox{loss}}{\\partial \\mbox{logits}} \\\\ gly2=\\frac{\\partial \\mbox{logits}}{\\partial y2}gll \\\\ gy2y1=\\frac{\\partial y2}{\\partial y1}gly2 \\\\ gy1y0=\\frac{\\partial y1}{\\partial y0}gy2y1 \\\\ (glw3,glb3)=(\\frac{\\partial \\mbox{logits}}{\\partial w3}gll,\\frac{\\partial \\mbox{logits}}{\\partial b3}gll) \\\\ (gy2w2,gy2b2)=(\\frac{\\partial y2}{\\partial w2}gly2,\\frac{\\partial y2}{\\partial b2}gly2) \\\\ (gy1w1,gy1b1)=(\\frac{\\partial y1}{\\partial w1}gy2y1,\\frac{\\partial y1}{\\partial b1}gy2y1) \\\\ (gy0w0,gy0b0)=(\\frac{\\partial y0}{\\partial w0}gy1y0,\\frac{\\partial y0}{\\partial b0}gy1y0) \\end{align}$$ 相對應的 tensorflow 代碼如下:12345678910111213141516gll = tf.gradients(ys=loss,xs=logits)gly2 = tf.gradients(ys=logits,xs=y2,grad_ys=gll)gy2y1 = tf.gradients(ys=y2,xs=y1,grad_ys=gly2)gy1y0 = tf.gradients(ys=y1,xs=y0,grad_ys=gy2y1)glw3 = tf.gradients(ys=logits,xs=w3,grad_ys=gll)glb3 = tf.gradients(ys=logits,xs=b3,grad_ys=gll)gy2w2 = tf.gradients(ys=y2,xs=w2,grad_ys=gly2)gy2b2 = tf.gradients(ys=y2,xs=b2,grad_ys=gly2)gy1w1 = tf.gradients(ys=y1,xs=w1,grad_ys=gy2y1)gy1b1 = tf.gradients(ys=y1,xs=b1,grad_ys=gy2y1)gy0w0 = tf.gradients(ys=y0,xs=w0,grad_ys=gy1y0)gy0b0 = tf.gradients(ys=y0,xs=b0,grad_ys=gy1y0) 用圖來表示為: Tensorflow 程式碼用上面一段的方式計算出所需參數的 gradients 後, update 使用最單純的 steepest descent, 這部分程式碼如下 12345678910111213update_w3 = tf.assign_add(w3,-rate*glw3[0])update_b3 = tf.assign_add(b3,-rate*glb3[0])update_w2 = tf.assign_add(w2,-rate*gy2w2[0])update_b2 = tf.assign_add(b2,-rate*gy2b2[0])update_w1 = tf.assign_add(w1,-rate*gy1w1[0])update_b1 = tf.assign_add(b1,-rate*gy1b1[0])update_w0 = tf.assign_add(w0,-rate*gy0w0[0])update_b0 = tf.assign_add(b0,-rate*gy0b0[0])training_operation = [update_w3, update_b3, update_w2, update_b2, update_w1, update_b1, update_w0, update_b0] 完整程式碼如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171import osimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.layers import flattenfrom tensorflow.examples.tutorials.mnist import input_datafrom sklearn.utils import shuffle\"\"\"Data Loading\"\"\"dataPath='../dataset/MNIST_data/'mnist = input_data.read_data_sets(dataPath, one_hot=True)# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]img_width = 28img_height = 28images = mnist.train.imagesimg_num, _ = images.shapeimages = np.reshape(images,(img_num,img_height,img_width))images = images[...,np.newaxis]print('(Input to CNN) Images with shape &#123;&#125;'.format(images.shape))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# load the validation setimages_valid = mnist.validation.imagesimg_num_valid = len(images_valid)images_valid = np.reshape(images_valid,(img_num_valid,img_height,img_width))images_valid = images_valid[...,np.newaxis]labels1Hot_valid = mnist.validation.labelsprint('Having %d number of validation images' % img_num_valid)# plotting sample imagesplt.figure(figsize=(15,5))for i in np.arange(2*7): random_idx = np.random.randint(0,img_num) plt.subplot(2,7,i+1) plt.imshow(images[random_idx][...,0],cmap='gray') plt.title(labels[random_idx][0])\"\"\"First define the hyper-parameters\"\"\"# Hyper-parametersEPOCHS = 30BATCH_SIZE = 512rate = 0.01depth_list = [512, 256, 128]cNum = 1\"\"\"Define the input output tensors\"\"\"# using one-hot decodingx = tf.placeholder(tf.float32, (None, img_height, img_width, cNum))one_hot_y = tf.placeholder(tf.int32, (None, n_classes))#one_hot_y = tf.one_hot(y, n_classes)\"\"\"Define the graph and construct it\"\"\"z0 = flatten(x)w0 = tf.get_variable('w0', shape=[img_width*img_height, depth_list[0]], initializer=tf.random_uniform_initializer(-0.1,0.1))b0 = tf.get_variable('b0', [depth_list[0]], initializer=tf.zeros_initializer)y0 = tf.nn.xw_plus_b(z0, w0, b0)z1 = tf.nn.relu(y0)w1 = tf.get_variable('w1', shape=[depth_list[0], depth_list[1]], initializer=tf.random_uniform_initializer(-0.1,0.1))b1 = tf.get_variable('b1', [depth_list[1]], initializer=tf.zeros_initializer)y1 = tf.nn.xw_plus_b(z1, w1, b1)z2 = tf.nn.relu(y1)w2 = tf.get_variable('w2', shape=[depth_list[1], depth_list[2]], initializer=tf.random_uniform_initializer(-0.1,0.1))b2 = tf.get_variable('b2', [depth_list[2]], initializer=tf.zeros_initializer)y2 = tf.nn.xw_plus_b(z2, w2, b2)z3 = tf.nn.relu(y2)w3 = tf.get_variable('w3', shape=[depth_list[2], n_classes], initializer=tf.random_uniform_initializer(-0.1,0.1))b3 = tf.get_variable('b3', [n_classes], initializer=tf.zeros_initializer)logits = tf.nn.xw_plus_b(z3, w3, b3)cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits)loss = tf.reduce_mean(cross_entropy)\"\"\"Define gradients\"\"\"gll = tf.gradients(ys=loss,xs=logits)gly2 = tf.gradients(ys=logits,xs=y2,grad_ys=gll)gy2y1 = tf.gradients(ys=y2,xs=y1,grad_ys=gly2)gy1y0 = tf.gradients(ys=y1,xs=y0,grad_ys=gy2y1)glw3 = tf.gradients(ys=logits,xs=w3,grad_ys=gll)glb3 = tf.gradients(ys=logits,xs=b3,grad_ys=gll)gy2w2 = tf.gradients(ys=y2,xs=w2,grad_ys=gly2)gy2b2 = tf.gradients(ys=y2,xs=b2,grad_ys=gly2)gy1w1 = tf.gradients(ys=y1,xs=w1,grad_ys=gy2y1)gy1b1 = tf.gradients(ys=y1,xs=b1,grad_ys=gy2y1)gy0w0 = tf.gradients(ys=y0,xs=w0,grad_ys=gy1y0)gy0b0 = tf.gradients(ys=y0,xs=b0,grad_ys=gy1y0)update_w3 = tf.assign_add(w3,-rate*glw3[0])update_b3 = tf.assign_add(b3,-rate*glb3[0])update_w2 = tf.assign_add(w2,-rate*gy2w2[0])update_b2 = tf.assign_add(b2,-rate*gy2b2[0])update_w1 = tf.assign_add(w1,-rate*gy1w1[0])update_b1 = tf.assign_add(b1,-rate*gy1b1[0])update_w0 = tf.assign_add(w0,-rate*gy0w0[0])update_b0 = tf.assign_add(b0,-rate*gy0b0[0])training_operation = [update_w3, update_b3, update_w2, update_b2, update_w1, update_b1, update_w0, update_b0]\"\"\"Define accuracy evaluation\"\"\"# calculate the average accuracy by calling evaluate(X_data, y_data)correct_prediction = tf.equal(tf.argmax(logits, axis=1), tf.argmax(one_hot_y, axis=1))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))def evaluate(X_data, y_data): num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] accuracy = sess.run(accuracy_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y&#125;) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples \"\"\"Run Session\"\"\"### Train your model here.import timeif not os.path.isdir('./models'): os.makedirs('./models')#saver = tf.train.Saver()accumulate_time = 0.0with tf.Session() as sess: sess.run(tf.global_variables_initializer()) num_examples = img_num print(\"Training...\") print() train_accuracy = np.zeros(EPOCHS) validation_accuracy = np.zeros(EPOCHS) for i in range(EPOCHS): stime = time.time() acc_train_accuracy = 0 X_train, y_train = shuffle(images, labels1Hot) for offset in range(0, num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] sess.run(training_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y&#125;) etime = time.time() accumulate_time += etime - stime validation_accuracy[i] = evaluate(images_valid, labels1Hot_valid) print(\"EPOCH &#123;&#125; ...\".format(i+1)) print(\"Validation Accuracy = &#123;:.3f&#125;\".format(validation_accuracy[i])) print() print('Cost time: ' + str(accumulate_time) + ' sec.') 訓練結果如下: 1234567891011121314151617EPOCH 1 ...Validation Accuracy = 0.553EPOCH 2 ...Validation Accuracy = 0.698EPOCH 3 ...Validation Accuracy = 0.771... 略EPOCH 28 ...Validation Accuracy = 0.936EPOCH 29 ...Validation Accuracy = 0.936EPOCH 30 ...Validation Accuracy = 0.936 這速度果然明顯比用 Adam 慢很多, 但至少說明了我們的確使用 Computational graph 的計算方式完成了 back propagation! 結論Tensorflow 使用計算圖譜的框架來計算函數的 gradients, 一旦這樣做, 神經網路的 backprop 很自然了. 事實上, 所有流行的框架都這麼做, 就連 Kaldi 原先在 nnet2 不是, 但到 nnet3 也改用計算圖譜來實作. Reference 李宏毅 Computational Graph tf.gradients說明 Colah’s Blog","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"Computational Graph","slug":"Computational-Graph","permalink":"http://yoursite.com/tags/Computational-Graph/"}]},{"title":"Notes for KKT Conditions","date":"2017-11-14T13:36:40.000Z","path":"2017/11/14/Notes-for-KKT-Conditions/","text":"2011年自己做的筆記, 放上來以免檔案丟失, 也方便隨時參考. 參考自 “Numerical Optimization” by Jorge Nocedal and Stephen J. Wright. 但是打算只用 Lagrange Multiplier Theorem 理解 KKT. :) 就像是一般微積分裡學到的一樣, 對於一個函式 $f(x)$ 若 $x^\\ast$ 為一 minimal/maximum point, 則必要條件為 $f’(x^\\ast)=0$. 而在 constraint optimization 版本必要條件變成 KKT conditions. 說更清楚一點就是, 若 $x^\\ast$ 為一 minimal/maximum point (+滿足某些神秘條件) , 則必要條件為在 $x^\\ast$ 滿足 KKT Conditions. 神秘條件稱為 Constraint Qualifications, 常見的為 LICQ, 在 Convex opt 裡為 Slater’s condition. wiki KKT 具體來說，我們要探討的是對於以下的問題，如果 $x^\\ast$ 為一 minimal point 且滿足式 (2) 的條件, 則會發生什麼事情 (我們找的是必要條件) $$\\begin{align} \\min f(x) \\\\ \\mbox{subject to } \\begin{array}{rcl} c_i(x)=0,i \\in \\mathbf{E} \\\\ c_i(x)\\geq 0, i \\in \\mathbf{I} \\\\ \\end{array} \\end{align}$$ Descent Direction一般這樣的問題不會有 closed form solution, 因此會使用數值最佳化的方式, 找出一個 sequence $(x_k)$ 來逼近 $x^\\ast$. 問題是如何讓這樣的 sequence 逼近一個 (local) minimum? 一個嘗試是至少先讓找到的每一個 $x_k$ 都比前一步更好. 換句話說就是要保證找到的 $x_k$ 滿足$f(x_k)&lt;f(x_{k-1})$ 有了這個想法, 再來就是該怎麼找下一個點, 或是說, 基於現在的點 $x_k$, 該往哪個方向 $d$, 走多遠 $t$? 也因此，我們也就衍生了一個問題, 往哪個方向走函數值會保證下降 (descent direction)? Descent direction 保證了在該方向上只要不要走太大步, 目標函數值一定會下降, 反過來說, 如果 $x^\\ast$ 已經是 (local) minimum 了, 在該點上不應該存在 descent direction. 基於上述的討論, 我們有必要了解清楚 descent direction. 定義如下 [Def]:令 $f \\in C^1(\\mathbb{R}^n,\\mathbb{R})$, 我們稱 $d$ 為 $x_0$ 的一個 descent direction 如果滿足: $\\exists t&gt;0$, such that $f(x_0+sd)&lt;f(x_0),\\forall s \\leq t$ 其實很容易證得: 只要該方向 $d$ 跟 gradient $\\triangledown f(x_0)$ 方向相反, 就會是 $x_0$ 的一個 descent direction [Thm1]:令 $f \\in C^1(\\mathbb{R}^n,\\mathbb{R})$, 如果滿足 $\\triangledown f(x_0)^Td&lt;0$ 則 $d$ 就會是 $x_0$ 的一個 descent direction [Pf]: 由微分的定義出發$$\\lim_{t\\rightarrow 0^+}\\frac{f(x_0+td)-f(x_0)-\\triangledown f(x_0)^Ttd}{\\parallel td \\parallel}=0 \\\\ \\Rightarrow \\lim_{t\\rightarrow 0^+}\\frac{f(x_0+td)-f(x_0)}{t \\parallel d \\parallel}=\\triangledown f(x_0)^T\\frac{d}{\\parallel d \\parallel} \\\\ \\Rightarrow \\lim_{t\\rightarrow 0^+}\\frac{f(x_0+td)-f(x_0)}{t}=\\triangledown f(x_0)^Td&lt;0 \\\\ \\Rightarrow \\exists t&gt;0,s.t.,f(x_0+sd)-f(x_0)&lt;0,for\\forall s \\leq t \\\\ \\Rightarrow \\exists t&gt;0,s.t.,f(x_0+sd)&lt;f(x_0),for\\forall s \\leq t$$ 很明顯 steepest descent $d=-\\triangledown f(x_0)$ 是 descent direction. 其實只要滿足這種形式 $d=-B\\triangledown f(x_0)$ 當 $B$ 是正定，就會是 descent direction. 而當 B 定義為 $\\triangledown ^2 f(x_0)$ (Hessian Matrix 是半正定, 通常是 full rank 就會正定), 這種形式就是牛頓法 $d=−\\triangledown ^2 f(x_0)\\triangledown f (x_0)$ 不過我們今天要處理的是 constrained opt, 會有等式或不等式的條件, 因此我們的搜尋空間只能在滿足這些條件下去搜尋, 稱該空間為 feasible set = {x|x滿足所有(2)式的條件}. 可以想像, 在 feasible set 的限制下, 能搜尋的 direction 會被限制. 因此 “Numerical Optimization” 這本書就展開了一系列的討論和證明, 可以得到在這個 feasible set 下, 這些 能搜尋的方向(我們稱為 limiting direction)所構成的集合 究竟長什麼樣. 且發生在最佳解上的 limiting directions 都不會是 descent direction (合理, 不然就找到更佳的解了). 此外, 看課本的話, 會繞更大一圈才會知道什麼是 KKT Conditions (但是相當嚴謹且豐富). 為了清楚了解 KKT, 我們繞過課本的方法, 完全採用微積分學過的 Lagrange Multiplier Theorem 來說明. 了解 KKT Conditions限制條件為等式其實 KKT 的表達全部圍繞在 Lagrange Multiplier Theorem 上. 一般課本上講的都是等式條件, 我們列出高維課本裏頭的定理:不想打 Latex 了 &gt;&lt;, 貼圖好了 [Thm2]: Lagrange Multiplier Theorem 考慮以下問題 $$\\min f(x) \\\\ \\mbox{subject to } \\begin{array}{rcl} c(x)=0 \\\\ \\end{array}$$ 我們可以得到, 若 $x^\\ast$ 為一個 local minimum, 由 Thm2 知道, 滿足 $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$, for some $\\lambda$此時的 $\\lambda$ 正負都有可能, 也就說明了兩個 gradients 是平行的. 用圖來說明如下: 限制條件為不等式考慮以下問題 $$\\min f(x) \\\\ \\mbox{subject to } \\begin{array}{rcl} c(x)\\geq 0 \\\\ \\end{array}$$ 當 $x^\\ast$ 為一個 local minimum 會發生什麼事? 分成兩種情況討論: $c(x^\\ast)=0$ $c(x^\\ast)&gt;0$ 第一種情況就退化成條件為等式的情形. 因此存在 $\\lambda$ 滿足 $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$. 如果 $\\lambda&lt;0$, 導致 $\\triangledown c(x^\\ast)$ 跟 $\\triangledown f(x^\\ast)$ 反方向的話, $\\triangledown c(x^\\ast)^T\\triangledown f(x^\\ast)&lt;0$ 導致 $\\triangledown c(x^\\ast)$ 變成一個 desent direction.則表示我們可以找到一個方向使得目標函數值下降且同時讓條件函數值上升(因此仍然是 feasible), 那麼與 $x^\\ast$ 是 local minimum 矛盾. 因此得到的結論是 $\\triangledown c(x^\\ast)$ 跟 $\\triangledown f(x^\\ast)$ 同方向, i.e., $\\lambda\\geq 0$. 圖示如下: 第二種情況是退化成 unconstrained opt, 因為 $x^\\ast$ 是在 feasible set 內, 換句話說 $x^\\ast$ 是 feasible set 的 interior point. 既然是 unconstrained opt, 且 $x^\\ast$ 為 local minimum, 則表示 $\\triangledown f(x^\\ast)=0$, 所以當然也可以寫成 $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$ 只不過此時的 $\\lambda=0$ 所以不管是第一種或是第二種情形, 我們都可以寫成 存在 $\\lambda\\geq 0$ 滿足 $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$ KKT Conditions到這裡為止, 我們基本上已經可以列出 KKT 了: [Thm3]: Karush‐Kuhn‐Tucker conditions[Pf]:Condition 1 只是說明具有 Lagrange Multiplier 的表達方式: $\\triangledown f(x^\\ast)=\\sum_i{\\lambda_i\\triangledown c_i(x^\\ast)}$Condition 2,3 是說明 $x^\\ast$ 是 feasible point, 這是廢話Condition 4 說明 若條件為不等式, 相對應的 Lagrange Multipliers 必須大於等於0, 我們在上一段討論了Condition 5 稱為 complementarity slackness (我知道很難念…), 這需要稍微說明一下如果 $c_i$ 是等式條件, 則 $c_i(x^\\ast)=0$, 因此滿足 Condition 5如果 $c_i$ 是不等式條件, 但是 $c_i(x^\\ast)=0$, 同樣也滿足 Condition 5最後一種情況是 $c_i$ 是不等式條件, 且 $c_i(x^\\ast)&gt;0$. 還記得我們上面針對此種情形的討論嗎? 我們會令他的 $\\lambda_i=0$, 所以還是滿足 Condition 5. 這裡沒有提到一件事情就是 LICQ, 全名 Linear Independent Constraint Qualification, 可參考 wiki KKT. LICQ 條件為: 對於某一點 feasible point x, 所有等式條件 (包含那些不等式條件但剛好在 x 變成等式) 在 x 這點的 gradients 都是線性獨立. 這個條件正好可以從 [Thm2]: Lagrange Multiplier Theorem 裡面看出來, Thm2 說明如果等式條件的 gradients 都線性獨立, 可以把 $\\mu=1$, 因此可以寫成 Condition 1: $\\triangledown f(x^\\ast)=\\sum_i{\\lambda_i\\triangledown c_i(x^\\ast)}$, 也因此可以滿足 KKT. 課本裡的證法課本裡的證明實在頗迂迴, 但是提供了很豐富和深刻的理解. 這裡還是努力記錄下來吧! {數學多, 謹慎服用} 我們分成5個步驟來討論: Limiting directions Limiting direction 與 Local minimum 的關聯 Limiting directions 的集合, 就稱為 F 吧 LICQ 成立時, “Limiting directions 都不是 descent direction” 與 “Lagrange Multipliers” 的等價關係 串起來變成 KKT 1. Limiting directions直觀來說, 對於某一點 $x_0$ (當然屬於 feasible set) 用在 feasible set 中的某條路徑去逼近它, 而逼近的最後方向就是 limiting direction. 另外, 一個 sequence ${z_k}$ 都屬於 feasible set , 都不等於 $x_0$, 且最後逼近 $x_0$, 我們稱為 feasible sequence. [Def]:若滿足以下條件稱 $d$ 是 $x_0$ 的 limiting direction. (當然 $x_0$ 是 feasible point)存在一個 feasible sequence $(z_k)_k$ 使得該 sequence 有一個 subsequence$$\\exists (z_{k_j})_j \\mbox{ such that } d = \\lim\\frac{(z_{k_j}-x_0)}{\\parallel z_{k_j}-x_0\\parallel}$$ 從定義上我們可以知道 limiting direction 長度為 1, 因為我們只在乎方向. 另外要特別說存在一個 subsequence 是因為 feasible sequence 不會只有一個 limiting direction. 例子如下: 2. Limiting direction 與 Local minimum 的關聯文章開頭有說明, “如果 $x^\\ast$ 已經是 (local) minimum 了, 在該點上不應該存在 descent direction.” 對於 constrained opt 的版本相當於 “如果 $x^\\ast$ 已經是 (local) minimum 了, 它的 limiting directions 都不能是 descent direction.” 用數學寫出來如下: [Thm4]:已知 $x^\\ast$ 是一個 local minimum, 則它所有的 limiting direction $d$ 都滿足 $\\triangledown f(x^\\ast)^Td \\geq 0$ 直觀上如果不滿足, 我們就可以找到一個 feasible sequence 從而得到該 limiting direction 會是一個 descent direction, 因此與 $x^\\ast$ 是 local minium 矛盾.我們在等下的第4個步驟可以看到此條件 “所有的 limiting direction $d$ 都滿足 $\\triangledown f(x^\\ast)^Td \\geq 0$” 等價於 KKT Conditions 的表達方式. 因此 Thm4 可以重寫成 “已知 $x^\\ast$ 是一個 local minimum, 則滿足 KKT Conditions”, 在最後第5步會串起來. 3. Limiting directions 的集合 (F) [Def]: Active Set對於某一 feasible point $x_0$, 它的 active set $\\mathbf{A}(x_0)$ 定義為$\\mathbf{A}(x_0) = \\mathbf{E} \\cup \\{i \\in \\mathbf{I} | c_i(x_0)=0 \\}$ [Def]: LICQ如果以下集合為線性獨立集, 則稱 LICQ 在 $x_0$ 成立$\\{\\triangledown c_i(x_0), i \\in \\mathbf{A}(x_0) \\}$ 其實我們在上面的討論都有使用這兩個定義, 這裡只不過用數學表示方便等下的討論.某一點它的所有 limiting direction 的集合 ($F$) 如下: [Thm5]:對於某一 feasible point $x_0$,$$F=\\left\\{ \\begin{array}{c|r} d &amp; \\begin{array}{rcl} d^T\\triangledown c_i(x_0)=0,i \\in \\mathbf{E} \\\\ d^T\\triangledown c_i(x_0) \\geq 0, i \\in \\mathbf{A}(x_0) \\cap \\mathbf{I} \\\\ \\parallel d \\parallel = 1\\\\ \\end{array} \\end{array} \\right\\}$$ 為了不模糊焦點, 證明就跳過, 想看的童鞋門就查一下舊的筆記 另外, 定義 $F_1=\\alpha F$, for $\\alpha\\geq 0$ (所以是 convex cone). 因此 $F1$ 只不過是把 $\\parallel d \\parallel =1$ 的條件去調. 4. LICQ 成立時, 關鍵的等價關係我們以下都假設 LICQ 成立, 這麼做就可以很方便地讓 limiting direction 的集合用 $F1$ 來表示. 還記得在 “2. Limiting direction 與 Local minimum 的關聯” 有提到我們希望找到某一 feasible point $x_0$ 的 $F1$ 都不是 descent direction, 因此該點就很有可能是我要找的 local optimum. 而這一個條件 “某一 feasible point $x_0$ 的 $F1$ 都不是 descent direction” 其實與 Lagrange Multipliers 息息相關, 也因此跟 KKT conditions 會產生連結. 下面定理可以證明這個條件可以等價於 KKT condition 的表達方式. [Thm6]:對於某一 feasible point $x_0$, Let $\\mathbf{A}(x_0)=(1…m)$, $\\mathbf{A}^T=[\\triangledown c_1(x_0)…\\triangledown c_m(x_0)]$ 則$$\\triangledown f(x-0)^Td\\geq 0,\\forall d \\in F1 \\Leftrightarrow\\\\ \\exists \\lambda \\in \\mathbb{R}^m \\mbox{ where } \\lambda_i \\geq 0 \\forall i \\in \\mathbf{A}(x_0) \\cap \\mathbf{I} \\mbox{, such that } \\triangledown f(x_0)=\\sum_{i=1}^m \\lambda_i \\triangledown c_i(x_0)=\\mathbf{A}^T$$ 同樣跳過, 證明可查看舊的筆記 我們可以仔細對照一下上面的 Lagrange Multiplier 那個條件, 其實它跟 “[Thm3]: Karush‐Kuhn‐Tucker conditions” 是一樣的, 只差在一個地方就是 complementarity slackness 沒有明確寫出來, 但我們知道一定存在 $\\lambda$ 可以滿足. 因此這個 Lagrange Multiplier 的條件也就是 KKT 的表達方式. 5. 串起來變成 KKT腦袋差不多都打結了, 目前為止到底得到了什麼關係? 我們來整理一下 Thm4 告訴我們一個最佳解一定會使得它的 limiting directions 都不是 descent direction.Thm5 告訴我們 limiting directions 的集合其實就是 $F1$ (or $F$).Thm6 告訴我們對於任一個 $F1$ 的 direction, 都不是 descent direction, 等同於滿足 KKT 的表達方式. 將 Thm4,5,6 串起來變成: 一個最佳解滿足 KKT 的表達方式. (當然前提是有滿足 LICQ) 打這篇累到不想有結論的結論不想有結論了, 乾脆來碎碎念吧. 最佳化是我唸書期間很愛的一門科目, 當時愈是唸它, 愈是不懂. 以前也很愛看 Stephen P. Boyd 的 convex opt 課程, 但現在腦袋裡似乎只剩教授名字和課程名字了. 喔對了, 我還記得一件事情, 就是在 convex 問題時, KKT condition 會變成 充要條件. 至於細節, 恩… [待補充]: 我有找到當時的筆記關於 convex 問題下的 KKT conditions, 以及它的 dual problem 討論. 只能說: 1. 學生可以很自由掌控自己的時間, 工作後的時間都是零碎的阿! 根本沒法長時間死嗑某一樣學科! 2. 當碼農工程師數學真的會退步, 碼農友碼農的好, 但希望自己也別忘記重要的數學觀念了. Reference Numerical Optimization 2nd edition, Jorge Nocedal 筆記 for the proof of Thm5,6","tags":[{"name":"Nonlinear Constraint Optimization","slug":"Nonlinear-Constraint-Optimization","permalink":"http://yoursite.com/tags/Nonlinear-Constraint-Optimization/"},{"name":"KKT","slug":"KKT","permalink":"http://yoursite.com/tags/KKT/"}]},{"title":"TF Notes (2), Speedup and Benchmark with Two GPU Cards","date":"2017-11-01T12:28:49.000Z","path":"2017/11/01/TF-Notes-Speedup-and-Benchmark-with-Two-GPU-Cards/","text":"這篇文章實作了官網的 同步式 data Parallelism 方法 ref，並且與原本只用一張GPU做個比較。實驗只使用兩張卡，多張卡方法一樣。主要架構如下圖 by TF 官網: 兩張卡等於是把一次要計算的 mini-batch 拆成兩半給兩個 (相同的) models 去並行計算 gradients，然後再交由 cpu 統一更新 model。詳細請自行參考官網。下面直接秀 Codes 和結果。 Machine Spec.GPU 卡為 Tesla K40c CPU 為 Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz 單 GPU 跑 MNIST直接上 Codes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201import gzipimport osimport tempfileimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.layers import flattenfrom tensorflow.examples.tutorials.mnist import input_datafrom sklearn.utils import shuffle\"\"\"Data Loading\"\"\"dataPath='../dataset/MNIST_data/'mnist = input_data.read_data_sets(dataPath, one_hot=True)# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]img_width = 28img_height = 28images = mnist.train.imagesimg_num, _ = images.shapeimages = np.reshape(images,(img_num,img_height,img_width))images = images[...,np.newaxis]print('(Input to CNN) Images with shape &#123;&#125;'.format(images.shape))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# load the validation setimages_valid = mnist.validation.imagesimg_num_valid = len(images_valid)images_valid = np.reshape(images_valid,(img_num_valid,img_height,img_width))images_valid = images_valid[...,np.newaxis]labels1Hot_valid = mnist.validation.labelsprint('Having %d number of validation images' % img_num_valid)# plotting sample imagesplt.figure(figsize=(15,5))for i in np.arange(2*7): random_idx = np.random.randint(0,img_num) plt.subplot(2,7,i+1) plt.imshow(images[random_idx][...,0],cmap='gray') plt.title(labels[random_idx][0])\"\"\"First define the hyper-parameters\"\"\"# Hyper-parametersEPOCHS = 30BATCH_SIZE = 512rate = 0.001drop_out_keep_prob = 0.5ksize = 5cnn_depth_list = [16, 32]mlp_depth_list = [256, 128]cNum = 1\"\"\"Define the input output tensors\"\"\"# using one-hot decodingx = tf.placeholder(tf.float32, (None, img_height, img_width, cNum))one_hot_y = tf.placeholder(tf.int32, (None, n_classes))#one_hot_y = tf.one_hot(y, n_classes)keep_prob = tf.placeholder(tf.float32) # probability to keep units\"\"\"Define the graph and construct it\"\"\"class MNISTCNN: def __init__(self, ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes): self._ksize = ksize self._cnn_depth_list = cnn_depth_list self._mlp_depth_list = mlp_depth_list self._img_height = img_height self._img_width = img_width self._cNum = cNum self._n_classes = n_classes self._mu = 0 self._sigma = 0.1 def create(self,x,keep_prob): conv = self._conv(x, self._cNum, self._ksize, self._cnn_depth_list[0], 'conv1') # Pooling. Input = 24x24xlayer_depth['layer_1']. Output = 12x12xlayer_depth['layer_1']. conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') for lidx in range(1,len(self._cnn_depth_list)): conv = self._conv(conv, self._cnn_depth_list[lidx-1], self._ksize, self._cnn_depth_list[lidx], 'conv&#123;&#125;'.format(lidx+1)) conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') fc = flatten(conv) bsize, fc_in_dim = fc.shape fc = self._fc(fc,fc_in_dim,self._mlp_depth_list[0],'fc1') fc = tf.nn.dropout(fc, keep_prob) # dropout for lidx in range(1,len(self._mlp_depth_list)): fc = self._fc(fc,self._mlp_depth_list[lidx-1],self._mlp_depth_list[lidx],'fc&#123;&#125;'.format(lidx+1)) fc = tf.nn.dropout(fc, keep_prob) # dropout with tf.variable_scope('logits') as scope: logits_w = tf.get_variable('logits_w', shape=[self._mlp_depth_list[-1],self._n_classes],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) logits_b = tf.get_variable('logits_b', shape=[self._n_classes],\\ initializer=tf.zeros_initializer) logits = tf.nn.xw_plus_b(fc, logits_w, logits_b, name=scope.name) print(logits.shape) return logits def _conv(self, x, in_depth, ksize, out_depth, scope_name, relu=True): bsize,h,w,cNum = x.shape assert(h-(ksize-1)&gt;=1) assert(w-(ksize-1)&gt;=1) with tf.variable_scope(scope_name) as scope: # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=(ksize, ksize, in_depth, out_depth),\\ initializer=tf.random_normal_initializer(self._mu,self._sigma)) biases = tf.get_variable('biases', shape=(out_depth),initializer=tf.zeros_initializer) out = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding='VALID',name=scope.name) + biases if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out def _fc(self, x, num_in, num_out, scope_name, relu=True): \"\"\"Create a fully connected layer.\"\"\" with tf.variable_scope(scope_name) as scope: # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=[num_in, num_out],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) biases = tf.get_variable('biases', [num_out],\\ initializer=tf.zeros_initializer) # Matrix multiply weights and inputs and add bias out = tf.nn.xw_plus_b(x, weights, biases, name=scope.name) if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out mnistCNN = MNISTCNN(ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes)logits = mnistCNN.create(x,keep_prob)\"\"\"Define loss and optimizer\"\"\"cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits)loss_operation = tf.reduce_mean(cross_entropy)optimizer = tf.train.AdamOptimizer(learning_rate = rate)training_operation = optimizer.minimize(loss_operation)# Define accuracy evaluation# calculate the average accuracy by calling evaluate(X_data, y_data)correct_prediction = tf.equal(tf.argmax(logits, axis=1), tf.argmax(one_hot_y, axis=1))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))def evaluate(X_data, y_data): num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] accuracy = sess.run(accuracy_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y, keep_prob: 1.0&#125;) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples \"\"\"Run Session\"\"\"### Train your model here.import timeif not os.path.isdir('./models'): os.makedirs('./models')#saver = tf.train.Saver()accumulate_time = 0.0with tf.Session() as sess: sess.run(tf.global_variables_initializer()) num_examples = img_num print(\"Training...\") print() train_accuracy = np.zeros(EPOCHS) validation_accuracy = np.zeros(EPOCHS) for i in range(EPOCHS): stime = time.time() acc_train_accuracy = 0 X_train, y_train = shuffle(images, labels1Hot) for offset in range(0, num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] sess.run(training_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y, keep_prob: drop_out_keep_prob&#125;) etime = time.time() accumulate_time += etime - stime validation_accuracy[i] = evaluate(images_valid, labels1Hot_valid) print(\"EPOCH &#123;&#125; ...\".format(i+1)) print(\"Validation Accuracy = &#123;:.3f&#125;\".format(validation_accuracy[i])) print() print('Cost time: ' + str(accumulate_time) + ' sec.') 同步式 data Parallelism 在兩張 GPU 跑 MNIST一樣直接上 Codes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287import gzipimport osimport tempfileimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.layers import flattenfrom tensorflow.examples.tutorials.mnist import input_datafrom sklearn.utils import shuffle\"\"\"Data Loading\"\"\"dataPath='../dataset/MNIST_data/'mnist = input_data.read_data_sets(dataPath, one_hot=True)# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]img_width = 28img_height = 28images = mnist.train.imagesimg_num, _ = images.shapeimages = np.reshape(images,(img_num,img_height,img_width))images = images[...,np.newaxis]print('(Input to CNN) Images with shape &#123;&#125;'.format(images.shape))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# load the validation setimages_valid = mnist.validation.imagesimg_num_valid = len(images_valid)images_valid = np.reshape(images_valid,(img_num_valid,img_height,img_width))images_valid = images_valid[...,np.newaxis]labels1Hot_valid = mnist.validation.labelsprint('Having %d number of validation images' % img_num_valid)plt.figure(figsize=(15,5))for i in np.arange(2*7): random_idx = np.random.randint(0,img_num) plt.subplot(2,7,i+1) plt.imshow(images[random_idx][...,0],cmap='gray') plt.title(labels[random_idx][0])\"\"\"First define the hyper-parameters\"\"\"# Hyper-parametersEPOCHS = 30BATCH_SIZE = 512rate = 0.001drop_out_keep_prob = 0.5ksize = 5cnn_depth_list = [16, 32]mlp_depth_list = [256, 128]cNum = 1\"\"\"Define the graph and construct it\"\"\"class MNISTCNN: def __init__(self, ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes): self._ksize = ksize self._cnn_depth_list = cnn_depth_list self._mlp_depth_list = mlp_depth_list self._img_height = img_height self._img_width = img_width self._cNum = cNum self._n_classes = n_classes self._mu = 0 self._sigma = 0.1 def create(self,x,keep_prob): conv = self._conv(x, self._cNum, self._ksize, self._cnn_depth_list[0], 'conv1') # Pooling. Input = 24x24xlayer_depth['layer_1']. Output = 12x12xlayer_depth['layer_1']. conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') for lidx in range(1,len(self._cnn_depth_list)): conv = self._conv(conv, self._cnn_depth_list[lidx-1], self._ksize, self._cnn_depth_list[lidx], 'conv&#123;&#125;'.format(lidx+1)) conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') fc = flatten(conv) bsize, fc_in_dim = fc.shape fc = self._fc(fc,fc_in_dim,self._mlp_depth_list[0],'fc1') fc = tf.nn.dropout(fc, keep_prob) # dropout for lidx in range(1,len(self._mlp_depth_list)): fc = self._fc(fc,self._mlp_depth_list[lidx-1],self._mlp_depth_list[lidx],'fc&#123;&#125;'.format(lidx+1)) fc = tf.nn.dropout(fc, keep_prob) # dropout with tf.variable_scope('logits') as scope: with tf.device('/cpu:0'): logits_w = tf.get_variable('logits_w', shape=[self._mlp_depth_list[-1],self._n_classes],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) logits_b = tf.get_variable('logits_b', shape=[self._n_classes],\\ initializer=tf.zeros_initializer) logits = tf.nn.xw_plus_b(fc, logits_w, logits_b, name=scope.name) print(logits.shape) return logits def _conv(self, x, in_depth, ksize, out_depth, scope_name, relu=True): bsize,h,w,cNum = x.shape assert(h-(ksize-1)&gt;=1) assert(w-(ksize-1)&gt;=1) with tf.variable_scope(scope_name) as scope: with tf.device('/cpu:0'): # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=(ksize, ksize, in_depth, out_depth),\\ initializer=tf.random_normal_initializer(self._mu,self._sigma)) biases = tf.get_variable('biases', shape=(out_depth),initializer=tf.zeros_initializer) out = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding='VALID',name=scope.name) + biases if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out def _fc(self, x, num_in, num_out, scope_name, relu=True): \"\"\"Create a fully connected layer.\"\"\" with tf.variable_scope(scope_name) as scope: with tf.device('/cpu:0'): # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=[num_in, num_out],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) biases = tf.get_variable('biases', [num_out],\\ initializer=tf.zeros_initializer) # Matrix multiply weights and inputs and add bias out = tf.nn.xw_plus_b(x, weights, biases, name=scope.name) if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out mnistCNN = MNISTCNN(ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes)# Averaging gradients for all tower models on GPUdef average_gradients(tower_grads): \"\"\"Calculate the average gradient for each shared variable across all towers. Note that this function provides a synchronization point across all towers. Args: tower_grads: List of lists of (gradient, variable) tuples. The outer list is over individual gradients. The inner list is over the gradient calculation for each tower. Returns: List of pairs of (gradient, variable) where the gradient has been averaged across all towers. \"\"\" average_grads = [] for grad_and_vars in zip(*tower_grads): # Note that each grad_and_vars looks like the following: # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN)) grads = [] for g, _ in grad_and_vars: # Add 0 dimension to the gradients to represent the tower. expanded_g = tf.expand_dims(g, 0) # Append on a 'tower' dimension which we will average over below. grads.append(expanded_g) # Average over the 'tower' dimension. grad = tf.concat(axis=0, values=grads) grad = tf.reduce_mean(grad, 0) # Keep in mind that the Variables are redundant because they are shared # across towers. So .. we will just return the first tower's pointer to # the Variable. v = grad_and_vars[0][1] grad_and_var = (grad, v) average_grads.append(grad_and_var) return average_grads# Construct model for each GPU, where variables are shared/updated by CPUwith tf.device('/cpu:0'): optimizer = tf.train.AdamOptimizer(learning_rate = rate)# Calculate the gradients for each model tower.tower_grads = []logits_list = []feed_x = []feed_one_hot_y = []keep_prob = tf.placeholder(tf.float32) # probability to keep unitswith tf.variable_scope(tf.get_variable_scope()): for i in range(2): with tf.device('/gpu:%d' % i): x = tf.placeholder(tf.float32, (None, img_height, img_width, cNum)) feed_x.append(x) one_hot_y = tf.placeholder(tf.int32, (None, n_classes)) feed_one_hot_y.append(one_hot_y) logits = mnistCNN.create(x,keep_prob) logits_list.append(logits) cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits) loss_op = tf.reduce_mean(cross_entropy) tf.get_variable_scope().reuse_variables() # Calculate the gradients for each batch of data on this model tower. grads = optimizer.compute_gradients(loss_op) # Keep track of the gradients across all towers. tower_grads.append(grads)with tf.device('/cpu:0'): # We must calculate the mean of each gradient. Note that this is the # synchronization point across all towers. grads = average_gradients(tower_grads) # Apply the gradients to adjust the shared variables. apply_gradient_op = optimizer.apply_gradients(grads) training_op = apply_gradient_op \"\"\"Prediction/Inference Part\"\"\"# Define accuracy evaluation, calculate the average accuracy by calling evaluate(X_data, y_data)# Using model that in the First GPU to calculatecorrect_prediction = tf.equal(tf.argmax(logits_list[0], axis=1), tf.argmax(feed_one_hot_y[0], axis=1))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))def evaluate(X_data, y_data): num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] accuracy = sess.run(accuracy_operation, feed_dict=&#123;feed_x[0]: batch_x, feed_one_hot_y[0]: batch_y, keep_prob: 1.0&#125;) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples \"\"\"Run Session\"\"\"# &#123;feed_x[0]:batch_x_1, feed_x[1]:batch_x_2,\\# feed_one_hot_y[0]:batch_y_1, feed_one_hot_y[1]:batch_y_1, keep_prob:drop_out_keep_prob&#125;def gen_feed_dict(batch_x, batch_y, drop_out_keep_prob): assert(len(batch_x)==len(batch_y)) assert(len(batch_x)%2==0) data_num = int(len(batch_x)/2) rtn_dict = &#123;&#125; rtn_dict[feed_x[0]] = batch_x[:data_num] rtn_dict[feed_x[1]] = batch_x[data_num:] rtn_dict[feed_one_hot_y[0]] = batch_y[:data_num] rtn_dict[feed_one_hot_y[1]] = batch_y[data_num:] rtn_dict[keep_prob] = drop_out_keep_prob return rtn_dict ### Train your model here.import timeif not os.path.isdir('./models'): os.makedirs('./models')#saver = tf.train.Saver()accumulate_time = 0.0with tf.Session() as sess: sess.run(tf.global_variables_initializer()) num_examples = img_num print(\"Training...\") print() train_accuracy = np.zeros(EPOCHS) validation_accuracy = np.zeros(EPOCHS) stime = time.time() for i in range(EPOCHS): stime = time.time() acc_train_accuracy = 0 X_train, y_train = shuffle(images, labels1Hot) for offset in range(0, num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] feed_dict = gen_feed_dict(batch_x, batch_y, drop_out_keep_prob) sess.run(training_op, feed_dict=feed_dict) etime = time.time() accumulate_time += etime - stime validation_accuracy[i] = evaluate(images_valid, labels1Hot_valid) print(\"EPOCH &#123;&#125; ...\".format(i+1)) print(\"Validation Accuracy = &#123;:.3f&#125;\".format(validation_accuracy[i])) print('Cost time: ' + str(accumulate_time) + ' sec.') 幾個注意處: 記得建立 variables (tf.get_variable) 時要使用 with tf.device(&#39;/cpu:0&#39;): 確保變量是存在 cpu 內 可以跑一小段 code: 12with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: sess.run(tf.global_variables_initializer()) 來觀察變量是否正確放在 cpu 上。 延續 2. 若使用 jupyter notebook 可以這樣做 jupyter notebook &gt; outputlog，執行完 2. 的 code 接著 cat outputlog | grep &#39;cpu&#39; 觀察變量是否存在。 使用 collections (如下) 來確認變數有正確分享 (養成好習慣)123trainable_collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)global_collection = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)print('Without Scope: len(trainable_collection)=&#123;&#125;; len(global_collection)=&#123;&#125;'.format(len(trainable_collection),len(global_collection))) Benchmark Results batch size = 128 時，使用兩張 GPU 花的時間為一張的 0.78 倍。而 batch size = 512 時的效果更明顯，為 0.69 倍。 一點小結論這種同步的架構適合在 batch size 大的時候，效果會更明顯。實驗起來兩張卡在 512 batch size 花的時間在一張卡的 0.7 倍。不過相比使用兩張卡，一張卡其實有一點優勢是在變量全部放在 GPU 上，因此省去了 CPU &lt;–&gt; GPU 的傳輸代價。這也是主要只到 0.7 倍，而沒有接近 0.5 倍的關鍵原因。 Reference Tensorflow 官網 同步式 data Parallelism 方法 Tensorflow github cifar10_multi_gpu_train.py","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"}]},{"title":"A Toy Example for Teacher Student Domain Adaptation","date":"2017-10-22T02:19:54.000Z","path":"2017/10/22/A-Toy-Example-for-Teacher-Student-Domain-Adaptation/","text":"看了這篇 2017 Microsoft AI and Research 的文章 “Large-Scale Domain Adaptation via Teacher-Student Learning“ 覺得滿有意思的，加上很容易實作，因此就分析一下這篇的可行性。 設計了一個 MNIST Toy Example 來展示 T/S Learning 的能力，自己也想知道這個方法有多可靠。相關的實驗 code 請參考 github TS Learning Methods介紹一下 TS Learning 的方法。他要解決的問題描述如下 假設我們已經有了一個訓練好的語音辨識模型，現在要辨識遠場的聲音，原 Domain (近場錄製的聲音)，可能效果就會不好。要解決最直接的方法就是重新錄製遠場語料，錄製的過程中很容易可以取得同時有近場和遠場未標記的語料 (放兩個麥克風，一個近一個遠)，不過關鍵是標記成本太高。因此這篇就是想利用未標記的語料直接針對原 Domain 的模型 Adapt 到新的 Domain 上。 以上面論文中的圖來說，左邊 Teacher network 只能在 Source Domain 有好的辨識能力，目標是希望得到右邊的 Student network 能在 Target Domain 針對同樣問題也有好的辨識能力。論文方法是一開始先將 Teacher network 拷貝一份給 Student network ，接著就開始餵 parallel data 給兩個 networks。 所謂 parallel data 意思是相同的資料來源，但是在不同 domain 蒐集，例如同一個人講同一句話，一個近場麥克風蒐集到，另一個遠場蒐集到。目標函式就是希望兩個 network 的後驗概率相同 (兩者的後驗概率平方誤差為0)，而我們只更新 Student network。在實作上不會使用後驗概率來計算兩個 network 的誤差，會使用未經過 softmax 的那層，也就是一般說的 logits 來計算。原因簡單說明如下: softmax 會將同樣是 negative 的類別的機率都壓到很低，但是 negative examples 也有分好壞，讀者可以試試 [10,2,1] 經過 softmax 後， 2 跟 1 之間的差異會被抹平。因此好的做法是，不要使用 softmax ，而是使用 logits。 Hinton 在這篇論文裡修改了 softmax 函式，多了一個 temperature $T$，論文裡推導這樣修改的 softmax，其實跟目標函式使用 logits 的平方誤差是一樣的 (在 “T跟logits差異很大” 且 “logits的分布均值為0” 的條件下) 這樣做的物理意義就相當於，將 “某聲音的遠場表現在 Student network 眼裡”，視為跟 “該聲音的近場表現在 Teacher network 眼裡” 認定為相同一件事情。因此就不需要針對 data 做標記了，只需要拿到這樣的一大堆 parallel data 就可以，而這很容易。 附上論文上的步驟如下: 演算法就這樣而已，很單純吧。但是究竟有多靠普? 好奇心下，就用 MNIST 設計了 toy example，就是接下來的內容囉。 MNIST Toy Example for TS Learning實驗設定 and Teacher Network首先設定兩個 Domain 為: 一個原圖 (原世界)，另一個上下顛倒的圖 (上下顛倒的世界)。 Teacher network 是一個很簡單的 “6” 和 “9” 的辨識器，當然是在原世界訓練好的。如果直接拿 teacher network 去看顛倒的 6，期望它認出一樣是 6 是辨識不出來的! (同樣期望 teacher network 看出顛倒的 9 仍然是 9 也是辦不到的) 之所以會選 6 和 9，是因為就算上下顛倒，顛倒的 6 和正向的 9 看起來仍然是不同的! 同樣的，顛倒的 9 和正向的 6 一樣看起來不同 ! 我們得到的 Teacher network 辨識情況如下: 12345678910Training...EPOCH 1 ...Train Accuracy = 0.967; Flip Accuracy = 0.098EPOCH 2 ...Train Accuracy = 0.999; Flip Accuracy = 0.151EPOCH 3 ...Train Accuracy = 0.999; Flip Accuracy = 0.110 明顯看到辨識率接近 100%，但是一旦上下顛倒，辨識率只剩 10%。有意思的是，由於我們只有兩個類別，對於上下顛倒的辨識率剩10%可以看做: 顛倒的 6，會被認成 9，而顛倒的 9 會被認為 6。但事實上，顛倒的 6 和 9 還是不一樣。 Student network 訓練我們將 MNIST 其他影像上下顛倒，做出 parallel dataset，然後按照論文的做法做 unsupervised training。有趣的是得到結果如下: 12345678910111213141516171819EPOCH 1 ...Acc loss = 3.871242271944786Train Accuracy = 0.156; Flip Accuracy = 0.998EPOCH 2 ...Acc loss = 0.40557907682784994Train Accuracy = 0.101; Flip Accuracy = 0.999EPOCH 3 ...Acc loss = 0.3005437100890939Train Accuracy = 0.103; Flip Accuracy = 0.999EPOCH 4 ...Acc loss = 0.2651689475203995Train Accuracy = 0.097; Flip Accuracy = 0.999EPOCH 5 ...Acc loss = 0.23342516055794454Train Accuracy = 0.116; Flip Accuracy = 0.999 Student network 可以成功辨識 顛倒的 6 和顛倒的 9 了! 注意，我們從來沒有給過 Student network 顛倒的 6 和顛倒的 9 這些訓練資料! 但是現在它有能力辨識這兩種圖了! 但是同樣的，如果給 student network 看一個正向的 6，在他的眼哩，看起來就如同 teacher network 看到 9 一樣。 也就是說，Student network 失去了原 Domain 的辨識能力。 這與論文原作者的結論不大一樣。 用 parallel data 非監督學習到底學到了什麼? 給 T/S 網路看過很多很多的 parallel data 後，Teacher 眼裡的圖，在 Student 眼裡看起來就反過來，反之亦然。因此這時候如果給 Student network 看一個 “正向的6”，它會認為: 啊!這在 Teacher 眼裡看到的是一個顛倒的 6 。(而 teacher network 會將顛倒的 6 看做是 9) 因此我認為，Student netowrk 很容易失去原先 domain 的辨識能力，就像這個例子 student network 無法認出正向的 6 一樣。 Summary如何讓一個 network 同時有原 Domain 和新 Domain 的辨識能力呢 ? 以上面的 toy example 為例，就是辨識兩個 classes class 1: 6 and 顛倒的6class 2: 9 and 顛倒的9 最直覺的做法，就是 T and S models 都跑一次辨識，然後將兩個後驗概率加起來後算 argmax。缺點就是 model size 立馬變成兩倍。 怎麼讓模型 size 不要變成兩倍呢? 簡單想了一個方式，就是讓 student model 改成這樣的模型: 其中 M model 的部分負責將 上下顛倒的 domain 轉換成原 domain 的 input，然後這樣的 input 就可以原封不動地用 teacher model 去辨識。剛好這個問題其實用一個 permuation matrix 可以做上下顛倒，因此實驗上就直接使用一個 linear layer (沒有 activation function)，當然 backprob 算出來的不會正好是 permutation matrix 就是了。 收斂情況如下: 基本上比原先要慢，因為原來是所有的 weights 都可以調整，而現在只能動一個 linear layer 12345678910111213141516171819EPOCH 1 ...Acc loss = 9.52696829760659Train Accuracy = 0.460; Flip Accuracy = 0.806EPOCH 2 ...Acc loss = 3.6580730849143146Train Accuracy = 0.364; Flip Accuracy = 0.955EPOCH 3 ...Acc loss = 2.454553008463332Train Accuracy = 0.304; Flip Accuracy = 0.980EPOCH 4 ...Acc loss = 1.823352760733923Train Accuracy = 0.277; Flip Accuracy = 0.988EPOCH 5 ...Acc loss = 1.4707165408316494Train Accuracy = 0.235; Flip Accuracy = 0.992 這樣做法雖然 model size 小了很多，但是要同時辨識正的和顛倒的仍然要跑兩遍的 model。 有沒有方法結合 TS learning unsupervised 的方式，且同時兼顧兩邊的 domain 辨識能力呢? 就再思考看看囉。 Reference Large-Scale Domain Adaptation via Teacher-Student Learning Distilling the Knowledge in a Neural Network Toy Example github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Adaptation","slug":"Adaptation","permalink":"http://yoursite.com/tags/Adaptation/"}]},{"title":"Word Embeddings (Encoder-Decoder 架構)","date":"2017-09-07T13:22:53.000Z","path":"2017/09/07/Word-Embeddings-and-Encoder-Decoder-Neural-Net/","text":"From Sparse Vector to Embeddings with Encoder–Decoder Structure 求 Embeddings Encoder–Decoder 結構 字典向量若我們字典裡有 $N$ 個 words, 第 $i$ 個字 $w^i$ 應該怎麼表示呢? 通常使用 one-hot vector 來表示: 把 $w^i$ 變成一個長度 $N$ 的向量 $x^i$。 恭喜! 有了 vector 我們就可以套用數學模型了。 問題是這樣的向量太稀疏了，尤其是當字典非常大的時候。 稀疏向量對於模型訓練很沒有效率。 我們需要轉換到比較緊密的向量，通常稱為 embedding。 下圖舉例將 $x$ 對應到它的緊密向量 $e$, 緊密向量有 embed_dim 維度 先假設已知如何對應到緊密向量已知一個 N * embed_dim 的矩陣 $E$，第 $i$ 個 row $e^i$ 就是 $w^i$ 的 embedding。 我們就可以使用 $e$ 來代替原先的稀疏向量 $x$ 進行訓練，讓訓練更好更容易。 以一個語言模型來說，使用 LSTM 模型如下: 恩，這樣大功告成，我們的模型可以順利訓練 …. ?? 不對，$E$ 這個 lookup table 怎麼決定? Lookup Table 使用矩陣相乘答案是讓模型自己訓練決定。要更了解內部運作，我們先將 lookup table 使用矩陣相乘的方式來看。 所以使用 lookup table LSTM 的語言模型變成如下 等等，矩陣相乘不就跟 neural net 一樣嗎? 這樣看起來這個 lookup table $E$ 就是一層的類神經網路而已 (沒有 activation function)。 我們用 LL (Linear Layer) 來代表，$E$ 就是 LL 的 weight matrix。 表示成 neural net 的方式，我們就直接可以 Backprob 訓練出 LL 的 weight $E$ 了。而 $E$ 就是我們要找的 embeddings。 Tensorflow 中做這樣的 lookup table 可以使用 tf.nn.embedding_lookup()。 Embedding 的作法可參考 tf 官網此處。 LL很弱怎麼辦?只用一層線性組合 (LL) 就想把特徵擷取做到很好，似乎有點簡化了。 沒錯，我們都知道，特徵擷取是 Deep neural net 的拿手好戲，所以我們可以將 LL 換成強大的 CNN。 這種先經過一層特徵擷取，再做辨識，其實跟 Encoder – Decoder 的架構一樣。 都是先經過 Encoder 做出 embeddings，接著使用 Embeddings decode 出結果。 Encoder 如果也採用 RNN 的話基本上就是 sequence-to-sequence 的架構了。 基本上拓展一下，對圖或影像做 Encode，而 Decoder 負責解碼出描述的文字。或是語言翻譯，語音辨識，都可以這麼看待。 Reference Embedding tf 官網 link Sequence to sequence learning link Udacity lstm github colah lstm","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Embedding","slug":"Embedding","permalink":"http://yoursite.com/tags/Embedding/"}]},{"title":"AutoEncoder","date":"2017-08-26T08:38:22.000Z","path":"2017/08/26/AutoEncoder/","text":"使用 MNIST and notMNIST 做了一個 AutoEncoder with Fully Connected DNN 的實驗。 依序將實驗結果根據如下步驟顯示出來，程式碼可以參考 [github] Data Loading and Plotting AutoEncoder Graph Constructiona. Define the input output tensorsb. Define the graph and construct itc. Define loss and optimizer Run Session Show some reconstructed images Plot Embeddings Do Image Generation by Decoder Data Loading and PlottingMNIST training data 有 55000 筆資料，是一個 28x28 的 image，值的範圍是 [0~1]，因此會對 input 都減去 0.5 正規化。 而 notMNIST 整理過後有 200000 筆，同樣也是 28x28 的 image，但值的範圍已經是 [-0.5~0.5]。值得一提的是，此資料還參雜著一些錯誤，如下圖就可發現，第二列的第二個應為 J，但是標記是 A。因此 notMNIST 相對來說很挑戰，但我們一樣可以看到 AutoEncoder 也會做出一些合理的壓縮。 AutoEncoder Graph ConstructionDefine the input output tensorsInput x 與 Output y 都是一樣 (沒有要做 Denoise AutoEncoder)，其中 code 是建立 Decoder 時的 input tensor。 1234x = tf.placeholder(tf.float32, (None, img_dim))y = tf.placeholder(tf.float32, (None, img_dim))embedding_dim = 2code = tf.placeholder(tf.float32, (None, embedding_dim)) Define the graph and construct it針對 Encoder 和 Decoder 都使用同一組參數，這樣的好處是參數量直接少約一半，同時減少 overfitting 的機會。當然我們沒有理由一定要將參數綁再一起，可以各自用自己的方法 (參數、模型結構) 去 Encode 和 Decocde。結構如下: Define loss and optimizer注意到 loss 的定義除了原來的影像重建誤差之外，還多了一個 embeddings 的 l2-norm。這是為了希望在 embedding space 上 encode 之後都接近 0，減少那種很大的 outliers 出現。參考李宏毅 Deep AutoEncoder 123loss_op = tf.reduce_sum(tf.pow(tf.subtract(reconstruct_auto, y), 2.0)) + l2_weight* tf.reduce_sum(tf.pow(embedded_auto, 2.0))optimizer = tf.train.AdamOptimizer(learning_rate = rate)training_op = optimizer.minimize(loss_op) Run SessionAdam optimizer 跑了 100 個 epochs Show some reconstructed images隨機選幾個 MNSIT 的重建圖: 隨機選幾個 notMNSIT 的重建圖: 可以看到 notMNIST 果然難多了。 Plot EmbeddingsMNIST 針對所有 training data 求得的 2-d embeddings 如下: notMNIST 針對所有 training data 求得的 2-d embeddings 如下: 如果只要做到 unsupervised dimension reduction 的話，使用 t-SNE 求得的 embedding 會比上圖都好看很多。但 t-SNE 沒有 Decoder，無法給定一個 embedding 去求回原先的 image。而這種 Encoder - Decoder 結構就相對彈性很多。 t-SNE 的 MNIST 圖如下: Do Image Generation by Decoder我們針對 Embedding Space 的一個區域去等距取出很多點，然後使用 Decoder 去 decode 出 image 來。 MNIST 的範圍選擇為， x 軸和 y 軸 [-1~1] 間隔 0.2，共 100 個點。(可參考上面 embedding space 了解選擇的範圍) notMNIST 的範圍選擇為， x 軸和 y 軸 [-2~2] 間隔 0.2，共 400 個點。(可參考上面 embedding space 了解選擇的範圍) 可以發現 embedding space 的兩個維度具有某些意義在! Reference 李宏毅 Deep AutoEncoder Distill t-SNE Reducing the Dimensionality of Data with Neural Networks (Hinton 2006) 本文之 [github]","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"auto-encoder","slug":"auto-encoder","permalink":"http://yoursite.com/tags/auto-encoder/"}]},{"title":"TF Notes (1), Variable Construct Share and Collect","date":"2017-08-23T10:46:48.590Z","path":"2017/08/23/TF-Notes-Variable-Construct-Share-and-Collect/","text":"隔了好久沒有用 tensorflow，老實說已經忘得差不多了，加上之前其實沒有仔細了解 tf.Variable() 的 class 和 variable scope 的意思。因此就趁這個機會了解一番，順便當成使用 tf 的熱身囉! 這是 TensorFlow 筆記系列的第一篇文，之後遇到什麼自己不懂的，就會再筆記下去。 前言什麼是 Scope ?簡單來說 Tensorflow 為每一個 object (含 variable) 都指定一個 獨一無二的 ID, 這個 ID 可以是使用 scope 來達成的舉例來說1234with tf.variable_scope(\"foo\"): with tf.variable_scope(\"bar\"): v = tf.get_variable(\"v\", [1])assert v.name == \"foo/bar/v:0\" 可以看到變數 v 的 ID (v.name) 為 &quot;foo/bar/v:0&quot; 有這樣獨一無二的 ID 後，對於所有的變數就非常好管理了! 變數的建立、分享和蒐集注意到建立和取得變數都使用 tf.get_variable()，而 tf.get_variable() 只有在第一次呼叫該 ID 的變數時才會建立，其它次呼叫如果沒有將 ID 的 reuse 設定為 true 則會報錯 (認為已經有變數存在了)，因此如果 reuse 設定為 true，tf.get_variable() 就會拿到之前建立好的變數，達到 share 變數的效果。 tf 針對變數提供了 tf.get_variable() 和 tf.variable_scope() 機制來管理變數 (link)， tf.get_variable(name, shape, initializer): Creates or returns a variable with a given name.tf.variable_scope(scope_name): Manages namespaces for names passed to tf.get_variable(). 並提供了 tf.get_collection() 來取得一個變數的 list，這樣的好處是有時候我們只希望 update 某些變數 (例如 GAN training), 因此就需要將這些變數加到一個 collection 中，或是使用 scope 命名來篩選 collection。 tf.get_collection(key, scope=None): Returns a list of object with the key of collection and scope name. 此筆記會學到: 使用 tf.get_variable() 建立變數 Share Variables Variable Scope 規則 Variable Collections 使用 tf.get_variable() 建立變數 tf.get_variable(name, shape, initializer): Creates or returns a variable with a given name. 常用的 initializer 可有 tf.constant_initializer(value): initializes everything to the provided value,tf.random_uniform_initializer(a, b): initializes uniformly from [a, b],tf.random_normal_initializer(mean, stddev): initializes from the normal distribution with the given mean and standard deviation. Code1123var1 = tf.get_variable('name1',[2,1],initializer=tf.zeros_initializer)print('Construct a variable with name \"var1\" with shape &#123;&#125;'.format(var1.shape))print(var1) output 為12Construct a variable with name &quot;var1&quot; with shape (2, 1)&lt;tf.Variable &apos;name1:0&apos; shape=(2, 1) dtype=float32_ref&gt; Share Variables延續 Code1 繼續執行下去 (以下每個Code片段都是依次序執行下去的結果)1var1_cpy = tf.get_variable('name1') 會報錯，因為 &#39;name1&#39; 的變數已經存在，因此需要將目前 scope 下的 ID 都指定為 reuse 才能 share &#39;name1&#39; 的變數，目前的 scope 是空字串 (‘’) Code2123456789with tf.variable_scope('',reuse=True): var1_cpy = tf.get_variable('name1')print(var1_cpy)# another way to specify reusing variables under the current scopewith tf.variable_scope(''): tf.get_variable_scope().reuse_variables() var1_cpy = tf.get_variable('name1')print(var1_cpy) output 為12&lt;tf.Variable &apos;name1:0&apos; shape=(2, 1) dtype=float32_ref&gt;&lt;tf.Variable &apos;name1:0&apos; shape=(2, 1) dtype=float32_ref&gt; Variable Scope 規則Scope 如果指定為 reuse, 則底下的 sub-scope 為自動繼承 範例來自 tf 官網 (link) Code31234567891011121314with tf.variable_scope(\"root\"): # At start, the scope is not reusing. assert tf.get_variable_scope().reuse == False with tf.variable_scope(\"foo\"): # Opened a sub-scope, still not reusing. assert tf.get_variable_scope().reuse == False with tf.variable_scope(\"foo\", reuse=True): # Explicitly opened a reusing scope. assert tf.get_variable_scope().reuse == True with tf.variable_scope(\"bar\"): # Now sub-scope inherits the reuse flag. assert tf.get_variable_scope().reuse == True # Exited the reusing scope, back to a non-reusing one. assert tf.get_variable_scope().reuse == False 或者將 scope 存起來，直接 pass 給其他 tf.variable_scope 使用 Code41234567891011with tf.variable_scope(\"foo\") as foo_scope: v = tf.get_variable(\"v\", [1])with tf.variable_scope(foo_scope): w = tf.get_variable(\"w\", [1])with tf.variable_scope(foo_scope, reuse=True): v1 = tf.get_variable(\"v\", [1]) w1 = tf.get_variable(\"w\", [1])assert v1 is vassert w1 is wprint(v1)print(w1) output 為 12&lt;tf.Variable &apos;foo/v:0&apos; shape=(1,) dtype=float32_ref&gt;&lt;tf.Variable &apos;foo/w:0&apos; shape=(1,) dtype=float32_ref&gt; 注意到如果之前有存起來一個 scope, e.g. scope_pre.name == &#39;cope1/scope2&#39;, 然後假設目前在的 scope 為 ‘scopeA/scopeB/scopeC/scopeD’則 tf.variable_scope(scope_pre) 一定會跳到 ‘scope1/scope2’ 不管現在在哪 When opening a variable scope using a previously existing scope we jump out of the current variable scope prefix to an entirely different one. This is fully independent of where we do it. Code51234567with tf.variable_scope(\"foo\") as foo_scope: assert foo_scope.name == \"foo\"with tf.variable_scope(\"bar\"): with tf.variable_scope(\"baz\") as other_scope: assert other_scope.name == \"bar/baz\" with tf.variable_scope(foo_scope) as foo_scope2: assert foo_scope2.name == \"foo\" # Not changed. Variable Collections得到 CollectionsCollection 顧名思義就是將許多 objects (如 Variables, Tensors, Ops …) 都蒐集成一個 list，預設情況下每一個 tf.Variable 都會被放到如下兩個 collections (link): tf.GraphKeys.GLOBAL_VARIABLES — variables that can be shared across multiple devices, tf.GraphKeys.TRAINABLE_VARIABLES— variables for which TensorFlow will calculate gradients. 抓取這些 Collection，可使用 tf.get_collection(key), 並將 collection 的 key 傳入 注意到 Code6 中將變數 b 的 trainable 屬性設定為 False，因此沒有被放進 tf.GraphKeys.TRAINABLE_VARIABLES 此 collection 中。 Code61234567with tf.variable_scope('rootScope') as scope: a = tf.get_variable('a',[2,3])b = tf.get_variable('b',[3,2], trainable=False)trainable_collections = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)global_collections = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)print('Collection with name \"TRAINABLE_VARIABLES\"=\\n&#123;&#125;'.format(trainable_collections))print('Collection with name \"GLOBAL_VARIABLES\"=\\n&#123;&#125;'.format(global_collections)) output 為1234Collection with name &quot;TRAINABLE_VARIABLES&quot;=[&lt;tf.Variable &apos;name1:0&apos; shape=(2, 1) dtype=float32_ref&gt;, &lt;tf.Variable &apos;foo/v:0&apos; shape=(1,) dtype=float32_ref&gt;, &lt;tf.Variable &apos;foo/w:0&apos; shape=(1,) dtype=float32_ref&gt;, &lt;tf.Variable &apos;rootScope/a:0&apos; shape=(2, 3) dtype=float32_ref&gt;]Collection with name &quot;GLOBAL_VARIABLES&quot;=[&lt;tf.Variable &apos;name1:0&apos; shape=(2, 1) dtype=float32_ref&gt;, &lt;tf.Variable &apos;foo/v:0&apos; shape=(1,) dtype=float32_ref&gt;, &lt;tf.Variable &apos;foo/w:0&apos; shape=(1,) dtype=float32_ref&gt;, &lt;tf.Variable &apos;rootScope/a:0&apos; shape=(2, 3) dtype=float32_ref&gt;, &lt;tf.Variable &apos;b:0&apos; shape=(3, 2) dtype=float32_ref&gt;] 利用 Scope 進一步篩選 Collections利用 tf.get_collection(key, scope) 得到的 object list, 會被 scope 再進一步篩選! Code7123456with tf.variable_scope('Hello') as HelloScope: hello = tf.get_variable('helloName',shape=()) with tf.variable_scope('World') as WorldScope: world = tf.get_variable('worldName',shape=())print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=HelloScope.name))print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=WorldScope.name)) output 為12[&lt;tf.Variable &apos;Hello/helloName:0&apos; shape=() dtype=float32_ref&gt;, &lt;tf.Variable &apos;Hello/World/worldName:0&apos; shape=() dtype=float32_ref&gt;][&lt;tf.Variable &apos;Hello/World/worldName:0&apos; shape=() dtype=float32_ref&gt;] 建立自己的 collection可以使用 tf.add_to_collection(&#39;my_collection_name&#39;,obj)，範例如下 Code8123456c = hellotf.add_to_collection('demo_used',c)d = tf.constant(np.random.randint(0,10,(5,2)),name='d',dtype='int32')tf.add_to_collection('demo_used',d)my_collection = tf.get_collection('demo_used')print('Collection with name \"demo_used\"=&#123;&#125;'.format(my_collection)) output 為1Collection with name &quot;demo_used&quot;=[&lt;tf.Variable &apos;Hello/helloName:0&apos; shape=() dtype=float32_ref&gt;, &lt;tf.Tensor &apos;d:0&apos; shape=(5, 2) dtype=int32&gt;] 將篩選或自建的 Collections 傳給 Optimizer我們可以將指定的 collection 傳給 optimizer 的 minimize() 函式, 通過這種方式, 可以明確指定只 update 那些變數，範例如下: 12op1 = tf.train.RMSPropOptimizer().minimize(g_loss, var_list=my_collection1)op2 = tf.train.AdamOptimizer().minimize(g_loss,var_list=my_collection2) 針對 Optimizer.minimize() 的 ‘var_list’ 參數說明為: Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES. Reference https://www.tensorflow.org/programmers_guide/variables https://www.tensorflow.org/versions/r1.2/programmers_guide/variable_scope API doc","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"}]},{"title":"Notes for Model Predictive Control","date":"2017-06-28T12:01:19.000Z","path":"2017/06/28/ModelPredictiveControl/","text":"從一開始決定上課後，經過了半年終於來到 Udacity Term2 最後一個 Project 了。只能說盡量讓自己把每一個做的 project 都寫一篇 blog 記錄，但這陣子時間真的不夠用，所以這篇就從 high level 的角度瀏覽一下內容。 目的我們用上圖來說明目的，MPC 要做的事情，就是給定一個指定的 reference trajectory (黃色的曲線，通常用一個 3rd polynomail 表示)，我們經由 motion model 來計算出最佳的控制 ($(\\delta,a)$分別表示車子輪子的角度跟加速度)，最佳的意思就是這樣的控制會產生出一個 predicted trajectory (綠色的曲線) 使得跟 reference trajectory cost 最小。這就等於將問題轉換成一個 nonlinear constraint optimization problem 了。另外剛才提到的控制項 $(\\delta,a)$，其中的 $\\delta$ 為下圖的 Wheel Orientation 角度:而 $a$ 表示加速度，正值是踩油門，負值是踩煞車。這邊我們當然假設油門和煞車同時只會有一個存在啦，開車技術沒這麼好。 Motion Model 這 6 個 states $(x,y,\\psi,v,cte,e\\psi)$ 分別表示 (車子x座標, 車子x座標, 車子heading角度, 車子速度, Cross Track Error, Error of 車子角度)CTE 或稱 XTE 是 reference position 跟 actual position 之間的誤差 同理 $e\\psi$ 就是 reference 的角度跟實際角度的差值了，注意到，由於 reference trajectory 可能是一個 3rd polynomail，我們可以算切線來求得 reference 的角度。 Tools of Nonlinear Constraint Opt兩個主要的 tool: IpoptInterior Point OPTimization，用來解 nonlinear constraint opt 問題。 CppAD在使用 Ipopt 的時候，需要計算 function 的 gradients，而 CppAD 可以幫我們自動計算。 一個很棒的使用兩個 tools 解 opt 問題的範例: link Consider with Latency通常下了一道 actuator 命令 (例如加速度要多少、輪子角度要多少)，到實際上車子運作會有一個 delay，而 Udacity simulator 設定這個 latency 是 0.1 second。 這個 latency 在車子速度較快的時候，影響會很大，導致車子無法正確開完。一個簡單的解法就是我們利用 motion model 去預測經過 latency 後的車子 states，然後後面所有流程都一模一樣即可。 Results [Video] With considering latency: [Video] Without considering latency: Referencemy github 目前解鎖成就","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"Model Predictive Control","slug":"Model-Predictive-Control","permalink":"http://yoursite.com/tags/Model-Predictive-Control/"},{"name":"Nonlinear Constraint Optimization","slug":"Nonlinear-Constraint-Optimization","permalink":"http://yoursite.com/tags/Nonlinear-Constraint-Optimization/"}]},{"title":"Structure Perceptron and Structure SVM","date":"2017-05-20T01:41:27.000Z","path":"2017/05/20/Structure-Perceptron-and-Structure-SVM/","text":"記得當年念博的時候，對於SVM頗有愛，也覺得掌握度很高惹，就是 kernel method + convex optimization 的完美合體。直到某天看到 structureSVM，看了老半天實在不得要領，當時就放下沒再管了。多年後 (2015)，剛好台大李宏毅教授教的課程最後一堂 Project demo，有請我們部門介紹做的一些內容給學生，才看到了強大的李老師的課程內容。他所教的 structure learning/svm 實在有夠清楚，又非常 general，真的是強到爆! 本人又年輕，又謙虛，我的新偶像阿!附上一張我與新偶像的合照… XD 以下內容為筆記用，方便日後回想，來源 大都是李老師的內容。 A General Framework (Energy-based Model)一般來說 ML 要學習的是 $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$ 這樣的一個 mapping function，使得在學習到之後，能夠對於新的 input $x$ 求得預測的 $y=f(x)$。簡單的情況是沒問題，例如 binary classification、multi-class classification 或 regression。但是萬一要預測的是複雜得多的 output，譬如 $\\mathcal{Y}$ 是一個 tree、bounding box、或 sequence，原來的架構就很難定義了。 所以將要學的問題改成如下的架構。 Training: $F: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ Inference: Given an object $x$, $\\tilde{y}=argmax_y F(x,y)$ $F$ 可以想成用來計算 $(x,y)$ 的匹配度。而這樣的 function 也稱為 Energy-based model。好了，定義成這樣看起來沒什麼不同，該有的問題還是在，還是沒解決。沒關係，我們繼續看下去，先把三個重要的問題列出來。 Problem 1: 怎麼定義 $F(x,y)$ ? Problem 2: 怎麼解決 $argmax_y$ ? Problem 3: 怎麼訓練 $F(x,y)$ ? 這些問題在某種情況會變得很好解，什麼情況呢? 若我們將 $F(x,y)$ 定義成 Linear Model (Problem 1 用 linear 定義)，我們發現訓練變得很容易 (Problem 3 好解) !! 疑?! Problem 2呢? 先當作已解吧，ㄎㄎ。 Linear Model of $F(x,y)=w\\cdot \\phi(x,y)$我們先假裝 Problem 2 已解 (Problem 2 要能解 depends on 問題的domain，和feature的定義)，我們來看一下要怎麼訓練這樣的 linear model。首先用李老師課程的範例 (辨識初音) 的例子舉例，其中 $y$ 是一個 bounding box:這個例子有兩個 training pairs: $(x^1,\\hat{y}^1)$ 和 $(x^2,\\hat{y}^2)$，我們希望求得一個 $w$ 使得紅色的圓圈投影到 $w$ 上後要大於所有藍色的圓圈。同理，紅色的星星要大於所有藍色的星星。其實我們仔細想想，這問題跟 perceptron learning 非常類似，perceptron learning 在做的是 binary classification，而如果把每一筆 training data $(x^i,\\hat{y}^i)$ 和 $(x^i,y:y\\neq\\hat{y}^i)$ 當作是 positive and negative classes，剛好就是一個 bineary classification problem (雖然不同筆 training data 會有各自的 positive and negative 資料，但不影響整個問題)所以如果有解 (linear separable)，則我們可以使用 Structure Perceptron 在有限步驟內求解。 Structure Perceptron 證明的概念跟 perceptron 一樣，就是假設有解，解為 $\\hat{w}$，要求每一次的 update $w^k$，會跟 $\\hat{w}$ 愈來愈接近，也就是 $$\\begin{align} cos\\rho_k = \\frac{\\hat{w}\\cdot w^k}{\\Vert{\\hat{w}}\\Vert\\cdot\\Vert{w^k}\\Vert} \\end{align}$$ 要愈大愈好!但我們也知道 $cos$ 最大就 1，因此就有 upper bound，所以會在有限步驟搞定。詳細推倒步驟可參考李老師講義，或看 perceptron 的收斂證明。 Cost Function用 cost function 的角度來說，其實 perceptron 處理的 cost 是計算錯誤的次數，如果將 cost 的 function 畫出來的話，會是 step function，而無法做微分求解。因此通常會將 cost function 改成可微分的方式，例如 linear or quadratic or what ever continuous function。改成可微分就有很多好處了，可以針對 cost function 做很多需要的修改，這些修改包括 1. 對不同的錯誤有不同的懲罰 2. 加入 regularization term … 等等，我們等下會談到。 Picture is from Duda Pattern Classification 左圖就是原來的 perceptron cost，而右圖就是將 cost 改成 linear cost。Linear cost 可定義如下:$$\\begin{align} C=\\sum_{n=1}^N C^n \\\\ C^n=(max_y[w\\cdot\\phi(x^n,y)])-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$所以，就gradient descent下去吧，跟原來的 perceptron learning 改 cost function 一樣。 讓錯誤的程度能表現出來這是什麼意思呢? 原來的 cost function 對於每一個錯誤的情形都一視同仁，也就是在找那個 $w$ 的時候，只要錯誤的例子投影在 $w$ 上比正確的還要小就好，不在忽小多少，但事實上錯誤會有好壞之分。下面是一個李老師的例子，例如右邊黃色的框框雖然跟正確答案紅框框不同 (所以被當成錯誤的例子)，但有大致上都抓到初音的臉了，因此我們可以允許他跟正確答案較接近。因此 cost function 可以修改一下: $$\\begin{align} C=\\sum_{n=1}^N C^n \\\\ C^n=max_y[w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)]-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ $\\triangle(\\hat{y}^n,y)$ 定義了這個錯誤的例子額外的 cost (需&gt;=0)，以 bounding box 而言，舉例來說兩個 set A and B，$\\triangle(A,B)$ 可定義為 $1-/frac{A \\cap B}{A \\cup B}$。不過需要特別一提的是，多增加這個額外的定義，有可能使得原來容易解的 $argmax_y$ (Problem 2) 變得無法解，所以要注意。 Minimize the upper bound很有趣的一點是，在我們引入了 $\\triangle(\\hat{y}^n,y)$ (稱為 margin, 在後面講到 structure SVM 可以看得出來) 後，可以用另一個觀點來看這個問題。 假設我們希望能將 $C’$ 最小化: $$\\begin{align} \\tilde{y}^n=argmax_y{w\\cdot \\phi(x^n,y)} \\\\ C&apos;=\\sum_{n=1}^N{\\triangle(\\hat{y}^n,\\tilde{y}^n)} \\end{align}$$ 結果我們發現其實 $\\triangle(\\hat{y}^n,\\tilde{y}^n)\\leq C^n$，因而變成 而我們上面都是在最小化 $C$，所以其實我們在做的事情就是在最小化 $C’$ 的 upper bound。上界的證明如下:這種藉由最佳化 upper bound 的方式，在 adaboost 也見過。普遍來說，原來的式子不容易最佳化的時候，我們藉由定義一個容易最佳化的upper bound，然後最小化它。另外，EM 演算法也有類似的概念。 Regularization直接加入 norm-2 regularization:$$\\begin{align} C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N C^n \\\\ C^n=max_y[w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)]-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ Structure SVM先講結論: 上面 Linear Model 最後的 cost function (包含marginal and regularization terms) 就是等價於 SVM。原先問題 P1: Find $w$ that minimize $C$$$\\begin{align} C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N C^n \\\\ C^n=max_y[w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)]-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ 改寫後的問題 P2: Find $w$ that minimize $C$$$\\begin{align} C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N C^n \\\\ For \\forall{y}: C^n\\geq w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ 觀察 P2，我們注意到給定一個 $w$ 時，它最小的 $C^n$ 應該會是什麼呢? (找最小是因為我們要 minimize $C$) 譬如我要求 $x\\leq{ 5,1,2,10 }$ 這個式子的 $x$ 最小是多少，很明顯就是 $x=max{ 5,1,2,10 }$。因此式 P2 的式 (13) 可以寫成 P1 的式 (11)。寫成 P2 有什麼好處? 首先將 $C^n$ 改成 $\\epsilon^n$，然後再稍微改寫一下得到如下的問題: 問題 P3: Find $w,\\epsilon^n,\\epsilon^2,…,\\epsilon^N$ that minimize $C$$$C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N \\epsilon^n \\\\ For \\forall{y}\\neq{\\hat{y}^n}: w\\cdot (\\phi(x^n,\\hat{y}^n)-\\phi(x^n,y))\\leq \\triangle (\\hat{y}^n,y)-\\epsilon^n,\\epsilon^n\\leq 0$$ 注意到，對於一個 n-th training pair $(x^n,\\hat{y}^n)$ 和給定一個 $y\\neq\\hat{y}^n$ 來說，我們都會得到一個 linear constraint。可以將上面式子的 constant 用 a, b來表示變成:$w\\cdot a \\leq b - \\epsilon^n \\\\$ 發現了嗎? 對於變數 $w$ 和 $\\epsilon^n$ 來說，這就是一個 linear constraint。 眼尖的讀者，可能就會覺得 P3 很眼熟。沒錯!它跟 SVM 長很像! 讓我們來跟 SVM 的 Primal form (不是 dual form) 做個比較吧。可以發現有兩點不同，原 SVM from wiki 列出如下: Margin term 的不同，P3 的 margin 比較 general，可以根據每個 negative case 都有自己的 margin，而原來 binary SVM 的 margin 是定為 1。 Constraint 個數的不同，原 SVM 個數為 training data 的個數，但是 P3 的個數為無窮多個。 呼! 所以 P3 這個問題，就是 SVM 的 general 版本，我們也稱之為 Structure SVM，這裡終於跟 SVM 連結上了! Cutting Plane Algorithm原先 SVM 有限的 constraint 下，我們直接用一個 QP solver可以很快處理掉。但在 Structure SVM 有無窮多的 constraints 究竟要怎麼解? 是個問題。首先觀察到，其實很多 constraints 都是無效的。例如:所以這個演算法策略就是從一個空的 working set $\\mathbb{A}^n$ 出發，每次 iteration 都找一個最 violate 的 constraint 加進去，直到無法再加入任何的 constraint 為止。這裡其實有兩個問題要討論，第一個是什麼是最 violate 的 constraint? 第二個是，這演算法會收斂嗎? 難道不會永遠都找得到 violate 的 constraint 一直加入嗎?我們先把演算法列出來，再來討論上面這兩個問題。 Most Violated Constraint直接秀李老師的投影片注意到在 Degree of Violation 的推導中，所有與變數 $y$ 無關的部分可以去掉。因此我們最後可以得到求 Most Violated Constraint 就是在求 Problem 2 ($argmax_y$)。注意到其實我們一直 “先假裝 Problem 2 已解” Convergence?論文中證明如果讓 violate 的條件是必須超過一個 threshold 才算 violated，則演算法會在有限步驟內收斂。嚴謹的數學證明要參考 paper。 最後的麻煩: Problem 2 argmax這篇實在打太長了，以至於我想省略這個地方了 (淚)，事實上解 Problem 2 必須看問題本身是什麼，以 POS (Part-Of-Speech) tagging 來說，Problem 2 可用 Viterbi 求解。而這也就是李教授下一個課程 Sequence Labeling Problem。POS 如何對應到 Structure Learning 實在非常精彩! 真的不得不佩服這些人的智慧! 有興趣的讀者請一定要看李教授的投影片內容! 簡單筆記一下: POS 使用 HMM 方式來 model，例如一句話 x = “John saw the saw” 對應到詞性 y = “PN V D N”。然後把詞性當作 state, word 當作 observation，就是一個典型的 HMM 結構。接著使用 Conditional Random Field (CRF) 將 $log P(x,y)$ 對應到 $w\\cdot\\phi(x,y)$ 的形式，在 $P(x,y)$ 是由 HMM 定義的情形下，我們可以寫出相對應的 $\\phi(x,y)$ 該如何定義。因此就轉成一個 structure learning 的格式了。詳細請參考李老師課程講義。 彩蛋 我心愛的晏寶貝三歲生日快樂! 這幾天會有一個很重大的決定發生! Reference Hung-yi Lee ML courses Perceptron Learning Convergence Proof Duda Pattern Classification structureSVM 原始論文","tags":[{"name":"Structure SVM","slug":"Structure-SVM","permalink":"http://yoursite.com/tags/Structure-SVM/"},{"name":"Structure Perceptron","slug":"Structure-Perceptron","permalink":"http://yoursite.com/tags/Structure-Perceptron/"},{"name":"Hung-yi Lee","slug":"Hung-yi-Lee","permalink":"http://yoursite.com/tags/Hung-yi-Lee/"}]},{"title":"統一的框架 Bayes Filter","date":"2017-05-10T14:15:16.000Z","path":"2017/05/10/Bayes-Filter-for-Localization/","text":"Bayes Filter Introduction前幾篇討論了很多 Kalman Filter 以及它相關的變形，如: EKF and UKF。這些方法我們都可以放在 Bayes Filter 的框架下來看，這麼做的話，KF 就只是其中一個特例了 (都是高斯分布的情形)。而如果我們只考慮幾個離散點的機率，並用蒙地卡羅法來模擬取樣的話，這種實作方式就會是 Particle Filter 。所以掌握了 Bayes Filter 背後的運作方式對於理解這些方法是很有幫助的。一些變數的意義仍然跟前幾篇一樣: z: measurement，也就是我們實際上經由 sensor 得到的測量值 (會有noise) x: state，我們希望估計出來的值，在 Localization 一般就是座標值 發現了嗎? 在上圖右 KF 的兩個步驟: Measurement Update 和 State Prediction 實際上就是上圖左邊的兩個數學式關係。搭配下圖文字一起看，Measurement Update 理解為得到一個觀察值 $z$ 後，我們用 Bayes Rule 可以估測出 state $x$ 的事後機率 $P(x|z)$，而該事後機率經由 motion model (eg. CTRV) 可以估測出下一個時間點的 x 機率分佈 $P(x’)$ (此步驟為 State Prediction)。得到新的 $P(x’)$ 就可以當成下一個時間點的事前機率，所以 Bayes rule 就可以接著下去重複此 loop。 與 Maximum a Posteriori (MAP) Adaptation 的關係事實上，這樣的框架也跟 MAP Adaptation 息息相關! 例如當事前機率是某些特別的機率分佈 (exponential family)，經由 Bayes rule 得到的事後機率，它的機率分佈會跟事前機率是同一類型的，(例如都是 Gaussian)。而這樣的選擇我們稱為 conjugate prior。由於 “事後” 與 “事前” 機率是同一種類型的機率分佈，因此把 “事後機率” 在當成下一次資料來臨時的 “事前機率” 也就很自然了! 這就是 MAP Adaptation 的核心概念，與 Bayes filter 一模一樣阿! Localization 詳細定義好的，我們來針對 Localization 詳細解釋吧，名詞定義如下: 觀測值 (time 1~t)、控制 (time 1~t)、和地圖 $m$ 都是假設已知，我們所不知的(要估測的)是目前 time t 的狀態值 $x$。舉例來說，一個一維的地圖如下:而觀測值 $z_{1:t}$ 如下:可以知道每一個時間點的觀測值是一個 dimension 為 k 的向量。 整個 Localization 的目的就是要計算對於位置 $x$ 我們有多少信心度，嚴謹地說，我們就是要計算如下:$$\\begin{align} bel(x_t)=p(x_t|z_{1:t},u_{1:t},m) \\end{align}$$ 意思是在已知目前所有的觀測值、控制、和地圖的情況下，位置 $x_t$ 的機率是多少，看數學式子的話，這不就正好就是 事後機率 嗎? 所以上面的 Bayes filter 架構就有發揮的空間了。另外一提的是，如果將地圖 $m$ 也當成未知的話，就是 SLAM 演算法了。(還沒有機會去讀這個演算法)下圖是一個一維的示意圖:但是要計算這樣的事後機率，必須要考慮從一開始到目前時間點的所有觀測值和控制，這樣的資料量實在太大，計算會非常沒有效率。因此，如果能只考慮目前的觀測值和控制，並用上一個時間的的事後機率就能推算出來的話，勢必會非常有效率。簡單來講，我們希望用遞迴的方式: 考慮 $bel(x_{t-1})$ 和目前的觀測值 $z_t$ 和控制 $u_t$ 就能推算 $bel(x)$。這就必須要簡化上面 $bel(x_t)$ 原始的定義了，要如何達到呢? 需借助 First-order Markov Assumption 。 First-order Markov Assumption 簡化 believe 假設目前的時間點為 $t$，我們知道要計算的 believe $bel(x_t)$ 代表事後機率，再套用 Bayes rule 之後，可以得到上面的表示。 事後機率 (Believe): 特別把時間點 t 的觀測值從原先定義拉出來，這是要強調我們在得到最新的觀測值 $z_t$ 後，希望去計算最新的 believe 事前機率 (Motion Model): 稱為 Motion Model 是因為假設我們目前在時間點 $t-1$，接著拿到下一次的控制 $u_t$ 後，我們希望估測出下一次的狀態值 $x_t$ 是什麼。有看過前幾篇的讀者應該馬上就能想到，可以利用 CTRV 之類的 motion model 去計算。 觀測值機率 (Observation Model): 這個是要計算當下的觀測值的機率分佈，這部分通常就是經由 sensor data 得到後，我們假設是高斯分布來計算。 Motion Model 遞迴 我們發現到，最後一行的結果，對照本文第一張圖的 State Prediction 式子是一樣的意思，差別只在一個是連續一個是離散。另一個差別是，此式子明顯寫出可以用上一次的事後機率做遞迴，所以第一張圖的 Measurement Update 藍色箭頭就這麼來的。 Observation Model 簡化 Bayes Filter Summary重新整理一下經由 “Motion Model 遞迴” 和 “Observation Model 簡化” 過後的事後機率 $bel(x_t)$，結果如下圖左。 (下圖右只是列出本文最開始的 Bayes Filter 式子來做對照)。結論是我們花了那麼大的力氣，用上了 1st Markov Assumption 去處理 Localization 的遞迴式子和簡化，結果不意外地就如同開始的 Bayes Filter 一樣。 另外，實作上如果所有的 pdf 都是高斯分布的話，結果就是 Kalman Filter。而如果透過 sampling 離散的狀態位置的話，結果就會是 Particle Filter。這部分就先不多說明了。(附上課程一張截圖) 有關 Particle Filter 的實作，在 Udacity Term2 Project3 中我們實作一個二維地圖的 localization。相關 Codes 可在筆者 github 中找到。 Reference Udacity 上課內容 MAP Adaptaion 部分詳細可參考: Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains","tags":[{"name":"Bayes Filter","slug":"Bayes-Filter","permalink":"http://yoursite.com/tags/Bayes-Filter/"},{"name":"Localization","slug":"Localization","permalink":"http://yoursite.com/tags/Localization/"},{"name":"Markov Localization","slug":"Markov-Localization","permalink":"http://yoursite.com/tags/Markov-Localization/"},{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"}]},{"title":"Notes for Unscented Kalman Filter","date":"2017-04-12T12:50:16.000Z","path":"2017/04/12/Unscented-Kalman-Filter-Notes/","text":"資料為 Udacity 課程內容。事實上 UKF 挺囉嗦的，單純看本文應該無法理解，必須搭配前兩篇 KF and EKF 和 CTRV。主要是筆記用，讓自己可以根據文章完整實做出來。 一切的一切都來自於 Kalman Filter 的 State-Space model 假設，我們來稍微回顧一下。 $$\\begin{align} x_k = F_kx_{k-1}+\\nu_k \\\\ z_k = H_kx_k+\\omega_k&Tab;\\\\ \\end{align}$$ 式(1)表示狀態值 $x$ 滿足線性的遞迴關係式，而式(2)表示觀測值 $z$ 是當下狀態值的線性關係式。這個線性的關係式是為了使得我們的高斯分布在轉換後仍然滿足高斯分布所做的假設。但實際上常常不滿足線性的關係，例如假設我們的 $x$ 包含了 Cartesian coordinate 的座標位置和速度的資訊，但是 RADAR 的觀測值 $z$ 卻是用 Polar coordinate 來表示，就會有一個非線性的座標轉換。另一個會造成非線性的情況是發生在式(1)，也就是我們如果使用更精確的 motion model，如 CTRV。EKF 解決的方法是用 Jacobian 做線性的逼近，但是非線性的關係式如果一複雜，算 Jacobian 就會太複雜且造成運算速度變慢。因此，本篇要介紹的 Unscented KF 有相對簡單的辦法，並且運算速度快，且實際效果好。UKF 概念上怎麼做呢? 我們看上圖就可了解，首先原始的高斯分布(上面的紅色橢圓)，經由非線性轉換 $f$ 後得到的 “實際分佈” 為下面的黃色曲線，而該實際分布的 mean 和 covariance matrix 所形成的的高斯分布為下面的紅色橢圓，但是我們不容易得到! 那麼怎麼逼近下面的紅色橢圓呢? UKF 做法就是在上圖選擇一些代表的點，稱為 Sigma Points，經過 $f$ 轉換後，可以得到下面的星星，然後就可以根據這些轉換後的星星去計算他們的 mean 和 covariance matrix，而得到藍色的橢圓。那麼我們馬上開始說明如何設定 Sigma Points 吧。 Sigma Points 選擇假設 state dimension 為 $n_x$，Sigma Points 就選擇 $2n_x+1$ 個點。我們以 $n_x=2$ 來舉例說明會比較清楚，而擴展到更高的維度也就非常 trivial 了。 可以知道我們需選擇5個點($2n_x+1$)，第一個點是 mean vector，接著針對每一個 dimension 都根據 mean vector 向該 dimension 去做正負方向的 perturb，而 $\\lambda$ 表示要 perturb 多遠(使用者給定的值)。但是要特別注意的是，這裡的 perturb dimension 必須是正規化後的方向 (Whitening)，否則若原來的高斯分布某一個方向特別大(想像一個很扁的橢圓)，使用原來的 covariance matrix 就會被該方向 dominate。上例的 sigma points 如下: CTRV Sigma Points我們來看 CTRV model 下的 sigma points 選擇，其中 state vector and noise term 分別定義如下 $$\\begin{align} x= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v \\\\ \\psi \\\\ \\dot{\\psi} \\end{array} \\right) \\end{align}$$ $$\\begin{align} v_k= \\left[ \\begin{array}{center} v_{a,k} \\\\ v_{\\ddot{\\psi},k} \\end{array} \\right]\\\\ v_{a,k}\\sim N(0,\\sigma_a^2),v_{a,k}\\sim N(0,\\sigma_{\\ddot{\\psi}}^2) \\\\ Q=E[v_k,v_k^T]= \\left[ \\begin{array}{clr} \\sigma_a^2 &amp; 0 \\\\ 0 &amp; \\sigma_{\\ddot{\\psi}}^2 \\\\ \\end{array} \\right] \\end{align}$$ $v_k$ 的第一個 term 是加速度的 noise，而第二個表示 yaw rate 的變化率。由於原始的 state recursion 還參雜了 $Stochastic_k$ 這樣的 vector (參考式(7)and(8))，因此要計算他們的 covariance matrix 會太難搞! (因為我們需要知道 covariance matrix 才能對每個 whitening 後的維度去 perturb 取點) $$\\begin{align} x_{k+1}=x_k+Deterministic_k+ \\left[ \\begin{array}{center} \\frac{1}{2}(\\Delta{t})^2cos(\\psi_k)\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2sin(\\psi_k)\\cdot v_{a,k} \\\\ \\Delta{t}\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2\\cdot v_{\\ddot{\\psi},k} \\\\ \\Delta{t}\\cdot v_{\\ddot{\\psi},k} \\end{array} \\right]\\\\ x_{k+1}=x_k+Deterministic_k+Stochastic_k \\end{align}$$ 比較簡單的作法是將 noise term (式(4)) 當成 state vector 的另外的維度，主要的好處是 covariance matrix 就變得很容易計算了。然後一樣用上述的方式產生 Sigma Points。因此整個流程如下圖: 可以看到原本維度從5變成7，因此要產生15點的 sigma points，而 augmentated state vector 的 covariance matrix 變得很容易定義。 Sigma Points Prediction產生了這些 sigma points 之後，我們就可以透過式(7)，做 nonlinear recursion 到下一個時間點的 state vector (注意到 noise term 也被 sigma points 取樣了，所以可以帶入式(7)中)! Mean and Covariance of Sigma Points還記得嗎? 將 sigma point transform 後，我們下一步就是要估計出 mean 和 covariance，忘記的同鞋們可以看一下本文最開始的圖 (藍色的高斯分布)。基本上根據一些 data points 算它們的高斯分布非常簡單，但是由於我們當初取的 sigma points 它們之間本來的機率就不同，因此在計算轉換後的高斯分布必須要考慮每個點的權重。權重的設定有不同方法，課程直接建議下面的設定，所以沒特別要說明的，就照公式計算而已: Measurement Prediction對於 RADAR 來說式(2)也是一個非線性的關係，因此也可以用 sigma points 的方法來逼近。假設我們在時間點 $k$ 取的 sigma points 為 $x$，經過非線性 state recursion 後得到時間點 $k+1$ 的 sigma points 為 $x’$，我們可以直接將 $x’$ 當作新取的 sigma points，拿來做 measurement 非線性轉換 $z’=h(x’)+w$，然後一樣用上面的公式算一下 measurement space 的高斯分布即可。RADAR 的 $h()$ 定義如下: $$\\begin{align} z=h(x)= \\left( \\begin{array}{clr} \\rho \\\\ \\phi \\\\ \\dot{\\rho} \\end{array} \\right) = \\left( \\begin{array}{clr} \\sqrt{p_x^2+p_y^2} \\\\ \\arctan(p_y/p_x) \\\\ \\frac{p_xcos(\\psi)v+p_ysin(\\psi)v}{\\sqrt{p_x^2+p_y^2}} \\end{array} \\right) \\end{align}$$ 稍微要注意的是，計算 covariance 時須考慮 noise 的 covariance (下圖紅色框起來的地方)，這跟計算 state space 中的高斯分布不同。這是因為在 measurement space 是兩個 independent 的高斯分布相加 (一個是 sigma point 估出來的，另一個是 noise 的高斯)，covariance 就是相加而已。 另外對於 LIDAR 來說 measurement 的轉換是線性關係，所以不使用 sigma point 的方法，因此在處理兩種 sensor data 時，記得區分一下 case。 Measurement Update終於來到最後的步驟了。我們費盡千辛萬苦根據時間點 $k$ 的 state vector 估計出了時間點 $k+1$ 的 measurement 值，而此時我們在時間點 $k+1$ 也收到了真正的 sensor data measurement。因此同樣可以使用 KF 的流程去計算所有的 update! 原因是我們其實全部都高斯化了 (透過 sigma points 方法)。 紅色框起來處為跟以前不同的地方，變成要計算 cross-correlation of “Measurement Prediction 那個 section 的第二張圖那兩排的 vectors” 心得其實概念並不困難，但是頗多計算流程和符號，同時也必須先了解 Kalman Filter 和 CTRV motion model，下一步就實作 Project 吧! 附上 predict 的結果:","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"Unscented Kalman Filter","slug":"Unscented-Kalman-Filter","permalink":"http://yoursite.com/tags/Unscented-Kalman-Filter/"}]},{"title":"CTRV Motion Model","date":"2017-04-11T14:15:41.000Z","path":"2017/04/11/CTRV-Motion-Model/","text":"Motion Models 資料為 Udacity 課程內容 在上一篇 EKF 中，我們其實假設的是 constant velocity model (CV)，也就是如下的關係式$$\\begin{align} x_k = Fx_{k-1}+\\nu_k \\\\ x_k= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v_x \\\\ v_y \\end{array} \\right), F= \\left( \\begin{array}{clr} 1 &amp; 0 &amp; \\Delta{t} &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\Delta{t} \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right) \\end{align}$$正好滿足 Kalman Filter 中 State-space model 的假設，但這樣的 motion model 很明顯太單純了，因為車子總是在變速且轉彎。因此真實在使用的時候不會用 CV model，那會用什麼呢? 以下為幾種可用的: constant turn rate and velocity magnitude model (CTRV) constant turn rate and acceleration (CTRA) constant steering angle and velocity (CSAV) constant curvature and acceleration (CCA) Udacity 在這次的 project 中讓我們使用了 CTRV，而此 model 的 state vector $x$ 定義如下:$$\\begin{align} x= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v \\\\ \\psi \\\\ \\dot{\\psi} \\end{array} \\right) \\end{align}$$其中 $p_x,p_y$ 是 $x,y$ 座標位置，$v$ 是速度的 magnitude，$\\psi$ 是速度的向量與水平軸的夾角稱 yaw angel，最後的 $\\dot{\\psi}$ 則是該夾角的變化率稱 yaw rate。而 CTRV 假設的是 $v$ 和 $\\dot{\\psi}$ 是 constant。而此 model 已不是一個線性系統了，也就是無法用 matrix 來表達，所以我們將式(1)改為如下的表達方式:$$\\begin{align} x_{k+1} = f(x_k,\\nu_k) \\end{align}$$如何將 function $f$ 寫成遞迴式子呢? 請看下一段 CTRV State Vector Recursion我們先忽略 noise $\\nu_k$ 這項，晚點再加回來。State vector 隨時間變化的式子如下:$$\\begin{align} x_{k+1}=x_k+\\int_{t_k}^{t_{k+1}}{ \\left[ \\begin{array} \\\\ \\dot{p}_x(t) \\\\ \\dot{p}_y(t) \\\\ \\dot{v}(t) \\\\ \\dot{\\psi}(t) \\\\ \\ddot{\\psi}(t) \\end{array} \\right] }dt\\\\ x_{k+1}=x_k+ \\left[ \\begin{array}{center} \\int_{t_k}^{t_{k+1}}{v(t)\\cdot cos(\\psi(t))}dt \\\\ \\int_{t_k}^{t_{k+1}}{v(t)\\cdot sin(\\psi(t))}dt \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] \\end{align}$$注意到 CTRV 的假設 $v$ 和 $\\dot{\\psi}$ 是 constant，也就會造成式(5)中 $\\dot{v}(t)=\\ddot{\\psi(t)}=0$，且從時間 $k$ 到 $k+1$ 的 $\\dot{\\psi}(t)$ 都等於 $\\dot{\\psi}_k$，也因此得到式(6)。但是我們仍然要處理式(6)前兩項的積分，首先一樣基於CTRV假設 $v(t)=v_k$ 對於時間 $k$ 到 $k+1$ 都是一樣，所以提到積分外面。然後由於 yaw rate 是 constant，因此 $\\psi(t)$ 可以明確表示出來，總之改寫如下:$$\\begin{align} x_{k+1}=x_k+ \\left[ \\begin{array}{center} v_k\\int_{t_k}^{t_{k+1}}{cos(\\psi_k+\\dot{\\psi}_k\\cdot(t-t_k))}dt \\\\ v_k\\int_{t_k}^{t_{k+1}}{sin(\\psi_k+\\dot{\\psi}_k\\cdot(t-t_k))}dt \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] \\end{align}$$然後沒什麼好說的，就積它吧:$$\\begin{align} x_{k+1}=x_k+ \\left[ \\begin{array}{center} \\frac{v_k}{\\dot{\\psi}_k}(sin(\\psi_k+\\dot{\\psi}_k\\Delta{t})-sin(\\psi_k)) \\\\ \\frac{v_k}{\\dot{\\psi}_k}(-cos(\\psi_k+\\dot{\\psi}_k\\Delta{t})+cos(\\psi_k)) \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] =x_k+Deterministic_k \\end{align}$$這邊有一個實作上需要避免的地方，就是當 $\\dot{\\psi}_k=0$ 時，上式的第1,2項會除0。不過我們知道當 $\\dot{\\psi}_k=0$ 表示車子是直直往前開，yaw angle不會改變，因此實際上可以用如下來計算:$$\\begin{align} x_{k+1}=x_k+ \\left[ \\begin{array}{center} v_kcos(\\psi_k)\\Delta{t} \\\\ v_ksin(\\psi_k)\\Delta{t} \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] =x_k+Deterministic&apos;_k \\end{align}$$ Recursion With Noise TermNoise term $v_k$ 這裡是假設如下:$$\\begin{align} v_k= \\left[ \\begin{array}{center} v_{a,k} \\\\ v_{\\ddot{\\psi},k} \\end{array} \\right] \\end{align}$$ 第一個 term 是加速度的 noise，而第二個表示 yaw rate 的變化率。考慮如果有這兩項 noises 的話，並且假設時間 $k$ 到 $k+1$ 這兩個 noises 的值是固定的，那麼 state vector 會變成如下:$$\\begin{align} x_{k+1}=x_k+Deterministic_k+ \\left[ \\begin{array}{center} \\frac{1}{2}(\\Delta{t})^2cos(\\psi_k)\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2sin(\\psi_k)\\cdot v_{a,k} \\\\ \\Delta{t}\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2\\cdot v_{\\ddot{\\psi},k} \\\\ \\Delta{t}\\cdot v_{\\ddot{\\psi},k} \\end{array} \\right]\\\\ x_{k+1}=x_k+Deterministic_k+Stochastic_k \\end{align}$$第三項是速度 $v$ 會被加速度 $v_{a,k}$ 這種 noise 怎麼影響，所以很明顯是線性增加，同理第四和第五項也很容易得到。第一和第二項，$x$ and $y$ 的位置這裡就比較麻煩，因此採用的是一個近似而已。這邊假設 yaw rate 沒有太高的情況下，下圖的兩個紅色圈圈位置應該是很接近，因此我們可以考慮走直線的紅色圈圈位置，也就得到了(11)第一二項的近似值。 Summary All CTRV省略解釋，寫出 state recursion 的計算。$$x= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v \\\\ \\psi \\\\ \\dot{\\psi} \\end{array} \\right)$$ if $\\dot{\\psi}_k\\neq0$, then$x_{k+1}=x_k+Deterministic_k+Stochastic_k$where$$Deterministic_k= \\left[ \\begin{array}{center} \\frac{v_k}{\\dot{\\psi}_k}(sin(\\psi_k+\\dot{\\psi}_k\\Delta{t})-sin(\\psi_k)) \\\\ \\frac{v_k}{\\dot{\\psi}_k}(-cos(\\psi_k+\\dot{\\psi}_k\\Delta{t})+cos(\\psi_k)) \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right]$$and$$Stochastic_k= \\left[ \\begin{array}{center} \\frac{1}{2}(\\Delta{t})^2cos(\\psi_k)\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2sin(\\psi_k)\\cdot v_{a,k} \\\\ \\Delta{t}\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2\\cdot v_{\\ddot{\\psi},k} \\\\ \\Delta{t}\\cdot v_{\\ddot{\\psi},k} \\end{array} \\right]$$otherwise $\\dot{\\psi}_k=0$, then$x_{k+1}=x_k+Deterministic&apos;_k+Stochastic_k$where$$Deterministic&apos;_k= \\left[ \\begin{array}{center} v_kcos(\\psi_k)\\Delta{t} \\\\ v_ksin(\\psi_k)\\Delta{t} \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right]$$ Unscented Kalman Filter 簡介由於 CTRV 是非線性的，會破壞 State-space model 的線性假設，例如下圖中原先紅色的高斯分布經過非線性轉換後分布為黃色。不過我們知道 EKF 可以利用 Jaccobian matrix 做線性逼近計算，所以我們同樣可以計算。但要計算上述非線性系統的 Jaccobian matrix 實在顯得有點複雜，好在 Unscented KF 可以完全避開這個麻煩。它利用選擇幾個代表的 candidates vectors，叫做 Sigma Points，去計算經過非線性轉換後的值，然後就可以得到 output domain 的 mean 和 covariance matrix，也就是上圖的綠色高斯分布。這邊要注意的是，output domain 的真實分佈不是高斯分布(黃色)，但我們仍然將它當成是高斯分布(綠色)去計算 mean 和 covariance matrix，因為這樣才能繼續套用 Kalman filter 的方法。說到這可知道 UKF 仍然只是逼近，不過根據 Udacity 的說法，實際應用上 UKF 是很快 (不用計算 Jaccobian) 且實際上效果很好!下回預告，UKF完整介紹。","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"Motion Model","slug":"Motion-Model","permalink":"http://yoursite.com/tags/Motion-Model/"},{"name":"Unscented Kalman Filter","slug":"Unscented-Kalman-Filter","permalink":"http://yoursite.com/tags/Unscented-Kalman-Filter/"}]},{"title":"Notes for Kalman Filter and Extended KF","date":"2017-04-03T08:56:13.000Z","path":"2017/04/03/Kalman-Filter-and-Extended-KF-Notes/","text":"Udacity term2 (Sensor Fusion, Localization, and Control) 的第一個 Project 就是用 KF and EKF 將 Lidar and Radar 的資訊做 fusion 並且可以 tracking。由於 KF/EKF 的數學符號很多，因此想筆記一下方便日後回想，所以主要以我自己看的角度，可能有些地方會沒有明確說明。本篇的筆記來源是 這裡，這篇真的講的超棒的，清楚易懂! 非常建議直接去看! Udacity 課程內容 若要實作所有的計算流程不管理論的話，可直接跳到 “7. 總結 Lidar and Radar Fusion”。 State Space Model這是整個 KF/EKF 的模型假設，寫出來如下: $$\\begin{align} x_k = F_kx_{k-1}+\\nu_k \\\\ z_k = H_kx_k+\\omega_k \\end{align}$$ \\(x_k\\) 是在時間點 \\(t\\) 的 states，也是我們希望能夠估計出來的(但是無法直接觀察到)。而 states 滿足 線性的一次遞迴 關係，也就是式子(1)。 \\(\\nu_k\\sim\\mathcal{N}(0,Q_k)\\) 是 state nose。\\(z_k\\) 是在時間點 \\(t\\) 的 observations，透過 \\(H_k\\) 將 states 轉換到 observations。 \\(\\omega_k\\sim\\mathcal{N}(0,R_k)\\) 是 sensor noise，而 \\(R_k\\) 基本上會由製造廠商提供。基本上兩個 noises 都跟所有人都 independent。 Prediction Stage整個 KF/EKF 都是基於 Gaussian distribution。因此假設我們有 \\(k-1\\) 時間點的 state 估計，所以我們知道 \\(x_{k}\\sim\\mathcal{N}(\\hat{x}_k,P_k)\\) 會變成如下的一個 Gaussian: $$\\begin{align} \\hat{x}_{k}=F_k\\hat{x}_{k-1} \\\\ P_k = F_kP_{k-1}F_k^T+Q_k \\end{align}$$ 式(3)and(4)即為 Prediction Stage。又因為我們知道 observation 跟 state 之間的關係為透過 \\(H_k\\) 轉換，在完全沒有 sensor noise 情況下，所以可以得知 prediction 的觀察值為: $$\\begin{align} z_{expected}\\sim\\mathcal{N}(\\mu_{expected},\\Sigma_{expected}) \\\\ \\mathcal{N}\\left( \\begin{array}{c} \\vec{\\mu}_{expected}=H_k\\hat{x}_{k}, &amp; \\Sigma_{expected} = H_kP_kH_k^T \\end{array} \\right) \\end{align}$$ Update Stage我們令實際上的觀察值為 \\(z_k\\sim\\mathcal{N}(\\vec{z}_k,R_k)\\)，將觀察值的 Gaussian 和 predict 的 Gaussian 畫出如下: 而將兩個 Gaussian pdfs 相乘的話:$$\\begin{align} \\mathcal{N}(x,\\mu_0,\\Sigma_0)\\cdot\\mathcal{N}(x,\\mu_1,\\Sigma_1)=\\mathcal{N}(x,\\mu&apos;,\\Sigma&apos;) \\\\ \\end{align}$$仍然會得到另一個 Gaussian:$$\\begin{align} K=\\Sigma_0(\\Sigma_0+\\Sigma_1)^{-1} \\\\ \\mu&apos;=\\mu_0+K(\\mu_1-\\mu_0) \\\\ \\Sigma&apos;=\\Sigma_0-K\\Sigma_0 \\end{align}$$ \\(K\\) 稱為 Kalman Gain。由式(10)可知，update 後的 covariance matrix 會愈來愈小，表示我們對於 prediction 的觀察值會愈來愈確定。另外由(9)可知，Kalman Gain 控制著要相信哪邊多一點。把估測的觀察值 pdf 和實際觀察值的 pdf，即 \\(z_k\\sim\\mathcal{N}(\\vec{z}_k,R_k)\\) 和式(5)兩個 pdfs 代入到式 (8)~(10) 得到如下: $$\\begin{align} H_k\\hat{x}_k&apos;=H_k\\hat{x}_k+K(\\vec{z}_k-H_k\\hat{x}_k) \\\\ H_kP_k&apos;H_k^T=H_kP_kH_k^T-KH_kP_kH_k^T \\\\ K=H_kP_kH_k^T(H_kP_kH_k^T+R_k)^{-1} \\end{align}$$ 把 (11)~(13) 開頭的 \\(H_k\\) 去掉，並且把 (12) and (13) 結尾的 \\(H_k^T\\) 去掉變成$$\\begin{align} \\hat{x}_k&apos;=\\hat{x}_k+K(\\vec{z}_k-H_k\\hat{x}_k) \\\\ P_k&apos;=P_k-KH_kP_k \\\\ K=P_kH_k^T(H_kP_kH_k^T+R_k)^{-1} \\end{align}$$ (14)~(16)就是 KF 的 Update Stage! 新的 states 估計值就被我們得到，然後這個值就可以被當成下一次 loop 的初始值。 KF Flow擷取網站上的圖片: Lidar/Radar 的一些設定state 定義為 \\(x=(p_x,p_y,v_x,v_y)\\) 分別是 (x 的位置, y 的位置, x 的速度, y 的速度)。 \\(F_k\\) 會根據兩次 sensor data 之間的時間間隔 \\(\\vartriangle t\\) 來表示: 另外我們將 加速度考慮為一個 mean = 0, covariance matrix = Q 的一個 random noise 的話，式 (1) and (4) 必須做修改。其中 \\(Q_v\\) 使用者自己設定調整，所以 state noise 的 covariance matrix 為 Lidar 只會觀察到位置，因此 Lidar 的 \\(H\\) 為:$$H_{lidar}= \\left( \\begin{array}{clr} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\end{array} \\right)$$ Radar 就比較特別了，它觀察到的是以 polar coordinate 來表示。所以它的 states 和 observation 之間的關係無法用一個 matrix \\(H\\) 來代表，是如下的 non-linear 式子:$$\\begin{align} h(x)= \\left( \\begin{array}{clr} \\rho \\\\ \\phi \\\\ \\dot{\\rho} \\end{array} \\right) = \\left( \\begin{array}{clr} \\sqrt{p_x^2+p_y^2} \\\\ \\arctan(p_y/p_x) \\\\ \\frac{p_xv_x+p_yv_y}{\\sqrt{p_x^2+p_y^2}} \\end{array} \\right) \\end{align}$$ 為了讓它符合 state-space model 的線性式子，只好使用 Taylor 展開式，只使用 Jaccobian matrix 針對 \\(h\\) 去展開，而這個就是 Extended KF。 EKF稍微改寫一下 Update Stage:$$\\begin{align} y=(\\vec{z}_k-H_k\\hat{x}_k) \\\\ S=H_kP_kH_k^T+R_k \\\\ \\hat{x}_k&apos;=\\hat{x}_k+Ky \\\\ P_k&apos;=P_k-KH_kP_k \\\\ K=P_kH_k^TS^{-1} \\end{align}$$在 EKF 中，由於我們使用 Taylor 展開式去逼近 \\(h\\)，因此上述的 \\(H_k\\) 必須使用如下式子計算:但是，這邊還有一個 tricky 的地方! 就是 式(18)直接使用式(17) \\(h\\) 的 non-linear function 計算!回想一下我們將 \\(h\\) 做 linearlization 的目的: 就是式(5),(6)下的那張圖的轉換。如果 Gaussian pdf 經過 nonlinear 轉換後會變成 “非Gaussian”，因此只好做線性逼近。既然線性轉換的 pdf 都已經是逼近了，不如就將 mean 使用最精確的值，因此 \\(y\\) 就直接使用式(17)計算。所以式(18)要改成:$$\\begin{align} y=(\\vec{z}_k-h(\\hat{x}_k)) \\end{align}$$ 總結 Lidar and Radar Fusion [Predict]$$\\hat{x}_{k}=F_k\\hat{x}_{k-1} \\\\ P_k = F_kP_{k-1}F_k^T+Q$$ where [Lidar Update]$$y=(\\vec{z}_k-H_{lidar}\\hat{x}_k) \\\\ S=H_{lidar}P_kH_{lidar}^T+R_k \\\\ \\hat{x}_k&apos;=\\hat{x}_k+Ky \\\\ P_k&apos;=P_k-KH_{lidar}P_k \\\\ K=P_kH_{lidar}^TS^{-1}$$ where \\(R_k\\) sensor noise covariance matrix 由廠商提供, and $$H_{lidar}= \\left( \\begin{array}{clr} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\end{array} \\right)$$ [Radar Update]$$y=(\\vec{z}_k-h(\\hat{x}_k)) \\\\ S=H_kP_kH_k^T+R_k \\\\ \\hat{x}_k&apos;=\\hat{x}_k+Ky \\\\ P_k&apos;=P_k-KH_kP_k \\\\ K=P_kH_k^TS^{-1}$$ where \\(R_k\\) sensor noise covariance matrix 由廠商提供, and $$h(x)= \\left( \\begin{array}{clr} \\rho \\\\ \\phi \\\\ \\dot{\\rho} \\end{array} \\right) = \\left( \\begin{array}{clr} \\sqrt{p_x^2+p_y^2} \\\\ \\arctan(p_y/p_x) \\\\ \\frac{p_xv_x+p_yv_y}{\\sqrt{p_x^2+p_y^2}} \\end{array} \\right)$$ Reference How a Kalman filter works, in pictures Udacity Term2 Lecture","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"Kalman Filter","slug":"Kalman-Filter","permalink":"http://yoursite.com/tags/Kalman-Filter/"},{"name":"Extended Kalman Filter","slug":"Extended-Kalman-Filter","permalink":"http://yoursite.com/tags/Extended-Kalman-Filter/"}]},{"title":"WGAN Part 2: 主角 W 登場","date":"2017-03-17T13:25:12.000Z","path":"2017/03/17/WGAN-Part-2/","text":"前情提要GAN 作者設計出一個 Minimax game，讓兩個 players: 生成器 G 和 鑑別器 D 去彼此競爭，並且達到平衡點時，此問題達到最佳解且生成器 G 鍊成。大致上訓練流程為先 optimize 鑑別器 D for some iterations，然後換 optimize 生成器 G (在 optimize G 時，此問題等價於最佳化 JSD 距離)，重複上述 loop 直到達到最佳解。但是仔細看看原來的最佳化問題之設計，我們知道在最佳化 G 的時候，等價於最佳化一個 JSD 距離，而 JSD 在遇到真實資料的時會很悲劇。怎麼悲劇呢? 原因是真實資料都存在 local manifold 中，造成 training data 的 p.d.f. 和 生成器的 p.d.f. 彼此之間無交集 (或交集的測度為0)，在這種狀況 JSD = log2 (constant) almost every where。也因此造成 gradients = 0。這是 GAN 很難訓練的一個主因。 也因此 WGAN 的主要治本方式就是換掉 JSD，改用 Wasserstein (Earth-Mover) distance，而修改過後的演算法也是簡單得驚人! Wasserstein (Earth-Mover) distance我們先給定義後，再用作者論文上的範例解釋定義如下:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_g)=\\inf_{\\gamma\\in\\prod(\\mathbb{P}_r,\\mathbb{P}_g)}E_{(x,y)\\sim \\gamma}[\\Vert x-y \\Vert] \\end{align}$$\\(\\gamma\\)指的是 real data and fake data 的 joint distribution，其中 marginal 為各自兩個 distributions。先別被這些符號嚇到，直觀的解釋為: EM 距離可以理解為將某個機率分佈搬到另一個機率分佈，所要花的最小力氣。 我們用下面這個例子明確舉例，假設我們有兩個機率分佈 f1 and f2:$$\\begin{align*} f_1(a)=f_1(b)=f_1(c)=1/3 \\\\\\\\ f_1(A)=f_1(B)=f_1(C)=1/3 \\end{align*}$$這兩個機率分佈在一個 2 維平面，如下:而兩個 \\(\\gamma\\) 對應到兩種 搬運配對法$$\\begin{align*} \\gamma_1(a,A)=\\gamma_1(b,B)=\\gamma_1(c,C)=1/3 \\\\\\\\ \\gamma_2(a,B)=\\gamma_2(b,C)=\\gamma_2(c,A)=1/3 \\end{align*}$$可以很容易知道它們的 marginal distributions 正好符合 f1 and f2 的機率分佈。則這兩種搬運法造成的 EM distance 分別如下:$$\\begin{align*} EM_{\\gamma_1}=\\gamma_1(a,A)*\\Vert a-A \\Vert + \\gamma_1(b,B)*\\Vert b-B \\Vert + \\gamma_1(c,C)*\\Vert c-C \\Vert \\\\\\\\ EM_{\\gamma_2}=\\gamma_2(a,B)*\\Vert a-B \\Vert + \\gamma_2(b,C)*\\Vert b-C \\Vert + \\gamma_2(c,A)*\\Vert c-A \\Vert \\end{align*}$$明顯知道 $\\theta=EM_{\\gamma_1}&lt;EM_{\\gamma_2}$而 EM distance 就是在算所有搬運法中，最小的那個，並將那最小的 cost 定義為此兩機率分佈的距離。這個距離如果是兩條平行 1 維的直線 pdf (上面的例子是直線上只有三個離散資料點)，會有如下的 cost: 對比此圖和上一篇的 JSD 的結果，EM 能夠正確估算兩個沒有交集的機率分佈的距離，直接的結果就是 gradient 連續且可微 ! 使得 WGAN 訓練上穩定非常多。 一個關鍵的好性質: Wasserstein (Earth-Mover) distance 處處連續可微原始 EM distance 的定義 (式(1)) 是 intractable一個神奇的數學公式 (Kantorovich-Rubinstein duality) 將 EM distance 轉換如下:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_\\theta)=\\sup_{\\Vert f \\Vert _L \\leq 1}{ E_{x \\sim \\mathbb{P}_r}[f(x)] - E_{x \\sim \\mathbb{P}_\\theta}[f(x)] } \\end{align}$$注意到 sup 是針對所有滿足 1-Lipschitz 的 functions f，如果改成滿足 K-Lipschitz 的 functions，則值會相差一個 scale K。但是在實作上我們都使用一個 family of functions，例如使用所有二次式的 functions，或是 Mixture of Gaussians，等等。而經過近幾年深度學習的發展後，我們可以相信，使用 DNN 當作 family of functions 是很洽當的選擇，因此假定我們的 NN 所有參數為 \\(W\\)，則上式可以表達成:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\approx\\max_{w\\in W}{ E_{x \\sim \\mathbb{P}_r}[f_w(x)] - E_{z \\sim p(z)}[f_w(g_{\\theta}(z))] } \\end{align}$$這裡不再是等式，而是逼近，不過 Deep Learning 優異的 Regression 能力是可以很好地逼近的。 我們還是需要保證整個 EM distance 保持處處連續可微分，這樣可以確保我們做 gradient-based 最佳化可以順利，針對這點，WGAN 作者很強大地證明完了，得到結論如下: 針對生成器 \\(g_\\theta\\)任何 feed-forward NN 皆可 針對鑑別器 \\(f_w\\)當 \\(W\\) 是 compact set 時，該 family of functions \\(\\{f_w\\}\\) 滿足 K-Lipschitz for some K。具體實現很容易，因為在 \\(R^d\\) space，compact set 等價於 closed and bounded，因此只需要針對所有的參數取 bounding box即可!論文裡使用了 [-0.01,0.01] 這個範圍做 clipping。 與 GAN 第一個不同點為: 鑑別器參數取 clipping。 EM distance 為目標函式所造成的不同我們將兩者的目標函式列出來做個比較$$\\begin{align} GAN: E_{x \\sim \\mathbb{P}_r} [\\log f_w(x)] + E_{z \\sim p(z)}[\\log (1-f_w(g_{\\theta}(z)))] \\\\ WGAN: E_{x \\sim \\mathbb{P}_r}[f_w(x)] - E_{z \\sim p(z)}[f_w(g_{\\theta}(z))] \\end{align}$$發現到 WGAN 不取 log，同時對生成器的目標函式也做了修改 與 GAN 第二個不同點為: WGAN 的目標函式不取 log，同時對生成器的目標函式也做了修改。 第三個不同點是作者實驗的發現 與 GAN 第三個不同點為: 使用 Momentum 類的演算法，如 Adam，會不穩定，因此使用 SGD or RMSProp。 WGAN 演算法總結一下與 GAN 的修改處 A. 鑑別器參數取 clipping。B. WGAN 的目標函式不取 log，同時對生成器的目標函式也做了修改。C. 使用 SGD or RMSProp。 WGAN 的優點一: 目標函式與訓練品質高度相關原始的 GAN 沒有這樣的評量指標，因此會在訓練中途用人眼去檢查訓練是否整個壞掉了。 WGAN 解決了這個麻煩。作者的範例如下，可以發現WGAN的目標函式 Loss 愈低，sampling出來的品質愈高。 二: 鑑別器可以直接訓練到最好原始的 GAN 需要小心訓練，不能一下子把鑑別器訓練太強導致導函數壞掉 三: 不需要特別設計 NN 的架構GNN 使用 MLP (Fully connected layers) 難以訓練，較成功的都是 CNN 架構，並搭配 batch normalization。而在 WGAN 演算法下， MLP架構可能穩定訓練 (雖然品質有下降) 四: 沒有 collapse mode (保持生成多樣性)作者自己說在多次實驗的過程都沒有發現這種現象 My Questions 原先 GAN 會有 collapse mode 看到有人討論是因為 KL divergence 不對稱的關係導致對於 “生成器生出錯誤的 sample” 比 “生成器沒生出所有該對的sample” 逞罰要大很多，不過這邊自己還是有疑問，因為 JSD 已經是對稱的 KL 了，還會有逞罰不同導致 collapse mode 的問題嗎? 需要再多看一下 paper 了解。 如何控制 sample 出來的 output，譬如 mnist 要 sampling 出某個 class。前提是希望不能對 data 有任何標記過，不然就沒有 unsupervised 的條件了。 Conditional GAN? 有空再研究一下這個課題 Tensorflow 範例測試主要參考此 github，用自己的寫法寫一次，並做些測試 用 MNIST dataset 做測試，原始 input 為 28x28，將它 padding 成 32x32，因此 input domain 為 32x32x1 生成器幾個重點，第一個是生成器用的是 conv2d_transpose (doc)，這是由於原先的 conv2d 無法將 image 的 size 變大，頂多一樣。因此要用 conv2d_transpose，以 第 15 行舉例。argument wc2 的 shape 為 [3, 3, 256, 512] 分別表示 [filter_h, filter_w, output_depth, input_depth]。argument [batch_size, 8, 8, 256] 表示 output layer 的 shape。後面兩個 argument 就很明顯了，分別是 strides [batch_stride, h_stride, w_stride, channel_stride] 和 padding。第二個重點是最後一層 out_sample = tf.nn.tanh(conv5)，由於我們會將 data 先 normalize 到 [-1,1]，因此使用 tanh 讓 domain 一致。 123456789101112131415161718192021222324252627282930313233z_dim = 128def generator_net(z): with tf.variable_scope('generator'): # Layer 1 - 128 to 4*4*512 wd1 = tf.get_variable(\"wd1\",[z_dim, 4*4*512]) bd1 = tf.get_variable(\"bd1\",[4*4*512]) dense1 = tf.add(tf.matmul(z, wd1), bd1) dense1 = tf.nn.relu(dense1) # reshape to 4*4*512 conv1 = tf.reshape(dense1, (batch_size, 4, 4, 512)) # Layer 2 - 4*4*512 to 8*8*256 wc2 = tf.get_variable(\"wc2\",[3, 3, 256, 512]) conv2 = tf.nn.conv2d_transpose(conv1, wc2, [batch_size, 8, 8, 256], [1,2,2,1], padding='SAME') conv2 = tf.nn.relu(conv2) # Layer 3 - 8*8*256 to 16*16*128 wc3 = tf.get_variable(\"wc3\",[3, 3, 128, 256]) conv3 = tf.nn.conv2d_transpose(conv2, wc3, [batch_size, 16, 16, 128], [1,2,2,1], padding='SAME') conv3 = tf.nn.relu(conv3) # Layer 4 - 16*16*128 to 32*32*64 wc4 = tf.get_variable(\"wc4\",[3, 3, 64, 128]) conv4 = tf.nn.conv2d_transpose(conv3, wc4, [batch_size, 32, 32, 64], [1,2,2,1], padding='SAME') conv4 = tf.nn.relu(conv4) # Layer 5 - 32*32*64 to 32*32*1 wc5 = tf.get_variable(\"wc5\",[3, 3, 1, 64]) conv5 = tf.nn.conv2d_transpose(conv4, wc5, [batch_size, 32, 32, 1], [1,1,1,1], padding='SAME') out_sample = tf.nn.tanh(conv5) return out_sample 鑑別器這個就是最一般的 CNN，output 最後是一個沒有過 log 的 scaler 且也沒有經過 activation function。比較重要的是變數都是使用 get_variable 和 scope.reuse_variables() (請參考 Sharing Variables)。具體的原因是因為我們會對 real data 呼叫一次鑑別器，而對於 fake data 也會在呼叫一次。若沒有 share variables，就會導致產生兩組各自的 weights。tf.get_variable() 跟 tf.Variable() 差別在於如果已經有名稱一樣的變數時 get_variable() 不會再產生另一個變數，而會 share，但是要真的 share 還必須多一個動作 reuse_variables 確保不是不小心 share 到的。 12345678910111213141516171819202122232425262728293031323334353637# Construct CriticNetdef conv2d(x, W, b, strides=1): x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME') x = tf.nn.bias_add(x, b) return tf.nn.relu(x)def critic_net(x, reuse=False): with tf.variable_scope('critic') as scope: size = 64 if reuse: scope.reuse_variables() # Layer 1 - 32*32*1 to 16*16*size wc1 = tf.get_variable(\"wc1\",[3, 3, 1, size]) bc1 = tf.get_variable(\"bc1\",[size]) conv1 = conv2d(x, wc1, bc1, strides=2) # Layer 2 - 16*16*size to 8*8*size*2 wc2 = tf.get_variable(\"wc2\",[3, 3, size, size*2]) bc2 = tf.get_variable(\"bc2\",[size*2]) conv2 = conv2d(conv1, wc2, bc2, strides=2) # Layer 3 - 8*8*size*2 to 4*4*size*4 wc3 = tf.get_variable(\"wc3\",[3, 3, size*2, size*4]) bc3 = tf.get_variable(\"bc3\",[size*4]) conv3 = conv2d(conv2, wc3, bc3, strides=2) # Layer 4 - 4*4*size*4 to 2*2*size*8 wc4 = tf.get_variable(\"wc4\",[3, 3, size*4, size*8]) bc4 = tf.get_variable(\"bc4\",[size*8]) conv4 = conv2d(conv3, wc4, bc4, strides=2) # Fully connected layer - 2*2*size*8 to 1 wd5 = tf.get_variable(\"wd5\",[2*2*size*8, 1]) bd5 = tf.get_variable(\"bd5\",[1]) fc5 = tf.reshape(conv4, [-1, wd5.get_shape().as_list()[0]]) logit = tf.add(tf.matmul(fc5, wd5), bd5) return logit Graph這裡有幾個重點，第一個是由於我們在最佳化過程中，會 fix 住一邊的參數，然後最佳化另一邊，接著反過來。此作法參考 link第二個重點是使用 tf.clip_by_value，可以看到我們對於所有透過 tf.get_collection 蒐集到的變數都增加一個 clip op。第三個重點是使用 tf.control_dependencies([opt_c]) link，這個定義了 op 之間的關聯性，它會等到 argument 內執行完畢後，才會接著執行下去。所以我們可以確保先做完 RMSPropOptimizer 才接著做 clip_by_value。另外 tf.tuple link 會等所有的 input arguments 都做完才會真的 return 出去，以確保每個 tensors 都做完 clipping 了。 1234567891011121314151617181920212223242526272829# build graphdef build_graph(): z = tf.placeholder(tf.float32, shape=(batch_size, z_dim)) fake_data = generator_net(z) real_data = tf.placeholder(tf.float32, shape=(batch_size, 32, 32, 1)) # Define loss and optimizer real_logit = critic_net(real_data) fake_logit = critic_net(fake_data, reuse=True) c_loss = tf.reduce_mean(fake_logit - real_logit) g_loss = tf.reduce_mean(-fake_logit) # get the trainable variables list theta_g = tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator') theta_c = tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic') # freezing or only update designated variables opt_g = tf.train.RMSPropOptimizer(learning_rate=lr_generator).minimize(g_loss, var_list=theta_g) opt_c = tf.train.RMSPropOptimizer(learning_rate=lr_critic).minimize(c_loss, var_list=theta_c) # then pass those trainable variables to clip function clipped_var_c = [tf.assign(var, tf.clip_by_value(var, clip_lower, clip_upper)) for var in theta_c] # wait until RMSPropOptimizer is done with tf.control_dependencies([opt_c]): # fetch the clipped variables and output as op opt_c = tf.tuple(clipped_var_c) return opt_g, opt_c, z, real_data WGAN Algorithm Flow照 paper 上的演算法 flow 123456789101112131415161718192021222324252627282930def wgan_train(): dataset = input_data.read_data_sets(\".\", one_hot=True) opt_g, opt_c, z, real_data = build_graph() saver = tf.train.Saver() config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True) config.gpu_options.allow_growth = True config.gpu_options.per_process_gpu_memory_fraction = 0.8 def next_feed_dict(): train_img = dataset.train.next_batch(batch_size)[0] train_img = 2*train_img-1 train_img = np.reshape(train_img, (-1, 28, 28)) npad = ((0, 0), (2, 2), (2, 2)) train_img = np.pad(train_img, pad_width=npad, mode='constant', constant_values=-1) train_img = np.expand_dims(train_img, -1) batch_z = np.random.normal(0, 1, [batch_size, z_dim]).astype(np.float32) feed_dict = &#123;real_data: train_img, z: batch_z&#125; return feed_dict with tf.Session(config=config) as sess: sess.run(tf.global_variables_initializer()) summary_writer = tf.summary.FileWriter(log_dir, sess.graph) for i in range(max_iter_step): print(\"itr = \",i) for j in range(c_iter): feed_dict = next_feed_dict() sess.run(opt_c, feed_dict=feed_dict) feed_dict = next_feed_dict() sess.run(opt_g, feed_dict=feed_dict) if i % 1000 == 999: saver.save(sess, os.path.join(ckpt_dir, \"model.ckpt\"), global_step=i) 一點小結論5.1. 上述架構沒有用 batch normalization，有用的話效果會好很多，生成器和鑑別器都可用。5.2. 鑑別器換成其他 CNN 架構也可以。5.3. MLP 架構也可以。 整體來說，對於熟悉 tensorflow 的人來說不難實作 (剛好我不是很熟)，尤其 WGAN 從根本上做的改進，讓整個 training 很容易!讓我們期待接下來的發展吧~ Reference GAN Wasserstein GAN，作者的 github 令人拍案叫绝的Wasserstein GAN A Tensorflow Implementation of WGAN: 使用 tf.contrib.layers，一個 higher level 的 API，比我現在的實作可以簡潔很多。 A GENTLE GUIDE TO USING BATCH NORMALIZATION IN TENSORFLOW: Batch Normalization, MLP, and CNN examples using tf.contrib.layers","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"generative model","slug":"generative-model","permalink":"http://yoursite.com/tags/generative-model/"}]},{"title":"WGAN Part 1: 先用 GAN 鋪梗","date":"2017-03-16T13:25:12.000Z","path":"2017/03/16/WGAN-Part-1/","text":"Open.ai 這張表達 generative modeling 的意思很清楚，忍不住就借用了。 筆者才疏學淺，如有錯誤，還請指正 Generative Adversarial Nets 提出了一個 NN 的 generative modeling 方法，在這之前，NN 要成為 p.d.f. 必須依賴於 sigmoid activation 的 Restricted Boltzmann Machines (RBM) 結構。例如 Deep Belief Net，整個 network 才會是一個 p.d.f.。然而學習這樣的一個 p.d.f. 必須使用 Contrastive Divergence 的 MCMC 方法， model 訓練完後要產生 sample 時也還是必須依賴 MCMC。加上在實用上，偏偏 sigmoid 很多時候效果不如 ReLu, maxout 等，例如 sigmoid 有嚴重的 gradient vanish problem。這使得 NN 在 generative modeling 又或是 unsupervised learning 上一直困難重重。 GAN 一出立即打破這個難堪的限制 ! 怎麼說呢? GAN 捨棄能夠明確表達出 p.d.f.的作法，寫不出明確的 p.d.f. 一點也沒關係，只要能生成 夠真的sample點，並且sample的機率跟training data一樣就好 然而 GAN 在實作上卻會遇上一些困難，例如生成的 samples 多樣性不足，訓練流程/架構 和 hyper-parameters 需要小心選擇，無法明確知道訓練的收斂狀況，這些問題等下會說明。 本篇的主角 (事實上下一篇才會登場) Wasserstein GAN (WGAN)，從本質上探討 GAN 目標函式中使用的 distance measure，進而根本地解決上述三個問題，這大大降低了 generative modeling 訓練難度 ! 我們還是來談談 GAN 怎麼一回事先吧。 Generative Adversarial NetsGAN 使用一個 two-player minimax gaming 策略。先用直觀說，我們有一個 生成器 \\(G\\)，用來生成夠真的 sample，另外還有一個 鑑別器 \\(D\\)，用來分辨 sample 究竟是真實資料 (training data) 來的呢，還是假的 (\\(G\\)產生的)。當這兩個模型互相競爭到一個平衡點的時候，也就是 \\(G\\) 能夠產生到 \\(D\\) 分辨不出真假的 sample，我們的生成器 \\(G\\) 就鍊成了。而 GAN 作者厲害的地方就在於 一: 將這兩個model的競爭規則轉換成一個最佳化問題二: 並且證明，當達到賽局的平衡點時(達到最佳解)，生成器就鍊成 (可以完美表示 training data 的 pdf，並且可sampling) 我們還是必須把上述策略嚴謹的表達出來 (寫成最佳化問題)，並證明當達到最佳化問題的最佳解時，就剛好完成生成器的鍊成。 Two-player Minimax Game原則上我們希望鑑別器 \\(D\\) 能分辨出真假 sample，因此 \\(D(x)\\) 很自然地可以表示為 sample \\(x\\) 為真的機率另外生成器 \\(G\\) 則是負責產生假 sample，也可以很自然地表達為 \\(G(z)\\)，其中 \\(z\\) 為 latent variables，且我們可以假設該 latent variables \\(z\\) follow 一個 prior distribution \\(p_z(z)\\)。 我們希望 \\(D(x)\\) 對來自於真實資料的 samples 能夠盡量大，而對來自於 \\(G\\) 產生的要盡量小，因此對於鑑別器來說，它的目標函式可定義為如下: $$\\begin{align} Maximize: E_{x \\sim p_{data}(x)} [\\log D(x)] + E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ 另一方面，我們希望 \\(G\\) 能夠強到讓 \\(D\\) 無法分辨真偽，因此生成器的目標函式為: $$\\begin{align} Minimize: E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ 結合上述兩個目標函式就是如下的 minmax problem了 $$\\begin{align} \\min_G{ \\max_D{V(D,G)} } = E_{x \\sim p_{data}(x)} [\\log D(x)] + E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ 這邊作者很漂亮地給出了上述問題的理論證明。證明了兩件事情: 上述最佳化問題 (式(3)) 達到 global optimum 時, \\( p_g = p_d \\)。 (生成器產生出來的 pdf 會等於真實資料的 pdf，因此生成器鍊成!) 使用如下的演算法可以找到 global optimum 接下來我們只討論第一個事情的證明，因為這關係到 GAN 的弱點，也就是 WGAN 要解決的問題根源! 證明 Global optimum 發生時，鍊成生成器大方向是這樣的 A. 假如給定 \\(G\\)，我們都可以找到一個相對應的 \\(D_G^*\\) 最佳化鑑別器的目標函式 (1)。B. 改寫原來的目標函式 \\(V(G,D)\\)，改寫後只跟 \\(G\\) 有關，我們定義為 \\(C(G)\\)，這是因為對於每一個 \\(G\\) 我們已經配給它相對應的 \\(D_G^*\\) 了，接著證明最佳解只發生在 \\( p_g = p_d \\) 的情況。 步驟 A: $$V(G,D)=\\int_{x}{p_d(x)\\log(D(x))dx}+\\int_{z}{p_z(z)\\log(1-D(g(z)))dz} \\\\ =\\int_x[p_d(x)\\log(D(x))+p_g(x)\\log(1-D(x))]dx$$ 而一個 function \\(f(x)=a\\log (y)+b\\log (1-y)\\) 的最佳解為 \\(y=\\frac{a}{a+b}\\)因此我們得到 \\( D_G^*(x) = \\frac{p_d(x)}{p_d(x)+p_g(x)} \\) 步驟 B: $$\\begin{align*} &amp; C(G)=\\max_{D}V(G,D) \\\\ &amp; =E_{x \\sim p_d}[\\log D_G^*(x)]+E_{z \\sim p_z}[\\log(1-D_G^*(G(z)))] \\\\ &amp; =E_{x \\sim p_d}[\\log D_G^*(x)]+E_{x \\sim p_g}[\\log(1-D_G^*(x))] \\\\ &amp; =E_{x \\sim p_d}[\\log{\\frac{p_d(x)}{p_d(x)+p_g(x)}}]+E_{x \\sim p_g}[\\log{\\frac{p_g(x)}{p_d(x)+p_g(x)}}] \\end{align*}$$ 然後我們特別觀察如果 \\(p_g = p_d\\)，上式會 $$\\begin{align} =E_{x \\sim p_d}[-\\log 2]+E_{x \\sim p_g}[-\\log 2]=-\\log4 \\end{align}$$ 重新改寫一下 \\(C(G)\\) 如下 $$\\begin{align} C(G)=-\\log4+KL(p_d\\vert\\frac{p_d+p_g}{2})+KL(p_g\\vert\\frac{p_d+p_g}{2}) \\\\ =-\\log4+2JSD(p_d \\vert p_g) \\end{align}$$ 馬上觀察到 \\(JSD\\geq0\\) 和 \\(JSD=0 \\Leftrightarrow p_g = p_d \\)這表示 \\(C(G)\\) 最佳值為 \\(-\\log4\\)，且我們已知當 \\(p_g = p_d\\) 時達到最佳值 (式(4))，因此為最佳解 結論整個 GAN 的流程:我們基於一個生成器 \\(G\\) 去最佳化 \\(D\\) 得到 \\(D_G^*\\)，接著要繼續最佳化生成器的時候，問題從目標函式 (3) 變成等價於要最佳化一個 JSD 的問題 (式(5))。藉由最佳化 JSD 問題，得到新的 \\(G\\)，然後重複上面步驟，最後達到式(3)的最佳解，而我們可以保證此時生成器鍊成， \\(p_g = p_d\\)。 問題出在哪? 問題就出在最佳化一個 JSD 的問題上面 ! JSD 有什麼問題?我們通過最佳化 JSD，而將 \\(p_g\\) 逐漸拉向 \\(p_d\\)。但是 JSD 有兩個主要的問題: A. 在 實際狀況 下，無法給初連續的距離值，導致 gradient 大部分都是 0，因而非常難以訓練B. 產生的樣本多樣性不足，collapse mode。 這邊要解釋一下 實際狀況 是什麼意思。一般來說，真實資料我們都會用非常高的維度去表示，然而資料的變化通常只被少數幾種變因所控制，也就是只存在高維空間中的 local manifold。例如一個 swiss roll 雖然是在 3 維空間中，但它是在一個 2 維的 manifold 空間裡。 這樣會造成一個問題就是， \\(p_d\\) 和 \\(p_g\\)，不會有交集，又或者交集處的集合測度為0!這樣的情況在JSD衡量兩個機率分布的時候會悲劇。作者給出了下面一個簡單易懂的例子: 兩個機率分布都是在一個 1 維的 manifold 直線上，x 軸的距離維 \\(\\theta\\)，此時的 JSD 值為右圖所示，全部都是 \\(\\log2\\)，除了在 \\(\\theta\\) 那點的值是 0 (pdf完全重疊)。這樣計算出的 Gradients 幾乎都是 0，這也就是為什麼 GAN 很難訓練的原因。 這問題在 WGAN 之前還是有人提出解決的方法，不過就很偏工程思考: 加入 noise 使得兩個機率分部有不可忽略的重疊。因此讓 GAN 先動起來，動起來之後，再慢慢地把 noise 程度下降。這是聰明工程師的厲害辦法! 但終歸來說還是治標。真正的治本方法，必須要替換掉 JSD 這樣的量測函式才可以。 本篇鋪梗結束 (這梗也太長了)。下篇終於輪到主角登場， WGAN 的 W !","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"generative model","slug":"generative-model","permalink":"http://yoursite.com/tags/generative-model/"}]},{"title":"Why-Aggregation-Work","date":"2017-03-13T13:29:47.000Z","path":"2017/03/13/Why-Aggregation-Work/","text":"為何三個臭皮匠會勝過一個諸葛亮?在 ML 中有一類的演算法稱為 Aggregation Methods，這方法的運作方式其實我們可能從小就接觸到了。有沒有遇過一種情況就是，當一群人遇到一個不知道最好答案的時候，最直接的方式就是大家的答案取平均。聽起來很直覺，但心裡老覺得怪怪的，因為根本不知道到底可不可靠。Aggregation methods 就是這樣的運作模式，這邊就給個結論，它很可靠! 以下的推導出自於林軒田教授的講義，這裡用自己的理解方式重新表達，主要作筆記用 開頭還是給先定義清楚一些 terms，對於理解式子才不會混淆 定義在先 Input: \\(x \\in X\\) 正確答案: \\(f(x)\\) 臭皮匠: \\(g_t(x),t=1,2,…\\) 臭皮匠們的決策結果: \\(G(x)=avg_t(g_t(x))\\) 衡量方法 \\(g\\) 的錯誤率: \\( Error(g)=E_x[(g(x)-f(x))^2]\\) 這邊要特別說的是衡量一個方法 \\(g\\) 的錯誤率，是針對所有的 input \\(x\\)，也就是針對 \\(X\\) domain 來算期望平方誤差 運算簡單但有點神奇的推導 我們先針對 一個固定的 x，來看看臭皮匠們統合的意見是否真的會得到較好的結果，由於input已經固定，所以下面會忽略 x 的 term首先是 “臭皮匠們各自的平方錯誤率” 的平均值$$avg_t((g_t-f)^2)$$將平方拆開後得$$=avg_t(g_t^2-2g_tf+f^2)$$將 avg 移入並用 G=avg(gt) 定義得到$$=avg_t(g_t^2)-2Gf+f^2$$再做如下的簡單代數運算$$=avg_t(g_t^2)-G^2+(G-f)^2 \\\\=avg_t(g_t^2)-2G^2+G^2+(G-f)^2 \\\\=avg_t(g_t^2-2g_tG+G^2)+(G-f)^2 \\\\=avg_t((g_t-G)^2)+(G-f)^2$$ 目前為止是針對 一個特定的輸入 x，而我們需要知道的是對 整個 domain X 的錯誤率因此真正要計算的是這個目標錯誤率$$avg_t(Error(g_t))=avg_t(E_x[(g_t(x)-f(x))^2])$$將 Expection for all x 代入進去剛剛上面針對一個 x 的結果，得到如下式子\\begin{eqnarray}=avg_t(E_x[(g_t(x)-G(x))^2])+E_x[(G(x)-f(x))^2] \\\\=avg_t(E_x[(g_t(x)-G(x))^2])+Error(G) \\\\\\geq Error(G) \\end{eqnarray} 怎麼解釋?重複一下最後的重要式子: $$avg_t(Error(g_t)) = avg_t(E_x[(g_t(x)-G(x))^2])+Error(G) \\\\\\geq Error(G)$$ 最直接的結論就是: “統合出來的結果”的錯誤率 會比 “各自決定”的平均錯誤率 還要低 可以看到針對 一組固定 的臭皮匠們 \\({g_t}\\)，不等式左邊 \\(avg_t(Error(g_t))\\) 是固定值，因此若要找一個統合大家意見的方法 \\(G\\)，而該方法有最小的錯誤率 (最小化 \\(Error(G)\\) )，很明顯就是要最大化 \\(avg_t(E_x(g_t-G)^2)\\)，而此最大化的結果 就是 \\(G\\) 是 \\({g_t}\\) 的平均值(uniform blending)，符合我們一開始說的最直覺的策略! 另一方面，如果我們選到兩組 set \\({g_t}\\) and \\({h_t}\\) 他們的 Error 相同: \\(avg_t(Error(g_t))= avg_t(Error(h_t))\\) ，那我們當然是要選擇意見最不同的那一組臭皮匠們，這是因為意見愈不同代表 \\(avg_t(E_x(g_t-G)^2)\\) 愈大，因而導致 \\(Error(G)\\) 會愈小。 小結 剛剛上面這個結論就很有趣，意見遇不同的話，統合起來的效果愈好，也就是你我之間的意見有很大的分歧時，這代表是好事! 事實上 Adaboost 就是採取這麼一個策略，每一次的 iteration 會選擇跟上次統合完的結果意見差最多那一位臭皮匠進來，有機會再補上 Adaboost，這是我很喜歡的一種 ML 演算法。 而這邊還可以引出一個方法, Bootstrap. Bootstrap aggregation方法很簡單。對我們的dataset每一次重新resampling (e.g. 取N’筆，每次取的data都再放回去，因此data可以重複。可重複這點造成dataset的point具有weight的性質，這在adaboost每一次iteration的re-weighting有同樣意思) 這個叫做bootstrap，針對該次的data算出我們的weak learner gt，iterate很多次後，把每一次的gt做uniform blending。 我認為 aggregation methods 就算放到現在的 Deep Learning 火熱的時代還是相當有用的，除了本身這些方法如 adaboost 好用之外，其概念也相當有用，例如 Deep Learning 的 dropout 事實上可以用 bootstrap 來解釋 (有機會再補上資料)","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"uniform blending","slug":"uniform-blending","permalink":"http://yoursite.com/tags/uniform-blending/"},{"name":"aggregation","slug":"aggregation","permalink":"http://yoursite.com/tags/aggregation/"},{"name":"adaboost","slug":"adaboost","permalink":"http://yoursite.com/tags/adaboost/"},{"name":"bootstrap","slug":"bootstrap","permalink":"http://yoursite.com/tags/bootstrap/"}]},{"title":"Vehicle-Tracking","date":"2017-03-12T14:27:13.000Z","path":"2017/03/12/Vehicle-Tracking/","text":"這個 Porject 目的是要偵測畫面中所有的車子, 大致上的流程是先訓練好 car/non-car 的 classifer, 然後用 sliding window 搭配不同的 window size 去偵測, 最後再把 bounding boxes 做一些後處理, 例如 merge boxes, 和對時間序列的處理以下為 git hub 的 REAMDE.md The goals / steps of this project are the following: Perform a Histogram of Oriented Gradients (HOG) feature extraction I implement HOG feature extraction and using a subset of training data to search a good settings of parameters. Images are stored in output_images/HOG_with_YCrCb.jpg and output_images/grid_search.jpg Train Classifier I trained a Linear SVM classifier with HOG + color_hist + bin_spatial which achieved 98% accuracy on test set. Sliding Window Search I implemented a sliding window search method with two scales of window. HOG features are extracted once for an given image. Showing Examples so far I showed 4 examples with the pipeline so far. Image is stored in output_images/example_before_post_processing.jpg Video Implementation I showed the results with a short video clip (test_video.mp4) as well as the final result that adopted post-processing below. Further Post-processing A buffer for heat-maps is used for keeping a 6 consecutive heat-maps in frames. This will filtered out some false accepts. Discussion A short discussion is made. – Rubric Points 1. Histogram of Oriented Gradients (HOG) Explain how (and identify where in your code) you extracted HOG features from the training images. Explain how you settled on your final choice of HOG parameters. I randomly selected examples of car and notcar and showed their HOG results in each channel of HLS space: In order to get a good enough setting for those parameters (orientations, pixels_per_cell and cells_per_block), I applied a grid searching method with a linear SVM on a small subset of training data. Grid searching space is defined as follows (24 combinations): 123orient_set = range(9,19,3)pix_per_cell_set = [4,8,16]cell_per_block_set = [1,2] The purpose of this stage is not finding the optimal, but rather, a good enough setting. So I choose orient=15, pix_per_cell=8, cell_per_block=2, cspace=&#39;RGB2YCrCb&#39; 2. Train Classifier Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them). Before training the classifier, dataset should be processed first.Since the vehicles/GTI*/*.png contains time-series data, I manually selected images to avoid train and test sets having identical images. In addition, 20% images in each training folder are treated as test images. The same partition method applied to non-vehicles images too. Then I trianed a Linear SVM model with HOG + color_hist + bin_spatial features which has performance: 1inside-acc=1.0, outside-acc=0.9802036199095022 3. Sliding Window Search Describe how (and identify where in your code) you implemented a sliding window search. How did you decide what scales to search and how much to overlap windows? The course provided a very useful code snippet that can extract HOG features once no matter how much windows are. So I reuse it as the feature extraction function!I used two types of scales, 1.5 and 1.2, which deal with large and small window respectively (car with near and far positions from camera). Also, I found that the overlaping of cells_per_step = 1 (more dense windows) has better results in my implementation. Before going through, it is worth checking the image values. Since feature extraction pipeline processed .png files with mpimg.imread, it reads images with values [0,1]. However, mpimg.imread reads the .jpg file with values within [0,255]. So it is necessary to divide 255 before calling the feature extraction pipeline while reading .jpg images with mpimg.imread. Make sure your images are scaled correctly The training dataset provided for this project ( vehicle and non-vehicle images) are in the .png format. Somewhat confusingly, matplotlib image will read these in on a scale of 0 to 1, but cv2.imread() will scale them from 0 to 255. Be sure if you are switching between cv2.imread() and matplotlib image for reading images that you scale them appropriately! Otherwise your feature vectors can get screwed up. To add to the confusion, matplotlib image will read .jpg images in on a scale of 0 to 255 so if you are testing your pipeline on .jpg images remember to scale them accordingly. And if you take an image that is scaled from 0 to 1 and change color spaces using cv2.cvtColor() you’ll get back an image scaled from 0 to 255. So just be sure to be consistent between your training data features and inference features! 4. Showing Examples Show some examples of test images to demonstrate how your pipeline is working. How did you optimize the performance of your classifier? The followings are some examples. As you can see in the example 2, there exists a false accept. This will be filtered out in the post-processing part. 5. Video Implementation Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.) Following is the final result (combined with post-processing as described below) 6. Further Post-processing Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes. A heat-map to further filtered out some false positives. Moreover, I used a buffer to keep the 6 consecutive frames of heat-maps, and then accumulated those heat-maps in buffer. The accumulated heat-map then thresholded and produced the final results. 7. Discussion Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust? There still have too much parameters that effect the robustness, like ystart, ystop, scale factors, thresholds for heat-maps, and etc. Moreover, with more challanging conditions, those settings might work in one condition but fail in others. I think the most important part in those pipelines is the classifier itself. The linear SVM I used in this project is not good enough as you can see in the video that still has few false accepts. So a deep-learning based classifier might achieve better results and actually helpful to the following pipelines. This would be my future work.","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"CV","slug":"CV","permalink":"http://yoursite.com/tags/CV/"}]},{"title":"Lane-Finding","date":"2017-02-27T02:12:28.000Z","path":"2017/02/27/Lane-Finding/","text":"以下是 github 上的 README, 全英文. 此 Project 主要都是在做 Computer Vision 相關的東西. 學到了許多使用 Python and CV 相關的技巧. 整理來說是個滿有趣的 project! The goals / steps of this project are the following: Compute the camera calibration matrix and distortion coefficients given a set of chessboard images. Apply a distortion correction to raw images. Use color transforms, gradients, etc., to create a thresholded binary image. Apply a perspective transform to rectify binary image (“birds-eye view”). Detect lane pixels and fit to find the lane boundary. Determine the curvature of the lane and vehicle position with respect to center. Warp the detected lane boundaries back onto the original image. Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position. Rubric Points1. Camera calibrationThe images for calculating the distortion and 3-D to 2-D mapping matrix are stored in ./camera_cal/calibration*.jpg.Firstly, I used cv2.findChessboardCorners to find out all those corner points (corners) in the images.Then I used cv2.calibrateCamera to calculate the distortion (dist) and mapping matrix (mtx) given the corners pts and their corresponding predifined 3-D pts objp 2. Provide an example of a distortion-corrected imageHere is an example of distortion-corrected image: 3. Create a thresholded binary image and provide exampleI used magnitude of gradients, direction of gradients, and L and S in HLS color space.A combined rule is used: 12combined[((mag_binary == 1) &amp; (dir_binary == 1)) |\\ ((hls_binary == 1) &amp; (dir_binary == 1) &amp; (bright_binary == 1))] = 1 Example masking image is showed: Moreover, I used widgets to help tunning the parameters of those masking functions. It can provide instantaneous binary result that really help for accelarating this step. The widgets codes are list here: 123456789def interactive_mask(ksize, mag_low, mag_high, dir_low, dir_high, hls_low, hls_high, bright_low, bright_high): combined = combined_binary_mask(image,ksize, mag_low, mag_high, dir_low, dir_high,\\ hls_low, hls_high, bright_low, bright_high) plt.figure(figsize=(10,10)) plt.imshow(combined,cmap='gray') interact(interactive_mask, ksize=(1,31,2), mag_low=(0,255), mag_high=(0,255),\\ dir_low=(0, np.pi/2), dir_high=(0, np.pi/2), hls_low=(0,255),\\ hls_high=(0,255), bright_low=(0,255), bright_high=(0,255)) 4. Perspective transformFirst, I defined the source and destination of perspective points as follows: Source Destination 585, 460 320, 0 203, 720 320, 720 1127, 720 960, 720 695, 460 960, 0 Then the perspective_warper function is defined which returns perspective image and the matrix warpM as well.warM is needed for the later step which does the inverse perspective back to the original image. 1perspective_img, warpM = perspective_warper(undist,src,dst) An example is showed here: 5. Lane line pixel and polynomial fittingI applied a windowing approach to identify the lane pixels In this example, I used 9 windows for both lane lines. The window is processed in an order from the buttom to the top. Pixels are detected by the following function 1def identify_lane_pixel(img, lcenter_in, rcenter_in, win_num=9, win_half_width=150, start_from_button=False): lcenter_in and rcenter_inare the centers (in horizontal coordinate) of windows. win_num defines how many windows are used. In this example, 9. win_half_width refers to the half length of window width start_from_button indicates how the initial centers of windows are set. Specifically, Let the current window as j and current frame index as i. If start_from_button=True, the center of window j will be initally set as window j-1. Otherwise, it will be initally set as window j in frame i-1. Then, by using the initial position just set, the lane pixels are identified if the histogram of that window is high enough. Finally, based on those identified pixels, update the center position of current widnow j. Next, a simple second order polynomial fitting is applied to both identified pixels 123# Fit a second order polynomial to eachleft_fit = np.polyfit(lpixely, lpixelx, 2)right_fit = np.polyfit(rpixely, rpixelx, 2) But wait! Since we are assuming “birds-eye view”, both lanes should be parallel! So I first tried a method that ties the polynomial coefficients except the shifting ones! this method results in the following example As can be seen in the figure, curves are indeed parallel. However, when I applied this method to the final video, I found that it wobbling a lot! (see “8. Video” below) After some investigation, I wonder that this problem is caused by the fixed source points of perspective. Since the pre-defined source points are always at the center of the camera while the lane curves are usually not, the result perspective curves is intrinsically not parellel! Hence, I applied a dynamic source point correction. Idea of method is showed in the follows: mapping inversely from coordinates in perspective images to original images can use the following formula: and results in the following example It works great! Unfortunately, if the lane curves are not stable, the resulting new source points may fail. This is the major difficulty of this method! (see “8. Video” below) 6. Radius of curvature of the lane and the position of the vehicleThe curvature is calculated based on the following formula. Udacity provides a very good tutorial here ! 1234a1, b1, c1 = left_fit_coefficientsa2, b2, c2 = right_fit_coefficientsr1 = ((1+(2*a1*height*ym_per_pix+b1)**2)**1.5)/(2*np.abs(a1))r2 = ((1+(2*a2*height*ym_per_pix+b2)**2)**1.5)/(2*np.abs(a2)) There’s no need to worry about absolute accuracy in this case, but your results should be “order of magnitude” correct. So I divide my result by 10 to make it seems more reasonable. And of course, the “order of magnitude” remains intact. 7. Warp the detected lane boundaries back onto the original imageIn order to warp back onto the original image, we need to calculate the inverse of perspective transform matrix warpMjust apply Minv = inv(warpM) which is from numpy.linalg import inv Then, simply apply cv2.warpPerspective with Minv as input. Note: use cv2.putText to print the curvature and position onto images 8. Video Simple poly-fit (Most stable! Simple is better ?!) Shared coefficients of poly-fit (Wobbling problem) Dynamic source points of perspective (Unstable, crash sometimes. If the lane curves are not stable, the resulting new source points may fail) DiscussionBasically, I applied those techniques suggested by Udacity. I did some efforts trying to parallize both curves in the perspective “bird eye view”. Two methods are applied Shared coefficients of polynomial fitting Dynamic source points of perspetive Each has its own issue. For (1.), wobbling, and for (2.) unstable. Future works will focus on solving the (2.) unstable issue. Maybe a smoothing method is a good idea. Moreover, for more difficult videos, pixels may not be detected which makes the pipeline crash. One way to overcome this problem is when this issue happens, the lane curve is set to be the same as previous frame. Generelizing this idea, a confidence measure of lane pixels is worth to apply. If the confidence is low, then set the lane curve as the same as previous frame might be a good way to better estimate result. Finally, finding a robust combination of masking rule and tweaking those parameters precisely might help too. 附上中文其他討論: Reviewer 給了很多有用的 article links! 這邊附上做未來參考 Perspective bird eye view:http://www.ijser.org/researchpaper%5CA-Simple-Birds-Eye-View-Transformation-Technique.pdfhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3355419/https://pdfs.semanticscholar.org/4964/9006f2d643c0fb613db4167f9e49462546dc.pdfhttps://pdfs.semanticscholar.org/4074/183ce3b303ac4bb879af8d400a71e27e4f0b.pdf Lane line pixel identification:https://www.researchgate.net/publication/257291768_A_Much_Advanced_and_Efficient_Lane_Detection_Algorithm_for_Intelligent_Highway_Safetyhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5017478/https://chatbotslife.com/robust-lane-finding-using-advanced-computer-vision-techniques-46875bb3c8aa#.l2uxq26sn lane detection with deep learning:http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w3/papers/Gurghian_DeepLanes_End-To-End_Lane_CVPR_2016_paper.pdfhttp://lmb.informatik.uni-freiburg.de/Publications/2016/OB16b/oliveira16iros.pdfhttp://link.springer.com/chapter/10.1007/978-3-319-12637-1_57 (chapter in the book Neural Information Processing)http://ocean.kisti.re.kr/downfile/volume/ieek1/OBDDBE/2016/v11n3/OBDDBE_2016_v11n3_163.pdf (in Korean, but some interesting insights can be found from illustrations)https://github.com/kjw0612/awesome-deep-vision (can be useful in project 5 - vehicle detection)噁心到吐血的真實挑戰: 還是老話一句, 真的要成為可用的產品, 難道超級無敵高阿!!","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"CV","slug":"CV","permalink":"http://yoursite.com/tags/CV/"}]},{"title":"Neural Art","date":"2017-02-13T14:04:36.000Z","path":"2017/02/13/Neural-Art/","text":"Art with Neural Network風格, 創作這種能力在現在Alpha Go已經稱霸的時代, 目前覺得還是人類獨有的不過有趣的是, 對於那些已經在 ImageNet 訓練得非常好的模型, 如: VGG-19, 我們通常已經同意模型可以辨別一些較抽象的概念那麼是否模型裡, 也有具備類似風格和創作的元素呢? 又或者風格在模型裡該怎麼表達? 本篇文章主要是介紹這篇 A Neural Algorithm of Artistic Style 的概念和實作, 另外一個很好的投影片 by Mark Chang 也很值得參考 先給出範例結果, 結果 = 原始的內容 + 希望的風格 Content Image Style Image Result Image 說在前頭的最佳化在講下去之前, 我們先講 NN 的事情, 一般情況, 我們是給定 input image x, 而參數 w 則是要求的變數, 同時對 loss (objective function) 做 optimize, 實作上就是 backprob.上面講到的三種東西列出來: x: input image (given, constant) w: NN parameters (variables) loss: objective function which is correlated to some desired measure 事實上, backprob 的計算 x and w 角色可以互換. 也就是將 w 固定為 constant, 而 x 變成 variables, 如此一來, 我們一樣可以用 backprob 去計算出最佳的 image x.因此, 如果我們能將 loss 定義得與風格和內容高度相關, 那麼求得的最佳 image x 就會有原始的內容和希望的風格了!那麼再來就很明確了, 我們要定義出什麼是 Content Loss 和 Style Loss 了 Content Loss針對一個已經訓練好的 model, 我們常常將它拿來做 feature extraction. 例如一個 DNN 把它最後一層辨識的 softmax 層拿掉, 而它的前一層的 response (做forward的結果), 就會是對於原始 input 的一種 encoding. 理論上也會有很好的鑑別力 (因最只差最後一層的softmax). Udacity 的 traffic-sign detection 也有拿 VGG-19, ResNet, 和 gooLeNet 做 feature extraction, 然後只訓練重新加上的 softmax layer 來得到很高的辨識率. 因此, 我們可以將 forward 的 response image 當作是一種 measure content 的指標!知道這個理由後, 原文公式就很好理解, 引用如下: So let p and x be the original image and the image that is generated and Pl and Fl their respective feature representation in layer l. We then define the squared-error loss between the two feature representations 簡單來說 Pl 是 content image P 在 l 層的 response, 而 Fl 是 input image x (記得嗎? 它是變數喔) 在 l 層的 response.這兩個 responses 的 squared-error 定義為 content loss, 要愈小愈好. 由於 response 為 input 的某種 encoded feature, 所以它們如果愈接近, input 就會愈接近了 (content就愈接近).引用 Mark Chang 的投影片: Style Loss個人覺得最神奇的地方就在這裡了! 當時自己怎麼猜測都沒猜到可以這麼 formulate.我個人的理解是基於 CNN 來解釋假設對於某一層 ConvNet 的 kernel 為 w*h*k (width, hieght, depth), ConvNet 的 k 通常代表了有幾種 feature maps說白一點, 有 k 種 filter responses 的結果, 例如第一種是線條類的response, 第二種是弧形類的responses … 等等而風格就是這些 responses 的 correlation matrix! (實際上用 Gram matrix, 但意義類似)基於我們對於 CNN 的理解, 愈後面的 layers 能處理愈抽象的概念, 因此愈後面的 Gram matrix 也就愈能代表抽象的 style 概念.原文公式引用如下: 總之就是計算在 l 層上, sytle image a 和 input image x 它們的 Gram matrix 的 L2-norm 值 一樣再一次引用 Mark Chang 的投影片:也可以去看看他的投影片, 有不同角度的解釋 實戰主要參考此 gitHub一開始 load VGG-19 model 就不說了, 主要的兩個 loss, codes 如下:123456789101112131415161718def content_loss_func(sess, model): \"\"\" Content loss function as defined in the paper. \"\"\" def _content_loss(p, x): # N is the number of filters (at layer l). N = p.shape[3] # M is the height times the width of the feature map (at layer l). M = p.shape[1] * p.shape[2] # Interestingly, the paper uses this form instead: # # 0.5 * tf.reduce_sum(tf.pow(x - p, 2)) # # But this form is very slow in \"painting\" and thus could be missing # out some constants (from what I see in other source code), so I'll # replicate the same normalization constant as used in style loss. return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(x - p, 2)) return _content_loss(sess.run(model['conv4_2']), model['conv4_2']) 12345678910111213141516171819202122232425262728293031323334353637383940414243# Layers to use. We will use these layers as advised in the paper.# To have softer features, increase the weight of the higher layers# (conv5_1) and decrease the weight of the lower layers (conv1_1).# To have harder features, decrease the weight of the higher layers# (conv5_1) and increase the weight of the lower layers (conv1_1).STYLE_LAYERS = [ ('conv1_1', 0.5), ('conv2_1', 1.0), ('conv3_1', 1.5), ('conv4_1', 3.0), ('conv5_1', 4.0),]def style_loss_func(sess, model): \"\"\" Style loss function as defined in the paper. \"\"\" def _gram_matrix(F, N, M): \"\"\" The gram matrix G. \"\"\" Ft = tf.reshape(F, (M, N)) return tf.matmul(tf.transpose(Ft), Ft) def _style_loss(a, x): \"\"\" The style loss calculation. \"\"\" # N is the number of filters (at layer l). N = a.shape[3] # M is the height times the width of the feature map (at layer l). M = a.shape[1] * a.shape[2] # A is the style representation of the original image (at layer l). A = _gram_matrix(a, N, M) # G is the style representation of the generated image (at layer l). G = _gram_matrix(x, N, M) result = (1 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2)) return result E = [_style_loss(sess.run(model[layer_name]), model[layer_name]) for layer_name, _ in STYLE_LAYERS] W = [w for _, w in STYLE_LAYERS] loss = sum([W[l] * E[l] for l in range(len(STYLE_LAYERS))]) return loss 一開始給定 random input image:style image 選定如下:隨著 iteration 增加會像這樣: 第一次的 backprob: 1000 iteration: 2000 iteration: 3000 iteration: 4000 iteration: 5000 iteration: 短節這之間很多參數可以調整去玩, 有興趣可以自己下載 gitHub 去測 上一篇的 “GTX 1070 參見” 有提到, 原來用 CPU 去計算, 1000 iteration 花了六個小時! 但是強大的 GTX 1070 只需要 6 分鐘! 不過, 就算是給手機用上GTX1070好了 (哈哈當然不可能), 6分鐘的一個結果也是無法接受!PRISMA 可以在一分鐘內處理完! 這必定不是這種要算 optimization 的方法可以達到的.事實上, 李飛飛的團隊發表了一篇論文 “Perceptual Losses for Real-Time Style Transfer and Super-Resolution“訓練過後, 只需要做 forward propagation 即可! Standford University 的 JC Johnson 的 gitHub 有完整的 source code!找時間再來寫這篇心得文囉!","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Art","slug":"Art","permalink":"http://yoursite.com/tags/Art/"}]},{"title":"GTX 1070","date":"2017-02-12T13:44:40.000Z","path":"2017/02/12/GTX-1070/","text":"NVIDIA GTX 1070 參見經過兩次的Udacity DNN Projects後, 我受不了用CPU訓練了! 這實在是太慢了!考量應該會長期使用GPU, AWS實在不怎麼便宜 (1hr=1USD @ Tokyo site), 加上local端訓練也比較方便, 就殺下去了!! 安裝 CUDA and cuDNN大致上的安裝流程如下, 並不複雜, 更詳細可參考 link 安裝 CUDA Drivers上述聯結中有下載路徑, 然後照頁面一步步選擇 (Operating System, Version, Installer Type)Installer Type如果網路不好建議選擇 exe local, 然後下載後執行安裝就對了Windows 環境變量中 CUDA_PATH 是 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0, 但是仍須加上 bin\\ 和 lib\\x64\\, 記得加上. 安裝 cuDNN要下載這個還要填一些登入資料, 需要再Accelerated Computing Developer Program註冊, 總之註冊後就可下載解壓後會有一個資料夾 cuda, 裡面三個子資料夾 bin, include, lib將上述的檔案放到相對應的 CUDA Driver 的安裝路徑內, 預設是在 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0 安裝 tensorflow-gpu最簡單的一步pip install tensorflow-gpu然後即可測試, 如果有成功會有以下畫面, 注意 successfully 字有無出現 1234567&gt;&gt;&gt; import tensorflow as tfI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally&gt;&gt;&gt; 測試 GTX 1070 強大能力使用兩個極端例子分別測試有無使用GPU速度上的差異 Neural Art 的例子: A Neural Algorithm of Artistic Style 這個例子是所有的東西都可以 load 進 memory 中, 因此沒有任何 I/O, 直接比拚運算能力! 因此可以直接看出 GPU 和 CPU 的計算能力差異 結果: 時間沒有很嚴格計算, 是看產生結果的時間稍微計算的, 但這效能已經很誇裝了, 60倍, 60倍, 60倍!跑出來的圖: Content Image Style Image Result Image 不是所有的情況都能把 training data 和 model 都 load 進 memory 中, 所以勢必會有其他拖慢速度的環節, 其中最慢的就是 I/O 剛好 Udacity 的 project 3 就是每筆 training data 都需要去 load image 並且 on-the-fly 運算一堆 preprocessing. 這個情況剛好是另一種可能的極端 結果跑一個epoch所花的時間為這種case看來只能加速到約 2倍. 沒辦法, 其他拖油瓶的動作佔太多比例了 短結大部分的情況下, 提升的速度範圍會落在 2~60 倍 之間, 總之是值得的! 就算不玩DNN, 電動也要把它打到物超所值!","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"NVIDIA","slug":"NVIDIA","permalink":"http://yoursite.com/tags/NVIDIA/"},{"name":"cuDNN","slug":"cuDNN","permalink":"http://yoursite.com/tags/cuDNN/"},{"name":"CUDA","slug":"CUDA","permalink":"http://yoursite.com/tags/CUDA/"}]},{"title":"Driving by Learning Your Style","date":"2017-02-05T13:58:07.000Z","path":"2017/02/05/Driving-by-Learning-Your-Style/","text":"Udacity Self Driving Project 3: behavioral cloningA great simulator is provided that can log your driving data (speed, throttle, brake, steering, and images) and test the driving algorithm.Two modes are provided, Training mode and Atuonomous mode. By using Training mode, you can collect training data to train the model. Then test the model with the Atuonomous mode. For those driving log data, steering and images are the most important features that we are going to use in this project. The goal is, given an image, find out the corresponding steering angle. Some might wonder that speed, throttle, and brake are features that are useful too.Also, driving images are time correlated, not just a given static image.With ignoring so much useful information, does the goal still reasonable? Nvidia just showed it works! and works pretty well!So our first step is to collect the data, and fortunately, Udacity provides data for us and I used it for training. Training Data Analysis8036 data are provided. Each data has 3 positions of images (left, center, right) with 1 corresponding steering angle.Most of angles are 0, and I found that randomly ignoring half of 0-angle data is fine and can speed up. Moreover, I duplicated some samples that has angles within the range +-[0.2, 1] in order to balance the data.Histograms of before/after data selection are shown below: Data AugmentationData augmentation is a practical way to avoid overfit and generalized the model. I used 5 types of augmentations: Flipping – Flipping is a useful way to balance both turns of data. For each data, a 1/2 probability is used to decide wheter to flip. Also, steering angle is multiplied by -1. Horizontal shift – [-20,+20] pixels are randomly selected as the shift value. By doing so, it can help to recover the vehicle when it goes outside the lane.By referencing this article, I added 0.004 steering angle units per pixel shift to the right, and subtracted 0.004 steering angle units per pixel shift to the left.Results in [-0.8~+0.8] steering values adjustment which corresponding to [-2~+2] degrees (steering value * 25 = degree) Brightness – Brightness is done in the “HSV” domain. I found that with a ratio of [0.5~1.1] for “V” domain works fine. Blurring – A Gaussian blur with kernel size 3 is applied. Not sure how useful of this method helps for robustness. Left/Right camera images – These left/right images are very useful for data augmentation and also help for recovering off-lane driving. Udacity: You also might wonder why there are three cameras on the car: center, left, and right. That’s because of the issue of recovering from being off-center.In the simulator, you can weave all over the road and turn recording on and off. In a real car, however, that’s not really possible. At least not legally.So in a real car, we’ll have multiple cameras on the vehicle, and we’ll map recovery paths from each camera. I adjusted the steering angles for left/right images with a naive method. Following figure shows how I correct the angle of right image: I found that setting offset = 6 or 5 is good enough. For large value, the car starts zig-zagging. An example of correction shows below, where the steering angles are indicated by red lines: Data Normalization Normalization – Images are normalized with (x-128)/128. Cropping – Images are trimmed with 40, 20, 20, and 20 pixels from top, bottom, left, and right respectively. This will cut most of the car hood and sky. Resizing – resized to 66 x 200, same as NVIDIA CNN. Model ArchitectureI adopted NVIDIA CNN with dropout layers: Generator and Training Generator: It is very useful to use a python generator to feed the training data batch-by-batch rather than loading all the data in memory at once.A useful link to learn python iterator/generator list here ( for those who doesn’t familiar with python just like me :) ). In order to further speed up. I tried pre-loading a chunck of data, e.g. 5000 images, into memory, and loaded another chunck if the batch data (required by generator) is outside the chunck in memory. However, it does not speed up! Somewhat weired. For each input images, a position is randomly chosen (left,center,right).Then flipping and shadowing are applied with a random fair coin. Finally, brighteness and horizonal shift are adopted with the corresponding angle adjustment. Training: Some hyper-parameters are listed: epoch–50 samples for each epoch – 8896 optimizer – Adam with 1e-4 batch-size – 64 Although Keras did shuffle, it only applies in the batched data. So I shuffled the entire training set for each epoch to get more de-correlated data. Driving PolicyI found that instead of giving a constant throttle, controlling to a constant speed is more stable to drive.So I used a simple policy that tries to keep speed near 20. 123456789speed = float(speed) if speed &gt; 25: throttle = 0.05 elif speed &gt; 20: throttle = 0.2 elif speed &gt; 10: throttle = 0.35 else: throttle = 0.5 ResultsSee below for the track1 drive. However, I failed on track2. Hit a wall during a right turn and still working on it.Hope some tweaks on data selection and model architecture might work~","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"}]},{"title":"traffic-sign-detection","date":"2017-01-18T14:35:21.000Z","path":"2017/01/18/traffic-sign-detection/","text":"前言終於來到 project 2 了, 這次的主要目的是練習使用 tensorflow 做交通號誌識別Dataset 為 German Traffic Sign Dataset有43種交通號誌, 是一種43選1的概念, 因為沒有考慮都不是這個選項, 理論上這類問題較簡單, 有researcher達到99.81%的辨識率 共 51839 張 training data, 而 testing 有 12630 張, 分佈如下, 可以看的出來資料分佈不均每種類別 random 挑一張出來如下圖Udacity 很好心的幫忙把所有的 image 幫你打包成只剩下 traffic sign Download, 且 cv2.resize(image,(32,32)) 了, 只需要 pickle.load 下來就搞定而原始的 data 是給你一大張image, 然後再告訴你那些traffic signs在image中的rectangular window座標, 還要再多處理較麻煩 要注意的一點是, dataset 是經由一秒鐘的 video 擷取下來, 因此鄰近的 data 會很相近 [1], 如果使用 train_test_split 會 random 選擇, 導致 train 和 validation 會相近而看不出差異 Input Data PreprocessingUdacity 建議我們可以處理幾個方向 將 data 數量弄得較 balance NN 算 loss 的時候不會根據每個類別數量的多寡作權重, 因此最單純的方法是就想辦法產生出一樣多的數量, 如第2點 可以增加 fake data 我的 image processing 實在很弱, 只單純的使用 rotation, 而且只敢稍微讓angle為正負5度, 怕那種有方向箭頭的號誌轉壞 1cv2.getRotationMatrix2D(image_center, angle, scale) 這樣的方式我實驗起來其實沒啥幫助, XD 我看到有人還使用 cv2.WarpPerspective, 果然專業多了! 我相信產生種類夠多的 fake data 一定會有幫助, 例如加 noise, blur 等等 將 data 做 normalization 做語音習慣了, 直覺就用 guassian normalization, mean=0, var=1, 結果整個大失敗! 只有不到1%辨識率, why?? 後來用 mean substraction, 然後除 abs 的最大值, 我只選擇使用 YUV 的 Y channel 當 input CNN 架構要設計和調整架構有點花時間, 加上我時間不多(懶), 所以我直接就用LeNet架構1234567layer_depth = &#123; 'layer_1': 6, 'layer_2': 16, 'fully_connected_1': 120, 'fully_connected_2': 84, 'out': n_classes,&#125; 自己多加了 dropout 和 l2 regularization, 原因是每次跑 training 的 accuracy 都要標到98 99, 但是 validation set 始終很難突破 93, 一直有 overfit 的感覺tensorflow 的 dropout 是設定要保留多少比例 (keep_prob), 在 training 的時候設定在最後的兩層 fully connected layers, keep_prob 愈小基本上愈難訓練也需要愈多 epoch另外記得在做 evaluation 的時候要把 keep_prob 設定成回 1 [1] 的架構想法不錯, 將較低層的 conv. layer 和較上層的 conv. layer 一併當作 fully connected layer 的 input, 這樣同時能夠有 low-level feature, higher-resolution 和 high-level feature, lower-resolution 兩種資訊一起當決策 其他 Hyper-parameters Optimizer: 說實話, 要不停的調整出最好的參數實在沒那個心力, 所以與其用SGD, 我就直接用 Adam 了 (Adagrad也是一種懶人選擇) pooling: 沒啥特別選, 因此用 max-pooling batch-size: 原先設定128, 有一次改成256就實在train不好, 就退回128了 learning rate: 0.001 l2 weight: 0.01 Learning Performancetest set accuracy = 0.893 自選測試圖片Udacity希望能學員自己找圖片來測試, 因此我就在德國的 google map 上找圖, (看著看著心都飄過去了)20張圖辨識結果如下:剛好錯10個, 只有 50% 正確率, 這實在有點悲劇其中有兩個錯誤值得注意右圖是top5辨識到的類別及機率, 可以發現除了正確答案的 traffic signal 在第二名外, 第一名的 general causion 其實跟 traffic signal 超像的 (只看灰階)看來必須把 input 的色彩資訊也加進去才能進一步改善了另一個是如下這個錯誤自己分析的原因是因為 training data 的 speed limit 都是圓的外框, 而此case剛好是一個長方形牌子, 裡面才是退色很嚴重的圓形, 所以導致辨識失敗或許真的 train 得很好的 CNN 有能力找出重要的判斷資訊, 因此會去忽略外面的方框, 而選擇去”看”外面退色的圓形和裡面的數字結論就是, 應該是我自己沒train好吧 ?! 短結小小做過一輪交通號誌辨識, 才比較有感覺真實狀況會有多困難阿~找時間來 visualize 一下每層的 hidden units 對什麼樣的 image 會有較高的 activation! This paper by Zeiler and Fergus with toolbox 要能 train 出好 model 除了參考文獻培養對 model 架構的好直覺外, engineering 的苦工也會是很大的關鍵! 後續嘗試對於目前的辨識率很不滿意. 不死心下就實作[1]的架構, 然後將 NN 的 model size 擴大, 並且將顏色資訊 YUV 的 U 加進去訓練 (結果上述因顏色錯誤的traffic signal就分對了)12345678910111213# Hyper-parametersEPOCHS = 30BATCH_SIZE = 128rate = 0.001drop_out_keep_prob = 0.5layer_depth = &#123; 'layer_1': 16, 'layer_2': 32, 'fully_connected_1': 256, 'fully_connected_2': 128, 'out': n_classes,&#125; 得到了 Test Accuracy = 0.953 ! 但是自選圖雖有進步仍很低 65%另外, 上述的參數設定下, 如果加了 l2_weight = 0.01 的話, validation 只能到 0.91x, 實在不大好訓練, 後來只好放棄第一次的 submission, reviewer 給了一些不錯的 reference 如下: Extra Important MaterialLately on slack few students asked for a good Deep Learning book.So after lot of research found a book which is also recommended by Elon Musk Deep Learning (Adaptive Computation and Machine Learning series) Github and on Amazon Fast.ai A Guide to Deep LearningFew Articles Traffic sign classification using brightness augmentation Dealing with unbalanced dataExtra Materials I noted a linkage here to discuss about how should we choose the batch_size of Stochastic Gradient Decent Since you might be interested into “Adam Optimizer”, here is a website that talks about it. You might like to learn the whole idea of Dropout It’s gives a brief analysis of the technique. reviewer 很用心阿!棒棒! Reference[1.] Traffic Sign Recognition with Multi-Scale Convolutional Networks","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"}]},{"title":"使用AWS訓練DNN步驟","date":"2017-01-16T13:47:43.000Z","path":"2017/01/16/aws-procedure/","text":"AWS Instance注意事項及連線建立AWS instance的時候, 由於我們使用jupyter需要port 8888, 需要 Configure the Security Group Running and accessing a Jupyter notebook from AWS requires special configurations. Most of these configurations are already set up on the udacity-carnd AMI. However, you must also configure the security group correctly when you launch the instance. By default, AWS restricts access to most ports on an EC2 instance. In order to access the Jupyter notebook, you must configure the AWS Security Group to allow access to port 8888.Click on “Edit security groups”.On the “Configure Security Group” page:Select “Create a new security group”Set the “Security group name” (i.e. “Jupyter”)Click “Add Rule”Set a “Custom TCP Rule”Set the “Port Range” to “8888”Select “Anywhere” as the “Source”Click “Review and Launch” (again) 成功建立AWS instance之後, 開啟git bashssh -i ‘C:\\Users\\bobon\\.ssh\\MyKeyPair.pem’ carnd@54.65.11.64其中54.65.11.64是instance的ip AWS上開啟jupyter notebook kernel首先先把project clone下來, 並設定好conda env1234git clone https://github.com/udacity/CarND-Traffic-Sign-Classifier-Projectcd CarND-Traffic-Sign-Classifier-Projectconda env create -f environment.ymlsource activate CarND-Traffic-Sign-Classifier-Project 接著安裝tensorflow-gpu1pip install tensorflow-gpu opencv 安裝1conda install -c https://conda.binstar.org/menpo opencv 剛剛已經建立conda的環境, 且activate CarND-Traffic-Sign-Classifier-Project, 所以可以直接開啟kernel1jupyter notebook 在local瀏覽器上輸入http://[all ip addresses on your system]:8888/例如aws ip為54.65.11.641http://54.65.11.64:8888/ 抓取AWS上的資料下來local端在自己local的terminal上12scp -i &apos;C:\\Users\\bobon\\.ssh\\MyKeyPair.pem&apos; carnd@54.65.11.64:/home/carnd/Traffic-sign/cnn-traffic-sign* ./models/scp -i &apos;C:\\Users\\bobon\\.ssh\\MyKeyPair.pem&apos; carnd@54.65.11.64:/home/carnd/Traffic-sign/checkpoint ./models/ 接著輸入密碼即可 (carnd)","tags":[{"name":"aws","slug":"aws","permalink":"http://yoursite.com/tags/aws/"}]},{"title":"Hexo 中文顯示 and Markdown 測試","date":"2017-01-08T13:47:43.000Z","path":"2017/01/08/chinese-encoding/","text":"除了將Hexo的_config.yml 設定成 language: zh-tw 之外文章如果用UltraEdit編輯的話的話, 要使用轉換編碼, 將ASCII轉UTF-8(Unicode編輯), 中文才能正常顯示 引言測試同一個區塊的引言 內容文字, 強調 引言測試二同一個引言測試二的區塊 無序清單, item1 仍然是item1的內容 item2 item3 有序item1 item2 仍然是item2的內容 item3 1234567891011121314151617181920212223bool text(const string inPath, const string outPath)&#123; ifstream ifs(inPath.c_str()); if (!ifs) return false; ofstream ofs(outPath.c_str()); if (!ofs) return false; string line; while (getline(ifs,line)) &#123; istringstream iss(line); string token; while (iss&gt;&gt;token) &#123; cout &lt;&lt; \"&lt;Token&gt;: \" &lt;&lt; token &lt;&lt; endl; ofs &lt;&lt; \"&lt;Token&gt;: \" &lt;&lt; token &lt;&lt; endl; &#125; &#125; ofs.close(); ifs.close(); return true;&#125; 新的item? Here is an example of AppleScript: tell application &quot;Foo&quot; beep end tell Normal paragrah 以下為分隔線 上線使用三個*中間有空格 上線使用三個*中間無空格 上線使用5個*中間有空格 上線使用三個-中間有空格 數學公式測試 $$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$\\(x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\ 方法:在文章要有 &lt;script type=”text/javascript” src=”http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default“ &gt;&lt;/script&gt; 這行指令然後安装插件 Hexo-math, 安装方法如下, 依次为1$ npm install hexo-math --save 在 Hexo 文件夹中执行：1$ hexo math install 在 _config.yml 文件中添加：1plugins: hexo-math","tags":[{"name":"markdown","slug":"markdown","permalink":"http://yoursite.com/tags/markdown/"},{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"Hello World","date":"2017-01-08T13:43:07.943Z","path":"2017/01/08/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]