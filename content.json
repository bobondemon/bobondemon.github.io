[{"title":"Stochastic Processes Week 2 Poisson Processes","date":"2021-12-12T00:42:36.000Z","path":"2021/12/12/Stochastic-Processes-Week-2-Poisson-Processes/","text":"Coursera Stochastic Processes èª²ç¨‹ç­†è¨˜, å…±ä¹ç¯‡: Week 0: ä¸€äº›é å‚™çŸ¥è­˜ Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes (æœ¬æ–‡) Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; ItÃ´ formula Week 8: LÃ©vy processes Week 2.1-4: Definition of a Poisson process as a special example of renewal process. Exact forms of the distributions of the renewal process and the counting processJust a recapä»¤æˆ‘å€‘æœ‰å¦‚ä¸Šé¢æ‰€è¿°çš„ renewal process[Poisson Processes Def1]:&emsp;A Poisson process æ˜¯ä¸€å€‹ renewal process, ä¸” $\\xi_i\\sim p(x)=\\lambda e^{-\\lambda x}\\cdot \\mathbf{1}(x&gt;0)$ which is exponential density function&emsp;æ­¤æ™‚ $\\mathbf{1}(\\cdot)$ ç‚º indicator function, è€Œ $\\lambda$ ç¨±ç‚º intensity parameter, æˆ– rate of Poisson process ç°¡å–®èªªå°±æ˜¯ exponential random variables çš„ renewal processæ­¤æ™‚çš„ renewal process çš„ $S_n$ (arrival time process) and $N_t$ (counting process) æœ‰ closed form [Thm] Arrival time process $S_n$ çš„ distribution and density functions:&emsp;$$F_{S_n}(x)=\\left\\{ \\begin{array} {rl} 1-e^{-\\lambda x} \\sum_{k=0}^{n-1}\\frac{(\\lambda x)^k}{k!}, &amp; x&gt;0 \\\\ 0, &amp; x&lt;0 \\end{array} \\right. \\\\ \\mathcal{P}_{S_n}(x)=\\lambda\\frac{(\\lambda x)^{n-1}}{(n-1)!}e^{-\\lambda x}\\cdot \\mathbf{1}(x&gt;0)$$ [Proof]:&emsp;æˆ‘å€‘è­‰æ˜ p.d.f., ä½¿ç”¨æ•¸å­¸æ­¸ç´æ³•&emsp;è€ƒæ…® $n=1$ æ™‚ $\\mathcal{P}_{S_1}=?$&emsp;$S_1=\\xi_1\\sim \\lambda e^{-\\lambda x}\\cdot \\mathbf{1}(x&gt;0)$&emsp;å‡è¨­ $n$ æˆç«‹, è€ƒæ…® $n+1$&emsp;$$\\mathcal{P}_{S_{n+1}}(x) = \\int_{y=0}^x \\mathcal{P}_{S_n}(x-y) \\cdot \\mathcal{P}_{\\xi_{n+1}}(y) dy \\\\ = \\int_{y=0}^x \\frac{\\lambda^n(x-y)^{n-1}}{(n-1)!}e^{-\\lambda (x-y)} \\lambda e^{-\\lambda y} dy \\\\ = \\int_{y=0}^x \\frac{\\lambda^{n+1}(x-y)^{n-1}}{(n-1)!}e^{-\\lambda x} dy \\\\ = \\frac{\\lambda^{n+1}}{(n-1)!}e^{-\\lambda x} \\int_{y=0}^x (x-y)^{n-1}dy \\\\ = \\frac{\\lambda^{n+1}}{(n-1)!} e^{-\\lambda x} \\cdot \\frac{x^n}{n} = \\lambda\\frac{(\\lambda x)^n}{n!} e^{-\\lambda x}$$ [Thm] Counting process $N_t$ æ˜¯ Poisson distribution, $\\mathcal{P}{N_t=n}\\sim Pois(\\lambda t)$:&emsp;$${\\color{orange} { \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\lambda t)=e^{-\\lambda t}\\frac{(\\lambda t)^n}{n!} } }$$[Proof]:&emsp;æˆ‘å€‘æœ‰ $\\mathcal{P}\\{N_t=n\\}=\\mathcal{P}\\{ S_n\\leq t\\} - \\mathcal{P}\\{ S_{n+1}\\leq t\\}\\ldots(\\star)$&emsp;L.H.S. æ„æ€æ˜¯, åˆ°æ™‚é–“ $t$ ç‚ºæ­¢æ­£å¥½ç™¼ç”Ÿ $n$ æ¬¡äº‹ä»¶çš„æ©Ÿç‡&emsp;R.H.S. ä»¥ä¸‹å…©ç¨®éƒ½å¯ä»¥è§£é‡‹:&emsp;1. åˆ°æ™‚é–“ $t$ ç‚ºæ­¢å·²ç™¼ç”Ÿè‡³å°‘ $n$ æ¬¡äº‹ä»¶çš„æ©Ÿç‡, æ‰£æ‰, åˆ°æ™‚é–“ $t$ ç‚ºæ­¢å·²ç™¼ç”Ÿè‡³å°‘ $n+1$ æ¬¡äº‹ä»¶çš„æ©Ÿç‡&emsp;&emsp;â€”&gt; å¾ˆé¡¯ç„¶åªæœƒå‰©ä¸‹åˆ°æ™‚é–“ $t$ ç‚ºæ­¢æ­£å¥½ç™¼ç”Ÿ $n$ æ¬¡äº‹ä»¶çš„æ©Ÿç‡&emsp;2. ç¬¬ $n$ å€‹äº‹ä»¶ç™¼ç”Ÿçš„æ™‚é–“æ¯” $t$ å°çš„æ©Ÿç‡, æ‰£æ‰, ç¬¬ $n+1$ å€‹äº‹ä»¶ç™¼ç”Ÿçš„æ™‚é–“æ¯” $t$ å°çš„æ©Ÿç‡.&emsp;&emsp;$\\{N_t=n\\}=\\{S_n\\leq t\\} \\cap \\{S_{n+1}&gt;t\\}$ ç”¨ $A \\cap B$ è¡¨ç¤º, ç”±æ–¼&emsp;$$\\left. \\begin{array}{r} A\\cap B=A\\backslash B^c \\\\ B^c\\subset A \\end{array} \\right\\} \\Rightarrow \\mathcal{P}(A\\cap B)=\\mathcal{P}(A)-\\mathcal{P}(B^c)$$&emsp;å› æ­¤&emsp;$$(\\star) = \\left( 1-e^{-\\lambda t} \\sum_{k=0}^{n-1}\\frac{(\\lambda t)^k}{k!} \\right) - \\left( 1-e^{-\\lambda t} \\sum_{k=0}^{n}\\frac{(\\lambda t)^k}{k!} \\right) \\\\ = e^{-\\lambda t}\\frac{(\\lambda t)^n}{n!}$$ Week 2.5: Memoryless property[Def] Memoryless Property:&emsp;A random variable $X$ possesses the memoryless property, $iff$&emsp;$\\mathcal{P}\\{X&gt;u+v\\} = \\mathcal{P}\\{X&gt;u\\} \\mathcal{P}\\{X&gt;v\\}$ If $\\mathcal{P}\\{x&gt;v\\}&gt;0$, then$\\mathcal{P}\\{ X&gt;u+v \\vert X&gt;v \\} = \\mathcal{P}\\{ X&gt;u \\}\\ldots(\\square)$é€™æ˜¯å› ç‚º$$\\mathcal{P}\\{X&gt;u+v\\} = \\mathcal{P}\\{X&gt;u+v , X&gt;v\\}=\\mathcal{P}\\{X&gt;u+v \\vert X&gt;v\\}\\mathcal{P}\\{X&gt;v\\} \\\\ \\text{mem.less prop.}\\Rightarrow \\mathcal{P}\\{X&gt;u\\} \\mathcal{P}\\{X&gt;v\\} = \\mathcal{P}\\{X&gt;u+v \\vert X&gt;v\\}\\mathcal{P}\\{X&gt;v\\} \\\\ \\Rightarrow \\mathcal{P}\\{X&gt;u\\} = \\mathcal{P}\\{X&gt;u+v \\vert X&gt;v\\}$$ å¾é€™å°±å¯ä»¥çœ‹å‡ºç‚ºä½•ç¨± memoryless. é€™æ˜¯å› ç‚º å·²ç¶“ç­‰äº† $v$ çš„æ™‚é–“, è¦å†å¤šç­‰ $u$ çš„æ™‚é–“, è·Ÿä¸€é–‹å§‹å°±ç­‰ $u$ çš„æ™‚é–“çš„æ©Ÿç‡æ˜¯ä¸€æ¨£çš„ å†ä¾†æœ‰ä¸€å€‹å®šç†å¯ä»¥çœ‹å‡º Poisson process é©ä¸é©åˆç”¨ä¾† model ä¸€å€‹å•é¡Œ [Thm] Memoryless is exactly exponential distribution:&emsp;Let $X$ be a random variable with density $p(x)$, å‰‡&emsp;$X\\text{ memoryless } \\Longleftrightarrow p(x)=\\lambda e^{-\\lambda x}\\text{, }X\\sim\\text{ exponential p.d.f.}$ ç¯„ä¾‹: å…¬è»Šæ¯ $20 \\pm 2$ åˆ†é˜ä¾†ä¸€ç­, ä¹Ÿå°±æ˜¯èªª $X$ æ˜¯å…¬è»Šåˆ°çš„æ™‚é–“çš„ r.v. å€¼åœ¨ $20 \\pm 2$. é€™å€‹å•é¡Œæ˜¯å¦å¯ä»¥ç”¨ Poisson process ä¾† model ? (æª¢å¯Ÿ r.v. $X$ æ˜¯å¦å…·æœ‰ memoryless property)ä»¤ $v=19$, $u=10$, å‰‡è€ƒæ…® $(\\square)$L.H.S. ç‚º $\\mathcal{P}\\{X&gt;29|X&gt;19\\}=0$R.H.S. ç‚º $\\mathcal{P}\\{ X&gt;10 \\}=1$å…©è€…ä¸ç›¸ç­‰, æ‰€ä»¥ä¸å…·æœ‰ memoryless property, å› æ­¤æ­¤ random variable ä¸èƒ½ç”¨åœ¨ renewal process è®“å®ƒæˆç‚º Poisson process [Quiz]: Week 2.6-7: Other definitions of Poisson processesä¹‹å‰å®šç¾© Poisson process æ˜¯ renewal process çš„ä¸€å€‹ç‰¹ä¾‹ (ç•¶ $\\xi$ æ˜¯ exponential distribution)ç¾åœ¨çµ¦å‡ºå¦ä¸€ç¨®å®šç¾©, é€™ç¨®å®šç¾©è·Ÿ Poisson process èˆ‡ Levy precess çš„é—œè¯æœ‰å¾ˆå¤§çš„é—œä¿‚. (ä¹‹å¾Œæ‰æœƒçŸ¥é“) [Poisson Processes Def2] from Levy precess:&emsp;Poisson process is an integer-valued process, $N_t$, with $N_0=0$ a.s. (almost surely), such that&emsp;1. $N_t$ has independent increments&emsp;&emsp;$\\forall t_0&lt;t_1&lt;...&lt;t_n$, we have $N_{t_1}-N_{t_0}, ..., N_{t_n}-N_{t_{n-1}}$ are independent&emsp;2. $N_t$ has stationary increments&emsp;&emsp;$N_t-N_s$ èˆ‡ $N_{t-s}$ å…·æœ‰ç›¸åŒçš„ distribution&emsp;3. $N_t-N_s\\sim Pois(\\lambda(t-s))$&emsp;&emsp;$${\\color{orange} { \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\lambda t)=e^{-\\lambda t}\\frac{(\\lambda t)^n}{n!} } }$$ ç‰¹æ€§ 3 å¯ä»¥æ¨å°å‡ºç‰¹æ€§ 2$X \\sim Pois(\\mu)$ has $\\mathbb{E}[X]=Var[X]=\\mu$ æˆ‘å€‘å†ä¾†æ¨å°ä¸€å€‹å®šç†, è©²å®šç†è®“æˆ‘å€‘æœ‰ç¬¬ä¸‰ç¨® Poisson process çš„å®šç¾©. ä¸”è·Ÿ queueing theory æœ‰é—œè¯.ç•¶åœ¨ä¸€å€‹æ¥µçŸ­çš„æ™‚é–“æ®µä¹‹å…§, Poisson process èƒ½åˆ†æˆä¸‰ç¨®æƒ…æ³:&emsp;1. æ²’æœ‰äº‹ä»¶ç™¼ç”Ÿ&emsp;2. äº‹ä»¶ç™¼ç”Ÿ $1$ æ¬¡&emsp;3. äº‹ä»¶ç™¼ç”Ÿ $\\geq 2$ æ¬¡ å®šç¾© $f(h)=o(g(h))$ ç‚º$\\lim_{h\\rightarrow0}\\frac{f(h)}{g(h)}=0$ ç›´è§€ç†è§£ç‚º $f(h)$ æ¯” $g(h)$ å°çš„æ›´å¿«. [Thm] Poisson process åœ¨æ¥µå°æ™‚é–“æ®µçš„è¡Œç‚º:&emsp;$$\\left\\{ \\begin{array}{rl} \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1 -\\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h), &amp; h\\rightarrow0 \\end{array} \\right.$$[Proof]:&emsp;æˆ‘å€‘å…ˆè­‰æ˜ $=0$ çš„æƒ…æ³&emsp;æ ¹æ“šä¹‹å‰çš„ å®šç¾©2 æˆ‘å€‘çŸ¥é“ $\\mathcal{P}\\{N_{t+h}-N_t=0\\}=e^{-\\lambda h}$, å‰‡&emsp;$$\\lim_{h\\rightarrow0}\\frac{1-\\mathcal{P}\\{N_{t+h}-N_t=0\\}}{h} = \\lim_{h\\rightarrow0}\\frac{1-e^{-\\lambda h}}{h}\\\\ \\text{by L&apos;Hospital&apos;s rule} = \\lim_{h\\rightarrow0}\\frac{\\lambda e^{-\\lambda h}}{1} = \\lambda$$&emsp;i.e., as $h\\rightarrow0$,&emsp;$$\\frac{1-\\mathcal{P}\\{N_{t+h}-N_t=0\\}}{h} = \\lambda + o(1) \\\\ \\Rightarrow \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1-\\lambda h + ho(1) \\\\ = 1-\\lambda h + o(h)$$&emsp;$o(h)$ çš„æ­£è² è™Ÿä¸é‡è¦, Q.E.D.&emsp;è€Œ $\\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h), h\\rightarrow0$ é€™ä¸€æ¢å¯ä»¥è—‰ç”±è¨ˆç®—&emsp;$$\\lim_{h\\rightarrow0}\\frac{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}{h} = \\lim_{h\\rightarrow0}\\frac{e^{-\\lambda h}\\lambda h}{h}\\\\ \\text{by L&apos;Hospital&apos;s rule} = \\lim_{h\\rightarrow0}\\frac{-\\lambda e^{-\\lambda h}\\lambda h + \\lambda e^{-\\lambda h}}{1} = \\lambda$$&emsp;i.e., as $h\\rightarrow0$,&emsp;$$\\frac{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}{h} = \\lambda + o(1) \\\\ \\Rightarrow \\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h)$$&emsp;Q.E.D. &emsp;æœ€å¾Œ, $\\mathcal{P}\\{N_{t+h}-N_t\\geq2\\}=1-\\mathcal{P}\\{N_{t+h}-N_t=0\\} - \\mathcal{P}\\{N_{t+h}-N_t=1\\}$ å¯ä»¥è¨ˆç®—å¾—åˆ°&emsp;æ³¨æ„åˆ° $-o(h)=o(h)$, $2o(h)=o(h)$&emsp;Q.E.D. [Poisson Processes Def3] from queueing process:&emsp;Poisson process is an integer-valued process, $N_t$, with $N_0=0$ a.s. (almost surely), such that&emsp;1. $N_t$ has independent increments&emsp;&emsp; $\\forall t_0&lt;t_1&lt;...&lt;t_n$, we have $N_{t_1}-N_{t_0}, ..., N_{t_n}-N_{t_{n-1}}$ are independent&emsp;2. $N_t$ has stationary increments&emsp;&emsp;$N_t-N_s$ èˆ‡ $N_{t-s}$ å…·æœ‰ç›¸åŒçš„ distribution&emsp;3. æ»¿è¶³:&emsp;&emsp;$\\lim_{h\\rightarrow0} \\frac{\\mathcal{P}\\{N_{t+h} - N_t\\geq2\\}}{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}=0$ å®šç¾© 2 å’Œå®šç¾© 3 çš„å·®åˆ¥åªåœ¨ç¬¬ 3 é»çš„æ¢ä»¶. ç„¶è€Œå…©ç¨®å®šç¾©ç­‰åƒ¹.æˆ‘å€‘å…¶å¯¦å·²ç¶“è­‰æ˜äº† (å®šç¾© 2 $\\Rightarrow$ å®šç¾© 3), ä½†å¦ä¸€å€‹æ–¹å‘é‚„æ²’æœ‰ Week 2.8-9: Non-homogeneous Poisson processesä¹Ÿå¯ä»¥åƒè€ƒ https://www.randomservices.org/random/poisson/Nonhomogeneous.html, ä½†å°æˆ‘ä¾†èªªæœ‰é»é›£æ‡‚ [Non-homogeneous Poisson Processes Def1]:&emsp;Let $\\Lambda(t)$ be a differentiable increasing function, and $\\Lambda(0)=0$,&emsp;$N_t$ is a N.H.P.P. (Non-Homogeneous Poisson Processes), if&emsp;1. $N_0=0$&emsp;2. $N_t$ has independent increments&emsp;3. $N_t-N_s \\sim Pois(\\Lambda(t)-\\Lambda(s))$&emsp;&emsp;$${\\color{orange} { \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\Lambda(t))=e^{-\\Lambda(t)}\\frac{\\Lambda(t)^n}{n!} } }$$&emsp;å®šç¾© $\\lambda(t) = \\Lambdaâ€™(t)$, ç¨± intensity function åœ¨ P.P. çš„å®šç¾© 2, æˆ‘å€‘çŸ¥é“ $N_t-N_s\\sim Pois(\\lambda(t-s))$, æ‰€ä»¥ç•¶ $\\Lambda(t)=\\lambda t$ çš„è©±, non-homogeneous P.P. ç­‰æ–¼ P.P. æˆ‘å€‘å¯ä»¥ç™¼ç¾ non-homogeneous å»æ‰ stationary increment ç‰¹æ€§, ä¹Ÿå°±æ˜¯ Poisson distribution çš„ rate $\\lambda$ åœ¨æ¯å€‹æ™‚é–“æ®µè½ä¸ä¸€å®šæœƒéƒ½ä¸€æ¨£, depends on $\\Lambda(t)$ [Properties of N.H.P.P.]:&emsp;1. $\\mathbb{E}[N_t]=\\Lambda(t)$&emsp;&emsp;æˆ‘å€‘ç®—ä¸€ä¸‹ $\\mathbb{E}[N_t]$ for N.H.P.P.:&emsp;&emsp;$N_t = N_t - N_0 \\sim Pois(\\Lambda(t)-\\Lambda(0))=Pois(\\Lambda(t))$&emsp;&emsp;$\\therefore \\mathbb{E}[N_t]=\\Lambda(t)$&emsp;2. å¦‚æœ $\\lambda(t)=\\text{const} \\Rightarrow \\Lambda(t) = \\text{const}\\cdot t$, å›é€€åˆ°åŸä¾†çš„ (homogeneous) P.P.&emsp;3. å› ç‚º $\\Lambda(t)$ is differentialble and increasing, æ‰€ä»¥ $\\Lambda^{-1}(t)$ å­˜åœ¨&emsp;&emsp;è®“æˆ‘å€‘å‡è¨­ $\\text{Image}(\\Lambda(t))=\\mathbb{R}^+$, æ‰€ä»¥ $\\Lambda^{-1}(t)$ å°æ–¼ $t\\in\\mathbb{R}^+$ éƒ½æ˜¯ well defined&emsp;&emsp;å°æ–¼ä¸€å€‹ N.H.P.P. çš„ $N_t$ ä¾†èªª, è€ƒæ…® $N_{\\Lambda^{-1}(t)}$ é€™äº› r.v.s çš„è©±, æœƒç™¼ç¾è®Šæˆäº† homogeneous P.P. äº†! ç¬¬ä¸‰é»æä¾›äº†ä¸€å€‹ N.H.P.P. èˆ‡ P.P. çš„å°æ‡‰æ–¹æ³•ä½†å…·é«”ä¾†èªª, å¦‚æœä¸€å€‹ N.H.P.P. å‰›å¥½æ˜¯ P.P. çš„è©±, iff ä»–ä¹Ÿæ˜¯å€‹ renewal process, ä¸‹ä¸€ç¯€è­‰æ˜ Week 2.10-12: Relation between renewal theory and non-homogeneous Poisson processesæˆ‘å€‘è©¦è‘—å¾ä¸€å€‹ $N_t$ æ˜¯ N.H.P.P. å»å»ºæ§‹ renewal process çœ‹çœ‹, æˆ‘å€‘æœ‰å¦‚ä¸‹çš„ N.H.P.P.Renewal process çš„ arrival time $S_n$ å¯ä»¥é€™éº¼æ§‹å»º$S_n=\\arg\\min_t\\{N_t=n\\}$ $\\{N_t=n\\}$ è¡¨ç¤ºç™¼ç”Ÿ $n$ æ¬¡äº‹ä»¶çš„æ™‚é–“çš„é›†åˆ, æ‰€ä»¥å¾ˆé¡¯ç„¶çš„å–æœ€å°é‚£å€‹å°±æ˜¯å‰›å‰›å¥½ç¬¬ $n$ å€‹äº‹ä»¶ç™¼ç”Ÿçš„æ™‚é–“, i.e. $=S_n$æœ‰ $S_n$ å°±å¯ä»¥å¾—åˆ° interarrival time $\\xi_n=S_n-S_{n-1}$ é€™æ¨£å­çš„ arrival times $S_n$ and interarrival times $\\xi_n$ æ˜¯ä¸€å€‹ renewal process å—?é¦–å…ˆæˆ‘å€‘çŸ¥é“è‹¥è¦æˆç‚ºä¸€å€‹ renewal process, $\\xi_1, \\xi_2, â€¦$ å¿…é ˆæ˜¯ i.i.d. æ‰è¡ŒNote that:$${ \\mathcal{P}\\{N_t=n\\} \\sim Pois(\\Lambda(t))=e^{-\\Lambda(t)}\\frac{\\Lambda(t)^n}{n!} }$$ å…ˆè­‰æ˜ä»¥ä¸‹å…©å€‹ç­‰å¼: $\\mathcal{P}_{\\xi_1}(x)=\\lambda(x)e^{-\\Lambda(x)}$:[Proof]:&emsp;$$\\mathcal{P}\\{\\xi_1\\leq x\\}=\\mathcal{P}\\{S_1\\leq x\\}=\\mathcal{P}\\{N_x\\geq 1\\} = 1 - \\mathcal{P}\\{N_x=0\\} \\\\ = 1- Pois(\\Lambda(x)) = 1-e^{-\\Lambda(x)}$$&emsp;ç”¨åˆ° $\\{S_n\\leq t\\}=\\{N_t\\geq n\\}$, ç„¶å¾Œå·¦å³ç­‰å¼å¾®åˆ†å¾—åˆ°:&emsp;$\\mathcal{P}_{\\xi_1}(x)=\\lambda(x)e^{-\\Lambda(x)}$&emsp;å…¶ä¸­ $\\lambda=\\Lambdaâ€™$, Q.E.D. $\\mathcal{P}_{(\\xi_2|\\xi_1)}(t|s)=\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)}$:[Proof]:&emsp;å…ˆè¨ˆç®—ä¸€ä¸‹å…©å€‹ r.v.s $\\xi_1,\\xi_2$ çš„ joint C.D.F. $\\mathcal{F}_{\\xi_1,\\xi_2}(s,t)$ &emsp;$$\\mathcal{F}_{(\\xi_1,\\xi_2)}(\\xi_1=s,\\xi_2=t)=\\mathcal{P}\\{\\xi_1\\leq s, \\xi_2\\leq t\\} \\\\ =\\int_{y=0}^s \\mathcal{P}\\{ \\xi_1\\leq s, \\xi_2\\leq t \\vert \\xi_1=y \\} \\mathcal{P}_{\\xi_1}(y)dy$$&emsp;$\\because y\\leq s$&emsp;$$=\\int_{y=0}^s \\mathcal{P}\\{ \\xi_2\\leq t \\vert \\xi_1=y \\} \\mathcal{P}_{\\xi_1}(y) dy \\\\ = \\int_{y=0}^s \\mathcal{P}\\{ N_{t+y} - N_y \\geq 1 \\vert \\xi_1 = y \\} \\mathcal{P}_{\\xi_1}(y) dy \\ldots(\\star)$$&emsp;ç”± N.H.P.P. çš„ independent increments ç‰¹æ€§çŸ¥é“ $(N_{t+y} - N_y),(N_y-N_0)$ é€™å…©å€‹ r.v.s ç‚º independent. å› æ­¤&emsp;$$\\mathcal{P}\\{N_{t+y} - N_y \\geq 1 | \\xi_1=y\\} = \\mathcal{P}\\{ N_{t+y} - N_y \\geq 1 | N_y-N_0=1 \\} \\\\ = \\mathcal{P}\\{N_{t+y} - N_y \\geq 1 \\}$$&emsp;æ¥çºŒ $(\\star)$&emsp;$$(\\star) = \\int_{y=0}^s \\mathcal{P}\\{ N_{t+y}-N_y \\geq 1 \\} \\cdot \\mathcal{P}_{\\xi_1}(y) dy \\\\ = \\int_{y=0}^s (1 - \\mathcal{P}\\{ N_{t+y}-N_y = 0 \\}) \\cdot \\mathcal{P}_{\\xi_1}(y) dy \\\\ = \\int_{y=0}^s (1 - e^{-\\Lambda(t+y)+\\Lambda(y)}) \\cdot \\lambda(y)e^{-\\Lambda(y)} dy$$&emsp;æ‰€ä»¥å…©å€‹ r.v.s $\\xi_1,\\xi_2$ çš„ joint C.D.F.&emsp;$\\mathcal{F}_{(\\xi_1,\\xi_2)}(s,t) = \\int_{y=0}^s (1 - e^{-\\Lambda(t+y)+\\Lambda(y)}) \\cdot \\lambda(y)e^{-\\Lambda(y)} dy$&emsp;å¾®åˆ†å¯ä»¥è¨ˆç®— P.D.F.&emsp;$$\\mathcal{P}_{(\\xi_1,\\xi_2)}(s,t) = \\frac{\\partial}{\\partial t} \\left( \\frac{\\partial}{\\partial s} \\mathcal{F}_{(\\xi_1,\\xi_2)}(s,t) \\right) \\\\ \\text{replace y by s} = \\frac{\\partial}{\\partial t} \\left( (1 - e^{-\\Lambda(t+s)+\\Lambda(s)}) \\cdot \\lambda(s)e^{-\\Lambda(s)} \\right) \\\\ = \\lambda(t+s) e^{-\\Lambda(t+s)+\\Lambda(s)}\\cdot \\lambda(s) e^{-\\Lambda(s)} \\\\ = \\lambda(t+s) e^{-\\Lambda(t+s)+\\Lambda(s)} \\cdot \\mathcal{P}_{\\xi_1}(s)$$&emsp;æ‰€ä»¥&emsp;$$\\mathcal{P}_{(\\xi_2|\\xi_1)}(t|s) = \\frac{\\mathcal{P}_{(\\xi_1,\\xi_2)}(s,t)}{\\mathcal{P}_{\\xi_1}(s)} \\\\ =\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)}$$&emsp;Q.E.D. å›åˆ°è€ƒæ…®ä»€éº¼æƒ…æ³ä¸‹çš„ N.H.P.P. çš„ $\\xi_1, \\xi_2, â€¦$ æ˜¯ i.i.d. é€™å€‹å•é¡Œæˆ‘å€‘å…ˆå‡è¨­ $\\xi_1, \\xi_2, â€¦$ æ˜¯ i.i.d., i.e. å‡è¨­ N.H.P.P. æ˜¯ renewal process, å‰‡$$\\mathcal{P}_{\\xi_2 | \\xi_1}(t|s)=\\mathcal{P}_{\\xi_2}(t) \\\\ \\because\\text{i.i.d.}=\\mathcal{P}_{\\xi_1}(t), \\forall t,s&gt;0$$å¸¶å…¥æˆ‘å€‘èŠ±å¾ˆå¤šåŠ›æ°£æ¨å°çš„ä¸Šè¿°å…©å€‹çµæœå¾—åˆ°:$\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)} = \\lambda(t)e^{-\\Lambda(t)}$ç„¶å¾Œå°å…©é‚Šéƒ½åšç©åˆ† $\\int_0^T\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)} dt = \\int_0^T \\lambda(t)e^{-\\Lambda(t)} dt \\ldots (\\square)$ $(\\square)$ R.H.S.:$$\\int_0^T \\lambda(t)e^{-\\Lambda(t)} dt = \\int_0^T e^{-\\Lambda(t)}d\\Lambda(t) \\left(= \\int_0^Te^{-y}dy\\right) \\\\ = -e^{-\\Lambda(T)}+e^{-\\Lambda(0)} = 1 -e^{-\\Lambda(T)}$$ $(\\square)$ L.H.S. åŒç†$$\\int_0^T\\lambda(t+s)e^{-\\Lambda(t+s)+\\Lambda(s)} dt = e^{\\Lambda(s)}\\int_0^T e^{-\\Lambda(t+s)} d\\Lambda(t+s) \\\\ \\left(\\text{this as: }e^{\\Lambda(s)}\\int e^{-y}dy\\right) \\\\ = e^{\\Lambda(s)} \\left[\\left. -e^{\\Lambda(t+s)}\\right|_{t=0}^T \\right] = e^{\\Lambda(s)}\\left[-e^{-\\Lambda(T+s)}+e^{-\\Lambda(s)}\\right] = -e^{-\\Lambda(T+s)+\\Lambda(s)}+e^0$$ æ‰€ä»¥ L.H.S = R.H.S.$$\\Rightarrow e^0-e^{-\\Lambda(T+s)+\\Lambda(s)} = 1 - e^{-\\Lambda(T)} \\\\ \\Rightarrow \\Lambda(T+s) - \\Lambda(s) = \\Lambda(T), \\forall T,s&gt;0$$ å› ç‚º $\\Lambda$ is increasing function, ä¸Šå¼çŸ¥é“æ˜¯ linear function, æ‰€ä»¥æœƒå¾—åˆ° $\\Lambda(t)=\\text{const}\\cdot t$è€Œé€™æ­£å¥½è¡¨æ˜äº† N.H.P.P. è®Šæˆäº† homogeneous P.P. äº† çµè«–æ˜¯:N.H.P.P. is a renewal process $\\Longleftrightarrow$ $\\Lambda (t)=\\lambda t$ (i.e. æ­¤æ™‚çš„ N.H.P.P. ä¹Ÿè®Šæˆ homogenous äº†) $(\\Longleftarrow)$ è­‰æ˜å¾ˆå®¹æ˜“, å› ç‚º homogeneous P.P. æ˜¯ renewal process æ›å¥è©±èªª N.H.P.P. is a renewal process $\\Longleftrightarrow$ it is homogenous P.P. Week 2.13: Elements of the queueing theory. M/G/k systems-1æˆ‘å€‘å…ˆå‰è­‰æ˜é [Thm] Poisson process åœ¨æ¥µå°æ™‚é–“æ®µ:&emsp;$$\\left\\{ \\begin{array}{rl} \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1 -\\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t=1\\} = \\lambda h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h), &amp; h\\rightarrow0 \\end{array} \\right.$$ å°æ–¼ N.H.P.P. ä¹Ÿæœ‰é¡ä¼¼çš„çµæœ [Thm] Non-homogeneous Poisson process åœ¨æ¥µå°æ™‚é–“æ®µ:&emsp;$$\\left\\{ \\begin{array}{rl} \\mathcal{P}\\{N_{t+h}-N_t=0\\} = 1 -{\\color{orange}{\\lambda(t)}} h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t=1\\} = {\\color{orange}\\lambda(t)} h + o(h), &amp; h\\rightarrow0 \\\\ \\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h), &amp; h\\rightarrow0 \\end{array} \\right.$$ æ‰€ä»¥é¡ä¼¼çš„å°æ–¼ N.H.P.P. æˆ‘å€‘ä¹Ÿæœ‰å¦ä¸€ç¨®å®šç¾© (é¡ä¼¼ [Poisson Processes Def3]) [Non-homogeneous Poisson Processes Def2]:&emsp;Let $\\Lambda(t)$ be a differentiable increasing function, and $\\Lambda(0)=0$,&emsp;$N_t$ is a N.H.P.P. (Non-Homogeneous Poisson Processes), if&emsp;1. $N_0=0$&emsp;2. $N_t$ has independent increments&emsp;3. æ»¿è¶³:&emsp;&emsp;$\\lim_{h\\rightarrow0} \\frac{\\mathcal{P}\\{N_{t+h} - N_t\\geq2\\}}{\\mathcal{P}\\{N_{t+h}-N_t=1\\}}=0$ (Non-homogeneous) Poisson processes å¾ˆé©åˆç”¨ä¾† modeling queueing processes. æˆ‘å€‘ç”¨ $M,D,G$ ä¾†è¡¨ç¤º distribution ç¨®é¡: $M$: exponential distribution. æ‰€ä»¥å¦‚æœç”¨ä¾†æè¿° arrival processes (memoryless) å°±æœƒè®Šæˆ Poisson processes $D$: deterministic (constant distribution, å³ä¸å— time $t$ çš„å½±éŸ¿? é‚„æ˜¯é€£ value éƒ½æ˜¯ constant?) $G$: general, è¡¨ç¤ºå¯ä»¥æ˜¯ä»»ä½• distribution Queueing processes åŒ…å«äº†ä¸‰å€‹å­—æ¯, e.g. $M/G/k$, åˆ†åˆ¥è¡¨ç¤º Arrival process, Service time, å’Œ Number of servers Arrival process: $\\in\\{M,D,G\\}$ Service time: $\\in\\{M,D,G\\}$. åŒæ™‚ç‚º I.I.D. Number of servers: $\\in\\{1, 2, ..., \\infty\\}$ æˆ‘å€‘ä»¥å­¸ç”Ÿä¸Šæ©Ÿæˆ¿ç”¨é›»è…¦ä¾†èˆ‰ä¾‹, è‹¥ç”¨ queueing process ç‚º $M/G/\\infty$ ä¾†æè¿°çš„è©±Arrival processes (by Poisson process) æè¿°äº†å­¸ç”Ÿä¾†çš„ $N(t)$Service time è¡¨ç¤ºå­¸ç”Ÿæœƒç”¨å¤šä¹…é›»è…¦, ç”¨ $G_Y(t)$ é€™å€‹ distribution æè¿°è€Œé›»è…¦ (server) çš„æ•¸é‡ç‚º $\\infty$, è¡¨ç¤ºå­¸ç”Ÿä¸€åˆ°å°±é¦¬ä¸Šæœ‰ä¸€å°é›»è…¦å¯ä»¥ç”¨, ç„¡éœ€ç­‰å¾… (æ›´è¤‡é›œçš„ queueing process æœƒè€ƒæ…®ç­‰å¾…æ™‚é–“)æ‰€ä»¥ $N(t)$ æ˜¯ä¸€å€‹ Poisson process, è€ƒæ…®ä¸€å€‹ fixed time $\\tau&gt;0$, æˆ‘å€‘æœƒæœ‰å…©å€‹ processes $N_1(t)$ and $N_2(t)$.$N_1(t)$ è¡¨ç¤ºæœ‰å¤šå°‘ é‚„åœ¨è™•ç† çš„äº‹ä»¶ at time $\\tau$ (å³åœ¨æ™‚é–“ $\\tau$ é‚„æœ‰å¤šå°‘å­¸ç”Ÿåœ¨ç”¨é›»è…¦)$N_2(t)$ è¡¨ç¤ºæœ‰å¤šå°‘ å·²è™•ç†å®Œ çš„äº‹ä»¶ at time $\\tau$ (å³åœ¨æ™‚é–“ $\\tau$ å·²æœ‰å¤šå°‘å­¸ç”Ÿç”¨å®Œé›»è…¦) $t$ and $\\tau$ çš„é—œä¿‚åœ¨è¨è«–å€æœ‰äººé€™éº¼å›ç­” è€ƒæ…® $N_1(t+h) - N_1(t)$, é€™å€‹å€¼è¡¨ç¤ºåœ¨é€™ä¸€æ®µæ™‚é–“å…§å…±ä¾†äº†å¤šå°‘äº‹ä»¶ä¸¦ä¸”é‚„åœ¨è™•ç† (å³é€™æ®µæ™‚é–“ä¾†äº†å¤šå°‘å­¸ç”Ÿ, ä¸¦ä¸”é€™äº›å­¸ç”Ÿéƒ½é‚„åœ¨ç”¨é›»è…¦), æ‰€ä»¥:$$\\mathcal{P}\\{ N_1(t+h)-N_1(t)=1 \\} \\\\ = \\mathcal{P}\\{ N(t+h)-N(t)=1 \\} \\cdot \\mathcal{P}\\{ Y&gt;\\tau-t \\} + o(h) \\ldots(\\blacktriangle)$$ $Y$ è¡¨ç¤º service time çš„ random variable, å…¶ distribution ç‚º $G_Y(t)$ â€œé€™æ®µ $h$ æ™‚é–“ä¾†äº†å¤šå°‘å­¸ç”Ÿ, ä¸¦ä¸”é€™äº›å­¸ç”Ÿéƒ½é‚„åœ¨ç”¨é›»è…¦â€ å¯ä»¥è¿‘ä¼¼æ–¼:â€œé€™æ®µ $h$ æ™‚é–“ä¾†äº† $1$ å€‹å­¸ç”Ÿ, ä¸¦ä¸”é€™ $1$ å€‹å­¸ç”Ÿé‚„åœ¨ç”¨é›»è…¦â€ + $o(h)$æ³¨æ„åˆ° $2$ å€‹å­¸ç”Ÿä»¥ä¸Šçš„æƒ…å½¢æˆ‘å€‘ç”¨ $o(h)$ è¡¨ç¤ºå³å¯, é€™æ˜¯å› ç‚º Poisson process çš„ä¸€å€‹æ€§è³ª:$\\mathcal{P}\\{N_{t+h}-N_t\\geq2\\} = o(h),h\\rightarrow0$ æ‰€ä»¥ç”¨ä¸Šé–‹é ­çš„ Theorem,$$(\\blacktriangle) = (\\lambda h + o(h)) \\cdot (1-G_Y(\\tau-t)) + o(h) \\\\ = \\lambda h(1-G_Y(\\tau-t)) + o(h)$$ é‡è¿°ä¸€é, æˆ‘å€‘æœ‰:$\\mathcal{P}\\{ N_1(t+h)-N_1(t)=1 \\} = \\lambda(1-G_Y(\\tau-t))h + o(\\delta)$ä»¤ $\\lambda(1-G_Y(\\tau-t))$ ç­‰æ–¼æŸå€‹ function $\\lambda_1(t)$, å‰‡ä¸Šå¼èˆ‡ N.H.P.P. çš„æ€§è³ªçµæœç›¸åŒ.åŒæ¨£å¯ä»¥å° $\\mathcal{P}\\{ N_1(t+h)-N_1(t)=0 \\}$ å’Œ $\\mathcal{P}\\{ N_1(t+h)-N_1(t)\\geq2 \\}$ æ¨å°å‡ºç›¸åŒçµæœ. æ‰€ä»¥æ ¹æ“š [Non-homogeneous Poisson Processes Def2] æˆ‘å€‘å¾—åˆ° $N_1$ æ˜¯ N.H.P.P. çš„çµè«– $N_1$ æ˜¯ N.H.P.P. ä¸”å…¶ intensity function $\\lambda_1(t)=\\lambda(1-G_Y(\\tau-t))$ æˆ‘å€‘å°æ–¼ $N_1$ çš„æ¨è«–åŒæ¨£ä¹Ÿå¯ä»¥ç”¨åœ¨ $N_2$ ä¸Š, çµæœä¹Ÿæ˜¯ä¸€æ¨£: $N_2$ æ˜¯ N.H.P.P. ä¸”å…¶ intensity function $\\lambda_2(t)=\\lambda \\cdot G_Y(\\tau-t)$ ä¸‹ä¸€ç¯€èª²æœƒè­‰æ˜ $N_1$ and $N_2$ ç‚ºäº’ç›¸ç¨ç«‹çš„ r.v.s Week 2.14: Elements of the queueing theory. M/G/k systems-2æ¬²è­‰ $\\mathcal{P}\\{N_1(t)=n_1, N_2(t)=n_2\\} = \\mathcal{P}\\{N_1(t)=n_1\\}\\mathcal{P}\\{ N_2(t)=n_2\\}$$$\\mathcal{P}\\{N_1(t)=n_1, N_2(t)=n_2\\}\\\\ = \\mathcal{P}\\{ N_1(t)=n_1, N_2(t)=n_2 | N(t)=n_1+n_2 \\} \\cdot \\mathcal{P}\\{N(t)=n_1+n_2\\} \\\\ = \\mathcal{C}_{n_1}^{n_1+n_2}(1-G(\\tau-t))^{n_1}(G(\\tau-t))^{n_2} \\cdot e^{-\\lambda t}\\frac{(\\lambda t)^{n_1+n_2}}{(n_1+n_2)!} \\ldots(=)$$$(=)$ ç‚º Binomial term, æŠŠâ€æˆåŠŸâ€çš„æ©Ÿç‡ç•¶æˆ â€œäº‹ä»¶åœ¨æ™‚é–“ $\\tau$ é‚„åœ¨servedçš„æ©Ÿç‡â€, é€™å€‹æ©Ÿç‡ç”± service time çš„ distribution probability å¯ä»¥çŸ¥é“:$\\mathcal{P}\\{Y&gt;(\\tau-t)\\}=1-\\mathcal{P}\\{Y\\leq(\\tau-t)\\}=1-G(\\tau-t)$ æ‰€ä»¥å¤±æ•—çš„è©±å°±æ˜¯ $G(\\tau-t)$å…±æœ‰ $n$ å€‹äº‹ä»¶, å…±æœ‰ $n_1$ å€‹äº‹ä»¶ $\\in N_1(t)$, and $n_2$ å€‹äº‹ä»¶ $\\in N_2(t)$, æ‰€ä»¥ $\\mathcal{C}_{n_1}^{n_1+n_2}$ $$(=)= \\frac {(\\lambda t(1-G(\\tau-t)))^{n_1}}{n_1!} e^{-\\lambda t(1-G(\\tau-t))} \\cdot \\frac {(\\lambda tG(\\tau-t))^{n_2}}{n_2!} e^{-\\lambda tG(\\tau-t)} \\\\ =\\mathcal{P}\\{N_1(t)=n_1\\}\\mathcal{P}\\{N_2(t)=n_2\\}$$ æˆ‘ä¸çŸ¥é“çš„æ˜¯ç‚ºä½• $\\frac {(\\lambda t(1-G(\\tau-t)))^{n_1}}{n_1!} e^{-\\lambda t(1-G(\\tau-t))} = \\mathcal{P}\\{N_1(t)=n_1\\}$å› ç‚ºåªçŸ¥é“ $N_1$ æ˜¯ N.H.P.P. ä¸”å…¶ intensity function $\\lambda_1(t)=\\lambda(1-G_Y(\\tau-t))$æ‰€ä»¥å¿…é ˆæ±‚å¾— $\\Lambda_1(t)$ æ‰èƒ½ä»£å…¥$\\mathcal{P}\\{N_t=n\\} \\sim Pois(\\Lambda(t))=e^{-\\Lambda(t)}\\frac{\\Lambda(t)^n}{n!}$æ‰€ä»¥çœ‹èµ·ä¾† $\\Lambda_1(t)=\\lambda t(1-G(\\tau-t))?$ ä¸æ‡‚â€¦ Q.E.D. Week 2.15-17: Compound Poisson processeså¯åƒè€ƒä¸€å€‹æ·ºé¡¯æ˜“æ‡‚çš„å®šç¾©: https://gtribello.github.io/mathNET/COMPOUND_POISSON_PROCESS.html [Compound Poisson Processes (C.P.P.) Def]:&emsp;$X_t=\\sum_{k=1}^{N_t} \\xi_k$&emsp;å…¶ä¸­&emsp;- $N_t$ æ˜¯ Poisson process with intensity $\\lambda$&emsp;- $\\xi_1,\\xi_2,â€¦$ are i.i.d.&emsp;- $\\xi_1,\\xi_2,â€¦$ and $N_t$ are independent $X_t$ çš„ distribution æ²’æœ‰ cloded form, ä½†è‹¥æ˜¯æŸäº›ç‰¹å®šçš„ $\\xi$ distribution å¯ä»¥ç®—å‡ºä¾†. æ³¨æ„åˆ°è‹¥ $\\xi_k=1$, å‰‡ $X_t=N_t$, å¯è—‰æ­¤æƒ³åƒä¸€ä¸‹ C.P.P. çš„ç‰©ç†æ„ç¾© å¦‚æœ $\\xi$ (C.P.P.) æ˜¯ non-negative integer values, æˆ‘å€‘ä½¿ç”¨ PGF å¹«åŠ©è¨ˆç®—[Probability Generating Function (PGF) Def]:&emsp;Let $\\xi$ æ˜¯ä¸€å€‹ integer çš„ random variable, with $\\geq 0$ values. å‰‡ PGF å®šç¾©ç‚º:&emsp;$\\varphi_\\xi(u)=\\mathbb{E}[u^\\xi], \\text{ where }|u|&lt;1$ æ ¹æ“šå®šç¾©æˆ‘å€‘å¯ä»¥å¾—åˆ° (expectation å¯«å‡ºä¾†), å¦‚æœ $\\xi_1 \\perp \\xi_2$, å‰‡ $\\varphi_{\\xi_1+\\xi_2}(u)=\\varphi_{\\xi_1}(u)\\varphi_{\\xi_2}(u)$ å¦‚æœ $\\xi$ (C.P.P.) æ˜¯ non-negative (real) values, æˆ‘å€‘ä½¿ç”¨ MGF å¹«åŠ©è¨ˆç®—[Moment Generating Function (MGF) Def]:&emsp;è·Ÿ Laplace transform å¯†åˆ‡ç›¸é—œ.&emsp;$\\mathcal{L}_\\xi(u)=\\mathbb{E}[e^{-u\\xi}], \\text{ where } \\xi\\geq0, u&gt;0$ å…¶å¯¦å°±æ˜¯ $\\mathcal{L}_f(s)=\\int_{x=0}^\\infty e^{-sx}f(x)dx$, å°‡ $f$ ä»¥ $\\xi$ çš„ P.D.F. ä»£å…¥ æ ¹æ“šå®šç¾©æˆ‘å€‘å¯ä»¥å¾—åˆ°, å¦‚æœ $\\xi_1 \\perp \\xi_2$, å‰‡ $\\mathcal{L}_{\\xi_1+\\xi_2}(u)=\\mathcal{L}_{\\xi_1}(u)\\mathcal{L}_{\\xi_2}(u)$. æ³¨æ„åˆ° ${\\xi_1+\\xi_2}$ å…¶ P.D.F. æ˜¯ $\\xi_1,\\xi_2$ çš„ P.D.F.s çš„ convolution. ä¹‹å‰æˆ‘å€‘ä¹Ÿè­‰é Laplace transform é€™å€‹æ€§è³ª å°æ–¼ $\\xi$ (C.P.P.) æ˜¯ general case çš„æƒ…æ³ä¾†èªª, æˆ‘å€‘éœ€å€ŸåŠ© characteristic function å¹«å¿™[Characteristic Function Def]:&emsp;For random variable $\\xi$, å®šç¾© characteristic function $\\Phi:\\mathbb{R}\\rightarrow \\mathbb{C}$ ç‚º&emsp;$\\Phi_\\xi(u) = \\mathbb{E}\\left[ e^{iu\\xi} \\right]$ åŒæ¨£æ ¹æ“šå®šç¾©æˆ‘å€‘å¯ä»¥å¾—åˆ°, å¦‚æœ $\\xi_1 \\perp \\xi_2$, å‰‡ $\\Phi_{\\xi_1+\\xi_2}(u)=\\Phi_{\\xi_1}(u)\\Phi_{\\xi_2}(u)$ [Characteristic Function of Increment of C.P.P.]:&emsp;For $t&gt;s\\geq0$, and $X_t$ is a C.P.P., we have&emsp;$\\Phi_{X_t-X_s}(u)=e^{\\lambda(t-s)(\\Phi_{\\xi_1}(u)-1)}$[Proof]:&emsp;$$\\Phi_{X_t-X_s}(u)=\\mathbb{E}\\left[ e^{iu(X_t-X_s)} \\right] \\\\ =\\sum_{k=0}^\\infty {\\color{orange} {\\mathbb{E}\\left[ \\left. e^{iu(X_t-X_s)} \\right| N_t-N_s=k\\right]} } \\cdot {\\color{green} {\\mathcal{P}\\{N_t-N_s=k\\}} }$$&emsp;æ³¨æ„åˆ°å·²çŸ¥ $N_t-N_s=k$ çš„æƒ…æ³ä¸‹, $X_t-X_s$ æ ¹æ“š C.P.P. çš„å®šç¾©å°±æ˜¯ $\\xi_1+â€¦+\\xi_k$, å†åŠ ä¸Šæˆ‘å€‘çŸ¥é“ $\\xi\\perp N_t$, æ‰€ä»¥æ©˜è‰²éƒ¨åˆ†çš„ condition å°±å¯ä»¥æ‹”æ‰. åŒæ™‚å·²çŸ¥ç¶ è‰²éƒ¨åˆ†ç‚º Poisson Processes.&emsp;å› æ­¤:&emsp;$$= \\sum_{k=0}^\\infty \\mathbb{E}\\left[e^{iu(X_t-X_s)}\\right] \\cdot Pois(\\lambda(t-s)) \\\\ = \\sum_{k=0}^\\infty (\\Phi_{\\xi_1}(u))^k \\cdot e^{-\\lambda(t-s)}\\frac{(\\lambda(t-s))^k}{k!} \\\\ =e^{-\\lambda(t-s)}\\sum_{k=0}^\\infty\\frac{(\\Phi_{\\xi_1}(u)\\lambda(t-s))^k}{k!} \\\\ = e^{-\\lambda(t-s)}e^{\\Phi_{\\xi_1}(u)\\lambda(t-s)} =e^{\\lambda(t-s)(\\Phi_{\\xi_1}(u)-1)}$$&emsp;Q.E.D. æ­¤å®šç†æè¿°äº† increment of C.P.P. çš„ characteristic function, å…¶å…·æœ‰ closed form solutionæ‰€ä»¥ characteristic function of $X_t$ is$\\Phi_{X_t}(u)=e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)}$ èª²ç¨‹è€å¸«èªªé€™å€‹ Theorem å¾ˆé‡è¦, å¯ä»¥æ ¹æ“šå®ƒæ¨å°å‡ºå¾ˆå¤š Corollaries [Expectation and Variance of C.P.P.]:&emsp;å°æ–¼æ­¤ç¯€å®šç¾©çš„ C.P.P. æˆ‘å€‘æœ‰&emsp;$$\\mathbb{E}[X_t]=\\lambda t\\mathbb{E}[\\xi_1] \\\\ Var[X_t]=\\lambda t\\mathbb{E}[\\xi_1^2]$$ åŸä¾†çš„ Poisson distribution $Pois(\\lambda t)$ çš„ mean and variance ç‚º $\\lambda t$, æ‰€ä»¥ C.P.P. ç­‰æ–¼å¤šä¹˜ä¸Š $\\xi_1$ çš„ moments [Proof]:&emsp;èª²ç¨‹è­‰ expectation, è€Œ variance å¯ç”¨åŒæ¨£æµç¨‹è­‰æ˜&emsp;$\\mathbb{E}[\\xi^r]&lt;\\infty\\Rightarrow\\Phi_\\xi(u)$ is r-times å¯å¾® at 0&emsp;å› ç‚º derivative and expectation éƒ½æ˜¯ç·šæ€§çš„, æ‰€ä»¥æˆ‘å€‘æœ‰&emsp;$\\Phi^{(1)}_\\xi(u)=\\frac{d}{du}\\mathbb{E}[e^{iu\\xi}]=\\mathbb{E}[(i\\xi)e^{iu\\xi}]$&emsp;$\\Phi^{(2)}_\\xi(u)=\\frac{d}{du}\\frac{d}{du}\\mathbb{E}[e^{iu\\xi}]=\\mathbb{E}[(i\\xi)^2 e^{iu\\xi}]$&emsp;$â€¦$&emsp;$\\Phi^{(r)}_\\xi(u)=\\mathbb{E}[(i\\xi)^r e^{iu\\xi}]$&emsp;$\\therefore \\Phi_\\xi^{(r)}(0)=i^r\\cdot\\mathbb{E}[\\xi^r]$&emsp;ç”±å‰é¢çš„ Theorem çŸ¥é“ $\\Phi_{X_t}(u)=e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)}$, æ‰€ä»¥&emsp;$$\\Phi_{X_t}^{(1)}(u)=\\frac{d}{du}\\Phi_{X_t}(u) =\\frac{d}{du}e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)} \\\\ =\\lambda t \\Phi_{\\xi_1}^{(1)}(u)e^{\\lambda t(\\Phi_{\\xi_1}(u)-1)} = \\lambda t \\Phi_{\\xi_1}^{(1)}(u)\\Phi_{X_t}(u)$$&emsp;è¨ˆç®— $\\mathbb{E}[X_t]$:&emsp;$$\\mathbb{E}[X_t]=\\frac{\\Phi_{X_t}^{(1)}(0)}{i} = \\frac{\\lambda t \\Phi_{ \\xi_1}^{(1)}(0) \\overbrace{\\Phi_{X_t}(0)}^{=1} } {i}$$&emsp;è€Œæ ¹æ“š characteristic function çš„ç‰¹æ€§&emsp;$\\frac{\\Phi_{\\xi_1}^{(1)}(0)}{i} = \\mathbb{E}[\\xi_1]$&emsp;æ‰€ä»¥ $\\mathbb{E}[X_t]=\\lambda t\\mathbb{E}[\\xi_1]$&emsp;Q.E.D. Applications of the Poisson Processes and Related ModelsApplications of the Poisson Processes and Related Models.pdf","tags":[{"name":"Coursera","slug":"Coursera","permalink":"http://yoursite.com/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"http://yoursite.com/tags/Stochastic-Processes/"},{"name":"Poisson Process","slug":"Poisson-Process","permalink":"http://yoursite.com/tags/Poisson-Process/"}]},{"title":"Stochastic Processes Week 1 Introduction & Renewal processes","date":"2021-12-11T12:56:53.000Z","path":"2021/12/11/Stochastic-Processes-Week-1-Introduction-Renewal-processes/","text":"Coursera Stochastic Processes èª²ç¨‹ç­†è¨˜, å…±ä¹ç¯‡: Week 0: ä¸€äº›é å‚™çŸ¥è­˜ Week 1: Introduction &amp; Renewal processes (æœ¬æ–‡) Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; ItÃ´ formula Week 8: LÃ©vy processes Week 1.2: Difference between various fields of stochasticsæ•¸å­¸ä¸Šçš„ stochastics è·Ÿä¸‰å€‹ä¸»è¦å­¸ç§‘æœ‰é—œ: Probability theory Mathematical statistics stochastic processes ç”¨ä¸€å€‹æ± å­è£¡é¢çš„é­šä¾†ç•¶ä¾‹å­, å‡è¨­æˆ‘å€‘è¦åˆ†ææŸå€‹æ™‚é–“é»æ± å­è£¡æœ‰å¤šå°‘é­š, i.e. $N$ æ¢é­šProbability theory å°±æ˜¯æ‰¾å‡ºé€™å€‹ $N$ çš„æ©Ÿç‡åˆ†å¸ƒ, ç„¶å¾Œå¯ä»¥åˆ†æå…¶ mean, variance â€¦è€Œ mathematical statistics æ˜¯è—‰ç”±ä¸€äº›çµ±è¨ˆå¯¦é©—, å»ä¼°è¨ˆå‡º $N$ èˆ‰ä¾‹ä¾†èªª, æŠ“ $M$ æ¢é­šåšè¨˜è™Ÿå¾Œæ”¾å›æ± å­. ç„¶å¾Œä¸€æ¬¡å¯¦é©—ç‚ºæŠ“ $n$ æ¢é­š, è‹¥ç™¼ç¾æœ‰ $m$ æ¢åšéè¨˜è™Ÿ, å‰‡é€™å€‹æ©Ÿç‡æˆ‘å€‘å¯ä»¥ç®—å‡ºä¾†:$\\mathcal{P}\\{\\#\\text{marked}=m\\}=(C_M^m\\cdot C_{N-M}^{n-m})/C_N^n$ æˆ‘å€‘é‡è¤‡é€™å€‹å¯¦é©— $q$ æ¬¡, å¾—åˆ°çš„åšéè¨˜è™Ÿçš„é­šçš„æ¬¡æ•¸ç‚º ${m_1,m_2,â€¦,m_q}$, å› æ­¤å¯ä»¥ç®—å‡º log-likelihood:$L(N)=\\sum_{k=1}^q \\log\\mathcal{P}\\{\\#\\text{marked}=m_k\\}$ æ‰€ä»¥æ±‚è§£ $\\arg\\max_N L(N)$å°æ–¼ stochastic processes æˆ‘å€‘ä¹Ÿå¯ä»¥å•åŒæ¨£çš„å•é¡Œ: $N$ æ˜¯å¤šå°‘? ä¸éæ­¤æ™‚æœƒå¤šè€ƒæ…® $N$ éš¨è‘—æ™‚é–“è®ŠåŒ– Week 1.3: Probability spaceå»ºè­°å…ˆé–±è®€[æ¸¬åº¦è«–] Sigma Algebra èˆ‡ Measurable function ç°¡ä»‹: https://ch-hsieh.blogspot.com/2010/04/measurable-function.html[æ©Ÿç‡è«–] æ·ºè«‡æ©Ÿç‡å…¬ç† èˆ‡ åŸºæœ¬æ€§è³ª: https://ch-hsieh.blogspot.com/2013/12/blog-post_7.html èª²ç¨‹ä½¿ç”¨å…©å€‹éš¨æ©Ÿå¯¦é©— (å¦‚åŒä¸Šé¢çš„åƒè€ƒé€£çµ): å¾é–‰å€é–“ $[0,1]$ ä¹‹ä¸­ ä»»é¸ä¸€å€‹æ•¸å­— åš $n$ æ¬¡çš„ä¸ŸéŠ…æ¿å¯¦é©— æˆ‘å€‘å¼•ç”¨åƒè€ƒé€£çµçš„å®šç¾©$\\mathcal{F}$ æ˜¯ä¸€å€‹ collection of subsets of $\\Omega$, ä¹Ÿå°±æ˜¯èªªæ¯ä¸€å€‹ element éƒ½æ˜¯ä¸€å€‹ $\\Omega$ çš„ subset. é™¤æ­¤ä¹‹å¤–, é‚„å¿…é ˆæ˜¯ $\\sigma$-algebra. $\\sigma$-algebra å’Œ topology å®šç¾©å¯åƒè€ƒ https://www.themathcitadel.com/topologies-and-sigma-algebras/ æ‰€ä»¥ä¸€å€‹ â€œäº‹ä»¶â€ $A$ æ˜¯ä¸€å€‹ subset of $\\Omega$ (i.e. $A\\subseteq\\Omega$), ä¹Ÿæ˜¯ä¸€å€‹ element of $\\mathcal{F}$, (i.e. $A\\in\\mathcal{F}$). ğŸ’¡ æ³¨æ„ $A$ æ˜¯ subset of $\\Omega$, é‚„ä¸å¤ . $A$ é‚„å¿…é ˆå¾æ˜¯ $\\sigma$-algebra çš„ $\\mathcal{F}$ è£¡é¢æŒ‘. æœ€å¾Œ $\\mathcal{P}:\\mathcal{F} \\rightarrow [0,1]$ æ­¤å‡½æ•¸å¿…é ˆæ»¿è¶³ä»¥ä¸‹å…¬ç†å°æ¯” measure çš„å®šç¾©, ç›¸ç•¶æ–¼å¤šå‡ºäº†ä¸€æ¢ $P(\\Omega)=1$, æ‰€ä»¥ probability measure æ˜¯ä¸€å€‹ç‰¹æ®Šçš„ measureå†å°æ¯” measure space çš„å®šç¾©, å·®åˆ¥åªåœ¨æ–¼ probability space ç”¨çš„ measure ç‚º probability measure Week 1.4: Definition of a stochastic function. Types of stochastic functions.é¦–å…ˆ random variables å…¶å¯¦æ˜¯ä¸€å€‹ measurable function $\\xi:\\Omega\\rightarrow\\mathbb{R}$, è¦ææ¸…æ¥šä»€éº¼æ˜¯ measurable function ä¹‹å‰, æˆ‘å€‘å…ˆè¦äº†è§£ Borel $\\sigma$-algebra, Measurable Space èˆ‡ Measurable sets ä¸€æ¨£, ä¸»è¦åƒè€ƒ [æ¸¬åº¦è«–] Sigma Algebra èˆ‡ Measurable function ç°¡ä»‹ $\\sigma(\\mathcal{E})$ è¡¨ç¤ºåŒ…å« $\\mathcal{E}$ çš„æœ€å° $\\sigma$-algebra, ä¸”ä¸€å®šå­˜åœ¨ (è­‰æ˜è«‹åƒè€ƒè©²æ–‡ç« )ä»¤ $X:=\\mathbb{R}$, å‰‡æˆ‘å€‘å¯ä»¥å–æ‰€æœ‰ open sets ç”¢ç”Ÿ Borel $\\sigma$-algebra, è¨˜åš $\\mathcal{B}(\\mathbb{R})$ $\\mathcal{B}(\\mathbb{R})$ å¯ä»¥æƒ³æˆå¯¦æ•¸è»¸ä¸ŠåŒ…å«æ‰€æœ‰ open sets çš„æœ€å° $\\sigma$-algebra, ç”± $\\sigma$-algebra å®šç¾©çŸ¥ä¹ŸåŒ…å« closed sets $[a,b]$, ${a}$, $(a,b]$, $[a,b)$, ä»¥åŠä¸Šè¿°é€™äº›çš„ countable union (complement) å¯ç¨± $\\sigma$-algebra ä¸­çš„å…ƒç´ ç‚º measurable setæ¥è‘—å®šç¾©å¯æ¸¬å‡½æ•¸:å°æ–¼ $f$ ä¾†èªª, å…¶ domain ($X$) and image ($Y$) spaces éƒ½é…å‚™äº†å°æ‡‰çš„ $\\sigma$-algebra $\\mathcal{A},\\mathcal{B}$å›åˆ° probability space, $(\\Omega, \\mathcal{F}, \\mathcal{P})$, æˆ‘å€‘çŸ¥é“ $\\mathcal{F}$ æ˜¯å®šç¾©åœ¨ sample space $\\Omega$ çš„ $\\sigma$-algebra, i.e. $(\\Omega,\\mathcal{F})$ æ˜¯ measurable space.è€Œ random variable $Z$ å…¶å¯¦æ˜¯å®šç¾©ç‚ºç”± $\\Omega$ æ˜ å°„åˆ° $\\mathbb{R}$ çš„ measurable function. $\\mathbb{R}$ é…å‚™ $\\mathcal{B}(\\mathbb{R})$.èˆ‰ä¸€å€‹ Khan Academy æ·ºé¡¯çš„ä¾‹å­, $X,Y$ ç‚ºå…©å€‹ r.v.s åˆ†åˆ¥æŠŠ outcomes å°æ‡‰åˆ° $\\mathbb{R}$ $Y$ çš„ outcomes æ˜¯$\\{(a_1,...,a_7):a_i\\in \\{\\text{head,tail} \\} \\}$å…± $2^7$ ç¨®å¯èƒ½Mapping åˆ° $\\mathbb{R}$ å¾Œæˆ‘å€‘å°±å¯ä»¥ç®—å°æ‡‰çš„æ©Ÿç‡, ä¾‹å¦‚$\\mathcal{P}(Y\\leq30)$ or $\\mathcal{P}(Y\\text{ is even})$è€Œéœ€è¦ r.v. æ˜¯ measurable function çš„åŸå› å¯ä»¥å¾é€™çœ‹å‡ºä¾†, å› ç‚ºæˆ‘å€‘è¦çŸ¥é“ pre-image:$\\{\\omega:Y(\\omega)\\leq30\\}$ä¹Ÿå°±æ˜¯æ»¿è¶³é€™æ¢ä»¶çš„ outcomes é›†åˆ, å¿…é ˆè¦ $\\in\\mathcal{F}$. æ‰€ä»¥å®ƒæ‰æœƒæ˜¯å€‹ â€œäº‹ä»¶â€é€™æ˜¯å› ç‚ºæˆ‘å€‘çš„ probability measure $\\mathcal{P}:\\mathcal{F}\\rightarrow [0,1]$, æ˜¯å®šç¾©åœ¨ $\\mathcal{F}$ (äº‹ä»¶çš„ $\\sigma$-algebra) ä¸Š Week 1.5: Trajectories and finite-dimensional distributions[Def]: Stochastic process &emsp;Stochastic process æ˜¯ä¸€å€‹ mapping $X:T\\times\\Omega\\rightarrow\\mathbb{R}$, è€Œé€šå¸¸ $T=\\mathbb{R}^+$, ä¸”æ»¿è¶³: &emsp;$\\forall t \\in T$, we have $X_t=X(t,\\cdot)$ is a random variable on probability space $(\\Omega,\\mathcal{F},\\mathcal{P})$ [Def]: Trajectory (sample path, or path) of a stochastic process &emsp;å°ä¸€å€‹ stochastic process $X$ ä¾†èªª, fixed $\\omega\\in\\Omega$, æˆ‘å€‘å¾—åˆ° $X(\\cdot,\\omega)$ ç‚º $T$ çš„å‡½æ•¸, é€™å°±æ˜¯ trajectory [Def]: Finite dimensional distributions &emsp;å°ä¸€å€‹ stochastic process $X$ ä¾†èªª, æˆ‘å€‘æ ¹æ“šæ™‚é–“å¯ä»¥æ‹¿åˆ° $n$ å€‹ random variables: &emsp;$(X_{t_1}, X_{t_2}, ..., X_{t_n})$, where $t_1, t_2,...,t_n \\in \\mathbb{R}$ åœ¨ probability theory è£¡é¢éƒ½æ˜¯å°‡é€™ $n$ å€‹ r.v.s è¦–ç‚ºç¨ç«‹, ä½†åœ¨ stochastic processes è£¡é¢ä¸èƒ½. é€™æ˜¯å¾ˆå¤§çš„ä¸åŒ. æ‰€ä»¥å°±ç®—æ˜¯ finite dimensional distribution, åœ¨ stochastic processes ä¹Ÿæ˜¯å¾ˆæŒ‘æˆ°çš„. video çš„è©¦é¡Œ: Week 1.6: Renewal process. Counting processå¯åƒè€ƒè©³ç´°è§£èªª: https://www.randomservices.org/random/renewal/Introduction.htmlç¬¬ä¸€å€‹äº‹ä»¶ç™¼ç”Ÿçš„æ™‚é–“ç‚º $T_1$, ç¬¬äºŒå€‹äº‹ä»¶ç™¼ç”Ÿçš„æ™‚é–“ç‚º $T_2$, â€¦æ¯ä¸€å€‹äº‹ä»¶è¦éš”å¤šä¹…ç™¼ç”Ÿéƒ½æ˜¯å¾ä¸€å€‹ random variable $X_i$ æ±ºå®šçš„å› æ­¤æˆ‘å€‘æœƒæœ‰ä¸€å€‹ sequence of interarrival times, $X=(X_1,X_2,â€¦)$æ‰€ä»¥ sequence of arrival times å°±æœƒæ˜¯ $T = (T_1, T_2, â€¦)$, å…¶ä¸­$T_n=\\sum_{i=1}^nX_i$ , $n\\in\\mathbb{N}$å®ƒå€‘çš„é—œä¿‚ç”¨åœ–ä¾†çœ‹å¦‚ä¸‹:æœ€å¾Œæˆ‘å€‘å¯ä»¥å®šç¾©ä¸€å€‹ random variable $N_t$ è¡¨ç¤ºåˆ°æ™‚é–“ $t$ ç‚ºæ­¢æœ‰å¤šå°‘å€‹â€äº‹ä»¶â€åˆ°é”äº†:$N_t=\\sum_{n=1}^\\infty \\mathbf{1}(T_n\\leq t)$, $t\\in[0,\\infty)$å…¶ä¸­ $\\mathbf{1}(\\cdot)$ è¡¨ç¤º indicator function.åˆæˆ–è€…å¯ä»¥ç”¨èª²ç¨‹ä¸Šçš„å®šç¾©:$N_t=\\arg\\max_n\\{T_n\\leq t\\}$æ‰€ä»¥å¯ä»¥å®šç¾© counting process ç‚ºä¸€å€‹ random process $N=(N_t:t\\geq 0)$æˆ‘å€‘å¯ä»¥å°‡ $N$ é€™å€‹ random processes çš„ trajectory (path) ç•«å‡ºä¾†.é€™å€‹æ„æ€å°±æ˜¯ fixed ä¸€å€‹ sample space çš„å€¼, ä¾‹å¦‚å›ºå®š $Xâ€™=(X_1=0.5,X_2=0.11,â€¦)$, ç„¶å¾Œå°æ¯å€‹æ™‚é–“é»çš„ $N_t$ çš„å€¼éš¨è‘—æ™‚é–“ç•«å‡ºä¾†. æˆ‘å€‘æœƒç™¼ç¾æ˜¯å¦‚ä¸‹çš„ increasing step function:$T_n\\leq t$ æ„æ€å°±æ˜¯ $n$ å€‹äº‹ä»¶ç™¼ç”Ÿçš„æ™‚é–“æ¯” $t$ å°. ç­‰åŒæ–¼åˆ°æ™‚é–“ $t$ ç‚ºæ­¢è‡³å°‘æœ‰ $n$ å€‹äº‹ä»¶å·²ç¶“ç™¼ç”Ÿ, i.e. $N_t\\geq n$ [Properties]: counting variables $N_t$ èˆ‡ arrival times $T_n$ çš„é—œè¯å¦‚ä¸‹:&emsp;1. ${N_t\\geq n}={T_n\\leq t}$ or ${N_t\\leq n}={T_n\\geq t}$&emsp;2. $\\{N_t=n\\}=\\{T_n\\leq t\\} \\cap \\{T_{n+1}&gt;t\\}$ Week 1.7: Convolutionæˆ‘å€‘æœ‰å…©å€‹äº’ç‚ºç¨ç«‹çš„ r.v.s $X\\bot Y$, ä¸”å·²çŸ¥ $X\\sim F_X$, $Y\\sim F_Y$ (in c.d.f.) æˆ–æ˜¯å¯«æˆ $X\\sim P_X$, $Y\\sim P_Y$ (in p.d.f.). $F_X$ and $F_Y$ æ˜¯ cumulated distribution function of $X$ and $Y$$P_X$ and $P_Y$ æ˜¯ probability density function of $X$ and $Y$$F_X(x)=P_X(X&lt;x)$ å‰‡ convolution of two independent random variables è¨˜åš: $F_X\\ast F_Y$ (convolution in terms of distribution function): $F_{X+Y}(x)=\\int_\\mathbb{R} F_X(x-y)dF_Y(y)$ æˆ– $P_X \\ast P_Y$ (convolution in terms of density function): $P_{X+Y}(x)=\\int_\\mathbb{R} P_X(x-y)P_y(y)dy$ ğŸ’¡ Convolution åŒæ¨£éƒ½æ˜¯ç”¨ $\\ast$ è¡¨ç¤º, ä½†æ ¹æ“šæ˜¯ c.d.f. or p.d.f. æœƒæœ‰ä¸åŒå®šç¾©, ç„¶è€Œå…©è€…ç‚ºç­‰åƒ¹ æŠŠ convolution æ“´å±•åˆ° $n$ å€‹ i.i.d. r.v.s $\\{X_1,X_2,...,X_n\\}$, å…¶ä¸­ $X_i \\sim F$, å‰‡:$S_n=X_1+...+X_n\\sim {\\color{orange}{F^{n\\ast}=\\underbrace{F\\ast ...\\ast F}_\\text{n times}}}$ æ³¨æ„åˆ°é€™è£¡çš„ convolution, $\\ast$, is in terms of distribution function æœ‰å¹¾å€‹ç‰¹æ€§: $F^{n\\ast}(x)\\leq F^n(x)$, if $F(0)=0$é€™æ˜¯å› ç‚º$$\\{X_1 + ... + X_n \\leq x\\}\\subseteq\\{X_1\\leq x, ..., X_n\\leq x\\} \\\\ \\therefore P\\{X_1 + ... + X_n \\leq x\\}\\leq \\prod_{i=1}^n P\\{X_i\\leq x\\} \\\\ =F^{n\\ast}(x)\\leq \\prod_{i=1}^n F(x)=F^n(x)$$ $F^{n\\ast}(x)\\geq F^{(n+1)\\ast}(x)$é€™æ˜¯å› ç‚º$\\{X_1 + ... + X_n \\leq x\\}\\supseteq\\{X_1 + ... + X_n + X_{n+1} \\leq x\\}$ [Thm] Expectation of counting process equals to renewal function $\\mathcal{U}(t)$:&emsp;è€ƒæ…® renewal process: $T_n=T_{n-1}+X_n$, ä¸” $X_1,X_2,â€¦$ are i.i.d. r.v.s with distribution function $F$. $\\mathcal{U}(t):=\\sum_{n=1}^\\infty F^{n\\ast}(t) &lt; \\infty$ èª²ç¨‹ç•¥éè­‰æ˜. æ­¤å®šç†å‘Šè¨´æˆ‘å€‘è©²åºåˆ—æ”¶æ–‚. $F^{n\\ast}(t)$ çš„æ„æ€æ˜¯ $n$ å€‹ i.i.d. r.v.s ç›¸åŠ çš„ CDF, i.e. $P(X_1+â€¦+X_n\\leq t)=P(T_n\\leq t)$, è€Œåœ¨ renewal process æŒ‡çš„å°±æ˜¯ arrival time $P(T_n\\leq t)$, i.e. åˆ°æ™‚é–“ $t$ ç‚ºæ­¢è‡³å°‘æœ‰ $n$ å€‹äº‹ä»¶å·²ç¶“ç™¼ç”Ÿçš„æ©Ÿç‡. ç„¶å¾Œ $\\sum_{n=1}^\\infty$ æœ‰é‚£ç¨®æŠŠæ‰€æœ‰å¯èƒ½çš„æƒ…æ³éƒ½è€ƒæ…®é€²å»çš„æ„æ€, å› æ­¤ $\\mathcal{U}(t)$ å¯èƒ½æœƒè·Ÿåˆ°æ™‚é–“ $t$ ç‚ºæ­¢ç™¼ç”Ÿâ€äº‹ä»¶â€çš„æ¬¡æ•¸çš„æœŸæœ›å€¼ ($\\mathbb{E} N_t$) æœ‰é—œ. è€Œäº‹å¯¦ä¸Šå°±æ˜¯. $\\mathcal{U}(t)$ ç¨± renewal function $\\mathbb{E} N_t = \\mathcal{U}(t)$&emsp;$$\\mathbb{E}N_t = \\mathbb{E}\\left[ \\#\\{n:T_n\\leq t\\} \\right] =\\mathbb{E}\\left[ \\sum_{n=1}^\\infty \\mathbf{1}(T_n\\leq t) \\right] \\\\ =\\sum_{n=1}^\\infty \\mathbb{E}\\left[ \\mathbf{1}(T_n\\leq t) \\right] =\\sum_{n=1}^\\infty P(T_n\\leq t) =\\sum_{n=1}^\\infty F^{n\\ast}(t)$$ é›–ç„¶ $\\mathbb{E} N_t = \\mathcal{U}(t) =\\sum_{n=1}^\\infty F^{n\\ast}(t)$ æˆ‘å€‘å¯ä»¥æ˜ç¢ºå¯«å‡ºä¾†, ä½†å°æ–¼ $F$ æ˜¯æ¯”è¼ƒ general form çš„è©±, å¹¾ä¹å¾ˆé›£ç®—å‡ºä¾†. å› ç‚ºè¦ç®— convolution, åˆè¦æ±‚ sequence çš„ limit. èª²ç¨‹è©¦é¡Œ: é™„ä¸Š exponential distributino å®šç¾© Week 1.8: Laplace transform. Calculation of an expectation of a counting process-1[Def]: Laplace transform&emsp;Given $f:\\mathbb{R}^+\\rightarrow\\mathbb{R}$, Laplace transform å®šç¾©ç‚ºå¦‚ä¸‹ç©åˆ†:&emsp;$\\mathcal{L}_f(s)=\\int_{x=0}^\\infty e^{-sx}f(x)dx$ æœ‰å¹¾å€‹ä¸»è¦çš„ properties: ä»¤ $f$ æ˜¯ p.d.f. of some random variable $\\xi$å‰‡ $\\mathcal{L}_f(s)=\\mathbb{E}\\left[ e^{-s\\xi} \\right]$ çµ¦å®šä»»å…©å€‹ functions (ä¸ä¸€å®šè¦æ˜¯ p.d.f. or c.d.f.) $f_1$, $f_2$, å‰‡ $\\mathcal{L}_{f_1\\ast f_2}(s)=\\mathcal{L}_{f_1}(s)\\cdot\\mathcal{L}_{f_2}(s)$å…¶ä¸­ $\\ast$ æ˜¯ convolution in terms of density[Proof]:Let $f(x)=f_1(x)*f_2(x)$ å‰‡ $$\\mathcal{L}_f(s)=\\int_{x=0}^\\infty e^{-sx}\\left(\\int_{y=0}^\\infty f_1(x-y)f_2(y)dy\\right)dx \\\\ =\\int_{y=0}^\\infty\\left(\\int_{x=0}^\\infty e^{-sx}f_1(x-y)dx\\right) \\\\ =\\int_{y=0}^\\infty\\left( e^{-sy}\\int_{x-y=y}^\\infty e^{-s(x-y)}f_1(x-y)d(x-y) \\right) f_2(y)dy \\\\ =\\mathcal{L}_{f_1}(s)\\cdot\\int_{y=0}^\\infty e^{-sy} f_2(y)dy = \\mathcal{L}_{f_1}(s)\\cdot\\mathcal{L}_{f_2}(s)$$ ä»¤ $F$ æ˜¯ c.d.f. ä¸” $F(0)=0$, $p=Fâ€™$ is p.d.f., å‰‡: $\\mathcal{L}_F(s)=\\frac{\\mathcal{L}_p(s)}{s}\\\\$ [Proof]: ä½¿ç”¨åˆ†éƒ¨ç©åˆ†, integration by part $$l.h.s.=-\\int_{\\mathbb{R}^+}F(x)\\frac{d(e^{-sx})}{s} = -\\left[F(x)e^{-sx}/s\\right]|_{x=0}^\\infty+\\frac{1}{s}\\int_{\\mathbb{R}^+}e^{-sx}dF(x) \\\\ = 0+\\frac{1}{s}\\int_{\\mathbb{R}^+}p(x)e^{-sx}dx = r.h.s.$$ [Example 1]: ä»¤ $f(x) = x^n$, æ±‚ $\\mathcal{L}_f(s)$&emsp;[sol]: ä¹Ÿæ˜¯ä½¿ç”¨åˆ†éƒ¨ç©åˆ†, integration by part$$\\mathcal{L}_{x^n}(s) = \\int_{\\mathbb{R}^+}x^n e^{-sx}dx =-\\int_{\\mathbb{R}^+}x^n\\frac{d(e^{-sx})}{s} \\\\ = \\frac{n}{s} \\int_{\\mathbb{R}^+}x^{n-1} e^{-sx}dx=...\\\\ =\\frac{n}{s}\\cdot\\frac{n-1}{s}\\cdot...\\cdot\\frac{1}{s}\\int_{\\mathbb{R}^+}e^{-sx}dx \\\\ =\\frac{n!}{s^n}\\cdot\\left[ -\\left.\\frac{1}{s}e^{-sx} \\right |_0^\\infty \\right] = \\frac{n!}{s^{n+1}}$$ [Example 2]: ä»¤ $f(x) = e^{ax}$, æ±‚ $\\mathcal{L}_f(s)$&emsp;[sol]: ç­”æ¡ˆç‚º $\\mathcal{L}_{e^{ax}}(s)=\\frac{1}{s-a}$, if $a&lt;s$&emsp;ç•¥â€¦ Week 1.9: Laplace transform. Calculation of an expectation of a counting process-2ç¾åœ¨æˆ‘å€‘è¦ä¾†è¨ˆç®— $\\mathbb{E}N_t$, å›é¡§ä¸€ä¸‹ $N_t$ æ˜¯ä¸€å€‹ r.v. è¡¨ç¤ºåˆ°æ™‚é–“ $t$ ç‚ºæ­¢æœ‰å¤šå°‘å€‹äº‹ä»¶åˆ°é”äº†è€Œæˆ‘å€‘ä¹Ÿè­‰æ˜äº† $\\mathbb{E} N_t = \\mathcal{U}(t) =\\sum_{n=1}^\\infty F^{n\\ast}(t)$, ç¾åœ¨æˆ‘å€‘åœ¨ä»”ç´°åˆ†æä¸€ä¸‹$$\\mathbb{E}N_t=\\mathcal{U}(t)=\\sum_{n=1}^\\infty F^{n\\ast}(t) \\\\ = F(t) + \\left( \\sum_{n=1}^\\infty F^{n\\ast} \\right) \\ast F(t) \\\\ = F(t) + \\mathcal{U}(t)\\ast F(t)$$å› æ­¤æˆ‘å€‘å¾—åˆ°: $\\mathcal{U}=F+\\mathcal{U}\\ast F$, å…¶ä¸­ convolution, $\\ast$, is in terms of distribution functionç„¶å¾Œæˆ‘å€‘å°ç­‰è™Ÿå…©é‚Šå¥—ç”¨ Laplace transfrom, ä½†æ˜¯æˆ‘å€‘æ³¨æ„åˆ° Laplace transfrom å¥—ç”¨çš„ convolution å¿…é ˆæ˜¯ density function, å› æ­¤è¦è½‰æ›å…¶å¯¦æˆ‘å€‘å¾å®šç¾©å¯ä»¥çŸ¥é“ $\\mathcal{U}\\ast_{cdf}F=\\mathcal{U}\\ast_{pdf}p$ å·²çŸ¥ $p=Fâ€™$$\\int_{\\mathbb{R}}\\mathcal{U}(x-y)dF(y) = \\int_{\\mathbb{R}}\\mathcal{U}(x-y)p(y)dy$ æ‰€ä»¥$\\mathcal{U}=F+\\mathcal{U}\\ast_{cdf} F = F+\\mathcal{U}\\ast_{pdf} p$, ç„¶å¾Œå°±å¯ä»¥å¥—ç”¨ Laplace transfrom, ä¸¦åˆ©ç”¨ä¸Šé¢æåˆ°çš„ properties 2 &amp; 3$$\\mathcal{L}_\\mathcal{U}(s) = \\mathcal{L}_F(s) + \\mathcal{L}_\\mathcal{U}(s) \\cdot \\mathcal{L}_p(s) \\\\ =\\frac{\\mathcal{L}_p(s)}{s} + \\mathcal{L}_\\mathcal{U}(s) \\cdot \\mathcal{L}_p(s)$$å› æ­¤$\\mathcal{L}_\\mathcal{U}(s) = \\frac{\\mathcal{L}_p(s)}{s(1-\\mathcal{L}_p(s))} \\ldots (\\star)$æ‰€ä»¥é›–ç„¶ç„¡æ³•ç›´æ¥è¨ˆç®—å‡º $\\mathbb{E} N_t = \\mathcal{U}(t)$, ä½†æˆ‘å€‘å¯ä»¥è¿‚è¿´åœ°é€éä»¥ä¸‹ä¸‰å€‹æ­¥é©Ÿä¾†ä¼°è¨ˆ: å¾ $F$ ç®—å‡º $\\mathcal{L}_p$ åˆ©ç”¨ $(\\star)$ å¾ $\\mathcal{L}_p$ ç®—å‡º $\\mathcal{L}_\\mathcal{U}$ åæ¨ä»€éº¼æ¨£çš„ $\\mathcal{U}$ æœƒå¾—åˆ° $\\mathcal{L}_\\mathcal{U}$, é€™ä¸€æ­¥æ˜¯æœ€å›°é›£çš„ ä¸‹ä¸€æ®µèª²ç¨‹å°‡æœƒçµ¦å‡ºä¸€å€‹å¦‚ä½•ä½¿ç”¨ä¸Šé¢ä¸‰æ­¥é©Ÿçš„ç¯„ä¾‹ Week 1.10: Laplace transform. Calculation of an expectation of a counting process-3[Example]: å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹ renewal process, $S_n=S_{n-1}+\\xi_n$&emsp;å…¶ä¸­ $\\xi_1, \\xi_2, ... \\sim p(x)=\\frac{e^{-x}}{2}+e^{-2x}$&emsp;æˆ‘å€‘è¦å¦‚ä½•è¨ˆç®— $\\mathbb{E}N_t$?, æˆ‘å€‘ä½¿ç”¨ä¸Šé¢æ‰€è¿°çš„ä¸‰æ­¥é©Ÿ:&emsp;&emsp;åœ¨æœ€å¾Œä¸€æ­¥çš„æ™‚å€™æˆ‘è¦éœ€è¦æ‰¾å‡º&emsp;ä»€éº¼æ¨£çš„ $\\mathcal{U}(t)$ æœƒæœ‰ $\\mathcal{L}_\\mathcal{U}(s)=\\frac{1}{s}$? Ans: $1$&emsp;ä»€éº¼æ¨£çš„ $\\mathcal{U}(t)$ æœƒæœ‰ $\\mathcal{L}_\\mathcal{U}(s)=\\frac{1}{s^2}$? Ans $t$&emsp;ä»€éº¼æ¨£çš„ $\\mathcal{U}(t)$ æœƒæœ‰ $\\mathcal{L}_\\mathcal{U}(s)=\\frac{1}{2s+3}$? Ans $\\exp\\{-\\frac{3}{2}t\\}$ Week 1.11: Limit theorems for renewal processesè€ƒæ…®ä¸€å€‹ renewal process, $S_n=S_{n-1}+\\xi_n$, å…¶ä¸­ $\\xi_1,\\xi_2,...$ are i.i.d. &gt;0 almost surelySLLN ç‚º Strong Law of Large Number; CLT ç‚º Central Limit TheoremThm1 ç›´è§€ä¸Šå¯ä»¥ç†è§£, å› ç‚º $N_t$ è¡¨ç¤ºåˆ°æ™‚é–“ $t$ ç‚ºæ­¢å…±æœ‰å¤šå°‘å€‹äº‹ä»¶ç™¼ç”Ÿäº†. ç•¶ $t$ å¾ˆå¤§çš„æ™‚å€™, æ¯å–®ä½æ™‚é–“ç™¼ç”Ÿçš„äº‹ä»¶æ¬¡æ•¸, i.e. $\\frac{N_t}{t}$, æ‡‰è©²æœƒååˆ†æ¥è¿‘é »ç‡, i.e. $\\frac{1}{\\mu}$Thm2 å°±ä¸å¥½ç›´æ¥ç†è§£äº†, å…¶ä¸­ $\\xrightarrow[]{d}$ è¡¨ç¤º convergence in distribution. æ³¨æ„åˆ° $\\frac{1}{\\mu}$ è¡¨ç¤ºå–®ä½æ™‚é–“æ˜¯é–“ç™¼ç”Ÿçš„æ¬¡æ•¸, æ‰€ä»¥ä¹˜ä¸Š $t$ å°±æ˜¯äº‹ä»¶ç™¼ç”Ÿçš„æ¬¡æ•¸, æ³¨æ„åˆ°é€™æ˜¯æœŸæœ›å€¼, è€Œ $N_t$ æ˜¯ç™¼ç”Ÿæ¬¡æ•¸çš„ random variable. å› æ­¤å¯ä»¥æƒ³åƒ mean æ‡‰è©²å°±æ˜¯ $t/\\mu$. å›°é›£çš„æ˜¯ variance æ˜¯ä»€éº¼, ä»¥åŠé€™å‰›å¥½æœƒ follow normal distribution.è€Œé€™å…©å€‹åˆ†åˆ¥è·Ÿ SLLN (Strong Law of Large Number) and CLT (Central Limit Theorem) ç›¸é—œ. ä»¥ä¸‹çµ¦å‡ºè­‰æ˜ Thm1 çš„è­‰æ˜: Thm2 çš„è­‰æ˜: é–‹é ­çš„ç¬¬ä¸€è¡Œ:$\\mathcal{P}\\left\\{ \\frac{\\xi_1 + \\xi_2 + ... + \\xi_n -n\\cdot\\mu}{\\sigma\\sqrt{n}} \\leq x \\right\\} = \\mathcal{P}\\left\\{ \\frac{S_n-n\\cdot\\mu}{\\sigma\\sqrt{n}} \\leq x \\right\\}$æ˜¯å¾ CLT å‡ºç™¼ç¬¬äºŒè¡Œåˆ°ç¬¬ä¸‰è¡Œç”¨åˆ°äº†, $\\mathcal{P}\\left\\{ S_n\\leq t \\right\\} = \\mathcal{P}\\left\\{ N_t \\geq n \\right\\}$ç„¶å¾Œç”± $t=n\\mu+\\sigma\\sqrt{n}x\\Rightarrow n=\\frac{t}{\\mu}-\\frac{\\sigma\\sqrt{n}}{\\mu}x$ ä¸¦å°‡ $n \\approx t/\\mu$ å¸¶å…¥ r.h.s. å¾—åˆ°$n=\\frac{t}{\\mu}-\\frac{\\sigma\\sqrt{t}}{\\mu^{3/2}}x$, ç„¶å¾Œä»£å›åˆ° $\\mathcal{P}\\left\\{ N_t \\geq n \\right\\}$, å†æ•´ç†ä¸€ä¸‹å¾—åˆ°æœ€å¾Œä¸€è¡Œè­‰æ˜æœ€å¾Œä¸€è¡Œæ˜¯ (è¢«æ“‹åˆ°):$\\mathcal{P}\\left\\{ Z_t \\leq x \\right\\} = \\mathcal{P}\\left\\{ Z_t &gt; -x \\right\\} \\rightarrow 1-\\Phi(-x) = \\Phi(x)$æˆ–å¯ä»¥åƒè€ƒ https://www.randomservices.org/random/renewal/LimitTheorems.html çš„ The Central Limit Theorem Applications of the Renewal ProcessesApplications of the Renewal Processes.pdf","tags":[{"name":"Coursera","slug":"Coursera","permalink":"http://yoursite.com/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"http://yoursite.com/tags/Stochastic-Processes/"},{"name":"Probability Space","slug":"Probability-Space","permalink":"http://yoursite.com/tags/Probability-Space/"},{"name":"Renewal Process","slug":"Renewal-Process","permalink":"http://yoursite.com/tags/Renewal-Process/"},{"name":"Counting Process","slug":"Counting-Process","permalink":"http://yoursite.com/tags/Counting-Process/"}]},{"title":"Stochastic Processes Week 0 ä¸€äº›é å‚™çŸ¥è­˜","date":"2021-12-11T12:01:14.000Z","path":"2021/12/11/Stochastic-Processes-Week-0-ä¸€äº›é å‚™çŸ¥è­˜/","text":"Coursera Stochastic Processes èª²ç¨‹ç­†è¨˜, å…±ä¹ç¯‡: Week 0: ä¸€äº›é å‚™çŸ¥è­˜ (æœ¬æ–‡) Week 1: Introduction &amp; Renewal processes Week 2: Poisson Processes Week3: Markov Chains Week 4: Gaussian Processes Week 5: Stationarity and Linear filters Week 6: Ergodicity, differentiability, continuity Week 7: Stochastic integration &amp; ItÃ´ formula Week 8: LÃ©vy processes æœ¬ç¯‡å›é¡§ä¸€äº›åŸºç¤çš„æ©Ÿç‡è¤‡ç¿’, é€™äº›åœ¨ä¹‹å¾Œèª²ç¨‹è£¡æœ‰ç”¨åˆ°.å¼·çƒˆå»ºè­°é–±è®€ä»¥ä¸‹æ–‡ç« : [æ¸¬åº¦è«–] Sigma Algebra èˆ‡ Measurable function ç°¡ä»‹ [æ©Ÿç‡è«–] æ·ºè«‡æ©Ÿç‡å…¬ç† èˆ‡ åŸºæœ¬æ€§è³ª A guide to the Lebesgue measure and integration Measure theory in probability ä»¥ä¸‹å›é¡§é–‹å§‹: å›é¡§æ©Ÿç‡çŸ¥è­˜ $e^x=\\sum_{k=0}^\\infty\\frac{x^k}{k!}$. Proof link. Independent of Random Variables $X\\perp Y \\Longleftrightarrow \\mathcal{P}(XY)=\\mathcal{P}(X)\\mathcal{P}(Y)$ $Cov(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]$[Proof]: $$Cov(X,Y)=\\mathbb{E}[(X-\\mu_x)(Y-\\mu_y)] \\\\ =\\mathbb{E}[XY]-\\mu_x\\mathbb{E}[Y]-\\mu_y\\mathbb{E}[X]+\\mu_x\\mu_y \\\\ =\\mathbb{E}[XY]-\\mu_x\\mu_y$$ $Var(X)=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2$[Proof]: $$Var(X)=Cov(X,X)=\\mathbb{E}[XX]-\\mathbb{E}[X]\\mathbb{E}[X] \\\\ = \\mathbb{E}[X^2]-(\\mathbb{E}[X])^2$$ $X,Y$ uncorrelated $\\Longleftrightarrow Cov(X,Y)=0 \\Longleftrightarrow \\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ $X\\perp Y \\Rightarrow$ $X,Y$ uncorrelated $\\Rightarrow \\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ Covariance æ˜¯ç·šæ€§çš„: $Cov(aX+bY,cZ)=acCov(X,Z)+bcCov(Y,Z)$[Proof]: $$Cov(aX+bY,cZ)=\\mathbb{E}[(aX+bY)cZ]-\\mathbb{E}[aX+bY]\\mathbb{E}[cZ] \\\\ = ac\\mathbb{E}[XZ]+bc\\mathbb{E}[YZ]-ac\\mathbb{E}[X]\\mathbb{E}[Z]-bc\\mathbb{E}[Y]\\mathbb{E}[Z] \\\\ = ac(\\mathbb{E}[XZ]-\\mathbb{E}[X]\\mathbb{E}[Z])+bc(\\mathbb{E}[YZ]-\\mathbb{E}[Y]\\mathbb{E}[Z]) \\\\ = acCov(X,Z)+bcCov(Y,Z)$$ [Characteristic Function Def]: For random variable $\\xi$, å®šç¾© characteristic function $\\Phi:\\mathbb{R}\\rightarrow \\mathbb{C}$ ç‚º $$\\Phi_\\xi(u) = \\mathbb{E}\\left[ e^{iu\\xi} \\right]$$ å¦‚æœ $\\xi_1\\perp\\xi_2$, å‰‡ $\\Phi_{\\xi_1+\\xi_2}(u)=\\Phi_{\\xi_1}(u)\\Phi_{\\xi_2}(u)$ [Proof]: $$\\Phi_{\\xi_1+\\xi_2}(u)=\\mathbb{E}[e^{iu(\\xi_1+\\xi_2)}]=\\mathbb{E}[e^{iu\\xi_1}e^{iu\\xi_2}] \\\\ = \\int_{\\xi_1,\\xi_2} \\mathcal{P}(\\xi_1,\\xi_2)e^{iu\\xi_1}e^{iu\\xi_2} d\\xi_1 d\\xi_2 \\\\ = \\int_{\\xi_1,\\xi_2} \\left(\\mathcal{P}(\\xi_1)e^{iu\\xi_1}\\right)\\left(\\mathcal{P}(\\xi_2)e^{iu\\xi_2}\\right) d\\xi_1 d\\xi_2 \\\\ =\\mathbb{E}[e^{iu\\xi_1}]\\mathbb{E}[e^{iu\\xi_2}] \\\\ =\\Phi_{\\xi_1}(u)\\Phi_{\\xi_2}(u)$$ $\\mathbb{E}[X+Y]=\\mathbb{E}[X]+\\mathbb{E}[Y]$. è·Ÿ $X,Y$ æ˜¯å¦ç¨ç«‹æˆ–ä¸ç›¸é—œç„¡é—œ. $|Cov(X,Y)|\\leq\\sqrt{Var(X)}\\sqrt{Var(Y)}$[Proof]: $Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)$[Proof]: $$Var(X+Y)=Cov(X+Y,X+Y)\\\\ =Cov(X,X)+2Cov(X,Y)+Cov(Y,Y)\\\\ =Var(X)+Var(Y)+2Cov(X,Y)$$ å¦‚æœ $X,Y$ uncorrelated (æ‰€ä»¥ $X\\perp Y$ ä¹Ÿæˆç«‹), å‰‡ $Var(X+Y)=Var(X)+Var(Y)$ Normal distribution of one r.v. $$X\\sim\\mathcal{N}(\\mu,\\sigma^2), \\text{ for }\\sigma&gt;0,\\mu\\in\\mathbb{R} \\\\ p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$ The characteristic function of normal distribution is: $\\Phi(u)=e^{iu\\mu-\\frac{1}{2}u^2\\sigma^2}$ ç¨ç«‹é«˜æ–¯åˆ†ä½ˆä¹‹å’Œä»ç‚ºé«˜æ–¯åˆ†ä½ˆ, mean and variance éƒ½ç›¸åŠ [Proof]: $(X_1,...,X_n) \\text{ where } X_k\\sim\\mathcal{N}(\\mu_k,\\sigma_k^2),\\forall k=1,...,n$ æˆ‘å€‘çŸ¥é“ $$X_k\\sim\\mathcal{N}(\\mu_k,\\sigma_k^2)\\longleftrightarrow \\Phi_k(u)=e^{iu\\mu_k-\\frac{1}{2}u^2\\sigma_k^2}$$ å‰‡ $$\\sum_k X_k \\longleftrightarrow \\prod_k \\Phi_k(u) = e^{iu(\\sum_k \\mu_k)-\\frac{1}{2}u^2(\\sum_k \\sigma_k^2)}$$ ç”±ç‰¹å¾µæ–¹ç¨‹å¼èˆ‡æ©Ÿç‡åˆ†ä½ˆä¸€å°ä¸€å°æ‡‰å¾—çŸ¥ $\\sum_k X_k \\sim \\mathcal{N}\\left(\\sum_k \\mu_k, \\sum_k \\sigma_k^2\\right)$ $K:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$ is symmetric positive semi-definite: çµ¦å®š $t_1&lt;t_2&lt;â€¦&lt;t_n$ ä¸€å€‹å¾ˆæœ‰ç”¨çš„æŠ€å·§ç‚º:å¯ä»¥è®Šæˆä»¥ä¸‹é€™äº› disjoint å€æ®µçš„ç·šæ€§çµ„åˆ $(t_2-t_1),...,(t_n-t_{n-1})$ å…·é«”å¦‚ä¸‹: $$\\sum_{k=1}^n \\lambda_k B_{t_k} = \\lambda_n(B_{t_n}-B_{t_{n-1}}) + (\\lambda_n+\\lambda_{n-1})B_{t_{n-1}} + \\sum_{k=1}^{n-2} \\lambda_k B_{t_k} \\\\ = \\sum_{k=1}^n d_k(B_{t_n}-B_{t_{n-1}})$$ æœƒæƒ³è¦é€™æ¨£è½‰æ›æ˜¯å› ç‚ºèª²ç¨‹æœƒå­¸åˆ° independent increment ç‰¹æ€§, è¡¨æ˜ disjoint å€æ®µçš„ random variables ä¹‹é–“äº’ç›¸ç¨ç«‹, å› æ­¤å¯ä»¥è®Šæˆäº’ç›¸ç¨ç«‹çš„ r.v.s ç·šæ€§ç›¸åŠ  Calculating Moments with Characteristic Functions (wiki)) Brownian Motion referenceBrownian Motion by Hartmann.pdf Lebesgue Measure and IntegrationA guide to the Lebesgue measure and integrationLebesgue integration from wiki[æ¸¬åº¦è«–] Sigma Algebra èˆ‡ Measurable function ç°¡ä»‹ Probability Theory[æ©Ÿç‡è«–] æ·ºè«‡æ©Ÿç‡å…¬ç† èˆ‡ åŸºæœ¬æ€§è³ªMeasure theory in probabilityDistribution function $F$ defined with (probability) measure $\\mu$ (ref)$F$ åˆç¨± cumulative distribution function (c.d.f.), æˆ–ç¨± cumulative functionå…¶å¾®åˆ†ç¨±ç‚º probability density function (p.d.f.), æˆ–ç°¡ç¨± density function. æ³¨æ„åˆ° density ä¸æ˜¯ (probability) measure $\\mu$!è€ŒæœŸæœ›å€¼å¯ä»¥ç”¨ Lebesgue measure or in probability measure ä¾†çœ‹å¾…:Given Probability space: $(\\Omega,\\Sigma,\\mathcal{P})$, æœŸæœ›å€¼å®šç¾©ç‚º æ‰€æœ‰ event $\\omega\\in\\Sigma$ çš„ probability measure $\\mathcal{P}(\\omega)$, ä¹˜ä¸Šè©² random variable çš„å€¼ $X(\\omega)$æ‰€æœ‰ outcome $\\omega\\in\\Omega$ çš„ probability measure $\\mathcal{P}(d\\omega)$, ä¹˜ä¸Šè©² random variable çš„å€¼ $X(\\omega)$ Lebesgueâ€™s Dominated Convergence Theorem: Exchanging $\\lim$ and $\\int$Let $(f_n)$ be a sequence of measurable functions on a measure space $(S,\\Sigma,\\mu)$. Assume $(f_n)$ converges pointwise to $f$ and is dominated by some (Lebesgue) integrable function $g$, i.e. $|f_n(x)|\\leq g(x), \\qquad \\forall n,\\forall x\\in S$ Then $f$ is (Lebesgue) integrable, i.e. $\\int_S |f|d\\mu&lt;\\infty$and $$\\lim_{n\\rightarrow\\infty}\\int_S |f_n-f|d\\mu=0 \\\\ \\lim_{n\\rightarrow\\infty}\\int_S f_nd\\mu = \\int_S\\lim_{n\\rightarrow\\infty}f_nd\\mu = \\int_S f d\\mu$$","tags":[{"name":"Coursera","slug":"Coursera","permalink":"http://yoursite.com/tags/Coursera/"},{"name":"Stochastic Processes","slug":"Stochastic-Processes","permalink":"http://yoursite.com/tags/Stochastic-Processes/"}]},{"title":"MCMC by Gibbs and Metropolis-Hasting Sampling","date":"2021-10-27T13:53:41.000Z","path":"2021/10/27/MCMC-by-Gibbs-and-Metropolis-Hasting-Sampling/","text":"PRML book sampling (chapter 11) é–‹é ­æŠŠå‹•æ©Ÿæè¿°å¾—å¾ˆå¥½, ä¹Ÿå¼•ç”¨ä¾†ç•¶é€™ç¯‡æ–‡ç« çš„å‰è¨€.åœ¨ç”¨ machine learning å¾ˆå¤šæ™‚å€™æœƒé‡åˆ°éœ€è¦è¨ˆç®—æŸå€‹ function $f(x)$ çš„æœŸæœ›å€¼, ç•¶ $x$ follow æŸå€‹ distribution $p(x)$ çš„æƒ…æ³, i.e. éœ€è¨ˆç®— $$\\begin{align} \\mu:=\\mathbb{E}_p[f]=\\int f(x)p(x)dx \\end{align}$$ ä¾‹å¦‚ EM algorithm æœƒéœ€è¦è¨ˆç®— $\\mathbb{E}_{p(z|x)}[f(x,z)]$, åƒè€ƒ ref çš„å¼ (23), (28)åˆæˆ–è€…æˆ‘å€‘è¦åš Bayesian çš„ prediction æ™‚, åƒè€ƒ ref çš„å¼ (2) é€™äº›æƒ…æ³å¤§éƒ¨åˆ†éƒ½ç„¡æ³•æœ‰ analytical form. ä¸éå¦‚æœæˆ‘å€‘èƒ½å¾çµ¦å®šçš„ distribution $p(x)$ å– $L$ å€‹ sample çš„è©±, å¼ (1) å°±èƒ½å¦‚ä¸‹é€¼è¿‘ $$\\begin{align} \\mathbb{E}_p[f] \\approx \\hat f:= \\frac{1}{L}\\sum_{l=1}^L f(x_l) \\end{align}$$ æˆ‘å€‘å…ˆä¾†çœ‹ä¸€ä¸‹ $\\hat f$ é€™å€‹ä¼°è¨ˆçš„æœŸæœ›å€¼æ˜¯ä»€éº¼: $$\\begin{align} \\mathbb{E}_p[\\hat f]=\\mathbb{E}_p\\left[ \\frac{1}{L}\\sum_{l=1}^L f(x_l) \\right] = \\frac{1}{L}\\sum_{l=1}^L\\mathbb{E}_p\\left[ f(x_l) \\right] = {E}_p [f] = \\mu \\end{align}$$ å¾—åˆ°ä¸€å€‹å¥½æ¶ˆæ¯æ˜¯æˆ‘å€‘åªè¦ä¼°è¶…å¤šæ¬¡çš„è©±, $\\hat f_1, \\hat f_2, â€¦$ é€™äº›ä¼°è¨ˆçš„å¹³å‡å°±æ˜¯æˆ‘å€‘è¦çš„å€¼ å…¶å¯¦é€™ç­‰åŒæ–¼ä¼°ä¸€æ¬¡å°±å¥½, ä½†ç”¨è¶…å¤§çš„ $L$ å»ä¼°è¨ˆ. å•é¡Œæ˜¯ $L$ è¦å¤šå¤§æ‰å¤  ? å¦‚æœè®Šæ•¸ $x$ çš„ç¶­åº¦å¢åŠ , éœ€è¦çš„ $L$ æ˜¯å¦ä¹Ÿè¦å¢åŠ æ‰æœƒæº–ç¢º ? i.e. æœƒä¸æœƒæœ‰ç¶­åº¦çˆ†ç‚¸çš„å•é¡Œ ? (åƒè€ƒ Curse of dimensionality [1]) æˆ‘å€‘å¯ä»¥è­‰æ˜ (see Appendix): $$\\begin{align} var[\\hat f]=\\frac{1}{L}var(f) \\end{align}$$ é€™å‘Šè¨´æˆ‘å€‘, éš¨è‘— sample æ•¸é‡ $L$ æ„ˆå¤§, æˆ‘å€‘ä¼°å‡ºä¾†çš„ $\\hat f$ çš„â€è®ŠåŒ–â€æœƒæ„ˆä¾†æ„ˆå° (æˆåæ¯”). æ›´é‡è¦çš„æ˜¯, é€™è·Ÿ input dimension ç„¡é—œ! æ‰€ä»¥ä¸æœƒæœ‰ç¶­åº¦çˆ†ç‚¸çš„å•é¡Œ. èª²æœ¬èªªé€šå¸¸ $L$ å–å€‹ 10 å€‹ 20 å€‹ä¼°å‡ºä¾†çš„ $\\hat f$ å°±å¾ˆæº–äº†. (å…¶å¯¦å¾ˆå¥½é©—è­‰) æ‰€ä»¥å‰©ä¸‹è¦è§£æ±ºçš„å•é¡Œä¾¿æ˜¯, è¦æ€éº¼å¾ä¸€å€‹çµ¦å®šçš„ distribution å– sample ? æœ¬ç¯‡æ­£æ–‡å¾é€™é–‹å§‹ å…ˆèªªæ˜ 1-d æƒ…æ³ä¸‹çš„ r.v. æ€éº¼ sampling å†ä¾†èªªæ˜å¦‚ä½•ç”¨ Markov chain sampling, ä¹Ÿå°±æ˜¯å¤§åé¼é¼çš„ MCMC (Markov Chain Monte Carlo) æœ€å¾Œä»‹ç´¹å…©å€‹å¯¦ä½œæ–¹æ³• Gibbs and Metropolis-Hasting sampling. ä»¥ä¸‹æ–‡ç« å…§å®¹çµ•å¤§å¤šæ•¸éƒ½æ˜¯å¾ Coursera: Bayesian Methods for Machine Learning èª²ç¨‹ä¾†çš„éå¸¸æ¨è–¦é€™é–€èª²ç¨‹! å¾ 1-D èªªèµ·Discrete caseå…ˆè¨è«– discrete distribution çš„æƒ…å½¢, æˆ‘å€‘ç¸½æ˜¯å¯ä»¥å– samples from uniform distribution [0, 1], i.e. $\\text{sample} \\sim \\mathcal{U}[0,1]$æ‰€ä»¥è‹¥è¦å¾ä¸‹åœ–ä¾‹å­çš„ discrete distribution å– samples å…¶å¯¦å¾ˆå®¹æ˜“, è‹¥è½åœ¨ [0, 0.6) å°± sample $a_1$, è½åœ¨ [0.6, 0.7) å– $a_2$, è½åœ¨ [0.7, 1) å– $a_3$. Gaussian caseå¦‚æœæ˜¯ continuous distribution å‘¢?è€ƒæ…®å¦‚ä¸‹çš„ standard Gaussian distribution $\\mathcal{N}(0,1)$ å¯ä»¥ä½¿ç”¨ Central Limit Theorem. èˆ‰ä¾‹ä¾†èªªæˆ‘å€‘å¯ä»¥å¾ $n$ å€‹ I.I.D. çš„ $\\mathcal{U}[0,1]$ å– samples, ç„¶å¾Œå¹³å‡èµ·ä¾†. CLT å‘Šè¨´æˆ‘å€‘ç•¶ $n$ å¾ˆå¤§çš„æ™‚å€™, çµæœåˆ†å¸ƒæœƒæ¥è¿‘ $\\mathcal{N}(0,1)$ General continuous caseé‚£å¦‚æœæ˜¯ general case å‘¢? æ–¹æ³•æ˜¯æ‰¾ä¸€å€‹å·²çŸ¥æœƒ sampling çš„åˆ†å¸ƒä¹˜ä¸Š constant value ä½¿å®ƒæˆç‚º upper boundä¾‹å¦‚åˆ©ç”¨ $2q(x)=2\\mathcal{N}(1,9)$ å¯ä»¥è®Šæˆ $p(x)$ çš„ upper bound å› æ­¤æˆ‘å€‘å¯ä»¥ sample $\\tilde{x}$ from $2q(x)$, èˆ‰ä¾‹ä¾†èªªå¾ˆæœ‰å¯èƒ½ $\\tilde{x}=0$ å› ç‚ºåœ¨ $0$ é™„è¿‘çš„æ©Ÿç‡æœ€å¤§. ä½†æ˜¯å°æ–¼æˆ‘å€‘çœŸå¯¦æƒ³è¦ samping çš„ $p(x)$ ä¾†èªª, $0$ åè€Œæ©Ÿç‡æ¯”è¼ƒå°. å› æ­¤æˆ‘å€‘è¦æœ‰ä¸€äº› rejection æ©Ÿåˆ¶. æ‰€ä»¥æµç¨‹å°±æ˜¯, é¦–å…ˆå…ˆå¾å·²çŸ¥çš„ $q(x)$ sample å‡º $\\tilde{x}$, ç”±æ–¼ $2q(x)$ æ˜¯ $p(x)$ çš„ upper bound, å› æ­¤æˆ‘å€‘å¯ä»¥æ ¹æ“šæ¯”ä¾‹ä¾†æ±ºå®šé€™ä¸€æ¬¡çš„ $\\tilde{x}$ æ˜¯å¦æ¥å—. ä¸Šåœ–ç´…è‰²ç‚º rejection è€Œç¶ è‰²ç‚º acception. å› æ­¤ acception æ©Ÿç‡ç‚º: $$\\begin{align} \\frac{p(x)}{2q(x)} \\end{align}$$ æˆ‘å€‘è§£é‡‹ä¸€ä¸‹ç‚ºä½•é€™æ–¹æ³•å¯ä»¥é‹ä½œ, é¦–å…ˆæ³¨æ„åˆ°æ‰€æœ‰å–å‡ºä¾†çš„ $\\tilde{x}$ (é‚„æ²’æ‹’çµ•ä¹‹å‰) æ˜¯å‡å‹»åˆ†å¸ƒåœ¨ $2q(x)$ curve ä¸‹çš„ (è¦‹ä¸‹åœ–). è€Œä¸€æ—¦å¼•å…¥æˆ‘å€‘ rejection çš„æ–¹æ³•, å–å‡ºä¾†çš„é»å°±æ˜¯å‡å‹»åˆ†å¸ƒåœ¨æˆ‘å€‘è¦çš„ $p(x)$ curve ä¸‹äº†. å¾ä¸Šé¢çš„èªªæ˜å¯ä»¥çœ‹å‡º, accept çš„æ¯”ä¾‹å…¶å¯¦å°±æ˜¯è—è‰²çš„æ¯”ä¾‹, å› æ­¤ upper bound æ„ˆç·Šå¯†æ•ˆæœæ„ˆå¥½.æ‰€ä»¥å¦‚æœ $p(x)\\leq Mq(x)$, å‰‡å¹³å‡ accept $1/M$ points. é€™æ˜¯å› ç‚º $p,q$ éƒ½æ˜¯æ©Ÿç‡åˆ†å¸ƒ, æ‰€ä»¥ area under curve éƒ½æ˜¯ $1$. å› æ­¤æ¯”ä¾‹å°±æ˜¯ $1/M$.æœ€å¾Œ, é€™å€‹æ–¹æ³•å¯ä»¥ç”¨åœ¨ä¸çŸ¥é“ normalization term $Z$ çš„æƒ…å½¢. ä¾‹å¦‚æˆ‘å€‘åªçŸ¥é“ $\\hat{p}(x)$, ä½†æˆ‘å€‘ä»ç„¶å¯ä»¥æ‰¾åˆ°ä¸€å€‹ distribution $q(x)$ ä¹˜ä¸Š constant $\\tilde{M}$ å¾Œæ˜¯ upper bound: $$\\hat{p}(x) \\leq \\tilde{M}q(x) \\\\ \\Longrightarrow p(x)=\\frac{\\hat{p}(x)}{Z} \\leq Mq(x)$$ ç¸½è§£ä¸€ä¸‹æ­¤æ³• çµè«–å°±æ˜¯é›–ç„¶å°å¤§éƒ¨åˆ† distribution éƒ½å¯ä»¥ç”¨, ä½†æ•ˆç‡ä¸å¥½. å°¤å…¶åœ¨ç¶­åº¦é«˜çš„æ™‚å€™æœƒå¤§éƒ¨åˆ†éƒ½ reject.é‚£æœ‰ä»€éº¼æ–¹æ³•å¯ä»¥å°ä»˜é«˜ç¶­åº¦å‘¢? ä¸‹é¢è¦ä»‹ç´¹çš„ MCMC with Gibbs/Metropolis-Hastings å°±èƒ½è™•ç†. Markov Chains Monte Carloé€™è£¡å‡è¨­å¤§å®¶å·²ç¶“ç†Ÿæ‚‰ Markov chain äº†, ä¸å¤šåšä»‹ç´¹.ä½¿ç”¨ Markov chain çš„ç­–ç•¥ç‚ºä»¥ä¸‹å¹¾å€‹æ­¥é©Ÿ: é‡é»åœ¨å¦‚ä½•è¨­è¨ˆä¸€å€‹ Markov chain (é€™è£¡ç­‰åŒæ–¼è¨­è¨ˆ transition probability $T$), æ”¶æ–‚çš„ stationary distribution æ­£å¥½å°±æ˜¯æˆ‘å€‘è¦çš„ $p(x)$é¦–å…ˆä¸æ˜¯æ¯å€‹ Markov chain éƒ½æœƒæ”¶æ–‚, ä½†æœ‰ä¸€äº›å……åˆ†æ¢ä»¶å¦‚ä¸‹åœ– Theorem: å°ç…§ Stochastic Processes è£¡çš„ç­†è¨˜ (ä¹‹å¾Œè£œ link), é€™è£¡çš„ theorem éš±å«äº†æ­¤ Markov chain ç‚º ergodic, i.e. 1-equivalence class, recurrent, and aperiodic. è€Œ ergodic Markov chain å¿…å®šå­˜åœ¨ stationary distribution. Gibbs samplingä¸Šé¢æåˆ°ä½¿ç”¨ Markov chain å– sample çš„è©±, æ€éº¼æ¨£çš„ $T$ æœƒè®“å®ƒæ”¶æ–‚åˆ° desired $p(x)$Gibbs sampling å¯ä»¥æƒ³æˆä¸€ç¨®ç‰¹æ®Šçš„ $T$ çš„è¨­è¨ˆæ–¹æ³•, å¯ä»¥ç¢ºä¿æ”¶æ–‚è‡³ $p(x)$å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹ 3-dim çš„ P.D.F., å¯ä»¥ä¸çŸ¥é“ normalization term $Z$: $$\\begin{align} p(x_1,x_2,x_3)=\\frac{\\hat{p}(x_1,x_2,x_3)}{Z} \\end{align}$$ å¾ $(x_1^0, x_2^0, x_3^0)$ é–‹å§‹, e.g. $(0,0,0)$å…ˆå°ç¬¬ä¸€ç¶­å– sample: $$\\begin{align} x_1^1 \\sim p(x_1 | x_2=x_2^0, x_3=x_3^0) \\\\ = \\frac{\\hat{p}(x_1,x_2^0,x_3^0)}{Z_1} \\end{align}$$ é‡å° 1-d distribution å– sample æ˜¯å¾ˆå®¹æ˜“çš„, å¯ä»¥ä½¿ç”¨ä¸Šä¸€ç¯€çš„åšæ³•æ¥è‘—å°ç¬¬äºŒç¶­å– sample: $$\\begin{align} x_2^1 \\sim p(x_2 | x_1=x_1^{\\color{red}{1}}, x_3=x_3^0) \\end{align}$$ æœ€å¾Œå°ç¬¬ä¸‰ç¶­å– sample: $$\\begin{align} x_3^1 \\sim p(x_3 | x_1=x_1^{\\color{red}{1}}, x_2=x_2^{\\color{red}{1}}) \\end{align}$$ ä»¥ä¸Šä¾¿æ˜¯ä¸€æ¬¡çš„ iteration, æ‰€ä»¥: é¡¯è€Œæ˜“è¦‹, é€™å€‹æ–¹æ³•ä¸èƒ½ parallel, ä¹‹å¾Œæœƒèªªæ€éº¼åŠ é€Ÿ (åˆ©ç”¨ Metropolis-Hastings) è­‰æ˜æ”¶æ–‚è‡³ desired distributionç¾åœ¨è¦è­‰æ˜é€™æ¨£çš„æ¡æ¨£æ–¹å¼å®šç¾©äº†ä¸€å€‹ Markov chain ä¸”æœƒæ”¶æ–‚åˆ° desired distribution $p(x)$, which is stationary!Markov chain çš„ states å®šç¾©ç‚º $p(x)$ çš„ domain, æˆ‘å€‘ä»¥ $n$-dim ä¾†èªªå°±æ˜¯ $(x_1,x_2,â€¦,x_n)$Transition probabilities $p_T(x\\rightarrow xâ€™)$ , i.e. å¾ state $x$ åˆ° $xâ€™$ çš„æ©Ÿç‡, ä½¿ç”¨ Gibbs sampling ä¾†å®šç¾©: $$\\begin{align} p_T(x\\rightarrow x&apos;)=p(x_1&apos;|x_2,x_3,...,x_n)p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) \\end{align}$$ é€™è£¡æˆ‘å€‘åšå€‹å‡è¨­, ä»¤ $p_T(x\\rightarrow xâ€™)&gt;0,\\forall x,xâ€™$, å‰‡ç”±å®šç†çŸ¥é“æ­¤ Markov chain å¿… $\\exists !$ stationary distribution. æ‰€ä»¥ç¾åœ¨å•é¡Œæ˜¯è©² stationary distribution æ˜¯æˆ‘å€‘è¦çš„ $p(x)$ å—?è¦è­‰æ˜ $p(x)$ æ˜¯ stationary, æˆ‘å€‘åªéœ€è­‰æ˜: $$\\begin{align} p(x&apos;)=\\sum_x p(x\\rightarrow x&apos;)p(x) \\end{align}$$ é€™è¡¨ç¤º $p(x)$ ç¶“é 1-step transition å¾Œ, åˆ†å¸ƒä»ç„¶æ˜¯ $p(x)$æ‰€ä»¥å†ä¾†å°±æ˜¯ç”¨ $p_T(x\\rightarrow xâ€™)$ ä»£å…¥, é©—è­‰çœ‹çœ‹å°ä¸å° $$\\begin{align} \\sum_x p_T(x\\rightarrow x&apos;)p(x) \\\\ = \\sum_x p(x_1&apos;|x_2,...,x_n)p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) p(x) \\\\ =p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_x p(x_1&apos;|x_2,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n)p(x) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_2,..,x_n} p(x_1&apos;|x_2,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\sum_{x_1}p(x) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_2,..,x_n} {\\color{orange}{p(x_1&apos;|x_2,...,x_n)}} ...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) {\\color{orange}{p(x_2,...,x_n)}} \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_2,..,x_n} {\\color{orange}{p(x_1&apos;,x_2,...,x_n)}} p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\ldots(\\star) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_3,..,x_n} {\\color{orange}{p(x_1&apos;,x_3,...x_n)}}p(x_2&apos;|x_1&apos;,x_3,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\\\ = p(x_n&apos;|x_1&apos;,...,x_{n-1}&apos;) \\sum_{x_3,..,x_n} {\\color{orange}{p(x_1&apos;,x_2&apos;,x_3,...,x_n)}}p(x_3&apos;|x_1&apos;,x_2&apos;,x_4,...,x_n)...p(x_{n-1}&apos;|x_1&apos;,...,x_{n-2}&apos;,x_n) \\ldots(\\square) \\end{align}$$ è§€å¯Ÿ $(\\star)$ åˆ° $(\\square)$, æ˜¯æ¶ˆè€—æ‰ $x_2$ çš„ summantion, åŒæ™‚ä¹Ÿæ¶ˆè€—æ‰å° $x_2$ çš„ gibbs sampling step. å› æ­¤æˆ‘å€‘å¯ä»¥å° $(\\square)$ åšä¸€æ¨£çš„äº‹æƒ…, å»æ¶ˆè€—æ‰ $x_3$ çš„ summantion ä»¥åŠå° $x_3$ çš„ gibbs step.é‡è¤‡åšæœƒå¾—åˆ°: $$\\begin{align} = p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;)\\sum_{x_n}p(x_1&apos;,...,x_{n-1} &apos;,x_n) \\\\ = p(x_n&apos;|x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) p(x_1&apos;,x_2&apos;,...,x_{n-1}&apos;) \\\\ = p(x_1&apos;,x_2&apos;,...,x_n&apos;)=p(x&apos;) \\end{align}$$ Q.E.D. ç¸½çµå¤§è‡´ä¸Šæœ‰å…©å€‹å‰æ: å›ºå®šå…¶ä»–ç¶­åº¦, å°æŸä¸€ç¶­åº¦å– samples æ˜¯å¾ˆå®¹æ˜“çš„ $p(x_i|x_1,...,x_{i-1}, x_{i+1}, ..., x_n)&gt;0$, é€™ä¿è­‰äº†æˆ‘å€‘é€é Gibbs sampling ç”¢ç”Ÿçš„ Markov chain ä¸€å®šæ”¶æ–‚åˆ° desired $p(x)$ å„ªé»ç‚º: å°‡ multi-dimensional sampling åŒ–ç°¡ç‚º 1-d sampling å®¹æ˜“å¯¦ä½œ ç¼ºé»ç‚º: Highly correlated samples, é€™ä½¿å¾—æˆ‘å€‘è·‘åˆ° stationary distribution å¾Œ, ä¹Ÿä¸èƒ½é€£çºŒçš„å– sample é» Slow convergence (mixing) Not parallel (æ¥ä¸‹ä¾†ä»‹ç´¹çš„ Metropolis Hastings å¹«å¿™å¯ä»¥æ”¹å–„) Metropolis-HastingsGibbs sampling ç¼ºé»æ˜¯ samples are too correlated, ä¸”ä¸èƒ½å¹³è¡ŒåŒ–. æ³¨æ„åˆ°åœ¨ Gibbs sampling æ–¹æ³•è£¡, å·²ç¶“å®šç¾©å¥½æŸä¸€å€‹ç‰¹åˆ¥çš„ Markov chain äº†. Metropolis-Hastings å‰‡å¯ä»¥å®šç¾©å‡ºä¸€å€‹ famliy of Markov chain éƒ½æ”¶æ–‚åˆ° desired distribution. å› æ­¤å¯ä»¥é¸æ“‡æŸä¸€å€‹ Markov chain å¯èƒ½æ”¶æ–‚è¼ƒå¿«, æˆ–æ˜¯ less correlated.Metropolis-Hastings ä¸­å¿ƒæƒ³æ³•å°±æ˜¯ â€œapply rejection sampling to Markov chainsâ€ Algorithm å…¶ä¸­ $Q(x^k\\rightarrow x)$ æ˜¯ä»»æ„äº‹å…ˆçµ¦å®šçš„ä¸€å€‹ transition probabilities (æ³¨æ„åˆ°éœ€æ»¿è¶³ $&gt;0,\\forall x,xâ€™$, é€™æ¨£æ‰èƒ½ä¿è­‰å”¯ä¸€æ”¶æ–‚)$A(x^k\\rightarrow x)$ è¡¨ç¤º given $x^k$ accept $x$ çš„æ©Ÿç‡, ç¨±ç‚º criticæ¼”ç®—æ³•æµç¨‹ç‚º: å…ˆå¾ $Q(x^k\\rightarrow x)$ å–æ¨£å‡º $xâ€™$, $xâ€™$ æœ‰ $A(x^k\\rightarrow xâ€™)$ çš„æ©Ÿç‡è¢«æ¥å—, ä¸€æ—¦æ¥å—å‰‡ $x^{k+1}=xâ€™$ å¦å‰‡ $x^{k+1}=x^k$, ç„¶å¾Œ iterate ä¸‹å»ä½¿ç”¨é€™ç¨®æ–¹å¼çš„è©±, æˆ‘å€‘å…¶å¯¦å¯ä»¥ç®—å‡º transition probability $T(x\\rightarrow xâ€™)$, å¦‚ä¸Šåœ–æ‰€ä»¥é—œéµå°±æ˜¯, æ€éº¼é¸æ“‡ $A(x^k\\rightarrow x)$ ä½¿å¾—é€™æ¨£çš„ Markov chain å¯ä»¥æ”¶æ–‚åˆ° desired probability $\\pi(x)$ æ€éº¼é¸æ“‡ Critic $A$ ä½¿å¾— Markov chain æ”¶æ–‚åˆ° $\\pi$æˆ‘å€‘å…ˆä»‹ç´¹ä¸€å€‹å……åˆ†æ¢ä»¶ (æ‰€ä»¥æœ‰å¯èƒ½ $\\pi(x)$ æ˜¯ stationary ä½†æ˜¯ä¸æ»¿è¶³ detailed balance equation) [Detailed Balance Equation]:è‹¥ $\\pi(x)T(x\\rightarrow xâ€™)=\\pi(xâ€™)T(xâ€™\\rightarrow x), \\forall x,xâ€™$, å‰‡ $\\pi(x)$ ç‚º stationary distribution, i.e. $\\pi(x&apos;)=\\sum_x \\pi(x)T(x\\rightarrow x&apos;)$ [Proof]: $$\\begin{align} \\sum_x \\pi(x)T(x\\rightarrow x&apos;) \\\\ \\text{by assumption} = \\sum_x \\pi(x&apos;)T(x&apos;\\rightarrow x) \\\\ = \\pi(x&apos;)\\sum_x T(x&apos;\\rightarrow x) = \\pi(x&apos;) \\end{align}$$ æ‰€ä»¥åªè¦é¸æ“‡çš„ $A(x\\rightarrow xâ€™)$ èƒ½å¤ è®“ $T(x\\rightarrow xâ€™)$ é‡å° $\\pi(x)$ æ»¿è¶³ detailed balance ç‰¹æ€§å°±èƒ½ä¿è­‰ Markov chain æ”¶æ–‚åˆ° $\\pi(x)$å› æ­¤æˆ‘å€‘è¨ˆç®—ä¸€ä¸‹, åªéœ€è€ƒæ…® $x\\neq xâ€™$ çš„æƒ…å½¢ (å› ç‚º $x=xâ€™$ ä¸€å®šæ»¿è¶³ detailed balance equation, é€™ä¸æ˜¯å»¢è©±å—) $$\\begin{align} \\pi(x)T(x\\rightarrow x&apos;)=\\pi(x&apos;)T(x&apos;\\rightarrow x) \\\\ \\Longleftrightarrow \\pi(x)Q(x\\rightarrow x&apos;)A(x\\rightarrow x&apos;) = \\pi(x&apos;)Q(x&apos;\\rightarrow x)A(x&apos;\\rightarrow x) \\\\ \\Longleftrightarrow \\frac{A(x\\rightarrow x&apos;)}{A(x&apos;\\rightarrow x)} = \\frac{\\pi(x&apos;)Q(x&apos;\\rightarrow x)}{\\pi(x)Q(x\\rightarrow x&apos;)} =: \\rho \\end{align}$$ æ‰€ä»¥ç•¶ $\\rho&lt;1$ æˆ‘å€‘è¨­å®š $$\\begin{align} \\left\\{ \\begin{array}{r} A(x\\rightarrow x&apos;)=\\rho \\\\ A(x&apos;\\rightarrow x)=1 \\end{array} \\right. \\end{align}$$ è€Œå¦‚æœ $\\rho&gt;1$ æˆ‘å€‘è¨­å®š $$\\begin{align} \\left\\{ \\begin{array}{r} A(x\\rightarrow x&apos;)=1 \\\\ A(x&apos;\\rightarrow x)=1/\\rho \\end{array} \\right. \\end{align}$$ ç¸½çµä¾†èªª $A$ å¯ä»¥é€™éº¼è¨­å®š $$\\begin{align} A(x\\rightarrow x&apos;)=\\min\\left\\{ 1, \\frac{\\pi(x&apos;)Q(x&apos;\\rightarrow x)}{\\pi(x)Q(x\\rightarrow x&apos;)} \\right\\} \\end{align}$$ æ³¨æ„åˆ° $\\rho$ æ˜¯å¯ä»¥ç›´æ¥ç®—å‡ºä¾†çš„, å› ç‚º $Q,\\pi$ éƒ½æ˜¯äº‹å…ˆçµ¦å®šå·²çŸ¥çš„, å› æ­¤æˆ‘å€‘å°±èƒ½è¨­å®šå‡ºå°æ‡‰çš„ acceptance distribution $A$. åŒæ™‚å¦‚æœæˆ‘å€‘åªæœ‰ unnormalized distribution, i.e. $\\hat\\pi(x)$, ç”± $A$ çš„è¨­å®šå¯ä»¥çœ‹å‡ºä¸å—å½±éŸ¿ $$\\begin{align} A(x\\rightarrow x&apos;)=\\min\\left\\{ 1, \\frac{ {\\color{orange}{\\hat\\pi(x&apos;)}} Q(x&apos;\\rightarrow x)}{ {\\color{orange}{\\hat\\pi(x)}} Q(x\\rightarrow x&apos;)} \\right\\} \\end{align}$$ æ€éº¼é¸æ“‡ $Q$é¦–å…ˆéœ€æ»¿è¶³ $Q(x\\rightarrow xâ€™)&gt;0,\\forall x,xâ€™$. é€™æ¨£æ‰æœƒæœ‰ä»¥ä¸Šçš„æ¨è«–.$Q$ æœƒå¸Œæœ›èƒ½èµ°â€å¤§æ­¥â€ä¸€é», ä¹Ÿå°±æ˜¯ transition ä¸è¦åªåœç¹åœ¨ç›¸é„°çš„é». å¥½è™•æ˜¯ç”¢ç”Ÿçš„ sample æœƒæ¯”è¼ƒç„¡é—œ.ä½†å¦‚æœèµ°å¤ªå¤§æ­¥, critic $A$ å°±æœ‰å¯èƒ½ä¸€ç›´ reject (why?) å°è‡´æ•ˆç‡å¤ªå·® æƒ³åƒå¦‚æœ $x$ å·²ç¶“åœ¨æ©Ÿç‡å¾ˆé«˜çš„åœ°æ–¹äº†, ä¾‹å¦‚ local maximum point. å¦‚æœ $Q$ èµ°å¤ªå¤§æ­¥åˆ° $xâ€™$, å‰‡å®¹æ˜“ $\\pi(xâ€™)&lt;&lt;\\pi(x)$, é€ æˆ $A$ å¤ªå°å®¹æ˜“ rejectæ‰€ä»¥å¦‚æœ $Q$ èµ°å°æ­¥ä¸€é», $xâ€™$ é‚„æ˜¯åœç¹åœ¨ $x$ é™„è¿‘, ç›¸å°ä¾†èªªå¯èƒ½æ©Ÿç‡å°±ä¸æœƒé‚£éº¼ä½ Example of Metropolis-Hastings1-d case toy example å‘Šè¨´æˆ‘å€‘ proposal çš„ distribution é¸æ“‡ä¹Ÿæ˜¯å¾ˆé‡è¦çš„. æœ€å¾Œå¯ä»¥ä½¿ç”¨ Metropolis Hastings ä¾†å¹³è¡ŒåŒ– Gibbs sampling!æˆ‘å€‘ä½¿ç”¨å¦‚ä¸‹åœ– â€œéŒ¯èª¤çš„â€ Gibbs sampling æ–¹æ³•, ä¸¦å°‡é€™æ–¹æ³•è¦–ç‚º Metropolis Hastings çš„ proposal $Q(x\\rightarrow xâ€™)$å› æ­¤å¯ä»¥å¹³è¡Œå°æ¯å€‹ç¶­åº¦å– sample! (å¥½è°æ˜!) çµèªMCMC è¢«è­½ç‚º 20 ä¸–ç´€åå€‹å‰å¤§çš„æ¼”ç®—æ³•ç™¼æ˜ä¹‹ä¸€ [3]. æ‰¾çŸ¥ä¹çš„æ–‡ç« å¯ä»¥çœ‹åˆ°é€™å€‹è¨è«–: æœ‰ä»€ä¹ˆç†è®ºå¤æ‚ä½†æ˜¯å®ç°ç®€å•çš„ç®—æ³•ï¼Ÿ[4] æœç„¶ MCMC ç†è«–ä¸æ˜¯ä¸€èˆ¬äººèƒ½åšçš„.å¾ŒçºŒå°æ–¼ Metropolis-Hastings çš„æ”¹é€²æœ‰ä¸€å€‹ç®—æ³•æ˜¯ Metropolis-adjusted Langevin algorithm [5] (MALA). è©²æ–¹æ³•æå‡ºä½¿ç”¨ Langevin dynamics [6] ç•¶ä½œ proposal, é€™æœƒä½¿å¾— random walk æœƒèµ°å‘æ©Ÿç‡æ¯”è¼ƒé«˜çš„åœ°æ–¹, å› æ­¤è¢«æ‹’çµ•æ©Ÿç‡è¼ƒä½. ä½†æ˜¯ MALA æˆ‘å¯¦åœ¨çœ‹ä¸æ‡‚, åªçŸ¥é“è·Ÿ Langevin dynamics sampling [7] æœ‰é—œ åœ¨ Generative Modeling by Estimating Gradients of the Data Distribution [8] çš„ Langevin dynamics æ®µè½è£¡æåˆ° MALA å¯ä»¥åªæ ¹æ“š score function ($\\nabla_x \\log p(x)$) å°±å¾ P.D.F. $p(x)$ å– samples! æœƒçœ‹åˆ° MALA æ˜¯å› ç‚ºé™¤äº† GAN ä¹‹å¤–æœ€è¿‘å¾ˆç†±é–€çš„ generative models: DPM [9]), å…¶æ ¸å¿ƒæŠ€è¡“ä¹‹ä¸€ç”¨åˆ°å®ƒ.çœ‹ä¾†è¦å…¨éƒ¨èæœƒè²«é€šç›®å‰æœƒå…ˆå¡é—œåœ¨é€™äº†. MALA ä½ ç­‰è‘—! åˆ¥è·‘å•Š, ä¸è¦ä»¥ç‚ºæˆ‘æ€•äº†ä½ , ç¸½æœ‰ä¸€å¤©æˆ‘ #$@^#@$Q (é€ƒ~) Appendixè­‰æ˜ $var[\\hat f]=\\frac{1}{L}Var(f)$ å¦‚ä¸‹: é¦–å…ˆå…©å€‹ independent r.v.s $X,Y$ æˆ‘å€‘çŸ¥é“å…¶ covariance ç‚º $0$: $$\\begin{align} 0 = Cov[XY] = \\mathbb{E}\\left[ (X-\\mu_x)(Y-\\mu_y) \\right] \\\\ = \\mathbb{E}[XY-X\\mu_y-\\mu_xY+\\mu_x\\mu_y] = \\mathbb{E}[XY] - \\mu_x\\mu_y \\\\ \\Rightarrow \\mathbb{E}[XY] = \\mu_x\\mu_y \\ldots(\\star) \\end{align}$$ ä¸”æœ‰ variance çš„æ€§è³ª: $Var(X)=\\mathbb{E}[X^2]-\\mu_x^2\\ldots(\\star\\star)$æ¥è‘—é–‹å§‹è¨ˆç®—: $$\\begin{align} Var[\\hat f]=\\mathbb{E}[(\\hat f - \\mathbb{E}[\\hat f])^2] = \\mathbb{E}[(\\hat f - \\mu)^2] = \\mathbb{E}[\\hat f^2] - \\mu^2 \\\\ = \\mathbb{E}\\left[ \\frac{1}{L}\\sum_k f(x_k) \\frac{1}{L}\\sum_m f(x_m) \\right] - \\mu^2 \\\\ = \\frac{1}{L^2}\\sum_k\\sum_m\\left[ \\mathbb{E}[f(x_k)f(x_m)] - \\mu^2 \\right] \\\\ \\text{by }(\\star)= \\frac{1}{L^2}\\sum_k \\left[ (\\mathbb{E}[f(x_k)^2]-\\mu^2)+(L-1)(\\mu^2-\\mu^2) \\right] \\\\ \\text{by }(\\star\\star) = \\frac{1}{L^2}\\sum_k Var(f(x_k)) \\\\ = \\frac{1}{L} Var(f) \\end{align}$$ Reference Curse of Dimensionality â€” A â€œCurseâ€ to Machine Learning Coursera: Bayesian Methods for Machine Learning The Best of the 20th Century: Editors Name Top 10 Algorithms æœ‰ä»€ä¹ˆç†è®ºå¤æ‚ä½†æ˜¯å®ç°ç®€å•çš„ç®—æ³•ï¼Ÿ Metropolis-adjusted Langevin algorithm: wiki Langevin dynamics: wiki æŠ½æ ·ç†è®ºä¸­æœ‰å“ªäº›ä»¤äººå°è±¡æ·±åˆ»(æœ‰è¶£)çš„ç»“è®º? Generative Modeling by Estimating Gradients of the Data Distribution What are Diffusion Models?","tags":[{"name":"Coursera","slug":"Coursera","permalink":"http://yoursite.com/tags/Coursera/"},{"name":"MCMC","slug":"MCMC","permalink":"http://yoursite.com/tags/MCMC/"},{"name":"Markov Chain","slug":"Markov-Chain","permalink":"http://yoursite.com/tags/Markov-Chain/"},{"name":"Gibbs Sampling","slug":"Gibbs-Sampling","permalink":"http://yoursite.com/tags/Gibbs-Sampling/"},{"name":"Metropolis Hastings","slug":"Metropolis-Hastings","permalink":"http://yoursite.com/tags/Metropolis-Hastings/"}]},{"title":"Gumbel-Max Trick","date":"2021-08-07T10:41:01.000Z","path":"2021/08/07/Gumbel-Max-Trick/","text":"æˆ‘å€‘åœ¨ä»‹ç´¹ VAE çš„æ™‚å€™æœ‰èªªæ˜åˆ° re-parameterization trick, å¤§æ„æ˜¯é€™æ¨£çš„ $y$ æ˜¯ sampling from distribution $\\alpha$, i.e., $y=\\text{Sampling}(\\alpha)$, å…¶ä¸­ $\\alpha=\\text{NN}_1(a;\\theta)$ç”±æ–¼æˆ‘å€‘æœ‰æ¡æ¨£, å› æ­¤ loss æ¡ç”¨æœŸæœ›å€¼. Loss function ç‚º: $$\\begin{align} L = \\mathbb{E}_{y\\sim\\alpha}[\\text{NN}_2(y;\\nu)] \\end{align}$$ Loss å° $\\theta$ åå¾®åˆ†çš„æ™‚å€™æœƒå¤±æ•—, ä¸»è¦æ˜¯å› ç‚º: $$\\begin{align} \\nabla_\\theta L = \\nabla_\\theta \\mathbb{E}_{y\\sim\\alpha}[\\text{NN}_2(y;\\nu)] \\\\ \\neq \\mathbb{E}_{y\\sim\\alpha}[\\nabla_\\theta \\text{NN}_2(y;\\nu)] \\end{align}$$ å¾®åˆ†ä¸èƒ½è·Ÿ Expectation äº’æ›æ˜¯å› ç‚º sampling çš„ distribution $\\alpha$ å…¶å¯¦ä¹Ÿæ˜¯ depends on $\\theta$. å› æ­¤åœ¨ VAE é‚£é‚Šçš„å‡è¨­å°±æ˜¯å°‡ $\\alpha$ å®šç¾©ç‚º Gaussian pdf. å› æ­¤å¯ä»¥è®Šæˆ: $$\\begin{align} \\nabla_\\theta L = \\nabla_\\theta \\mathbb{E}_{y\\sim\\alpha}\\left[ \\text{NN}_2(y;\\nu) \\right] \\\\ = \\nabla_\\theta \\mathbb{E}_{\\varepsilon\\sim N(0,I)}\\left[ \\text{NN}_2(\\mu+\\sigma\\varepsilon; \\nu) \\right] \\\\ = \\mathbb{E}_{\\varepsilon\\sim N(0,I)}\\left[ \\nabla_\\theta \\text{NN}_2(\\mu+\\sigma\\varepsilon; \\nu) \\right] \\end{align}$$ æ¡æ¨£è®Šæˆå¾ä¸€å€‹ è·Ÿ $\\theta$ ç„¡é—œçš„åˆ†å¸ƒ, å› æ­¤å¾®åˆ†è·ŸæœŸæœ›å€¼å°±èƒ½äº’æ›, æ‰€ä»¥å¯ä»¥åš backprop. ç¾åœ¨çš„æƒ…æ³æ˜¯å¦‚æœæ˜¯ Gaussian çš„æƒ…å½¢å¾ˆå¥½åšè®Šæ›, ä½†å¦‚æœæ˜¯ categorical distribution è©²æ€éº¼è¾¦å‘¢? ä»€éº¼æƒ…æ³æœƒé‡åˆ° categorical distribution? åœ¨ reinforcement learning æ™‚, $\\text{NN}_1$ predict å‡ºä¾‹å¦‚ 4 å€‹ actions çš„æ©Ÿç‡, æˆ‘å€‘éœ€è¦éš¨æ©Ÿæ¡æ¨£ä¸€ç¨® action, ç„¶å¾Œå‚³çµ¦å¾Œé¢çš„ NN å»è¨ˆç®— reward.(å…¶å¯¦æˆ‘ä¸ç†Ÿ RL, çœ‹ç¶²è·¯ä¸Šçš„æ–‡ç« èªªçš„) Gumbel max trick å°±æä¾›äº†è§£æ³•! Gumbel Distribution and Gumbel Max Samplingé€™ä¸€ç¯‡æ–‡ç«  The Humble Gumbel Distribution æä¾›äº†éå¸¸æ¸…æ™°çš„è§£é‡‹, ååˆ†æ¨è–¦é–±è®€ å‡è¨­æˆ‘å€‘ç¶“ç”±ä¸€å€‹ network ç®—å‡º logits $(x_k)_k$, ä¸€èˆ¬æˆ‘å€‘å¦‚æœè¦ sampling çš„è©±é‚„å¿…é ˆé softmax è®“å®ƒè®Šæˆæ©Ÿç‡ $(\\alpha_k)_k$, ç„¶å¾Œåœ¨ç”¨ä¾‹å¦‚ np.random.choice æ ¹æ“šæ©Ÿç‡æ¡æ¨£å‡ºçµæœ. ç¾åœ¨ sampling æµç¨‹æ”¹ç‚º: å…ˆå¾æ¨™æº– Gumbel åˆ†ä½ˆ (å…ˆä¸ç®¡é€™åˆ†ä½ˆé•·ä»€éº¼æ¨£) æ¡æ¨£å‡º $N$ å€‹å€¼, ä»¤ç‚º $(G_k)_k$, è®“å®ƒè·Ÿ logits ç›¸åŠ : $z_k=x_k+G_k$, ç„¶å¾Œ $\\text{argmax}_k (z_k)$ å°±æ˜¯æˆ‘å€‘é€™æ¬¡çš„æ¡æ¨£çµæœ åœ–ç¤ºç‚º: æ³¨æ„åˆ°æˆ‘å€‘å”¯ä¸€çš„ä¸€å€‹æ¡æ¨£å‹•ä½œå®Œå…¨è·Ÿ network çš„åƒæ•¸ $\\theta$ ç„¡é—œ! å› æ­¤ re-parameterization trick å°±èƒ½ç”¨ä¸Š. (å…ˆå‡è¨­ $\\text{argmax}_k (z_k)$ å¯å¾®, å› æ­¤å¯ä»¥ backprop, é€™ç­‰ä¸‹æœƒèªª)å‰©ä¸‹å”¯ä¸€ä¸ç¢ºå®šçš„å°±æ˜¯, é€™æ¨£çš„æ¡æ¨£è¡Œç‚ºå‡ºä¾†çš„çµæœ, æœƒè·Ÿä½¿ç”¨ $(\\alpha_k)_k$ çš„æ©Ÿç‡åˆ†ä½ˆæ¡æ¨£å‡ºä¾†ä¸€æ¨£å— ?æ›å¥è©±èªª, $\\text{argmax}_k (z_k)$ å‡ºä¾†çš„çµæœ, å…¶çµæœçš„åˆ†ä½ˆæ˜¯ä¸æ˜¯ç¬¦åˆ $(\\alpha_k)_k$ ?ç¨‹å¼é©—è­‰å¯åƒè€ƒ The Humble Gumbel Distribution, å°‡æœ€ä¸»è¦çš„éƒ¨åˆ†ä¿®çŸ­æ“·å–å¾Œå¦‚ä¸‹: 123456789101112131415161718192021222324252627282930313233343536373839# Modified from http://amid.fish/humble-gumbelimport numpy as npimport matplotlib.pyplot as plt# Assign categorical probabilities, for example:probs = [0.13114754, 0.01639344, 0.21311475, 0.24590164, 0.19672131, 0.06557377, 0.13114754]n_classes = len(probs)logits = np.log(probs) # logits is log probability (with constant offset)n_samples = 10000 # experimental number of samplingdef gumbel_sampling(logits): noise = np.random.gumbel(size=len(logits)) sample = np.argmax(logits + noise) return samplesamples_with_gumbel_max_trick = [gumbel_sampling(logits) for _ in range(n_samples)]samples_from_true_distribution = np.random.choice(np.arange(n_classes), size=n_samples , p=probs)# Plotting area, comparing `samples_with_gumbel_max_trick` and `samples_from_true_distribution`def plot_estimated_probs(samples, n_classes): estd_probs, _, _ = plt.hist(samples, bins=np.arange(n_classes + 1), align='left', edgecolor='white', density=True) plt.xlabel(\"Category\") plt.ylabel(\"Estimated probability\") return estd_probsplt.figure()plt.subplot(1, 2, 1)plot_estimated_probs(samples_from_true_distribution, n_classes)plt.title('Sampling from true pdf')plt.subplot(1, 2, 2)estd_probs = plot_estimated_probs(samples_with_gumbel_max_trick, n_classes)plt.title('Sampling with Gumbel-max trick')plt.tight_layout()plt.show() å¯ä»¥çœ‹åˆ°ç”¨ Gumbel-max trick æ¡æ¨£å‡ºä¾†çš„ samples å…¶åˆ†ä½ˆè·ŸçœŸå¯¦çš„æ©Ÿç‡åˆ†ä½ˆååˆ†æ¥è¿‘.äº‹å¯¦ä¸Šå¯ä»¥è­‰æ˜æœƒæ˜¯ä¸€æ¨£çš„, åœ¨ä¸‹ä¸€ç¯€æˆ‘å€‘å°‡è­‰æ˜å¯«å‡ºä¾†.å†å›‰å—¦ä¸€ä¸‹, ä¸è¦å¿˜è¨˜äº†, ä½¿ç”¨ np.random.choice å°çœŸå¯¦åˆ†ä½ˆæ¡æ¨£æ˜¯æ²’æœ‰è¾¦æ³•åš backprop çš„ (è¦‹ eq (2) (3))è€Œé€é Gumbel-max trick æˆ‘å€‘å¯ä»¥å¾ä¸€å€‹èˆ‡è¦ optimize çš„åƒæ•¸ $\\theta$ ç„¡é—œçš„åˆ†ä½ˆ (Gumbel distribution) é€²è¡Œæ¡æ¨£, æ‰èƒ½åˆ©ç”¨ re-parameterization trick åš backprop (ä¾‹å¦‚ eq (4)~(6) çš„æ¦‚å¿µ) å…¶å¯¦æˆ‘å°‘è¬›äº†ä¸€ä»¶äº‹, np.argmax ä¸å¯å¾®, æ‰€ä»¥ä¸èƒ½ backprop. å› æ­¤ä¸€å€‹å¯¦éš›çš„åšæ³•æ˜¯ä½¿ç”¨ softmax (with temperature) è¿‘ä¼¼: $$\\begin{align} \\text{softmax}(z_k,\\tau)=\\frac{\\exp(z_k/\\tau)}{\\sum_{i=1}^N\\exp(z_i/\\tau)} \\end{align}$$ å¯¦ä½œä¸Šæœƒå…ˆè®“ temperature $\\tau$ å¾æ¯”è¼ƒå¤§çš„å€¼é–‹å§‹ (æ¯”è¼ƒä¸é‚£éº¼å‡¸é¡¯å€¼ä¹‹é–“å¤§å°çš„å·®ç•°), ä¹‹å¾Œæ…¢æ…¢è®Šå°æ¥è¿‘ $0$ (ç­‰åŒæ–¼ argmax). åƒè€ƒ paper çš„åœ–: Proof of Gumbel-Max Trick for Discrete Distributionså…¶å¯¦å®Œå…¨åƒè€ƒ The Gumbel-Max Trick for Discrete Distributions, ä½†æœ€å¾Œä¸€è¡Œçš„æ¨å°ç”¨çœ‹çš„å¯¦åœ¨æ²’çœ‹å‡ºä¾†, å› æ­¤è‡ªå·±è£œé½Šå®Œæ•´ä¸€é» Math warning, å¾ˆæ¯ç‡¥ Gumbel PDF: $f(z;\\mu)=\\exp\\left[-(z-\\mu)-\\exp\\left[-(z-\\mu)\\right]\\right]$ $f(z;0)=\\exp\\left[-z-\\exp\\left[-z\\right]\\right]$ Gumbel CDF: $F(z;\\mu)=\\exp\\left[-\\exp\\left[-(z-\\mu)\\right]\\right]$ $F(z;0)=\\exp\\left[-\\exp\\left[-z\\right]\\right]$ Categorical distribution ä¾‹å¦‚åˆ†æˆ $N$ é¡, NN é€šå¸¸æœ€å¾Œæœƒè¼¸å‡ºä¸€å€‹ logits vector, $(x_k)_k$, $k=1â€¦N$ $z_k=x_k+G_k$, å…¶ä¸­ $G_k$ æ˜¯ä¸€å€‹æ¨™æº– Gumbel distribution (mean=0, scale=1) $$\\begin{align} \\Pr(k\\text{ is largest}|\\{x_i\\},z_k) = \\Pr(\\max_{i\\neq k}z_i&lt;z_k) \\\\ =\\prod_{i\\neq k}\\Pr(z_i&lt;z_k) = \\prod_{i\\neq k}\\Pr(x_i+G_i&lt;z_k) \\\\ =\\prod_{i\\neq k}\\Pr(G_i&lt;z_k-x_i) \\\\ =\\prod_{i\\neq k}F(z_k-x_i;0) \\\\ =\\prod_{i\\neq k}\\exp\\{-\\exp\\{-z_k+x_i\\}\\} \\end{align}$$ $$\\begin{align} \\therefore \\Pr(k\\text{ is largest}|\\{x_i\\})=\\int\\Pr(z_k)\\Pr(k\\text{ is largest}|\\{x_i\\},z_k)dz_k \\\\ = \\int f(z_k-x_k;0)\\prod_{i\\neq k}\\exp\\{-\\exp\\{-z_k+x_i\\}\\} \\\\ = \\int \\left(\\exp\\{-z_k+x_k-e^{-z_k+x_k}\\}\\right) \\prod_{i\\neq k}\\exp\\{-e^{-z_k+x_i}\\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k\\}\\prod_{i=1}^N{ \\exp\\{-e^{-z_k+x_i}\\} } dz_k \\\\ = \\int \\exp\\{-z_k+x_k\\} \\cdot \\exp\\{-\\sum_{i=1}^Ne^{-z_k+x_i}\\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k-\\sum_{i=1}^Ne^{-z_k+x_i} \\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k-e^{-z_k} {\\color{orange}{\\sum_{i=1}^Ne^{x_i}}} \\} dz_k \\\\ =\\int \\exp\\{-z_k+x_k- {\\color{orange}A} e^{-z_k} \\} dz_k \\end{align}$$ é€™è£¡æˆ‘å€‘ç‚ºäº†æ–¹ä¾¿å®šç¾© $A=\\sum_{i=1}^N e^{x_i}$ $$\\begin{align} =\\int \\exp\\{-z_k+x_k - {\\color{orange}{e^{\\ln A}}} e^{-z_k} \\} dz_k \\\\ = e^{x_k} \\int \\exp\\{-z_k-e^{-z_k + \\ln A}\\} dz_k \\\\ = e^{x_k} \\int \\exp\\{-z_k {\\color{orange}{+\\ln A-\\ln A}} -e^{-z_k + \\ln A}\\} dz_k \\\\ = e^{x_k}\\cdot e^{-\\ln A} \\int \\exp\\{-(z_k-\\ln A)-e^{-(z_k-\\ln A)}\\} dz_k \\\\ = \\frac{e^{x_k}}{A} \\int f(z_k;\\ln A) dz_k \\\\ = \\frac{e^{x_k}}{\\sum_{i=1}^N e^{x_i}} \\end{align}$$ Reference The Humble Gumbel Distribution The Gumbel-Max Trick for Discrete Distributions The Gumbel-Softmax Trick for Inference of Discrete Variables ã€ä¸€æ–‡å­¦ä¼šã€‘Gumbel-Softmaxçš„é‡‡æ ·æŠ€å·§ Categorical Reparameterization with Gumbel-Softmax","tags":[{"name":"Gumbel distribution","slug":"Gumbel-distribution","permalink":"http://yoursite.com/tags/Gumbel-distribution/"},{"name":"Gumbel max trick","slug":"Gumbel-max-trick","permalink":"http://yoursite.com/tags/Gumbel-max-trick/"},{"name":"Gumbel max sampling","slug":"Gumbel-max-sampling","permalink":"http://yoursite.com/tags/Gumbel-max-sampling/"},{"name":"Re-parameterization trick","slug":"Re-parameterization-trick","permalink":"http://yoursite.com/tags/Re-parameterization-trick/"}]},{"title":"Noise Contrastive Estimation (NCE) ç­†è¨˜","date":"2021-06-05T02:15:04.000Z","path":"2021/06/05/Noise-Contrastive-Estimation-NCE-ç­†è¨˜/","text":"ä¹‹å‰è½äººä»‹ç´¹ wav2vec [3] æˆ–æ˜¯çœ‹å…¶ä»–äººçš„æ–‡ç« å¤§éƒ¨åˆ†éƒ½åªæœ‰ä»‹ç´¹ä½œæ³•, ç›´åˆ°æœ‰ä¸€å¤©è‡ªå·±å»çœ‹è«–æ–‡æ‰ç™¼ç¾çœ‹ä¸æ‡‚ CPC [2] (wav2vec ä½¿ç”¨ CPC æ–¹æ³•). å› æ­¤æ‰æ±ºå®šå¥½å¥½è®€ä¸€ä¸‹ä¸¦è¨˜éŒ„. å…ˆå°‡é€™äº›æ–¹æ³•é—œä¿‚æ¢³ç†ä¸€ä¸‹, NCE â€“&gt; CPC (infoNCE) â€“&gt; wav2vec. æ­¤ç¯‡ç­†è¨˜ä¸»è¦ç´€éŒ„ NCE (Noise Contrastive Estimation) åœ¨åš ML æ™‚å¸¸å¸¸éœ€è¦ä¼°è¨ˆæ‰‹ä¸Š training data çš„ distribution $p_d(x)$. è€Œæˆ‘å€‘é€šå¸¸æœƒä½¿ç”¨åƒæ•¸ $\\theta$, ä½¿å¾—åƒæ•¸çš„æ¨¡å‹è·Ÿ $p_d(x)$ ä¸€æ¨£. åœ¨ç¾åœ¨ DNN çµ±æ²»çš„å¹´ä»£å¯èƒ½æœƒèªª, ä¸ç„¶å°±ç”¨ä¸€å€‹ NN ä¾†è¨“ç·´å§, å¦‚ä¸‹åœ–: çµ¦ input $x$, ä¸Ÿçµ¦ NN å¸Œæœ›ç›´æ¥åå‡º $p_\\theta(x)$. ä¸Šåœ–çš„æ¶æ§‹æ˜¯ $x$ å…ˆä¸Ÿçµ¦åƒæ•¸ç‚º $\\theta_f$ çš„ NN, è©² NN æœ€å¾Œä¸€å±¤çš„ outputs å†ä¸Ÿçµ¦åƒæ•¸ç‚º $w$ çš„ linear layer æœ€å¾Œåå‡ºä¸€å€‹ scalar å€¼, è©²å€¼å°±æ˜¯æˆ‘å€‘è¦çš„æ©Ÿç‡.è€Œè¨“ç·´çš„è©±å°±ä½¿ç”¨ MLE (Maximum Likelihood Estimation) ä¾†æ±‚åƒæ•¸ $\\theta$. æ©, å•é¡Œä¼¼ä¹å¾ˆå–®ç´”ä½†çœŸæ­£å¯¦ä½œèµ·ä¾†å»å›°é›£é‡é‡. ä¸€å€‹å•é¡Œæ˜¯ NN outputs è‹¥è¦ä¿æŒ p.d.f. å‰‡å¿…é ˆé softmax, ç¢ºä¿ sum èµ·ä¾†æ˜¯ 1 (ä¹Ÿå°±æ˜¯è¦ç®— $Z_\\theta$). $$\\begin{align} p_\\theta(x)=\\frac{u_\\theta(x)}{Z_\\theta}=\\frac{e^{G(x;\\theta)}}{Z_\\theta} \\\\ \\text{where } Z_\\theta = \\sum_x u_\\theta(x) \\end{align}$$ å¼ (1) ç‚º energy-based model, åœ¨åš NN classification æ™‚, NN çš„ output å°±æ˜¯ $G(x;Î¸)$, ä¹Ÿå°±æ˜¯å¸¸çœ‹åˆ°çš„ logit, ç¶“é softmax å°±ç­‰åŒæ–¼å¼ (1) åœ¨åšçš„äº‹ è€Œåšé€™ä»¶äº‹æƒ…åœ¨ $x$ æ˜¯ discrete space ä½†æ•¸é‡å¾ˆå¤š, ä¾‹å¦‚ NLP ä¸­ LM vocabulary å¾ˆå¤§æ™‚, è¨ˆç®—è³‡æºæœƒæ¶ˆè€—éå¤§.æˆ–æ˜¯ $x$ æ˜¯ continuous space ä½†æ˜¯ç®— $Z_\\theta$ çš„ç©åˆ†æ²’æœ‰å…¬å¼è§£çš„æƒ…å½¢æœƒåšä¸ä¸‹å». (ä¸ç„¶å°±è¦ç”¨ sampling æ–¹æ³•, å¦‚ MCMC) NCE å·§å¦™çš„å°‡æ­¤ MLE å•é¡Œè½‰åŒ–æˆ binary classification å•é¡Œ, å¾è€Œå¾—åˆ°æˆ‘å€‘è¦çš„ MLE è§£. ä¸éåœ¨æ­¤ä¹‹å‰, æˆ‘å€‘å…ˆä¾†çœ‹çœ‹ MLE çš„ gradient é•·ä»€éº¼æ¨£. MLE æ±‚è§£å¯«å‡º likelihood: $$\\begin{align} \\text{likilhood}=\\prod_{x\\sim p_d} p_\\theta(x) \\end{align}$$ Loss å°±æ˜¯ negative log-likelihood $$\\begin{align} -\\mathcal{L}_{mle}=\\mathbb{E}_{x\\sim p_d}\\log p_{\\theta}(x)= \\mathbb{E}_{x\\sim p_d}\\log \\frac{u_\\theta(x)}{Z_\\theta}\\\\ \\end{align}$$ è¨ˆç®—å…¶ gradient: $$\\begin{align} -\\nabla_{\\theta}\\mathcal{L}_{mle}= \\mathbb{E}_{x\\sim p_d} \\left[ \\nabla_{\\theta}\\log{u_\\theta(x)} - \\color{orange}{\\nabla_{\\theta}\\log{Z_\\theta}} \\right] \\\\ \\color{orange}{\\nabla_{\\theta}\\log{Z_\\theta}} = \\frac{1}{Z_\\theta}\\nabla_{\\theta}Z_\\theta = \\frac{1}{Z_\\theta} \\sum_x \\nabla_{\\theta} e^{G(x;\\theta)} \\\\ =\\frac{1}{Z_\\theta} \\sum_x e^{G(x;\\theta)} \\nabla_{\\theta}G(x;\\theta) = \\sum_x \\left[ \\frac{1}{Z_\\theta}e^{G(x;\\theta)} \\right] \\nabla_{\\theta}G(x;\\theta) \\\\ =\\sum_x p_{\\theta}(x) \\nabla_{\\theta} \\log u_{\\theta}(x) = \\mathbb{E}_{x \\sim p_{\\theta}} \\nabla_{\\theta} \\log u_{\\theta}(x) \\\\ \\therefore \\text{ } -\\nabla_{\\theta}\\mathcal{L}_{mle} = \\mathbb{E}_{x\\sim p_d} \\left[ \\nabla_{\\theta} \\log u_{\\theta}(x) - \\color{orange}{\\mathbb{E}_{x \\sim p_{\\theta}} \\nabla_{\\theta} \\log u_{\\theta}(x)} \\right] \\\\ = \\mathbb{E}_{x\\sim p_d} \\nabla_{\\theta} \\log u_{\\theta}(x) - \\mathbb{E}_{x \\sim p_{\\theta}} \\nabla_{\\theta} \\log u_{\\theta}(x)\\\\ = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\nabla_{\\theta} \\log u_{\\theta}(x) \\\\ \\end{align}$$ å¾ (11) å¼å¯ä»¥çœ‹åˆ°, ä¼°è¨ˆçš„ pdf èˆ‡ training data çš„ pdf å·®è¶Šå¤§ gradient æ„ˆå¤§, ç•¶å…©è€…ç›¸åŒæ™‚ gradient ç‚º 0 ä¸ update. Sigmoid or Logistic Functionåœ¨èªªæ˜ NCE ä¹‹å‰å…ˆè«‡ä¸€ä¸‹ sigmoid function. å‡è¨­ç¾åœ¨æˆ‘å€‘åšäºŒåˆ†é¡å•é¡Œ, å…©å€‹é¡åˆ¥ $C=1$ or $C=0$. ä»¤ $p$ æ˜¯æŸå€‹ input $x$ å±¬æ–¼ class 1 çš„æ©Ÿç‡ (æ‰€ä»¥ $1-p$ å°±æ˜¯å±¬æ–¼ class 0 çš„æ©Ÿç‡)å®šç¾© log-odd ç‚º (å…¶å¯¦ä¹Ÿç¨±ç‚º logit): $$\\begin{align} \\text{log-odd} = \\log \\frac{p}{1-p} \\end{align}$$ æˆ‘å€‘çŸ¥é“ sigmoid function $\\sigma(x)=\\frac{1}{1+e^{-x}}$ å°‡å¯¦æ•¸ input mapping åˆ° 0 ~ 1 å€é–“çš„å‡½å¼. è‹¥æˆ‘å€‘å°‡ log-odd ä»£å…¥æˆ‘å€‘å¾ˆå®¹æ˜“å¾—åˆ°: $$\\begin{align} \\sigma(\\text{log-odd})=...=p \\end{align}$$ ç™¼ç¾ sigmoid å›å‚³çµ¦æˆ‘å€‘çš„æ˜¯ $x$ å±¬æ–¼ class 1 çš„æ©Ÿç‡å€¼, i.e. $\\sigma(\\text{log-odd})=p(C=1|x)$. æ‰€ä»¥åœ¨äºŒåˆ†é¡å•é¡Œä¸Š, æˆ‘å€‘å°±æ˜¯è¨“ç·´ä¸€å€‹ NN èƒ½ predict logit å€¼. NCE çš„ Network æ¶æ§‹é¦–å…ˆ NCE å¼•å…¥äº†ä¸€å€‹ Noise distribution $q(x)$. è«–æ–‡æåˆ°è©² $q$ åªè¦æ»¿è¶³ç•¶ $p_d(x)$ nonzero å‰‡ $q(x)$ ä¹Ÿå¿…é ˆ nonzero å°±å¯ä»¥. äºŒåˆ†é¡å•é¡Œç‚º, å‡è¨­è¦å–ä¸€å€‹æ­£ä¾‹ (class 1), å°±å¾ training data pdf $p_d(x)$ å–å¾—. è€Œè‹¥è¦å–ä¸€å€‹åä¾‹ (class 0) å‰‡å¾ noise pdf $q(x)$ å–å¾—.æˆ‘å€‘å¯ä»¥å– $N_p$ å€‹æ­£ä¾‹ä»¥åŠ $N_n$ å€‹åä¾‹, ä»£è¡¨ prior ç‚º: $$\\begin{align} p(C=1)=\\frac{N_p}{N_p+N_n} \\\\ p(C=0)=1-p(C=1) \\\\ \\end{align}$$ å› æ­¤å°±å¯ä»¥å¾—åˆ°ä¸€å€‹ batch å…± $N_p+N_n$ å€‹ samples, ä¸Ÿå…¥ä¸‹åœ–çš„ NN structure åšäºŒåˆ†é¡å•é¡Œ: Network å‰åŠæ®µé‚„æ˜¯è·ŸåŸä¾†çš„ MLE æ¶æ§‹ä¸€æ¨£, åªæ˜¯æˆ‘å€‘æœŸæœ› $NN_{\\theta}$ åå‡ºä¾†çš„æ˜¯ logit, ç”±ä¸Šé¢ä¸€å€‹ section æˆ‘å€‘çŸ¥é“ç¶“é sigmoid å¾—åˆ°çš„æœƒæ˜¯ $x$ å±¬æ–¼ class 1 çš„æ©Ÿç‡. å› æ­¤å¾ˆå®¹æ˜“å°±ç”¨ xent loss å„ªåŒ–. ç¥å¥‡çš„ä¾†äº†, NCE å‘Šè¨´æˆ‘å€‘, optimize é€™å€‹äºŒåˆ†é¡å•é¡Œå¾—åˆ°çš„ $\\theta$ ç­‰æ–¼ MLE è¦æ‰¾çš„ $\\theta$! $$\\begin{align} \\theta_{nce} = \\theta_{mle} \\end{align}$$ ä¸” NN è¨ˆç®—çš„ logit ç›´æ¥å°±è®Šæˆ MLE è¦ç®—çš„ $p_{\\theta}(x)$. åŒæ™‚è—‰ç”±æ›æˆäºŒåˆ†é¡å•é¡Œ, ä¹Ÿé¿é–‹äº†å¾ˆé›£è¨ˆç®—çš„ $Z_{\\theta}$ å•é¡Œ.ç‚ºäº†ä¸å½±éŸ¿é–±è®€æµæš¢åº¦, æ¨å°éç¨‹è«‹åƒç…§ Appendix æ‰€ä»¥æˆ‘å€‘å¯ä»¥é€éå¼•å…¥ä¸€å€‹ Noise pdf ä¾†é”åˆ°ä¼°è¨ˆ training data çš„ generative model äº†. é€™ä¹Ÿæ˜¯ç‚ºä»€éº¼å«åš Noise Contrastive Estimation. Representationç”±æ–¼é€é NCE è¨“ç·´æˆ‘å€‘å¯ä»¥å¾—åˆ° $\\theta$, æ­¤æ™‚åªéœ€è¦ç”¨ $\\theta_f$ çš„ NN ä¾†ç•¶ä½œ feature extractor å°±å¯ä»¥äº†. ç¸½çµæœ€å¾Œæµç¨‹å¯ä»¥ç¸½çµæˆä¸‹é¢é€™å¼µåœ–: æœ€å¾ŒèŠä¸€ä¸‹ CPC (Contrastive Predictive Coding) [2]. æˆ‘è¦ºå¾—è·Ÿ NCE å°±å…©é»ä¸åŒ: æˆ‘å€‘ç•«çš„ NCE åœ–è£¡çš„ $w$, æ”¹æˆè«–æ–‡è£¡çš„ $c_t$, æ‰€ä»¥è®Šæˆ network æ˜¯ä¸€å€‹ conditioned çš„ network ä¸æ˜¯ä¸€å€‹äºŒåˆ†é¡å•é¡Œ, æ”¹æˆ N é¸ 1 çš„åˆ†é¡å•é¡Œ (batch size $N$, æŒ‡å‡ºå“ªä¸€å€‹æ˜¯æ­£ä¾‹), å› æ­¤ç”¨ categorical cross-entorpy ç•¶ loss æ‰€ä»¥æ–‡ç« ç¨±é€™æ¨£çš„ loss ç‚º infoNCE loss åŒæ™‚ CPC [2] è«–æ–‡ä¸­å¾ˆæ£’çš„ä¸€é»æ˜¯å°‡é€™æ¨£çš„è¨“ç·´æ–¹å¼ä¹Ÿè·Ÿ Mutual Information (MI) é€£æ¥èµ·ä¾†.è­‰æ˜äº†æœ€å°åŒ– infoNCE loss å…¶å¯¦å°±æ˜¯åœ¨æœ€å¤§åŒ– representation èˆ‡æ­£ä¾‹çš„ MI (çš„ lower bound). é€™äº›èƒŒå¾Œæ•¸å­¸æ’èµ·äº†æ•´å€‹åˆ©ç”¨ CPC åœ¨ SSL (Self-Supervised Learning) çš„åŸºç¤. ç°¡å–®è¬›å°±æ˜¯ä¸éœ€è¦æ˜‚è²´çš„ label å…¨éƒ¨éƒ½ unsupervised å°±èƒ½å­¸åˆ°å¾ˆå¥½çš„ representation.è€Œè¿‘æœŸ facebook æ›´åˆ©ç”¨ SSL å­¸åˆ°çš„å¥½ representation çµåˆ GAN åœ¨ ASR é”åˆ°äº† 19 å¹´çš„ STOA WER. è«–æ–‡: Unsupervised Speech Recognition or see [9] SSL å¥½æ±è¥¿, ä¸è©¦è©¦çœ‹å—? AppendixPrior pdf:$$\\begin{align} p(C=1)=\\frac{N_p}{N_p+N_n} \\\\ p(C=0)=1-p(C=1) \\\\ \\end{align}$$ Generative pdf:$$\\begin{align} p(x|C=1)=p_{\\theta}(x) \\\\ p(x|C=0)=q(x) \\end{align}$$ å› æ­¤ Posterior pdf:$$\\begin{align} p(C=1|x)=\\frac{p(C=1)p(x|C=1)}{p(C=1)p(x|C=1)+p(C=0)p(x|C=0)}=\\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_r q(x)} \\\\ p(C=0|x)=\\frac{p(C=0)p(x|C=0)}{p(C=1)p(x|C=1)+p(C=0)p(x|C=0)}=\\frac{N_r q(x)}{p_{\\theta}(x)+N_r q(x)} \\\\ \\end{align}$$å…¶ä¸­ $N_r=\\frac{N_n}{N_p}$ å› æ­¤ likelihood ç‚º:$$\\begin{align} \\text{likilhood}=\\prod_{t=1}^{N_p} p(C_t=1|x_t) \\cdot \\prod_{t=1}^{N_n} p(C_t=0|x_t) \\end{align}$$ Loss ç‚º negative log-likelihood:$$\\begin{align} - \\mathcal{L}_{nce} = \\sum_{t=1}^{N_p} \\log p(C_t=1|x_t) + \\sum_{t=1}^{N_n} \\log p(C_t=0|x_t) \\\\ = N_p \\left[ \\frac{1}{N_p} \\sum_{t=1}^{N_p} \\log p(C_t=1|x_t) \\right] + N_n \\left[ \\frac{1}{N_n} \\sum_{t=0}^{N_n} \\log p(C_t=0|x_t) \\right] \\\\ \\propto \\left[ \\frac{1}{N_p} \\sum_{t=1}^{N_p} \\log p(C_t=1|x_t) \\right] + N_r \\left[ \\frac{1}{N_n} \\sum_{t=0}^{N_n} \\log p(C_t=0|x_t) \\right] \\end{align}$$ ç•¶å›ºå®š $N_r$ ä½†æ˜¯è®“ $N_p\\rightarrow\\infty$ and $N_n\\rightarrow\\infty$. æ„å‘³è‘—æˆ‘å€‘å›ºå®šæ­£è² æ¨£æœ¬æ¯”ä¾‹, ä½†å–ç„¡çª®å¤§çš„ batch. é‡å¯«ä¸Šå¼æˆ:$$\\begin{align} - \\mathcal{L}_{nce} = \\mathbb{E}_{x\\sim p_d} \\log p(C=1|x) + N_r \\mathbb{E}_{x\\sim q} \\log p(C=0|x) \\\\ \\therefore \\text{} -\\nabla_{\\theta}\\mathcal{L}_{nce} = \\nabla_{\\theta}\\left[ \\mathbb{E}_{x\\sim p_d} \\log \\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_rq(x)} + N_r\\mathbb{E}_{x\\sim q} \\log \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\right] \\\\ = \\mathbb{E}_{x\\sim p_d} \\color{orange}{\\nabla_{\\theta} \\log \\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_rq(x)}} + N_r \\mathbb{E}_{x\\sim q} \\color{green}{\\nabla_{\\theta} \\log \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} } \\end{align}$$ è¨ˆç®—æ©˜è‰²å’Œç¶ è‰²å…©é …, ä¹‹å¾Œå†ä»£å›ä¾†: $$\\begin{align} \\color{orange}{\\nabla_{\\theta} \\log \\frac{p_{\\theta}(x)}{p_{\\theta}(x)+N_rq(x)}} = \\nabla_{\\theta}\\log\\frac{1}{1+N_r\\frac{q(x)}{p_{\\theta}(x)}} = -\\nabla_{\\theta}\\log \\left( 1+\\frac{N_rq(x)}{p_{\\theta}(x)} \\right) \\\\ = -\\frac{1}{1+\\frac{N_rq(x)}{p_{\\theta}(x)}}\\nabla_{\\theta}\\frac{N_rq(x)}{p_{\\theta}(x)} = -\\frac{N_rq(x)}{1+\\frac{N_rq(x)}{p_{\\theta}(x)}}\\nabla_{\\theta}\\frac{1}{p_{\\theta}(x)} \\\\ = -\\frac{N_rq(x)}{1+\\frac{N_rq(x)}{p_{\\theta}(x)}} \\frac{-1}{p_{\\theta}^2(x)} \\nabla_{\\theta} p_{\\theta}(x) \\\\ = \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\left[ \\frac{1}{p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) \\right] \\\\ = \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta} \\log p_{\\theta}(x) \\end{align}$$ $$\\begin{align} \\color{green}{\\nabla_{\\theta} \\log \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)}} = -\\nabla_{\\theta} \\log\\left( 1+\\frac{p_{\\theta}(x)}{N_rq(x)} \\right) = -\\frac{1}{1+\\frac{p_{\\theta}(x)}{N_rq(x)}} \\nabla_{\\theta} \\frac{p_{\\theta}(x)}{N_rq(x)} \\\\ = -\\frac{1}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) \\\\ = -\\frac{p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\left[ \\frac{1}{p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) \\right] \\\\ = -\\frac{p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} \\log p_{\\theta}(x) \\end{align}$$ å°‡ (34), (38) ä»£å›å» (29) å¾—åˆ°: $$\\begin{align} - \\nabla_{\\theta}\\mathcal{L}_{nce} = \\mathbb{E}_{x\\sim p_d} {\\color{orange}{\\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta} \\log p_{\\theta}(x)}} - N_r \\mathbb{E}_{x\\sim q} {\\color{green}{\\frac{p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} \\log p_{\\theta}(x)}} \\\\ = \\sum_x \\left[ p_d(x) \\frac{N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta} \\log p_{\\theta}(x) \\right] - \\sum_x \\left[ q(x) \\frac{N_r p_{\\theta}(x)}{N_rq(x)+p_{\\theta}(x)} \\nabla_{\\theta} \\log p_{\\theta}(x)\\right] \\\\ = \\sum_x \\frac{(p_d(x)-p_{\\theta}(x))N_rq(x)}{p_{\\theta}(x)+N_rq(x)} \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ = \\sum_x \\frac{(p_d(x)-p_{\\theta}(x))q(x)}{\\frac{p_{\\theta}(x)}{N_r}+q(x)} \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ \\end{align}$$ ç•¶ $N_r\\rightarrow\\infty$ æ„å‘³è‘—æˆ‘å€‘è®“è² æ¨£æœ¬é å¤šæ–¼æ­£æ¨£æœ¬, ä¸Šå¼è®Šæˆ:$$\\begin{align} \\lim_{N_r\\rightarrow\\infty} - \\nabla_{\\theta}\\mathcal{L}_{nce} = \\sum_x \\frac{(p_d(x)-p_{\\theta}(x))q(x)}{0+q(x)} \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ = \\sum_x (p_d(x)-p_{\\theta}(x)) \\nabla_{\\theta}\\log p_{\\theta}(x) \\\\ = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\left( \\nabla_{\\theta}\\log u_{\\theta}(x) -\\nabla_{\\theta}\\log Z_{\\theta} \\right) \\end{align}$$ æ­¤æ™‚æˆ‘å€‘ç™¼ç¾é€™ gradient ä¹Ÿèˆ‡ Noise pdf $q(x)$ ç„¡é—œäº†! æœ€å¾Œæˆ‘å€‘å°‡ MLE and NCE çš„ gradient æ‹‰å‡ºä¾†å°æ¯”ä¸€ä¸‹:$$\\begin{align} -\\nabla_{\\theta}\\mathcal{L}_{mle} = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\nabla_{\\theta} \\log u_{\\theta}(x) \\\\ -\\nabla_{\\theta}\\mathcal{L}_{nce} = \\sum_x \\left[ p_d(x) - p_{\\theta}(x) \\right] \\left( \\nabla_{\\theta}\\log u_{\\theta}(x) -\\nabla_{\\theta}\\log Z_{\\theta} \\right) \\end{align}$$ æˆ‘å€‘ç™¼ç¾ MLE and NCE åªå·®åœ¨ä¸€å€‹ normalization factor (or partition) $Z_{\\theta}$.æœ€é­”è¡“çš„åœ°æ–¹å°±åœ¨æ–¼ NCE è«–æ–‡ [1] è­‰æ˜æœ€ä½³è§£æœ¬èº«çš„ logit å·²ç¶“æ˜¯ probability å‹å¼, å› æ­¤ä¹Ÿä¸éœ€è¦ normalize factor. è«–æ–‡è£¡èªªç¤™æ–¼ç¯‡å¹…æ²’çµ¦å‡ºè­‰æ˜, ä¸»è¦æ˜¯ä¾†è‡ª Theorem 1 çš„çµæœ: æ‰€ä»¥æˆ‘å€‘ä¸å¦¨å°‡ $Z_{\\theta}=1$, çµæœæœ‰: $$\\begin{align} \\color{red} {\\nabla_{\\theta}\\mathcal{L}_{mle} = \\nabla_{\\theta}\\mathcal{L}_{nce}} \\\\ \\color{red} {\\Rightarrow \\theta_{mle} = \\theta_{nce}} \\\\ \\end{align}$$ Reference 2010: Noise-contrastive estimation: A new estimation principle for unnormalized statistical models 2019 DeepMind infoNCE/CPC: Representation learning with contrastive predictive coding 2019 FB: wav2vec: Unsupervised pre-training for speech recognition 2020 MIT &amp; Google: Contrastive Representation Distillation Noise Contrastive Estimation å‰ä¸–ä»Šç”Ÿâ€”â€”ä» NCE åˆ° InfoNCE â€œå™ªå£°å¯¹æ¯”ä¼°è®¡â€æ‚è°ˆï¼šæ›²å¾„é€šå¹½ä¹‹å¦™ [è¯‘] Noise Contrastive Estimation The infoNCE loss in self-supervised learning High-performance speech recognition with no supervision at all","tags":[{"name":"Noise Contrastive Estimation","slug":"Noise-Contrastive-Estimation","permalink":"http://yoursite.com/tags/Noise-Contrastive-Estimation/"},{"name":"NCE","slug":"NCE","permalink":"http://yoursite.com/tags/NCE/"},{"name":"infoNCE","slug":"infoNCE","permalink":"http://yoursite.com/tags/infoNCE/"}]},{"title":"Distributed Data Parallel and Its Pytorch Example","date":"2020-12-20T04:19:38.000Z","path":"2020/12/20/Distributed-Data-Parallel-and-Its-Pytorch-Example/","text":"è¨“ç·´æ™‚å€™çš„å¹³è¡ŒåŒ–å¯åˆ†ç‚º: Model Parallel: æ‰€æœ‰ GPUs è·‘åŒä¸€å€‹ batch ä½†æ˜¯å„è‡ªè·‘æ¨¡å‹ä¸åŒéƒ¨åˆ† Data Parallel: GPUs è·‘ä¸åŒçš„ batches, ä½†è·‘åŒä¸€å€‹å®Œæ•´çš„æ¨¡å‹ ç”±æ–¼ Data Parallel è·‘åŒä¸€å€‹å®Œæ•´æ¨¡å‹ä¸”å„ GPU éƒ½ç”¨è‡ªå·±è¤‡è£½çš„ä¸€ä»½, åœ¨ update åƒæ•¸æ™‚è¦å¦‚ä½•ç¢ºä¿æ›´æ–°ä¸€è‡´? å¯åˆ†ç‚º synchronous å’Œ asynchronous update. (æ–‡ç« å¾Œé¢æœƒè©³ç´°è¨è«–) æœ¬æ–‡è¨è«– Data Parallel with asynchronous update. æ—¢ç„¶è¦åš data parallel, ç¬¬ä¸€ä»¶äº‹æƒ…ä¾¿æ˜¯å¦‚ä½•å°ä¸åŒ GPU åˆ†æ´¾ä¸åŒçš„ batches, æ¥ä¸‹ä¾†æˆ‘å€‘å°±ä½¿ç”¨ PyTorch åšé€™ä»¶äº‹. æŒ‡æ´¾ä¸åŒ Batch çµ¦ä¸åŒ GPUç›´æ¥ä¸Šä¸€å€‹ toy example (minimal_distributed_data_example.py) 123456789101112131415161718192021222324252627282930313233343536# file: minimal_distributed_data_example.pyimport ...class SimpleDataset(torch.utils.data.Dataset): def __init__(self, start, end): assert(start &lt; end) self.start, self.end, self.data_num = start, end, end - start def __len__(self): return self.data_num def __getitem__(self, idx): return idx + self.startif __name__ == '__main__': # ===== Distributed Settings world_size = int(os.environ.get('WORLD_SIZE', 1)) local_rank = 0 is_distributed = world_size &gt; 1 if is_distributed: torch.distributed.init_process_group(backend='nccl') local_rank = torch.distributed.get_rank() torch.cuda.set_device(local_rank) device = torch.device(\"cuda\", local_rank) # ===== Dataset/DataLoader Settings dataset = SimpleDataset(0, 4*6) sampler = DistributedSampler(range(4*6), shuffle=False, seed=1111) # Shuffle here (set True) if needed rather than in DataLoader print(f'========== device:&#123;device&#125;') data_parallel_dl = DataLoader(dataset, batch_size=4, num_workers=8, shuffle=False, sampler=sampler) # since we use sampler, so we set shuffle to False (default) in DataLoader # ===== Traverse All Data arr = [] for sample_batch in data_parallel_dl: arr += sample_batch.tolist() t = np.random.randint(100)/100.0 sample_batch.to(device) print('sleep &#123;:.2f&#125;; device:&#123;&#125;\\t&#123;&#125;'.format(t, device, sample_batch)) time.sleep(t) print(f'device:&#123;device&#125;\\n&#123;np.sort(np.array(arr))&#125;') [Line 23~27 æœ‰é—œ Dataset/DataLoader] Line 24 dataset åªæ˜¯ä¸€å€‹ 0 åˆ° 23 çš„ int list. Line 27 DataLoader åœ¨åˆ†é… batches çµ¦ä¸åŒ GPUs æ™‚åªéœ€è¦å°‡ sampler ä½¿ç”¨ DistributedSampler å‰µå»ºå°±å¯ä»¥. DistributedSampler åœ¨åˆ†é…ä¸€å€‹ batch é™¤äº†æœƒæŒ‡å®šè³‡æ–™æ˜¯é‚£äº› index ä¹‹å¤–, é‚„æœƒæŒ‡å®šè©²ç­† batch æ˜¯è¦åˆ†åˆ°å“ªå€‹ gpu. [Line 14~22 æœ‰é—œ Distributed Settings]åœ¨åŸ·è¡Œé€™å€‹æª”æ¡ˆçš„æ™‚å€™, æˆ‘å€‘æœƒä½¿ç”¨ torch.distributed.launch, ç¯„ä¾‹æŒ‡ä»¤å¦‚ä¸‹: 1CUDA_VISIBLE_DEVICES=2,3 python -m torch.distributed.launch --nproc_per_node=2 --use_env minimal_distributed_data_example.py æ­¤æ™‚ PyTorch æœƒé–‹å•Ÿå…©å€‹ processes å»åŸ·è¡Œä½ çš„ .py, é€™è£¡æ³¨æ„ä¸æ˜¯ threads, é€™æ˜¯å› ç‚º python Global Interpreter Lock (GIL) çš„åŸå› , ä½¿ç”¨ thread æ•ˆç‡æœƒä¸é«˜. å¦å¤–ä½¿ç”¨ --use_env å‰‡æœƒåœ¨å„è‡ªçš„ process è£¡è¨­å®šç’°å¢ƒè®Šæ•¸: WORLD_SIZE (ç¯„ä¾‹ = 2) LOCAL_RANK (ç¯„ä¾‹ = 0 or 1) å› æ­¤ line 17 æˆ‘å€‘ä¾¿å¯è—‰ç”± world_size å¾—çŸ¥æ˜¯å¦ç‚º distributed ç’°å¢ƒ. æ˜¯çš„è©± line 20 å°±å¯ä»¥æ‹¿åˆ°é€™å€‹ process çš„ local_rank (å¯ä»¥æƒ³æˆæ˜¯ worker çš„ç·¨è™Ÿ, ä¹Ÿå°±æ˜¯ç¬¬å¹¾å€‹å¹³è¡Œçš„å–®ä½), æ¥è‘— line 21, 22 å°±å¯ä»¥æ ¹æ“š local_rank è¨­ç½® gpu. [Line 28~36 æœ‰é—œ go through all data] åœ¨åŸ·è¡Œæ™‚, å„å€‹ process æœƒæ‹¿åˆ°ç›¸å°æ‡‰å€‹ batches. Line 35 æ¨¡æ“¬è™•ç†è©²ç­†è³‡æ–™æ‰€èŠ±çš„æ™‚é–“. Line 36 ç‚ºç¢ºèªè‡ªå·±é€™å€‹ process ç¸½å…±æ‹¿åˆ°é‚£äº› batches. ä»¥ç¯„ä¾‹ä¾†èªª, å…©å€‹ gpus æ‡‰è©²è¦æ‹¿åˆ° exclusive çš„å…©å€‹ sets å…¶è¯é›†æ˜¯ {0,1, â€¦, 23}. çµæœå¦‚ä¸‹: Good Job! ç¾åœ¨æˆ‘å€‘æœƒæŠŠæ¯å€‹ GPU éƒ½åˆ†é…ä¸åŒçš„ batches äº†, ä¸éé‚„æœ‰ä¸€å€‹é—œéµçš„å•é¡Œ: è©²æ€éº¼å„è‡ªè¨ˆç®— gradients ç„¶å¾Œ update? é€™å°±é–‹å§‹è¨è«– update çš„å…©ç¨® case, synchronous and asynchronous update. Asynchronous Update Synchronous: æ¯ä¸€æ¬¡æ¨¡å‹ update è¦ç­‰åˆ°æ‰€æœ‰ device çš„ batch éƒ½çµæŸ, çµ±åˆå¾Œ update Asynchronous: æ¯å€‹ device ç®—å®Œè‡ªå·±çš„ batch å¾Œå³å¯ç›´æ¥ update å¯ä»¥æƒ³åƒéåŒæ­¥çš„åŒ–å¯ä»¥æ›´æ–°çš„æ¯”è¼ƒæœ‰æ•ˆç‡, ä½†å¯èƒ½æ•ˆæœæœƒä¸å¦‚åŒæ­¥çš„æ–¹å¼.Asynchronous æœƒé‡åˆ°çš„ç‹€æ³æ˜¯ç®—å®Œ gradient å¾Œè¦ update parameters æ™‚, parameters å·²ç¶“è¢«å…¶ä»– process update éäº†, é‚£ç‚ºä»€éº¼é‚„å¯ä»¥ work? Asynchronous ç‹€æ³ 1ç¯„ä¾‹å‡è¨­å…©å€‹ GPU (1&amp;2) å…¶åƒæ•¸ç©ºé–“éƒ½åœ¨ $\\theta_a$. Step 1. å‡è¨­ GPU2 å…ˆç®—å®Œ $\\Delta P_2(\\theta_a)$ ä¸¦ä¸” update åˆ° $\\theta_b$: $$\\begin{align} \\theta_b = \\theta_a + \\Delta P_2(\\theta_a) \\end{align}$$ Step2. é€™æ™‚å€™ GPU1 ç®—å®Œ gradient äº†, ç”±æ–¼ç•¶æ™‚ç®— gradient æ˜¯åŸºæ–¼ $\\theta_a$, å› æ­¤ gradient ç‚º $\\Delta P_1(\\theta_a)$, ä½†æ˜¯è¦ update çš„æ™‚å€™ç”±æ–¼å·²ç¶“è¢« GPU2 æ›´æ–°åˆ° $\\theta_b$ äº†, æ‰€ä»¥æœƒæ›´æ–°åˆ° $\\theta_c$: $$\\begin{align} \\theta_c = \\theta_b + \\Delta P_1(\\theta_a) \\end{align}$$ é€™è£¡è®€è€…å¯èƒ½æœƒç–‘å•, è¨ˆç®— gradient èˆ‡ update æ™‚æ ¹æ“šçš„åƒæ•¸æ˜¯ä¸åŒ, é€™æ¨£ update æœƒä¸æœƒå‡ºå•é¡Œ? ä»¥ä¸Šé¢é€™å€‹ä¾‹å­ä¾†èªª, é‚„å‰›å¥½æ²’äº‹. åŸå› æ˜¯å…¶å¯¦ç­‰åŒæ–¼ synchronous update: $$\\begin{align} \\theta_c = \\theta_a + \\left[ \\Delta P_2(\\theta_a) + \\Delta P_1(\\theta_a) \\right] \\end{align}$$ é‚£å¯èƒ½æœƒç¹¼çºŒå•, é€™åªæ˜¯å‰›å¥½, å¦‚æœä¸€å€‹ GPU æ¯”å¦ä¸€å€‹æ…¢å¾ˆå¤š, æœƒæ€æ¨£? æˆ‘å€‘çœ‹çœ‹ case 2 Asynchronous ç‹€æ³ 2GPU2 å¤ªå¿«äº†â€¦ å·²ç¶“ update å¥½å¹¾è¼ª å¥½å§â€¦ æƒ³æˆé¡ä¼¼æœ‰ momentum æ•ˆæœå§ å¯¦å‹™ä¸Šæœƒåœ¨å¹¾æ¬¡çš„ update éå¾Œå¼·åˆ¶ synchronize update ä¸€æ¬¡, å¯ä»¥æƒ³åƒå¦‚æœä¸€äº›æ¢ä»¶æˆç«‹ (è­¬å¦‚ gradients æ˜¯ bounded), æ‡‰è©²èƒ½ä¿è­‰æ”¶æ–‚ (é€™é‚Šæˆ‘æ²’åšåŠŸèª²é˜¿, ç´”ç²¹çŒœæ¸¬) Synchronous Updateæ¯å€‹ gpu éƒ½ç®—å®Œå„è‡ª batch çš„ gradients å¾Œ, çµ±ä¸€æ•´ç† update parameters, å¸¸è¦‹å…©ç¨®æ–¹å¼: Parameter Server Ring Allreduce æ¥è‘—ä»‹ç´¹çš„é€™å…©ç¨®æ–¹æ³•åœ–ç‰‡ä¸»è¦å¾ Baidu: Bringing HPC techniques to deep learning [Andrew Gibiansky] ç­†è¨˜ä¸‹ä¾†. Parameter Server çš„ Synchronous Updateä¸€æ¬¡ Update åˆ†å…©æ­¥é©Ÿ GPU 0 å…¨éƒ¨éƒ½æ‹¿åˆ° GPU 1~4 çš„ Gradients å¾Œ, æ›´æ–° parameters GPU 0 æŠŠ model ç™¼é€çµ¦ GPU 1~4 å‡è¨­æœ‰ $N$ å€‹ GPU, é€šä¿¡ä¸€æ¬¡èŠ±è²»æ™‚é–“ $K$, å‰‡ PS æ–¹æ³•æˆæœ¬ç‚º: Gradients passing: $(N-1)K$ Model passing: $(N-1)K$ Total $2K(\\color{orange}{N}-1)$, è·Ÿ GPU æ•¸é‡æ­£æ¯” Ring Allreduce æ¯”è¼ƒå¤šåœ–, ç‰¹åˆ¥æ‹‰å‡ºä¸€å€‹ section èªªæ˜ Ring Allreduce çš„ Synchronous Updateæ¯ä¸€å€‹ GPU éƒ½åˆ†åˆ¥æœ‰ä¸€å€‹å‚³é€å’Œæ¥æ”¶çš„å°è±¡ GPU, åˆ†é…èµ·ä¾†æ­£å¥½å½¢æˆä¸€å€‹ç’°. å‡è¨­æ¯å€‹ GPUs éƒ½ç®—å¥½ gradients äº†, ä¸¦ä¸”æˆ‘å€‘å°‡ gradients åˆ†æˆè·Ÿ GPU æ•¸é‡ä¸€æ¨£çš„ $N$ å€‹ chunks: é€™æ–¹æ³•åˆ†å…©æ­¥é©Ÿ: Scatter Reduce All Gather 1. Scatter Reduce åšå®Œ $N-1$ æ¬¡ iteration å¾Œå¯ä»¥ç™¼ç¾æ¯å¼µ GPU éƒ½æœƒæœ‰ä¸€å€‹æ˜¯å®Œæ•´çš„ chunk. 2. All Gather åšå®Œ $N-1$ æ¬¡ iteration å¾Œå¯ä»¥ç™¼ç¾æ¯å¼µ GPU éƒ½æ‹¿åˆ°æ‰€æœ‰å®Œæ•´çš„ chunk. All Gather æµç¨‹è·Ÿ Scatter Reduce æ˜¯ä¸€æ¨£, åªæ˜¯å°‡ç´¯åŠ è¡Œç‚ºè®Šæˆå–ä»£è€Œå·². æˆæœ¬æ¯å€‹ GPUs éƒ½å¾—åˆ°çµ±åˆå¾Œçš„ gradients, å› æ­¤ å„å€‹ GPU ä¸Šçš„ model å¯ä»¥å„è‡ª update (gradients ç›¸åŒ, æ‰€ä»¥ update å¾Œçš„ models ä¹Ÿç›¸åŒ) å‡è¨­æœ‰ $N$ å€‹ GPU,å‰‡æˆæœ¬ç‚º: é€šä¿¡ä¸€æ¬¡èŠ±è²»æ™‚é–“ $K/N$ (å› ç‚ºæˆ‘å€‘åˆ†æˆ $N$ å€‹ chunks åŒæ™‚å‚³è¼¸) Scatter reduce: $(N-1)K/N$ All gather: $(N-1)K/N$ Total $2K(\\color{orange}{N}-1)/\\color{orange}{N}$, è·Ÿ GPU æ•¸é‡ç„¡é—œ PyTorch: Model with DDPé‚„è¨˜å¾—æœ€é–‹é ­çš„ç¯„ä¾‹å—? æˆ‘å€‘åšåˆ°äº†æŠŠæ¯å€‹ GPU éƒ½åˆ†é…ä¸åŒçš„ batches, ä½†é‚„ä¸æœƒå°‡å„è‡ªè¨ˆç®— gradients çµ±åˆç„¶å¾Œ update. å…¶å¯¦æˆ‘å€‘åªéœ€è¦é‡å°ä¸Šé¢ç¯„ä¾‹çš„ minimal_distributed_data_example.py åšé»ä¿®æ”¹å°±å¯ä»¥. é‡å° model ä½œå¦‚ä¸‹æ”¹å‹•: 1model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank) é€™æ¨£å°±ä½¿å¾— model çš„ backward() æˆç‚º sync op. ä¹Ÿå°±æ˜¯åœ¨å‘¼å« loss.backward() æœƒç­‰åˆ°æ¯å¼µ GPU çš„ gradient éƒ½ç®—å®Œä¸” sync äº† (PS or All Gather éƒ½å¯ä»¥) æ‰æœƒæ¥ä¸‹å»åŸ·è¡Œ. æ³¨æ„äº‹é … ç”±æ–¼æ¯å€‹ process éƒ½æœ‰è‡ªå·±çš„ optimizer(scheduler), è€Œ momentum æœƒæ ¹æ“šç•¶å‰çš„ gradient update, å¦‚ä½•ç¢ºä¿æ¯å€‹ optimizers éƒ½ç›¸åŒ?Ans: ç”±æ–¼ .backward() æ˜¯ sync op, å› æ­¤ opt.step() æ™‚æ¯å€‹ processes çš„ gradients å·²ç¶“åŒæ­¥äº†, æ‰€ä»¥ momentum æœƒæ ¹æ“šç›¸åŒçš„ gradient update Batch-norm çš„ statistics åŒæ­¥?Ans: See torch.nn.SyncBatchNorm Save checkpoint æ™‚åœ¨ä¸€å¼µå¡ä¸Šå­˜å°±å¯ä»¥ (é€šå¸¸ç”¨ LOCAL_RANK=0 çš„é‚£å€‹ process) æ€éº¼ç¢ºä¿æ¯å€‹ process ä¸Šçš„ model random initial ç›¸åŒçš„ weights?Ans: DistributedDataParallel åœ¨ init æ™‚å°±æœƒç¢ºä¿ parameters/buffers sync éäº†, see here model ç¶“é DistributedDataParallel åŒ…éå¾Œ name æœƒå¤šä¸€å€‹å‰ç¶´ module., å¦‚æœè¨“ç·´å’ŒåŠ è¼‰æ¨¡å‹ä¸€å€‹ä½¿ç”¨ DDP ä¸€å€‹æ²’æœ‰ load_state_dict æœ‰å¯èƒ½æœƒå› æ­¤å‡ºéŒ¯, éœ€è‡ªè¡Œè™•ç† ä¸€äº› metrics å¦‚ accuracy/loss ç”±æ–¼åœ¨å„å€‹ GPUs è¨ˆç®—, å¯ä»¥åˆ©ç”¨ torch.distributed.all_reduce, torch.distributed.all_gather ç­‰ä¾† syncSee DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED æœ‰ä¸€å€‹ä¸éŒ¯çš„ DDP ç¯„ä¾‹ [2] å¦‚æœå¯ä»¥çš„è©±, æ¨è–¦ä½¿ç”¨ PyTorch Lightning, ç›´æ¥å¹«ä½ æŠŠé€™äº›ç¹ç‘£çš„ç´°ç¯€åŒ…å¥½, å‘Šè¨´å®ƒè¦ç”¨å¹¾å¼µ GPUs å°±çµæŸäº†. Reference[1] Bringing HPC Techniques to Deep Learning[2] A good example of DDP in PyTorch","tags":[{"name":"Distributed Data Parallel (DDP)","slug":"Distributed-Data-Parallel-DDP","permalink":"http://yoursite.com/tags/Distributed-Data-Parallel-DDP/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://yoursite.com/tags/PyTorch/"}]},{"title":"Quantization çš„é‚£äº›äº‹","date":"2020-10-03T01:35:24.000Z","path":"2020/10/03/Quantization-çš„é‚£äº›äº‹/","text":"NN åœ¨åš quantization æ™‚æ¡ç”¨çš„æ˜¯éå°ç¨±çš„æ–¹å¼, real ($r$) å’Œ quantized ($q$) values å°æ‡‰é—œä¿‚å¦‚ä¸‹: å…¶ä¸­ zero point $Z$ æœƒè·Ÿ $q$ ç›¸åŒ type, ä¾‹å¦‚ int8, è€Œ scaling value $S$ å‰‡æœƒè·Ÿ $r$ ç›¸åŒ, ä¾‹å¦‚ float. ä»¥ uint3 (0~7) åš quantization, å¦‚ä¸‹åœ–æ‰€ç¤º: æœ¬ç¯‡è¨è«–ä»¥ä¸‹å…©é»: åŒä¸€å€‹ real å€¼å¦‚ä½•åœ¨ä¸åŒçš„ $Z$/$S$ åšè½‰æ›, e.g.: $q_1$ with ($Z_1$/$S_1$) å¦‚ä½•å°æ‡‰åˆ° $q_2$ with ($Z_2$/$S_2$) PyTorch çš„ Quantization Aware Training (QAT) è¨è«– åœ¨ä¸åŒ $Z$/$S$ è½‰æ›æœ‰å…©å€‹å¸¸è¦‹ç†ç”±: åœ¨åš NN çš„ quantization æ™‚å€™, æ¯å€‹ layer çš„ output domain éƒ½ä¸åŒ, é€™å°è‡´äº†ä½¿ç”¨ä¸åŒçš„ $Z$/$S$. åˆæˆ–è€…ä¸Ÿçµ¦ NN åš inference ä¹‹å‰, mfcc/mfb éœ€è¦å…ˆè½‰æ›åˆ° NN input çš„ $Z$/$S$ quantized domain ä¸Š. é¡å¤–æä¸€é» PyTorch çš„ quantized Tensor å…¶å¯¦å°±åªæ˜¯æ¯”åŸæœ¬çš„ Tensor å¤šäº† $Z$ and $S$. ä¾‹å¦‚çµ¦å®š $Z$ and $S$, torch.quantize_per_tensor æœƒå°‡ä¸€å€‹æ­£å¸¸çš„ tensor å¾ $r$ è½‰æˆ $q$, å®˜ç¶²ç¯„ä¾‹: 12345&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)tensor([-1., 0., 1., 2.], size=(4,), dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()tensor([ 0, 10, 20, 30], dtype=torch.uint8) ä»¥ä¸‹æˆ‘å€‘éƒ½ä»¥ uint8 ç•¶ä½œ quantized çš„ type, real value ä»¥ float (4 bytes) ç‚ºæº–. è€Œ int ç‚º 4 bytes. å…ˆä½¿ç”¨ Float è½‰æ›è¦å°‡ç¬¬ä¸€å€‹ domain ($Z_1$/$S_1$) çš„æ•¸å€¼è½‰æ›åˆ°ç¬¬äºŒå€‹ domain ($Z_2$/$S_2$) æœ€ç°¡å–®çš„æ–¹æ³•å°±æ˜¯å…ˆæŠŠç¬¬ä¸€å€‹ domain çš„ $r_1$ ç®—å‡ºä¾†, å†åˆ©ç”¨ç¬¬äºŒå€‹ domain çš„ $Z_2$/$S_2$ æ±‚å¾— $q_2$ $$\\begin{align} \\color{orange}{r_1}=(float)\\left( \\left( (int32)q_1-Z_1 \\right)*S_1 \\right) \\\\ q_2=\\text{uint8_saturated_round}\\left( \\frac{\\color{orange}{r_2}}{S_2}+Z_2 \\right) \\end{align}$$ ç”±æ–¼ $r_2=r_1$ å› æ­¤ (2) å¯è¨ˆç®—å‡º $q_2$. ä½†é€™æ¨£è¨ˆç®—é‚„æ˜¯ç”¨åˆ° float, å…¶å¯¦æˆ‘å€‘å¯ä»¥å®Œå…¨ä½¿ç”¨ integer é‹ç®—ä¾†é”æˆ. ç´”ç”¨ Integer é‹ç®— å…¶ä¸­ $M&gt;1.0$ æ˜¯æ²’æœ‰æ„ç¾©çš„, e.g. $S_1&gt;S_2$. å¦‚ä¸‹åœ–èˆ‰ä¾‹ä¾†èªª, data domain åˆ†å¸ƒåªæœƒåœ¨ 8 å€‹é»ä½ç½®ä¸Š, ä½¿ç”¨æ›´ç´°çš„ resolution å»å­˜æ²’æ„ç¾©. $M_0$ å¾ˆæ˜é¡¯å¯ä»¥ç”¨ Q0.31 çš„ int32 ä¾†ä¿å­˜, æ‰€ä»¥ $M_0$ èˆ‡ $(q_1-Z_1)$ ç›¸ä¹˜çš„æ™‚å€™ä½¿ç”¨ fractional multiplication, æœ€å¾Œ $2^{-n}$ ä½¿ç”¨ shift å³å¯. ä»€éº¼æ˜¯ fractional multiplication? ä¸€å¼µåœ–è¡¨ç¤ºå°±çŸ¥é“: æœ€å¾Œæˆ‘å€‘è¦é©—è­‰çš„è©±å…¶å¯¦å¯ä»¥è·Ÿä¸Šä¸€æ®µè¬›çš„ Float ç‰ˆæœ¬å°æ¯”å°±å¯ä»¥. çŸ©é™£é‹ç®—çš„ Quantization è½‰æ›å…¶å¯¦ convolution è£¡çš„çŸ©é™£é‹ç®—åªæ˜¯åŸä¾†çš„ $r_2=r_1$ è®Šæˆ $r_3=r_1r_2$ çš„é—œä¿‚è€Œå·², å…¶é¤˜éƒ½ç›¸åŒ. è²¼ä¸€å¼µè«–æ–‡çš„å…§å®¹å³å¯. æ›´å¤šå…§å®¹å¯ä»¥åƒè€ƒè«–æ–‡ ref [1], ä¾‹å¦‚ä½¿ç”¨ ReLU6 æ›¿ä»£ ReLU, å› ç‚ºå¦‚æœæˆ‘å€‘ä½¿ç”¨ uint8 çš„è©±ç”±æ–¼ ReLU6 å°‡ domain é™åˆ¶åœ¨ [0,6] ä¹‹é–“, é€™æ¨£ 8 bits å¯ä»¥ç”¨ $Z=0$, $S=1.0/2^5=0.03125$ ä¾†è¡¨ç¤º. åŒæ™‚æœ€å¾Œå†è½‰æ›æˆ quantization model æ™‚å¯ä»¥ç›´æ¥æ‹¿æ‰ ReLU6 (å› ç‚ºç›´æ¥ä½¿ç”¨ quantization å°±å¥½) Symmetric Fixed Pointå‚³çµ±ä¸Šå¸¸è¦‹çš„ fixed point æ¡ç”¨çš„æ˜¯ symmetric quantization, ä¾‹å¦‚ Q4.3 é€™ç¨® int8 çš„è¡¨ç¤ºæ–¹å¼ (-8.0 ~ 7.875). ä½†å®ƒå…¶å¯¦åªæ˜¯ asymmetric quantization çš„ç‰¹ä¾‹. Q4.3 åŸºæœ¬ä¸Šå°±æ˜¯ $Z=0$ å’Œ $S=1.0/2^3=0.125$ çš„ asymmetric quantization. PyTorch çš„ Quantization Aware Training (QAT) ç­†è¨˜PyTorch 1.7.0 quantization doc ä¸€é–‹å§‹è¦å…ˆå°ä½ çš„ NN Module å…ˆä½œå¦‚ä¸‹æ”¹å‹•: åœ¨è‡ªå·±å®šç¾©çš„ NN Module è£¡, æ‰€æœ‰ç”¨åˆ° torch.nn.functional çš„ op éƒ½è½‰æ›æˆ torch.nn.Module åœ¨è‡ªå·±å®šç¾©çš„ NN Module è£¡, forward æ™‚å…ˆå°‡ input é QuantStub(), ç„¶å¾Œæœ€å¾Œ output é DeQuantStub(). QuantStub() æœƒå°‡æ­£å¸¸çš„ input tensor è®Šæˆ quantized tensor (è£¡é¢åŒ…å« $Z$/$S$), ç„¶å¾Œ DeQuantStub() æœƒå°‡ quantized tensor è½‰æ›æˆæ­£å¸¸çš„ tensor. åœ¨è‡ªå·±å®šç¾©çš„ NN Module è£¡, ä½¿ç”¨ torch.quantization.fuse_modules å®šç¾©ä½ çš„ fuse_model function. ç›®å‰ PyTorch åªæ”¯æ´æœ‰é™ç¨® modules fusion (see function fuse_known_modules in fuse_modules.py). æ¥è‘— QAT ç‚ºä»¥ä¸‹å¹¾å€‹æ­¥é©Ÿ: å°‡ NN çš„ object (net) è¨­å®šç‚º net.train() (å¦‚æœåªæ˜¯åš post-quantization å‰‡ç”¨ net.eval()).é€™æ˜¯å› ç‚º QAT è¦åœ¨ training æ™‚æ¨¡æ“¬ inference çš„ quantization precision loss, æ‰€ä»¥è¦æ’å…¥å¾ˆå¤š fake-quantization çš„ op. å¯ä»¥åƒè€ƒè«–æ–‡ ref [1] çš„ Figure C.4 åˆ° Figure C.8. è€Œå¦‚æœåªæ˜¯ post-quantization å‰‡åœ¨åŸä¾†æ­£å¸¸çš„ floating trianing å®Œå¾Œ, å°‡ net.eval() è¨­å®šå¥½ç›´æ¥å°± fuse model äº† (torch.quantization.fuse_modules å°æ˜¯ train or eval æœ‰ä¸åŒçš„ fuse è¡Œç‚º). å‘¼å« net.fuse_model().ä¾‹å¦‚å‡è¨­æˆ‘å€‘è¦ fuse [&#39;conv1&#39;, &#39;bn1&#39;, &#39;relu1&#39;], PyTorch æœƒå°‡ç¬¬ä¸€å€‹ Module è®Šæˆ fused Module, å‰©ä¸‹çš„å…©å€‹ç‚º Identity() Module å°‡ net è¨­å®š attribute qconfig.ä¾‹å¦‚: net.qconfig= torch.quantization.get_default_qat_qconfig(&#39;fbgemm&#39;) å‘¼å« torch.quantization.prepare_qat(net, inplace=True).æ­¤ function ä¸»è¦å¹«ä½ åšå…©ä»¶äº‹æƒ…: a. propagate qconfig: å°æ‰€æœ‰å­ Module è¨­å®šç›¸å°æ‡‰çš„ qconfig (å› ç‚ºæ­¥é©Ÿ3æˆ‘å€‘åªé‡å° root Module è¨­å®š qconfig) b. add observer/fake-quantization: observer ç‚ºç°¡å–®çš„ min/max ç·šæ€§é‡åŒ–æ–¹å¼(æˆ– histogram æ–¹å¼ç­‰). å°‡åœ–éœ€è¦ quantization çš„åœ°æ–¹å®‰æ’å¥½é€™äº› observer/fake-quantization. åŸ·è¡Œä¸€èˆ¬ training æµç¨‹.åœ¨ training çš„éç¨‹ä¸­å°±æœƒé †ä¾¿çµ±è¨ˆå¥½å°æ‡‰çš„ min/max ç­‰, ç„¶å¾Œæ¯å€‹ tensor çš„ $Z$/$S$ ä¹Ÿæœƒå°æ‡‰å¾—åˆ° (é€šå¸¸ç”¨ moving average æ–¹å¼åš smoothing). æœ€å¾Œè½‰æ›æˆ quantized model torch.quantization.convert(net, inplace=True) ä»¥ä¸Šä¸€å€‹æœ€å°ç¯„ä¾‹å¦‚ä¸‹: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import torchimport torch.nn as nnfrom torch.quantization import QuantStub, DeQuantStubimport torch.quantizationclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.bn1 = nn.BatchNorm2d(6) self.relu1 = nn.ReLU() self.quant = QuantStub() self.dequant = DeQuantStub() def forward(self, x): x = self.quant(x) x = self.relu1(self.bn1(self.conv1(x))) x = self.dequant(x) return x # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization # This operation does not change the numerics def fuse_model(self): torch.quantization.fuse_modules(self, ['conv1', 'bn1', 'relu1'], inplace=True)net = Net()print('===== Before fuse_model:')print(net)print('===== After fuse_model:')net.train()net.fuse_model()print(net)print('===== Setting qconfig:')# Specify quantization configuration# Start with simple min/max range estimation and per-tensor quantization of weightsnet.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')print(net.qconfig)print('===== After torch.quantization.prepare:')torch.quantization.prepare_qat(net, inplace=True)print(net)# Do your regular trainingtraining_loop(net)print('===== After torch.quantization.convert:')torch.quantization.convert(net, inplace=True)print(net) æœ€å¾Œé™„ä¸Šä¸€å€‹å¾ˆæ£’çš„ convolution and batchnorm fusion è§£èªª [é€£çµ], ä½œè€…æ˜¯ Nenad MarkuÅ¡ Reference Paper: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference (BETA) STATIC QUANTIZATION WITH EAGER MODE IN PYTORCH Nenad MarkuÅ¡: Fusing batch normalization and convolution in runtime","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://yoursite.com/tags/PyTorch/"},{"name":"Asymmetric Quantization","slug":"Asymmetric-Quantization","permalink":"http://yoursite.com/tags/Asymmetric-Quantization/"},{"name":"Symmetric Quantization","slug":"Symmetric-Quantization","permalink":"http://yoursite.com/tags/Symmetric-Quantization/"},{"name":"Quantization Aware Training (QAT)","slug":"Quantization-Aware-Training-QAT","permalink":"http://yoursite.com/tags/Quantization-Aware-Training-QAT/"}]},{"title":"TF Notes (7), Some TF2.x Eager Mode Practices","date":"2020-06-26T02:52:18.000Z","path":"2020/06/26/TF-Notes-some-TF2-x-eager-mode-practices/","text":"ç‚ºäº†å­¸ç¿’ TF2.x åªå¥½æŠŠä»¥å‰ç·´ç¿’çš„ä¸€äº› projects é‡å¯«ä¸€æ¬¡, ä½†å¾Œä¾†æ™‚é–“æ–·æ–·çºŒçºŒçš„, æ‰€ä»¥åªåšäº†ä¸€éƒ¨åˆ†. ç¸½ä¹‹å…ˆè¨˜éŒ„ä¸€ä¸‹ç›®å‰çš„ç·´ç¿’é€²åº¦å§. TFDataset ç·´ç¿’: jupyter notebook if map() has random ops: dataset.shuffle().batch().cache().map().prefetch() map() has NO random ops: dataset.shuffle().batch().map().cache().prefetch() NeuralStyleTransfer: jupyter notebook ç·´ç¿’ optimization è®Šæ•¸æ˜¯ input x è€Œä¸æ˜¯åŸä¾†çš„ weights w TSLearning ToyExample: jupyter notebook å›ºå®šæŸä¸€éƒ¨åˆ†çš„ model æœ€åŸå§‹çš„ distillation AutoEncoder jupyter notebook: decoder éƒ¨åˆ†ä½¿ç”¨ deconvolution jupyter notebook: å…¨éƒ¨ FC ä½† decoder æ˜¯ encoder çš„ transpose (share weights) GAN jupyter notebook: MMGAN jupyter notebook: WGAN jupyter notebook: WGAN-div Adversarial Domain Adaptation jupyter notebook å’Œ ä»‹ç´¹åŠå¯¦é©—çµæœ é‚„æœ‰å¾ˆå¤šæ²’ç·´ç¿’åˆ°, VAE, seq2seq, transformer ç­‰â€¦.åªå¥½å†èªªäº†","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"}]},{"title":"CTC Model and Loss","date":"2020-05-31T10:07:24.000Z","path":"2020/05/31/CTC-Model-and-Loss/","text":"CTC model æ˜¯ä¸€å€‹ decoder éƒ¨åˆ†ç‚ºç°¡å–®çš„ (independent) linear classifer çš„ seq2seq model. å› æ­¤ input frame æœ‰ $T$ å€‹, å°±æœƒæœ‰ $T$ å€‹ output distribution vectors. æ­£å¸¸ä¾†èªª (ex: ASR) output token æ•¸é‡ $N&lt;T$, æ‰€ä»¥æœƒæœ‰ alignment å•é¡Œ. ä»¥å¾€çš„ alignment (HMM) å¼·è¿«æ¯å€‹ frame index éƒ½éœ€å°æ‡‰åˆ°ä¸€å€‹ phoneâ€™s state, ä½† CTC å…è¨±å°æ‡‰åˆ° â€œç©ºâ€ çš„ state (null or blank). é€™è®“ CTC çš„ alignment æ¯” HMM æ›´æœ‰å½ˆæ€§. RNN-T æ˜¯å¦ä¸€ç¨®æ¯” CTC æ›´æœ‰å½ˆæ€§çš„ alignment è¡¨é”æ–¹å¼. CTC çš„ gradient å¯ä»¥éå¸¸æœ‰æ•ˆç‡çš„ç”¨ dynamic programming æ±‚å¾— (forward/backward æ¼”ç®—æ³•, ä¸‹åœ–). å› æ­¤æ¡ç”¨ gradient-based optimization æ–¹æ³•å°±å¾ˆåˆé©. æœ¬æ–‡æœƒè©³ç´°ä»‹ç´¹ä¸Šé¢æåˆ°çš„å¹¾é». Decoding éƒ¨åˆ†ä¸ä»‹ç´¹. CTC Model and Loss: PDF Slide é€£çµ Link PDF","tags":[{"name":"CTC","slug":"CTC","permalink":"http://yoursite.com/tags/CTC/"}]},{"title":"Exp of Adversarial Domain Adaptation","date":"2020-05-17T09:15:21.000Z","path":"2020/05/17/Exp-of-Adversarial-Domain-Adaptation/","text":"Domain Adaptation æ˜¯å¸Œæœ›åœ¨ source domain æœ‰ label ä½†æ˜¯ target domain ç„¡ label çš„æƒ…æ³ä¸‹, èƒ½é‡å° target domain (æˆ–åŒæ™‚ä¹Ÿèƒ½å° source domain) é€²è¡Œåˆ†é¡ä»»å‹™. â€œAdversarialâ€ çš„æ„æ€æ˜¯åˆ©ç”¨ GAN çš„ â€œå°æŠ—â€ æƒ³æ³•: Label predictor é›–ç„¶åªèƒ½ä¿è­‰ source domain çš„åˆ†é¡. ä½†ç”±æ–¼æˆ‘å€‘æŠŠ feature ç”¨ GAN æ¶ˆé™¤äº† domain ä¹‹é–“çš„å·®ç•°, å› æ­¤æˆ‘å€‘æ‰èƒ½æœŸæœ›é€™æ™‚å€™çš„ source domain classifier ä¹Ÿèƒ½ä½œç”¨åœ¨ target domain. é€™ç¯‡æ–‡ç«  å¼µæ–‡å½¥, é–‹é ­çš„åœ–å‚³é”çš„æ„æ€å¾ˆç²¾ç¢º, è«‹é»é€²å»åƒè€ƒ. æ¥è‘—å˜—è©¦è¤‡ç¾äº†ä¸€æ¬¡ Domain-Adversarial Training of Neural Networks çš„ mnist(source) to mnist_m(target) çš„å¯¦é©—. ä¸Šä¸€ç¯‡èªªæ˜ GAN çš„ framework: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} D(x) - E_{x\\sim P_G}f^*(D(x)) \\right] \\\\ G^*=\\arg\\min_G{Div\\left(P_d\\|P_G\\right)} + reg(G) \\\\ \\end{align}$$ å°æ–¼ Adversarial Domain Adaptation ä¾†èªªåªè¦åœ¨æ­£å¸¸ GAN çš„ training æµç¨‹ä¸­, update $G$ æ™‚å¤šåŠ ä¸€å€‹ regularization term $reg(G)$ å°±å¯ä»¥äº†. è€Œ $reg(G)$ å°±æ˜¯ Label Predictor çš„ loss, ä½œç”¨å°±æ˜¯ train $G$ æ™‚é™¤äº†è¦æ¬ºé¨™ $D$, åŒæ™‚è¦èƒ½é™ä½ prediction error. å¯¦é©— source domain ç‚ºæ¨™æº–çš„ mnist, è€Œ target domain æ˜¯ modified mnist, å¦‚ä½•ç”¢ç”Ÿå¯ä»¥åƒè€ƒDaipuwei/DANN-MNIST. ä¸‹åœ–æ˜¯ mnist_m çš„ä¸€äº›ç¯„ä¾‹: æˆ‘å€‘å…ˆä¾†çœ‹ä¸€ä¸‹åˆ†ä½ˆ, è—è‰²çš„é»æ˜¯ mnist, ç´…è‰²æ˜¯ mnist_m, ç”¨ tSNE è·‘å‡ºä¾†çš„çµæœæ˜é¡¯çœ‹åˆ°å…©å€‹ domain åˆ†ä½ˆä¸åŒ: æˆ‘å€‘ä¹‹å‰èªªé, ä¸ç”¨ç®¡ GRL (Gradient Reversal Layer), å°±ä¸€èˆ¬çš„ GAN æ¶æ§‹, åŠ ä¸Š regularization term å°±å¯ä»¥. è½èµ·ä¾†å¾ˆå®¹æ˜“, æˆ‘å°±éš¨æ‰‹è‡ªå·±ç”¨äº†å¹¾å€‹ CNN åœ¨ generator, å¹¾å±¤ fully connected layers çµ¦ classifier å’Œ discriminator å°±åšäº†èµ·ä¾†. ç™¼ç¾æ€éº¼å¼„éƒ½è¨“ç·´ä¸èµ·ä¾†! ç”¢ç”Ÿä¸‹é¢å…©ç¨®æƒ…å½¢: GAN too weak:é‡æ–°èª¿æ•´äº†ä¸€ä¸‹ $reg(G)$ çš„æ¯”é‡å¾Œâ€¦. GAN too strong:å…©å€‹ domain çš„ features å¹¾ä¹å®Œå…¨ overlap, ç„¶å¾Œ classifier å¹¾ä¹ç„¡ä½œç”¨ (ä¹Ÿçœ‹ä¸å‡ºæœ‰10å€‹åˆ†ç¾¤). è©±èªª, é€™åœ–å¾ˆåƒè…¦çš„ç´‹è·¯? è²ªé£Ÿè›‡? è¿·å®®? è‚šå­è£¡çš„è›”èŸ²? å¾Œä¾†åœ¨å˜—è©¦èª¿äº†å¹¾å€‹åƒæ•¸å¾Œä»ç„¶è¨“ç·´ä¸èµ·ä¾†. é€™è®“æˆ‘æ„Ÿåˆ°å¾ˆæŒ«æŠ˜. å¯¦åœ¨å—ä¸äº†å¾Œ, åƒè€ƒäº†ç¶²è·¯ä¸Šçš„åšæ³•æ”¹æˆä»¥ä¸‹å¹¾é»: WGAN æ”¹æˆç”¨ MMGAN RMSProp(1e-4) æ”¹æˆ Adam(1e-3) ä½¿ç”¨ç¶²è·¯ä¸Šä¸€å€‹æ›´ç°¡å–®çš„æ¶æ§‹ github æ”¹æˆç”¨ MMGAN å¾Œ, å»æ‰ BN layer å°±èƒ½è¨“ç·´èµ·ä¾† ç„¶å¾Œå°±å¯ä»¥è¨“ç·´èµ·ä¾†äº†(ç¿»æ¡ŒxN), è¨“ç·´å¾Œçš„çµæœå¦‚ä¸‹: å¯ä»¥çœ‹åˆ°åœ¨ mnist è¾¨è­˜ç‡ ~99% çš„æƒ…å½¢ä¸‹, mnist_m èƒ½å¤ æœ‰ 83.6% çš„è¾¨è­˜ç‡ (æ²’åš adaptation åªæœ‰ç´„50%) Feature çš„åˆ†å¸ƒå¦‚ä¸‹åœ– (è—è‰²çš„é»æ˜¯ mnist, ç´…è‰²æ˜¯ mnist_m): é›–ç„¶é‚„æœ‰ä¸€äº› feature æ²’æœ‰å®Œå…¨ match åˆ°, ä½†å·²ç¶“å¾ˆé‡ç–Šäº†. åŒæ™‚æˆ‘å€‘ä¹Ÿèƒ½æ˜é¡¯åˆ°åˆ° 10 ç¾¤çš„åˆ†é¡. çµè«–é›–ç„¶ç†è«–ä¸Šçš„ç†è§£å¾ˆå®¹æ˜“, ä½†å¯¦ä½œèµ·ä¾†å»ç™¼ç¾å¾ˆé›£èª¿æ•´. GAN å°±æ˜¯é‚£éº¼é›£æé˜¿â€¦. Reference GAN framework Domain-Adversarial Training of Neural Networks åƒè€ƒç”¢ç”Ÿ mnist_m çš„ codes Daipuwei/DANN-MNIST Domain-Adversarial Training of Neural Networks with TF2.0: lancerane/Adversarial-domain-adaptation å¼µæ–‡å½¥ Domain-adaptation-on-segmentation è‡ªå·±å¯¦é©—çš„ jupyter notebook","tags":[{"name":"GAN","slug":"GAN","permalink":"http://yoursite.com/tags/GAN/"},{"name":"ADDA","slug":"ADDA","permalink":"http://yoursite.com/tags/ADDA/"}]},{"title":"Framework of GAN","date":"2020-05-11T12:29:12.000Z","path":"2020/05/11/Note-for-Framework-of-GAN/","text":"èªªä¾†æ±—é¡, è‡ªå¾17å¹´ä¸‰æœˆç­†è¨˜å®Œ WGAN å¾Œ, å°±æ²’å†ç¢° GAN ç›¸é—œçš„æ±è¥¿äº†. æƒ¡è£œäº†ä¸€ä¸‹ æå®æ¯…GAN çš„èª²ç¨‹å’Œå…¶ä»–ç›¸é—œè³‡æ–™, å› æ­¤ç­†è¨˜ä¸€ä¸‹. MMGAN(æœ€åŸå§‹çš„GAN), NSGAN(è·ŸMMGANå·®åˆ¥åœ¨ G çš„ update ç›®æ¨™å‡½å¼æœ‰é»ä¸åŒ), f-GAN, WGAN, ADDA (Adversarial Discriminative Domain Adaptation), infoGAN, VAE-GAN ç­‰â€¦ é€™äº›å…¨éƒ¨éƒ½æ˜¯ follow ä¸‹é¢é€™æ¨£çš„ framework: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} D(x) - E_{x\\sim P_G}f^*(D(x)) \\right] \\\\ G^*=\\arg\\min_G{Div\\left(P_d\\|P_G\\right)} + reg(G) \\\\ \\end{align}$$ å…¶ä¸­ $P_d$ ç‚º real data pdf, $P_G$ ç‚º generator ç”¢ç”Ÿçš„ data pdf. $f^*$ å¸¶å…¥ä¸åŒçš„å®šç¾©æœƒç”¢ç”Ÿä¸åŒçš„ divergence, é€™ä¹‹å¾Œæœƒå†èªªæ˜. å¼ (1) å®šç¾©äº† $P_G$ èˆ‡ $P_d$ çš„ divergence, å…¶ä¸­é€™å€‹ divergence çš„å€¼ç‚ºè—‰ç”±è§£é€™å€‹æœ€ä½³åŒ–å•é¡Œæ±‚å¾—çš„. å¼ (2) è¡¨ç¤ºè¦æ‰¾çš„ $G$ å°±æ˜¯ divergence æœ€å°çš„é‚£å€‹. Divergence æœ€å° ($=0$) åŒæ™‚ä¹Ÿè¡¨ç¤º $P_G=P_d$ (ç”Ÿæˆå™¨éŠæˆ). å¦‚æœåŒæ™‚è€ƒæ…® regularization term, $reg(G)$, å‰‡æœƒæœ‰å¾ˆå¤šè®ŠåŒ–ç”¢ç”Ÿ, å¦‚ ADDA, infoGAN, VAE-GANâ€¦ æˆ‘å€‘æ¥è‘—ä¾†çœ‹ MMGAN, NSGAN, f-GAN, WGAN, ADDA, infoGAN, VAE-GAN é€™äº›æ€éº¼ fit é€²é€™å€‹æ¡†æ¶. MMGANMMGAN æ˜¯ MinMax GAN çš„ç¸®å¯«, æŒ‡çš„æ˜¯æœ€åŸå§‹çš„ GAN. å°‡ (1) ä¸­çš„ $D(x)$ ä½¿ç”¨ $\\log D(x)$ æ›¿æ›, ä¸¦ä¸” $f^*(t)=-\\log(1-exp(t))$ æ›¿æ›å¾—åˆ°å¦‚ä¸‹å¼å­: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} \\log D(x) - E_{x\\sim P_G}[-\\log(1-D(x))] \\right] \\\\ \\end{align}$$ ç¨å¾®å†æ•´ç†ä¸€ä¸‹: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_D\\left[ E_{x\\sim P_d} \\log D(x) + E_{x\\sim P_G}[\\log(1-D(G(z)))] \\right] \\\\ \\end{align}$$ é€™å°±æ˜¯ GAN discriminator åŸå§‹çš„å¼å­. è€Œæˆ‘å€‘çŸ¥é“çµ¦å®š $G$ ä¸Šè¿°çš„æœ€ä½³è§£ç‚º \\( D_G^*(x) = \\frac{P_d(x)}{P_d(x)+P_G(x)} \\), ä¸¦å¸¶å…¥ (4) æˆ‘å€‘å¾—åˆ°: $$\\begin{align} Div\\left(P_d\\|P_G\\right) = -\\log4+KL(p_d\\vert\\frac{p_d+p_g}{2})+KL(p_g\\vert\\frac{p_d+p_g}{2}) \\\\ =-\\log4+2JSD(p_d \\vert p_g) \\end{align}$$ å› æ­¤ discriminator çš„æœ€å¤§åŒ–ç›®çš„æ˜¯è¨ˆç®—å‡º JS divergence. è€Œ generator $G$ æ±‚è§£æ²’ä»€éº¼å¥½èªª, ç›´æ¥å° (3) æœ€å°åŒ–: $$\\begin{align} G^*=\\arg\\min_G E_{x\\sim P_G}[\\log(1-D(x))] \\end{align}$$ æ³¨æ„åˆ°èˆ‡ (2) å°æ¯”, MMGAN åªæ˜¯æ²’æœ‰ regularization term è€Œå·². NSGANNSGAN ç‚º Non-Saturating GAN ç¸®å¯«, èˆ‡ MMGAN åªå·®åœ¨ generator $G$ æ±‚è§£å¼å­ä¸åŒ, åŸæœ¬æ˜¯å¸Œæœ›åœ¨ä¸€é–‹å§‹ generator æ¯”è¼ƒå·®çš„æƒ…å½¢ä¸‹ç”¨ (7) ç®—çš„ gradient æœƒå¤ªå°, å› æ­¤æ”¹æˆä¸‹å¼, ä½¿å¾— gradient èƒ½åœ¨ä¸€é–‹å§‹çš„æ™‚å€™æ¯”è¼ƒå¤§, è®“ update å‹•èµ·ä¾†. NSGAN generator $G$ ç‚º: $$\\begin{align} G^*=\\arg\\min_G E_{x\\sim P_G}[-\\log(D(x))] \\end{align}$$ å¦‚æœæˆ‘å€‘å°‡ \\( D_G^*(x) = \\frac{P_d(x)}{P_d(x)+P_G(x)} \\) å¸¶å…¥ä¸¦æ•´ç†, æˆ‘å€‘æœƒç™¼ç¾: $$\\begin{align} G^*=\\arg\\min_G E_{x\\sim P_G}[-\\log(D^*(x))] \\\\ =\\arg\\min_G \\left[ KL(P_G\\|P_d)-2JSD(P_d\\|P_G) \\right] \\end{align}$$ ç”¢ç”Ÿäº†å…©å€‹äº’ç›¸ trade-off çš„ objective funtionâ€¦ é€™é€ æˆäº†çŸ›ç›¾ è©³ç´°æ¨å°è«‹åƒè€ƒ ä»¤äººæ‹æ¡ˆå«ç»çš„Wasserstein GAN ä¸€æ–‡, éå¸¸æ£’çš„æ–‡ç« . å¼•ç”¨æ–‡ç« å…§çš„èªªæ˜: ä¸€å¥è¯æ¦‚æ‹¬ï¼šæœ€å°åŒ–ç¬¬äºŒç§ç”Ÿæˆå™¨losså‡½æ•°ï¼Œä¼šç­‰ä»·äºæœ€å°åŒ–ä¸€ä¸ªä¸åˆç†çš„è·ç¦»è¡¡é‡ï¼Œå¯¼è‡´ä¸¤ä¸ªé—®é¢˜ï¼Œä¸€æ˜¯æ¢¯åº¦ä¸ç¨³å®šï¼ŒäºŒæ˜¯collapse modeå³å¤šæ ·æ€§ä¸è¶³ã€‚ f-GANæˆ‘å€‘åœ¨ MMGAN æ™‚æåˆ° â€œå°‡ (1) ä¸­çš„ $D(x)$ ä½¿ç”¨ $\\log D(x)$ æ›¿æ›, ä¸¦ä¸” $f^*(t)=-\\log(1-exp(t))$ æ›¿æ›â€ å‰‡æœƒå¾—åˆ° discriminator å°±æ˜¯åœ¨æ±‚è§£ JS divergence. é‚£éº¼æœ‰æ²’æœ‰å…¶ä»–è¨­å®šæœƒç”¢ç”Ÿå…¶ä»– divergence å‘¢? æœ‰çš„, è—‰ç”± f-GAN çš„å®šç¾©å¯ä»¥å›Šæ‹¬å„å¼å„æ¨£çš„ divergence. ä½¿ç”¨æè€å¸«çš„èªªæ˜æµç¨‹ç­†è¨˜: é¦–å…ˆå®šç¾© f-divergence, å¯ä»¥ç™¼ç¾ JSD, KL, reverse-KL, Chi square ç­‰ç­‰éƒ½å±¬æ–¼å…¶ä¸­çš„ç‰¹ä¾‹. æ¥è‘—èªªæ˜ convex function çš„ conjugate function. æœ€å¾Œæ‰èªªæ˜æ€éº¼è·Ÿ GAN ç”¢ç”Ÿé—œè¯ (ç¥å¥‡çš„é€£çµ). f-divergence$$\\begin{align} Div_f(P\\|Q)=\\int_x q(x)f\\left( \\frac{p(x)}{q(x)} \\right) dx \\\\ \\text{where } f \\text{ is }\\color{orange}{convex} \\text{ and } f(1)=0 \\end{align}$$ æ˜é¡¯çŸ¥é“ $p(x)=q(x)$ æ™‚ $Div_f(P|Q)=0$, åŒæ™‚å¯ä»¥è­‰æ˜ $Div_f(P|Q)\\geq 0$, å› æ­¤æ»¿è¶³ divergence å®šç¾©(search â€œDivergence (statistics) wikiâ€ for definition): $$\\begin{align} Div_f(P\\|Q)=\\int_x q(x)f\\left( \\frac{p(x)}{q(x)} \\right) dx \\\\ \\geq f\\left( \\int_x q(x)\\frac{p(x)}{q(x)} dx \\right)=f(1)=0 \\\\ \\end{align}$$ $f$ æ˜¯ convex é€™é»å¾ˆé‡è¦, æ‰èƒ½å°‡ (13) åˆ° (14) ä½¿ç”¨ Jensenâ€™s inequality. å®šç¾©ä¸åŒ $f$ æœƒç”¢ç”Ÿä¸åŒ divergence, å¸¸è¦‹çš„ç‚º(æè€å¸«slide): ç”±æ–¼ $f$ æ˜¯ convex, è€Œæ¯ä¸€å€‹ convex function éƒ½æœƒæœ‰ä¸€å€‹ conjugate function $f^*$ (å®ƒä¹Ÿæ˜¯ convex), åˆ©ç”¨é€™å€‹ç‰¹æ€§æœ€å¾Œå¯ä»¥è·Ÿ GAN é€£èµ·ä¾†. å› æ­¤ä»¥ä¸‹å…ˆèªªæ˜ conjugate function. Fenchel ConjugateEvery convex function $f$ has a conjugate function $f^*$: $$\\begin{align} f^*(t)=\\max_{x\\in dom(f)}\\{xt-f(x)\\} \\end{align}$$ è€å¸«çš„æŠ•å½±ç‰‡éå¸¸å½¢è±¡çš„è¡¨ç¤ºå‡º $f$ èˆ‡ $f^*$ çš„é—œä¿‚L é‚„å…·é«”èˆ‰äº†å€‹ç•¶ $f(x)=x\\log x$ çš„ä¾‹å­: èˆ‡ GAN çš„é—œè¯é€™æ˜¯æˆ‘è¦ºå¾—éå¸¸å²å®³çš„åœ°æ–¹. é¦–å…ˆ $f^*$ çš„ conjugate å°±è®Šå› $f$ äº†, å®ƒå€‘äº’ç‚º conjugate. $$\\begin{align} f^*(t)=\\max_{x\\in dom(f)}\\{xt-f(x)\\}\\longleftrightarrow f(x)=\\max_{t\\in dom(f^*)}\\{xt-f^*(t)\\} \\end{align}$$ å°‡ (11) åˆ©ç”¨ conjugate çš„é—œä¿‚é‡æ–°è¡¨ç¤ºä¸€ä¸‹ $$\\begin{align} Div_f(P\\|Q)=\\int_x q(x)f\\left( \\frac{p(x)}{q(x)} \\right) dx \\\\ =\\int_x q(x) \\left( \\max_{t\\in dom(f^*)} \\left[ \\frac{p(x)}{q(x)}t - f^*(t) \\right] \\right) dx \\end{align}$$ å²å®³çš„åœ°æ–¹ä¾†äº†â€¦. å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹ function $D$ å¯ä»¥ç›´æ¥å¹«æˆ‘å€‘è§£å‡º (18) çš„é‚£å€‹ $t$ æ˜¯ä»€éº¼, ä¹Ÿå°±æ˜¯: $$\\begin{align} D(x)=\\hat{t}=\\arg\\max_{t\\in dom(f^*)} \\left[ \\frac{p(x)}{q(x)}t - f^*(t) \\right] \\end{align}$$ é‚£éº¼ $Div_f(P||Q)$ ç›´æ¥å°±æ˜¯ $$\\begin{align} Div_f(P||Q)=\\int_x q(x) \\left[ \\frac{p(x)}{q(x)}\\hat{t} - f^*(\\hat{t})) \\right] dx \\end{align}$$ å¯¦ä½œä¸Š $D$ çš„è¡¨é”èƒ½åŠ›æœ‰é™, åŒæ™‚è®“æˆ‘å€‘æ‰¾åˆ°æœ€æº–çš„é‚£å€‹å«åš $\\hat{D}$, å› æ­¤åªèƒ½æ±‚å¾—ä¸€å€‹ä¸‹ç•Œä¸¦æ•´ç†ä¸€ä¸‹å¾—åˆ°: $$\\begin{align} Div_f(P||Q)\\geq \\int_x q(x) \\left[ \\frac{p(x)}{q(x)}\\hat{D}(x) - f^*(\\hat{D}(x))) \\right] dx \\\\ \\approx \\int_x q(x) \\left[ \\frac{p(x)}{q(x)}\\hat{D}(x) - f^*(\\hat{D}(x))) \\right] dx \\\\ = \\int_x {p(x)\\hat{D}(x)}dx - \\int_x{q(x)f^*(\\hat{D}(x))} dx \\\\ = E_{x\\sim P}\\left[ \\hat{D}(x) \\right] - E_{x\\sim Q}\\left[ f^*( \\hat{D}(x) ) \\right] \\\\ = \\max_D \\left[ E_{x\\sim P}\\left[ D(x) \\right] - E_{x\\sim Q}\\left[ f^*( D(x) ) \\right] \\right] \\\\ \\end{align}$$ è«‹æŠŠ (25) è·Ÿ (1) æ¯”è¼ƒ, å…¶å¯¦å°±ä¸€æ¨¡ä¸€æ¨£. å› æ­¤, åªè¦ $f$ æ˜¯ convex function , ä¸” $f(1)=0$, discriminator $D$ çš„æœ€ä½³åŒ–å•é¡Œ ((1) ç”¨ $f$ çš„ conjugate, $f^*$, å¸¶å…¥) å°±æ˜¯åœ¨è¨ˆç®—å…©å€‹åˆ†å¸ƒçš„ f-divergence. è«–æ–‡ç›´æ¥çµ¦å‡ºå„ç¨® f-divergence çš„ $f$ and $f^*$ å› æ­¤æˆ‘å€‘å¯ä»¥ç™¼ç¾ MMGAN å’Œ LSGAN éƒ½æ˜¯ f-GAN çš„ä¸€ç¨®ç‰¹ä¾‹. WGANå…·é«”è«‹åƒè€ƒä¹‹å‰è‡ªå·±ç­†è¨˜çš„æ–‡ç«  æè€å¸«çš„è¬›ç¾©å°æ–¼ Earth Moverâ€™s Distance (æˆ–ç¨± Wasserstein distance) è¬›è§£å¾—å¾ˆæ¸…æ¥š, å…¶ä¸­çš„ä¸€å€‹åƒè€ƒé€£çµæ›´è§£é‡‹äº† Wasserstein distance å¦‚ä½•è½‰æ›æˆæ±‚è§£ $\\max_D$ ä¸” $D$ å¿…é ˆé™åˆ¶åœ¨ Lipschitz æ¢ä»¶ä¸‹. ç¸½ä¹‹é€™è£¡è¦èªªçš„æ˜¯, Wasserstein distance ä¸å±¬æ–¼ f-divergence, ä½†ä¹Ÿå®Œå…¨ follow æˆ‘å€‘ä¸€é–‹å§‹èªªçš„ (1) &amp; (2) çš„æ¶æ§‹: ä»¤ $f^*(x)=x$ åŒæ™‚å¤šä¸€å€‹é™åˆ¶æ˜¯ $D\\in k-Lipschitz$ $$\\begin{align} Div\\left(P_d\\|P_G\\right) = \\max_{D\\in k-Lipschitz}\\left[ E_{x\\sim P_d} D(x) - E_{x\\sim P_G}D(x) \\right] \\\\ \\end{align}$$ æ±‚è§£ discriminator çš„æœ€ä½³åŒ–å•é¡Œå…¶å¯¦å°±æ˜¯åœ¨ä¼°ç®—å…©å€‹åˆ†å¸ƒçš„ divergence. åŸå§‹è«–æ–‡é‡å° $D\\in k-Lipschitz$ çš„é™åˆ¶ç›´æ¥ç”¨å¾ˆæš´åŠ›çš„ weight clipping æ–¹æ³•è§£æ‰. å› æ­¤å¾Œé¢æœ‰ä¸€ç¯‡ WGAN-GP (Gradient Panelty) çš„æ–¹å¼è£œå¼·. é€™è£¡ä¸å±•é–‹è¨è«–, å› ç‚ºæˆ‘ä¹Ÿæ²’ä»€éº¼ç ”ç©¶, ç°¡å–®å¸¶éä¸€é»å¾…è®€çš„è«–æ–‡. å¦å¤–æœ‰ä¸€ç¯‡ SN-GAN â€œSpectral Normalization for Generative Adversarial Networksâ€œ çœ‹èµ·ä¾†æ˜¯ä¸€ç¨®ä¸»æµè¨“ç·´ WGAN çš„æ–¹å¼, äº‹å…ˆå°±å°‡ gradient éƒ½é™åˆ¶ norm&lt;=1. é€™ç¯‡æ–‡ç« å¤§è‡´æ•´ç†å„ç¨®è®Šé«”, åƒè€ƒé€£çµ. é—œæ–¼ regularization term, $reg(G)$Adversarial Domain Adaptationæˆ‘å€‘å…ˆèªª Domain-Adversarial Training of Neural Networks é€™ç¯‡ç¶“å…¸çš„æ–‡ç« . Generator ç¾åœ¨åšçš„æ˜¯ feature extractor çš„å·¥ä½œ, è€Œæˆ‘å€‘å¸Œæœ› target domain çš„ feature èƒ½è·Ÿ source domain çš„ feature åˆ†ä½ˆä¸€æ¨£, é€™æ¨£åœ¨ source domain (æœ‰ label) è¨“ç·´å¥½çš„ model, å°±èƒ½ç›´æ¥åœ¨ target domain (ç„¡ label) ä¸Šä½œç”¨. è¦åšåˆ°ç„¡æ³•å€åˆ†å‡ºé€™å€‹ feature æ˜¯ source or target domain é€™ä»¶äº‹æƒ…â€¦.æ­£å¥½å°±å¯ä»¥ç”¨ GAN çš„æ–¹å¼é”åˆ°. ä¸çœ‹ Label Predictor çš„éƒ¨åˆ†çš„è©±, å°±æ˜¯ä¸€å€‹å…¸å‹çš„ GAN. ä½œç”¨å°±æ˜¯æŠŠ source and target çš„ feature æŠ•å½±åˆ°å…±åŒçš„ç©ºé–“, ä¸¦ä¸”åˆ†ä¸é–‹. ä½†ç¼ºå°‘ Label Predictor æœ‰å¯èƒ½é€ æˆ feature extractor ç”¢ç”Ÿ trivial solution (ä¾‹å¦‚å…¨éƒ¨ map åˆ° constant) é€™æ¨£ä¹Ÿèƒ½ä½¿ discriminator åˆ†ä¸é–‹. å› æ­¤åŠ ä¸Š Label Predictor é™¤äº†é¿å…é€™ä»¶äº‹å¤–, ä¹Ÿä¿è­‰åœ¨ source domain èƒ½å¤ å¾ˆå¥½çš„å®Œæˆæˆ‘å€‘çš„åˆ†é¡ä»»å‹™. æ³¨æ„, å› ç‚º label åªæœ‰åœ¨ source domain, å› æ­¤ label predictor åªèƒ½ä¿è­‰ source domain çš„åˆ†é¡. ä½†ç”±æ–¼æˆ‘å€‘æŠŠ feature ç”¨ GAN æ¶ˆé™¤äº† domain ä¹‹é–“çš„å·®ç•°, å› æ­¤æˆ‘å€‘æ‰èƒ½æœŸæœ›é€™æ™‚å€™çš„ source domain classifier ä¹Ÿèƒ½ä½œç”¨åœ¨ target domain. è«–æ–‡ä½¿ç”¨äº†ä¸€å€‹å«åš Gradient Reversal Layer (GRL), å…¶å¯¦æˆ‘å€‘å¯ä»¥å¿½ç•¥é€™ä»¶äº‹æƒ…, å› ç‚ºé€™åªæ˜¯ discriminator and generator ä¸€å€‹ maximize å¦ä¸€å€‹ minimize, è€Œä½¿å¾—è¦ update generator æ™‚ç•¶æ™‚ç®—çš„ discriminator gradient è¦å–è² è™Ÿ. æˆ‘å€‘ç…§æ­£å¸¸çš„ GAN training å°±å¯ä»¥äº†. Label Predictor çš„ loss å…·é«”å°±æ˜¯ (2) çš„ regularization term, $reg(G)$. é€™æ˜¯å¸Œæœ›æˆ‘å€‘ train $G$ çš„æ™‚å€™é™¤äº†è¦æ¬ºé¨™ $D$, åŒæ™‚è¦èƒ½é™ä½ $reg(G)$ (prediction loss). å¾ŒçºŒæœ‰ä¸€ç¯‡ Advesarial Discriminative Domain Adaptation ç®—æ˜¯è±å¯Œäº†é€™ç¨®æ¶æ§‹. è«–æ–‡è£¡å° source and target çš„ feature extractor ä½¿ç”¨ä¸åŒçš„ neural networks. ä¸¦ä¸”ä¸€é–‹å§‹çš„ source domain feature extractor æ˜¯äº‹å…ˆè¨“ç·´å¥½çš„. ç„¶å¾Œå¾Œé¢çš„ GAN éƒ¨åˆ†è¨“ç·´çš„æ™‚å€™, target domain çš„ feature extractor è¦å»åŒ¹é… source domain çš„. é€™æ¨£åšçš„å¥½è™•æ˜¯è‡³å°‘ä¸€é‚Šçš„åˆ†ä½ˆæ˜¯å›ºå®šä½çš„, æ¯”è¼ƒå®¹æ˜“è¨“ç·´. åŒæ™‚ä¹Ÿç°¡åŒ–äº†è¨“ç·´æµç¨‹, è¦‹ä¸‹åœ–: infoGANè©³ç´°å°±ä¸è§£é‡‹äº†, äº‹å¯¦ä¸Šæ¨å°è¼ƒè¤‡é›œä½†å¯¦ä½œä¸Šå»ç•°å¸¸å®¹æ˜“, ä¹‹å¾Œæœ‰æ©Ÿæœƒå†è¨˜éŒ„ä¸€ä¸‹. ç¸½ä¹‹åœ¨åŸå§‹ GAN æ¶æ§‹ä¸Šå¤šäº†ä¸€å€‹ Decoder, ç”¨ä¾†é‚„åŸ generator input ä¸­æ‰€æŒ‡å®šçš„éƒ¨åˆ†($c$). Decoder å¸Œæœ›èƒ½å°‡ $c$ ç„¡æçš„é‚„åŸ, é‚£éº¼ä»€éº¼å«ç„¡æ? æŒ‡çš„å°±æ˜¯ Mutual Information of $c$ and $\\hat{c}$ æœ€å¤§. å…¶ä¸­ $\\hat{c}$ è¡¨ç¤ºç”± Decoder é‚„åŸå‡ºä¾†çš„çµæœ. é‚„åŸçš„ loss term åŸºæœ¬å°±æ˜¯ $reg(G)$, åŒæ¨£çš„ç†è§£, $G$ é™¤äº†è¦é¨™é $D$ ä¹‹å¤–, å¤šäº†ä¸€å€‹ä»»å‹™å°±æ˜¯ä½¿å¾—é‚„åŸçš„ loss æ„ˆå°æ„ˆå¥½. é™„ä¸Šæå®æ¯…æ•™æˆèª²ç¨‹çš„å…©å¼µåœ–ç‰‡: VAE-GANç›´æ¥ä¸Šè€å¸«çš„ slides ä»¥ GAN çš„è§’åº¦ä¾†çœ‹, $G$ é™¤äº†è¦æ¬ºé¨™ $D$ ä¹‹å¤–, é‚„å¤šäº† VAE çš„ loss ($reg(G)$) ç”¨ä¾† reconstruct åŸæœ¬çš„ input image. å° GAN ä¾†èªªæ˜¯æœ‰å¥½è™•çš„, å› ç‚º GAN é›–ç„¶èƒ½å¤ ç”¢ç”Ÿå¤ çœŸçš„ image, ä½†æ˜¯æœƒè‡ªå·±â€æé€ â€, å› æ­¤å¤šäº† VAE çš„ $reg(G)$ æœƒè®“æé€ çš„æƒ…æ³é™ä½. ä»¥ VAE çš„è§’åº¦ä¾†çœ‹, GAN çš„ loss è®Šæˆäº† regularization term äº†. ä¹Ÿå°±æ˜¯èªª VAE é™¤äº†è¦ç”¢ç”Ÿè·ŸåŸæœ¬æ¥è¿‘çš„ image (pixel-level), é‚„è¦èƒ½é¨™é $D$. é€™æ˜¯ç‚ºäº†è£œè¶³ VAE çš„ç¼ºé», åŸå§‹ VAE çš„ç›®æ¨™å‡½å¼æ˜¯ pixel-level çš„ l2-norm, é€™è·Ÿäººé¡èªç‚ºçš„çœŸå¯¦ä¸çœŸå¯¦ä¸ä¸€è‡´, å› æ­¤ AVE æœƒç”¢ç”Ÿæ¨¡ç³Šçš„ image. ç”¨ GAN çš„ loss ç•¶æˆ regularization term å‰‡è£œè¶³äº† VAE é€™é». å› æ­¤ VAE-GAN é€™æ˜¯å€‹äº’æƒ çš„çµæ§‹, å¾ˆæ¼‚äº®. é€™å€‹çµæ§‹æ–°çš„ä¸€ç¯‡ Adversarial Latent Autoencoders ç²—ç•¥è¬›ä¹Ÿæ˜¯ VAE-GAN æ¶æ§‹, åªæ˜¯ reconstruction ä¸æ˜¯å† image, è€Œæ˜¯åœ¨ latent space. è«–æ–‡çµæœååˆ†é©šè‰·, github. çµè«–æœ¬ç¯‡é–‹é ­èªªæ˜çš„ framework åŸºæœ¬å¯ä»¥è§£é‡‹äº†ä¸Šè¿°å„ç¨® GAN. ä½†ç”±æ–¼æœ¬é­¯æ‰ç–å­¸æ·º, é‚„æœ‰ä¸€å¤§å †æ²’çœ‹çš„è®Šç¨®, EBGAN, BEGAN, CycleGAN, â€¦etc. åªèƒ½èªªä¹‹å¾Œè®€åˆ°çš„æ™‚å€™, çœ‹çœ‹èƒ½å¦è©¦è‘—é€™éº¼è§£é‡‹. GAN å¯¦åœ¨å¤ªå¤šäº†, å¯ä»¥çœ‹çœ‹ GAN Zoo æœ‰å¤šå°‘ç”¨ GAN ä¾†å‘½åçš„æ¶æ§‹(ä¼¼ä¹åœæ­¢æ›´æ–°). Reference æå®æ¯…GAN f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization ä»¤äººæ‹æ¡ˆå«ç»çš„Wasserstein GAN WGANç­†è¨˜ Wasserstein GAN and the Kantorovich-Rubinstein Duality Spectral Normalization for Generative Adversarial Networks GANè®ºæ–‡é˜…è¯»ç¬”è®°3ï¼šWGANçš„å„ç§å˜ä½“ by æ—å°åŒ— InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets Domain-Adversarial Training of Neural Networks Advesarial Discriminative Domain Adaptation Autoencoding beyond pixels using a learned similarity metric Adversarial Latent Autoencoders","tags":[{"name":"GAN","slug":"GAN","permalink":"http://yoursite.com/tags/GAN/"},{"name":"ADDA","slug":"ADDA","permalink":"http://yoursite.com/tags/ADDA/"},{"name":"fGAN","slug":"fGAN","permalink":"http://yoursite.com/tags/fGAN/"},{"name":"WGAN","slug":"WGAN","permalink":"http://yoursite.com/tags/WGAN/"},{"name":"infoGAN","slug":"infoGAN","permalink":"http://yoursite.com/tags/infoGAN/"},{"name":"VAE-GAN","slug":"VAE-GAN","permalink":"http://yoursite.com/tags/VAE-GAN/"}]},{"title":"Notes for (conditional/cross-)Entropy, Mutual-information, ...","date":"2020-05-02T03:37:12.000Z","path":"2020/05/02/Notes-for-conditional-cross-Entropy-Mutual-information/","text":"æ•´ç†ä¸‹ entropy çš„ä¸€äº›æ±è¥¿, ä¸ç„¶ä¹…æ²’çœ‹è€æ˜¯å¿˜è¨˜. Entropy of a r.v. $X$: $H(X)$ Conditional Entropy of $Y$ given $X$: $H(Y|X)$ Cross(Relative) Entropy of two pdf, $p$ and $q$: $D(p\\Vert q)$ Mutual Information of two r.v.s: $I(X;Y)$ æ–‡ç« æœƒæ˜ç¢ºå®šç¾©æ¯ä¸€é …, ç„¶å¾Œåœ¨æ¨å°å®ƒå€‘ä¹‹é–“é—œä¿‚çš„åŒæ™‚æœƒè§£é‡‹å…¶ç‰©ç†æ„ç¾©. æœ€å¾Œå…¶å¯¦å°±å¯ä»¥æ•´ç†æˆé¡ä¼¼é›†åˆé—œä¿‚çš„åœ– (wiki) Entropy$$\\begin{align} H(X) = \\sum_{x\\in X}{p(x)\\log{\\frac{1}{p(x)}}} \\end{align}$$ ä¸€å€‹ r.v. $X$ å‡è¨­é…çµ¦ä»–çš„ pdf ç‚º $p$, å‰‡å¯ä»¥ç®—å‡ºç›¸å°æ‡‰çš„ entropy $H(X)$, æ‰€ä»¥æˆ‘å€‘å…¶å¯¦å¯ä»¥çœ‹æˆæ˜¯ $H(p)$.å¯ä»¥çŸ¥é“ç•¶ $p$ æ˜¯ uniform distribution æ™‚ entropy é”åˆ°æœ€å¤§ (å¾Œé¢å†è­‰æ˜). åŒæ™‚è©²å®šç¾©å¯ä»¥å¾ˆå®¹æ˜“çœ‹å‡º $H(X)\\geq 0$. ç›´è§€è§£é‡‹: å› ç‚º uniform distribution æ™‚æœ€å¤§ (æœ€ç„¡æ³•ç¢ºå®šå“ªä¸€å€‹ outcome æœ€æœ‰å¯èƒ½), å¦ä¸€å€‹æ¥µç«¯ç‚º $X$ ç‚º constant (r.v. åªæœ‰ä¸€å€‹ outcome) æ‰€ä»¥æˆ‘å€‘å¯ä»¥ç†è§£ç‚º entropy ç‚ºé‡æ¸¬ä¸€å€‹ r.v. $X$ çš„ä¸ç¢ºå®šæ€§ å¦‚æœè¦å°æ¯ä¸€å€‹ outcome æ±ºå®šè¦ç”¨å¤šå°‘ bit ä¾†ä»£è¡¨, æˆ‘å€‘å¸Œæœ›å¹³å‡èŠ±çš„ bits æ•¸èƒ½æœ€å°, ç›´è§€ä¸Šæ©Ÿç‡æ„ˆå¤§çš„ outcome ç”¨æ„ˆå°çš„ bits ä¾†è¡¨é”. å› æ­¤ outcome $x_i$ æˆ‘å€‘ç”¨ $\\log{\\frac{1}{p(x_i)}}$ é€™éº¼å¤š bits è¡¨ç¤º, å‰‡ entropy ä»£è¡¨äº† encode æ‰€æœ‰ outcome æ‰€éœ€è¦çš„å¹³å‡ bits æ•¸, å‰‡é€™å€‹æ•¸æ˜¯æœ€å°çš„. (é€™å¯ä»¥å¾ä¸‹é¢çš„ cross entropy å¾—åˆ°è­‰æ˜) æˆ‘å€‘å¯ä»¥ç”¨ entropy ä¾†è¡¨ç¤ºè©² r.v. æ‰€åŒ…å«çš„è³‡è¨Šé‡. å› ç‚ºæ¯” entropy æ›´å¤šçš„è³‡è¨Šé‡éƒ½æ˜¯ reduandant çš„ (ç”±ä¸Šä¸€é»å¯çœ‹å‡º) ç‚ºäº†æ–¹ä¾¿æˆ‘å€‘æœƒäº¤å‰ä½¿ç”¨ è³‡è¨Šé‡ æˆ–æ˜¯ encode çš„æœ€å°å¹³å‡ bits æ•¸ ä¾†è¡¨ç¤º entropy çš„ç‰©ç†æ„ç¾©. Cross(Relative) Entropyç‚ºé‡æ¸¬å…©å€‹ pdfs, $p(x)$ and $q(x)$ ä¹‹é–“çš„ â€œdivergenceâ€, ä¹Ÿç¨±ç‚º KL divergence. æ³¨æ„ divergence ä¸éœ€æ»¿è¶³ä¸‰è§’ä¸ç­‰å¼ä¹Ÿä¸éœ€æ»¿è¶³å°ç¨±æ€§, å› æ­¤ä¸æ˜¯ä¸€å€‹ â€œmetricâ€. ä½†ä¹Ÿè¶³å¤ ç•¶æˆæ˜¯æŸç¨®â€è·é›¢â€ä¾†çœ‹å¾… (ä¸æ˜¯æŒ‡æ•¸å­¸ä¸Šçš„ norm) $$\\begin{align} D(p\\Vert q) = \\sum_x{p(x)\\log{\\frac{p(x)}{q(x)}}} \\end{align}$$ ç”± Jensenâ€™s inequality å¯ä»¥æ¨å¾— $D(p\\Vert q) \\geq 0$, å¦å¤– $D(p\\Vert q)=0$ iff $p=q$ $$\\begin{align} D(p\\Vert q) = - \\sum_x{p(x)\\log{\\frac{q(x)}{p(x)}}} \\\\ \\text{by Jensen&apos;s inequality: } \\geq \\log\\sum_x{p(x)\\frac{q(x)}{p(x)}} = 0 \\end{align}$$ é‡å¯«ä¸€ä¸‹ (2):$$\\begin{align} 0\\leq D(p\\Vert q) = \\sum_x{p(x)\\log{\\frac{1}{q(x)}}} - H(p) \\end{align}$$ é€™èªªæ˜äº†å°æ–¼ outcome $x_i$ (true distribution ç‚º $p(x)$) æˆ‘å€‘ç”¨ $\\log{\\frac{1}{\\color{red}{q}(x_i)}}$ é€™éº¼å¤š bits è¡¨ç¤ºæ˜¯æµªè²»çš„. æ‰€ä»¥è­‰æ˜äº†ä¸Šé¢ Entropy ç¬¬äºŒç¨®è§£é‡‹. ç›´è§€è§£é‡‹: $D(p\\Vert q)$ è¡¨ç¤ºä½¿ç”¨éŒ¯èª¤çš„ distribution $q$ ä¾† encode è¦å¤šèŠ±çš„å¹³å‡ bits æ•¸é‡ Conditional Entropy$$\\begin{align} H(Y|X) = \\sum_{x\\in X}{p(x)H(Y|x=X)} \\\\ =\\sum_x{p(x)\\sum_y{p(y|x)\\log{\\frac{1}{p(y|x)}}}} \\end{align}$$ $H(Y|x=X)$ è§£é‡‹ç‚º given $x$ encode $Y$ çš„æœ€å°å¹³å‡ bits æ•¸ (å°±æ˜¯ entropy åªæ˜¯åˆé‡è¤‡èªªäº†ä¸€æ¬¡), ä½†æ˜¯æ¯ä¸€å€‹ $x$ æœ‰è‡ªå·±çš„æ©Ÿç‡, å› æ­¤å° $x$ å†å–ä¸€æ¬¡å¹³å‡. case 1:å¦‚æœ $Y$ å®Œå…¨å¯ä»¥ç”± $X$ æ±ºå®š, å› æ­¤ä¸€æ—¦çµ¦å®š $x$, $Y$ å°±æ˜¯ä¸€å€‹ constant, æ‰€ä»¥ $H(Y|x=X)=0$. å†å° $X$ ç®—æœŸæœ›å€¼ä»ç„¶æ˜¯ 0. case 2:å¦‚æœ $X$ and $Y$ independent. $$\\begin{align} (7)=\\sum_x{p(x)\\sum_y{p(y)\\log{\\frac{1}{p(y)}}}}=\\sum_x{p(x)H(Y)}=H(Y) \\end{align}$$ å¾—åˆ°çµè«– $H(Y|X)=H(Y)$ Case 1 and 2 èªªæ˜äº† $X$ and $Y$ åœ¨å®Œå…¨ä¾è³´å’Œå®Œå…¨ç„¡é—œæ™‚ $H(Y|X)$ åˆ†åˆ¥ç‚º $0$ and $H(Y)$. ç›´è¦ºä¸Šæˆ‘å€‘å¯ä»¥èªç‚º $0\\leq H(Y|X) \\leq H(Y)$, å› ç‚ºå‰›å¥½æ˜¯å…©å€‹æ¥µç«¯æƒ…å½¢. ä½†ç›´è¦ºæ˜¯ä¸å¤ çš„, æˆ‘å€‘è­‰æ˜ä¸€ä¸‹. ä½¿ç”¨å®šç¾©, æˆ‘å€‘å¯ä»¥å¾ˆå®¹æ˜“æ¨å°å‡º $$\\begin{align} \\color{orange}{I(X;Y)\\equiv}H(Y)-H(Y|X)=\\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)p(y)}}} \\\\ =D(p(x,y)\\Vert p(x)p(y)) \\geq 0 \\end{align}$$ æˆ‘å€‘é€™è£¡å…ˆå·è·‘å‡ºäº† mutual information $I(X;Y)$ çš„å®šç¾©, ä¸‹é¢æœƒå†è©³ç´°è¬›. ç›´è§€è§£é‡‹:$H(Y|X)$ è¡¨ç¤ºçµ¦äº† $X$ çš„è³‡è¨Šå¾Œ, $Y$ å‰©ä¸‹çš„è³‡è¨Šé‡. å…¶ä¸­ $0\\leq H(Y|X) \\leq H(Y)$ Chain Rule for Entropyæˆ‘å€‘åœ¨æŠŠ (7) åšå€‹é‹ç®—: $$\\begin{align} H(Y|X)=(7)=-\\sum_x{p(x)\\sum_y{p(y|x)\\log{p(y|x)}}} \\\\ =-\\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)}}} \\\\ =-\\sum_{x,y}{p(x,y)\\log{p(x,y)}}+\\sum_{x,y}{p(x,y)\\log{p(x)}} \\\\ =H(X,Y) - H(X) \\end{align}$$ ç›´è§€è§£é‡‹:çµ¦å®š $X$ å¾Œ $Y$ å‰©ä¸‹çš„è³‡è¨Šé‡ ($H(Y|X)$) å°±ç­‰æ–¼ $X,Y$ æ•´é«”çš„è³‡è¨Šé‡æ‰£æ‰å–®ç¨ $X$ éƒ¨åˆ†çš„è³‡è¨Šé‡ Mutual Information(9) å·²ç¶“å…ˆçµ¦å‡ºäº†å®šç¾©, æˆ‘å€‘é€™è£¡é‡è¤‡ä¸€æ¬¡: $$\\begin{align} I(X;Y)\\equiv\\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)p(y)}}} =D(p(x,y)\\Vert p(x)p(y))\\\\ \\text{(ç”¨ entropy å®šç¾©å¾—åˆ°) }=H(Y)-H(Y|X) \\\\ \\text{(ç”¨ entropy å®šç¾©å¾—åˆ°) }=H(X)-H(X|Y) \\end{align}$$ åœ¨ç”¨ Chain Rule for Entropy: $H(Y)=H(X,Y)-H(X|Y)$ ä»£é€² (16) å¾—åˆ° $$\\begin{align} I(X;Y)=H(X,Y)-H(Y|X)-H(X|Y) \\end{align}$$ æˆ‘çŸ¥é“é€™éº¼å¤šå¼å­ä¸€å®šçœ¼èŠ±äº†â€¦.å¥½åœ¨å®Œå…¨å¯ä»¥ç”¨é›†åˆçš„ Venn diagram è¡¨ç¤º! æ‰€æœ‰å¼å­çš„ç›´è§€è¡¨é” Reference wiki mutual information Dr. Yao Xie, ECE587, Information Theory, Duke University æœ¬ç¯‡å…§å®¹åªæ˜¯ lecture 1 ~ 4 çš„ç¯„åœ","tags":[{"name":"Entropy","slug":"Entropy","permalink":"http://yoursite.com/tags/Entropy/"},{"name":"Conditional Entropy","slug":"Conditional-Entropy","permalink":"http://yoursite.com/tags/Conditional-Entropy/"},{"name":"Cross Entropy","slug":"Cross-Entropy","permalink":"http://yoursite.com/tags/Cross-Entropy/"},{"name":"Mutual Information","slug":"Mutual-Information","permalink":"http://yoursite.com/tags/Mutual-Information/"}]},{"title":"Determinant of Covariance Matrix","date":"2019-07-15T13:41:13.000Z","path":"2019/07/15/Determinant-of-Covariance-Matrix/","text":"ç­†è¨˜ covariance matrix $R$ çš„ determinant æ„ç¾©ä»¥åŠä»–çš„ bound. é€™æ˜¯åœ¨è®€ Time-delay estimation via linear interpolation and cross correlation æ™‚çš„ appendix è­‰æ˜. è¦ºå¾—æœ‰ç”¨å°±ç­†è¨˜ä¸‹ä¾†. é–‹é–€è¦‹å±±, $det(R)$ å¯ä»¥æƒ³æˆ volumn (ç­‰æ–¼æ‰€æœ‰ eigenvalues ç›¸ä¹˜), ç„¶å¾Œ upper bound å°±æ˜¯æ‰€æœ‰å°è§’é …å…ƒç´ ç›¸ä¹˜. $$\\begin{align} det(R)=\\prod_i \\lambda_i \\leq \\prod_i r_{ii} \\end{align}$$ $\\lambda_i$ æ˜¯ i-th eingenvalue. äº‹å¯¦ä¸Šåªè¦ $R$ æ˜¯ square matrix, å‰‡ $|det(R)|$ ç­‰æ–¼ç”¨æ¯å€‹ row vector åšå‡ºä¾†çš„ â€œå¹³è¡Œå…­é¢é«”â€ çš„é«”ç© [ref] ä»¥ä¸‹ç­†è¨˜è«–æ–‡ä¸­è­‰æ˜ $det(R)$ çš„ upper bound, å¾é€™å€‹ bound æˆ‘å€‘ä¹Ÿèƒ½çœ‹å‡ºç‰©ç†æ„ç¾©. Upper bound of the determinant of positive definite matrix[Theorem]: ä»¤ $H$ ç‚º $L$ by $L$ æ­£å®šçŸ©é™£, å‰‡$$\\begin{align} det(H) \\leq \\prod_{i=1}^{L} h_{ii} \\end{align}$$ [Proof]:å…ˆå°‡ $H$ ä½œå¦‚ä¸‹æ‹†è§£$$\\begin{align} H=\\left( \\begin{array}{cc} \\tilde{H} &amp; h \\\\ h^T &amp; h_{LL} \\end{array} \\right) \\end{align}$$ å…¶ä¸­ $\\tilde{H}$ æ˜¯ $L-1$ by $L-1$ çŸ©é™£.å¾ Determinant of block matrices æˆ‘å€‘çŸ¥é“:$$\\begin{align} det(H)=det(\\tilde{H})(h_{LL}-h^T \\tilde{H}^{-1}h) \\end{align}$$ å› ç‚º $H$ æ˜¯æ­£å®š, æ‰€ä»¥ $\\tilde{H}$ ä¹Ÿæ˜¯æ­£å®š, åŒ…å«å…¶ inverse Every principal submatrix of a positive definite matrix is positive definite. æ­£å®šçš„ $det&gt;0$, ä»¥åŠæ­£å®šçš„äºŒæ¬¡å¼ $&gt;0$, å¸¶å…¥åˆ° (4) å°±ä¸é›£ç™¼ç¾$$\\begin{align} det(H)\\leq h_{LL}det(\\tilde{H}) \\end{align}$$ é‡è¤‡æ­¤æ­¥é©Ÿå°±èƒ½æ¨å°å‡º (2) Determinant of covariance matrixæˆ‘å€‘çŸ¥é“ covariance matrix $R$ æ˜¯æ­£å®š (åš´æ ¼ä¸Šç‚ºåŠæ­£å®š, å¦‚æœæ²’æœ‰å…©å€‹å®Œå…¨ linear depedent çš„ç¶­åº¦çš„è©±, å°±æ˜¯æ­£å®š), å› æ­¤ç¬¦åˆ upper bound (2). è§€å¯Ÿç•¶ coordinate ä¹‹é–“ç‚º independent æ™‚, è¡¨ç¤ºéå°è§’é …éƒ½æ˜¯ $0$, åªå‰©ä¸‹å°è§’é … (æ¯å€‹ç¶­åº¦çš„ variance). é€™æ™‚ (2) çš„ä¸ç­‰å¼è®Šæˆç­‰å¼, å°è§’é …ç›¸ä¹˜æ„ç¾©ç›¸ç•¶æ–¼ç®— volumn å¯ä»¥çœ‹å‡ºå…©é»çµè«– covariance matrix å°è§’é …çš„ç›¸ä¹˜ç¸½æ˜¯æœƒæ¯” $det$ å¤§ coordinate ä¹‹é–“æ˜¯ independent å‰‡ covariance matrix å°è§’é …çš„ç›¸ä¹˜æœƒç­‰æ–¼ $det$ Correlation matrixæˆ‘å€‘çŸ¥é“ correlation matrix å°è§’é …éƒ½æ˜¯ $1$, ä¸”æ˜¯æ­£å®šæ ¹æ“šä»¥ä¸Šçš„è¨è«–çŸ¥é“:$$\\begin{align} 0\\leq det(\\mbox{corr}(X))\\leq 1 \\end{align}$$ Take Home Messagesä»¤ $R$ ç‚º covariance matrix, $\\tilde{R}$ æ˜¯ correlation matrix $R$ å°è§’é …çš„ç›¸ä¹˜ç¸½æ˜¯æœƒæ¯” $det(R)$ å¤§ coordinate ä¹‹é–“æ˜¯ independent å‰‡ $R$ å°è§’é …çš„ç›¸ä¹˜ç­‰æ–¼ $det(R)$ $det(\\tilde{R})$ åœ¨ 0 å’Œ 1 ä¹‹é–“ (åŒ…å«) ä»¤ $A$ ç‚º square matrix, å‰‡ $|det(A)|$ ç­‰æ–¼ä»¥æ¯å€‹ row vector åšå‡ºä¾†çš„ â€œå¹³è¡Œå…­é¢é«”â€ çš„é«”ç© [ref] Reference Time-delay estimation via linear interpolation and cross correlation Determinants and Volumes","tags":[{"name":"Covariance matrix","slug":"Covariance-matrix","permalink":"http://yoursite.com/tags/Covariance-matrix/"},{"name":"Correlation matrix","slug":"Correlation-matrix","permalink":"http://yoursite.com/tags/Correlation-matrix/"},{"name":"determinant","slug":"determinant","permalink":"http://yoursite.com/tags/determinant/"}]},{"title":"TF Notes (6), Candidate Sampling, Sampled Softmax Loss","date":"2019-07-02T12:34:12.000Z","path":"2019/07/02/TF-Notes-Candidate-Sampling/","text":"NN åšåˆ†é¡æœ€å¾Œä¸€å±¤é€šå¸¸ä½¿ç”¨ softmax loss, ä½†å¦‚æœé¡åˆ¥æ•¸é‡å¾ˆå¤§æœƒå°è‡´è¨ˆç®— softmax çš„ cost å¤ªé«˜, é€™æ¨£æœƒè®“è¨“ç·´è®Šå¾—å¾ˆæ…¢. å‡å¦‚ç¸½å…±çš„ class æ•¸é‡æ˜¯ 10000 å€‹, candidate sampling çš„æƒ³æ³•å°±æ˜¯å°æ–¼ä¸€å€‹ input $x$ æ¡æ¨£å‡ºä¸€å€‹ subset (ç•¶ç„¶éœ€è¦åŒ…å«æ­£ç¢ºçš„ label), è­¬å¦‚åªç”¨ 50 å€‹ classes, æ‰£æ‰æ­£ç¢ºçš„é‚£å€‹ class, å‰©ä¸‹çš„ 49 å€‹ classes å¾ 9999 å€‹æ¡æ¨£å‡ºä¾†. ç„¶å¾Œè¨ˆç®— softmax åªåœ¨é‚£ 50 å€‹è¨ˆç®—. é‚£éº¼å•é¡Œä¾†äº†, é€™æ¨£çš„æ¡æ¨£æ–¹å¼æœ€çµ‚è¨“ç·´å‡ºä¾†çš„ logits æœƒæ˜¯å°çš„å—? å®ƒèˆ‡æœªæ¡æ¨£å‰ (full set) çš„ logtis æœ‰ä½•å°æ‡‰é—œä¿‚? æ¡ç”¨ candidate sampling æ–¹å¼çš„ softmax loss åœ¨ tensorflow ä¸­å·²ç¶“ç›´æ¥æœ‰ op äº†, åƒè€ƒ tf.nn.sampled_softmax_loss. æ–‡æª”è£¡æœ€çµ‚æ¨å°å¾—åˆ°å¦‚ä¸‹çš„ä¸€å€‹å¼å­: $$\\begin{align} \\log(P(y|x_i,C_i))=\\log(P(y|x_i))-\\log(Q(y|x_i))+K&apos;(x_i,C_i) \\end{align}$$ æ¨å°éç¨‹è‡ªè¡Œçœ‹æ–‡æª”å°±å¯ä»¥, é‡è¦çš„æ˜¯äº†è§£å¼å­çš„ç‰©ç†æ„ç¾©.$C_i$ æ˜¯å° input $x_i$ æ¡æ¨£å‡ºçš„ subset, åŒ…å«äº† ä¸€å€‹æ­£ç¢ºçš„é¡åˆ¥æ¨™ç±¤ å’Œ å…¶ä»–æ¡æ¨£å‡ºçš„é¡åˆ¥ $S_i$. $Q(y|x_i)$ æ˜¯åŸºæ–¼ input $x_i$, label $y$ è¢«é¸ä¸­æˆç‚º $S_i$ çš„æ©Ÿç‡. $Kâ€™$ æ˜¯è·Ÿ $y$ ç„¡é—œçš„, æ‰€ä»¥å°æ–¼å¼å­ä¾†èªªæ˜¯ constant. æ³¨æ„åˆ°å¼å­çš„è®Šæ•¸æ˜¯ $y$ ä»£è¡¨äº†æ˜¯ softmax çš„å“ªä¸€å€‹ output node. å¼ (1) çš„è§£é‡‹ç‚º: â€œåœ¨ candidate set $C_i$ ä¸‹çš„ logits çµæœâ€ ç­‰æ–¼ â€œåœ¨ full set ä¸‹çš„ logtis çµæœæ¸›å» $\\log Q(y|x_i)$â€, $Kâ€™$ æœƒç›´æ¥è¢« $\\log P(y|x_i)$ å¸æ”¶, å› ç‚º logits åŠ ä¸Š constant å°æ–¼ softmax ä¾†èªªæœƒåˆ†å­åˆ†æ¯æ¶ˆæ‰, æ‰€ä»¥ä¸å½±éŸ¿. ä»¥ä¸‹æˆ‘å€‘é †ä¾¿è¤‡ç¿’ä¸€ä¸‹, ç‚ºä»€éº¼ logits å¯ä»¥å¯«æˆ â€œ$\\mbox{const}+\\log P(y|x)$â€ é€™ç¨®å½¢å¼. (åŒ…å«è¤‡ç¿’ Entropy, cross-entropy, softmax loss) Entropy å®šç¾©$$\\begin{align} \\sum_i{q(x_i)\\log{\\frac{1}{q(x_i)}}} \\end{align}$$ å°æ–¼ input $x_i$, å…¶æ©Ÿç‡ç‚º $q(x_i)$, è‹¥æˆ‘å€‘ä½¿ç”¨ $\\log{\\frac{1}{q(x_i)}}$ é€™éº¼å¤š bits çš„æ•¸é‡ä¾† encode å®ƒçš„è©±, å‰‡ä¸Šé¢çš„ entropy ä»£è¡¨äº† encode æ‰€æœ‰ input æ‰€éœ€è¦çš„å¹³å‡ bits æ•¸, è€Œé€™å€‹æ•¸æ˜¯æœ€å°çš„. ç”¨éŒ¯èª¤çš„ encoding æ–¹å¼æˆ‘å€‘å‡è¨­ç”¨ $\\log{\\frac{1}{p(x_i)}}$ é€™éº¼å¤š bits çš„æ•¸é‡ä¾† encode çš„è©±, å‰‡å¹³å‡ encode bits æ•¸ç‚º: $$\\begin{align} \\sum_i{q(x_i)\\log{\\frac{1}{p(x_i)}}} \\end{align}$$ é€™å€‹æ•¸é‡ä¸€å®šæœƒæ¯” entropy ä¾†çš„å¤§, è€Œå¤§å‡ºä¾†çš„å€¼å°±æ˜¯æˆ‘å€‘ä½¿ç”¨éŒ¯èª¤çš„ encoding é€ æˆçš„ä»£åƒ¹ (cross-entropoy). Cross-entropyå¦‚ä¸Šé¢æ‰€èªª, éŒ¯èª¤çš„ encoding æ–¹å¼é€ æˆçš„ä»£åƒ¹å¦‚ä¸‹: $$\\begin{align} \\mbox{Xent}(p,q)\\triangleq\\sum_i{q(x_i)\\log{\\frac{1}{p(x_i)}}} - \\sum_i{q(x_i)\\log{\\frac{1}{q(x_i)}}} \\\\ =\\sum_i{q(x_i)\\log{\\frac{q(x_i)}{p(x_i)}}} \\\\ \\end{align}$$ Sparse softmax lossæœ€å¸¸è¦‹çš„æƒ…å½¢ç‚ºç•¶åªæœ‰ $q(x_j)=1$ è€Œå…¶ä»– $x\\neq x_j$ æ™‚ $q(x)=0$ çš„è©± ($q$ è®Šæˆ one-hot), ä¸Šé¢çš„ corss-entropy è®Šæˆ: $$\\begin{align} \\mbox{SparseSoftmaxLoss}\\triangleq\\mbox{Xent}(p,q\\mbox{ is one-hot})=-\\log p(x_j) \\\\ =-\\log\\frac{e^{z_j}}{\\sum_i{e^{z_i}}}=-\\log e^{z_j} + \\log\\sum_i{e^{z_i}} \\\\ =-z_j + \\log\\sum_i{e^{z_i}} \\end{align}$$ å…¶ä¸­ $z_i$ è¡¨ç¤º i-th logtis, åƒè€ƒ tf.nn.sparse_softmax_cross_entropy_with_logits Logits çš„è§£é‡‹j-th logtis $z_j$ å¯è§£é‡‹ç‚º â€œconst + class $j$ çš„ log probabilityâ€. $$\\begin{align} z_j = \\mbox{cosnt} + \\log p(j) \\end{align}$$ ç‚ºä»€éº¼å‘¢? é€™æ˜¯å› ç‚º logtis ç¶“é softmax å¾Œæœƒè®Šæˆæ©Ÿç‡, æˆ‘å€‘å‡è¨­ç¶“é softmax å¾Œ node $j$ çš„æ©Ÿç‡ç‚º $pâ€™(j)$, è¨ˆç®—ä¸€ä¸‹é€™å€‹å€¼: $$\\begin{align} p&apos;(j)=\\frac{e^{z_j}}{\\sum_i e^{z_i}} \\\\ =\\frac{e^{\\log p(j)}e^{\\mbox{const}}}{e^{\\mbox{const}}\\sum_i e^{\\log p(i)}} \\\\ =\\frac{p(j)}{\\sum_i p(i)} \\\\ =p(j) \\end{align}$$ é€™æ™‚å€™æˆ‘å€‘å†å›å»å°ç…§é–‹å§‹çš„å¼ (1), å°±èƒ½æ¸…æ¥šçš„è§£é‡‹ candidate sampling çš„ logtis å’Œ full set çš„ logits ä¹‹é–“çš„é—œä¿‚äº†. Sampled softmax lossç”±å¼ (1) æˆ‘å€‘å·²ç¶“çŸ¥é“ candidate sampling çš„ logtis å’Œ full set çš„ logits ä¹‹é–“çš„é—œä¿‚. å› æ­¤åœ¨è¨“ç·´çš„æ™‚å€™, æ­£å¸¸ forward propagation åˆ° logits æ™‚, é€™æ™‚å€™çš„ logits æ˜¯ full set çš„. ä½†ç”±æ–¼æˆ‘å€‘è¨ˆç®— softmax åªæœƒåœ¨ candidate set ä¸Š. å› æ­¤è¦æŠŠ full set logits æ¸›å» $\\log Q(y|x_i)$, æ¸›å®Œå¾Œæ‰æœƒæ˜¯æ­£ç¢ºçš„ candiadtes logits. å°æ–¼ inference éƒ¨åˆ†, å‰‡å®Œå…¨ç…§èˆŠ, å› ç‚ºåŸæœ¬ forward propagation çš„çµæœå°±æ˜¯ full set logits äº†. é€™ä¹Ÿæ˜¯ tf å®˜ç¶²ç¯„ä¾‹é€™éº¼å¯«çš„åŸå› : 123456789101112131415if mode == \"train\": loss = tf.nn.sampled_softmax_loss( weights=weights, biases=biases, labels=labels, inputs=inputs, ..., partition_strategy=\"div\")elif mode == \"eval\": logits = tf.matmul(inputs, tf.transpose(weights)) logits = tf.nn.bias_add(logits, biases) labels_one_hot = tf.one_hot(labels, n_classes) loss = tf.nn.softmax_cross_entropy_with_logits( labels=labels_one_hot, logits=logits) Reference tf.nn.sampled_softmax_loss Candidate Sampling tf.nn.sparse_softmax_cross_entropy_with_logits","tags":[{"name":"Entropy","slug":"Entropy","permalink":"http://yoursite.com/tags/Entropy/"},{"name":"Candidate sampling","slug":"Candidate-sampling","permalink":"http://yoursite.com/tags/Candidate-sampling/"},{"name":"Sampled softmax loss","slug":"Sampled-softmax-loss","permalink":"http://yoursite.com/tags/Sampled-softmax-loss/"}]},{"title":"SphereFace Paper Study and Implementation Notes","date":"2019-06-18T13:13:46.000Z","path":"2019/06/18/SphereFace-paper-study-and-implementation-notes/","text":"SphereFace: Deep Hypersphere Embedding for Face Recognition ä½¿å¾—è¨“ç·´å‡ºä¾†çš„ embeddings å¯ä»¥å¾ˆå¥½çš„ä½¿ç”¨ cosine similarity åš verification/identification. å¯ä»¥å…ˆç¶²è·¯ä¸Šæœå°‹ä¸€ä¸‹å…¶ä»–äººçš„ç­†è¨˜å’Œè¨è«–, ç•¶ç„¶ç›´æ¥çœ‹è«–æ–‡æœ€å¥½.ä¸€èˆ¬ä¾†èªªæˆ‘å€‘å°è¨“ç·´é›†çš„æ¯å€‹äººç”¨ classification çš„æ–¹å¼è¨“ç·´å‡º embeddings, ç„¶å¾Œåœ¨æ¸¬è©¦çš„æ™‚å€™å¯ä»¥å°æ¯”å…©å€‹äººçš„ embeddings ä¾†åˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€å€‹äºº. ä½¿ç”¨ verification ç•¶ä¾‹å­, å¯¦ç”¨ä¸Šæ¸¬è©¦çš„äººä¸æœƒå‡ºç¾åœ¨è¨“ç·´é›†ä¸­, æ­¤æƒ…å½¢ç¨±ç‚º openset è¨­å®š. æ³¨æ„åˆ° embedding æ˜¯ä½¿ç”¨ classification æ–¹å¼è¨“ç·´å‡ºä¾†, ä¹Ÿå°±æ˜¯èªª, å¦‚æœè¨“ç·´é›†æœ‰ 1000 å€‹äºº, æœ€å¾Œä¸€å±¤çš„ softmax å°±æœ‰ 1000 å€‹ nodes. ç„¶å¾Œ embedding ä¸€èˆ¬å– softmax å‰ä¸€å±¤ (å‰å…©å±¤ä¹Ÿå¯).æ¸¬è©¦æ™‚å¸¸è¦‹çš„åšæ³•å°±æ˜¯è¨ˆç®—å…©å€‹ embeddings çš„ cosine similarity, ç›´è§€ä¸Šç›¸åŒçš„äººä»–å€‘çš„ embedding æœƒæ¥è¿‘, å› æ­¤å¤¾è§’å° (cosine å¤§), è€Œä¸åŒçš„äººå¤¾è§’å¤§ (cosine å°).ä½†å•é¡Œä¾†äº†, ç•¶åˆè¨“ç·´ embedding æ™‚ä¸¦æ²’æœ‰é‡å° classification ç”¨å¤¾è§’ä¾†åˆ†é¡, ä¹Ÿå°±ä¸èƒ½ä¿è­‰ softmax loss å°æ–¼ä½¿ç”¨ cosine similarity æ˜¯æœ€æœ‰æ•ˆçš„. Modified softmax loss (M-softmax loss) å’Œ Angular softmax loss (A-softmax loss) å°±èƒ½é‡å°é€™ç¨®æƒ…å½¢ (æ¸¬è©¦æ™‚ä½¿ç”¨ cosine similarity) è¨ˆç®— loss. A-softmax loss æ¯” M-softmax loss æ¢ä»¶æ›´åš´è‹›, é™¤äº†å¸Œæœ›é‡å° angular åšåˆ†é¡å¤–, é‚„å¸Œæœ›åŒä¸€é¡çš„å¤¾è§’èƒ½èšå†ä¸€èµ·, ä¸åŒé¡çš„å¤¾è§’èƒ½ç›¡é‡åˆ†é–‹. ä¸‹é¢å°±èªªæ˜ä¸€ä¸‹ softmax loss, M-softmax loss and A-softmax loss, ç„¶å¾Œä»¥ tensorflow çš„å¯¦ä½œä¾†èªªæ˜ Softmax Losså…¶å¯¦æ²’ä»€éº¼å¥½èªªæ˜çš„, å…¬å¼å¦‚ä¸‹ Decision boundary ä»¥å…©é¡ä¾†çœ‹å¦‚ä¸‹: $$\\begin{align} (W_1 - W_2)x+b_1 - b_2=0 \\end{align}$$ M-Softmax Losså¦‚æœæˆ‘å€‘å°‡ $W_j$ çš„ norm é™åˆ¶ç‚º 1, ä¸”å»æ‰ biases, $b_j=0$, å‰‡åŸä¾†çš„ softmax loss è®Šæˆå¦‚ä¸‹: Decision boundary ä»¥å…©é¡ä¾†çœ‹å¦‚ä¸‹: $$\\begin{align} \\parallel x \\parallel (\\cos \\theta_1 - \\cos \\theta_2)=0 \\Rightarrow \\cos \\theta_1 = \\cos \\theta_2 \\end{align}$$ æˆ‘å€‘å¯ä»¥ç™¼ç¾ decision boundary å®Œå…¨ç”±å¤¾è§’ä¾†æ±ºå®šäº†! è«–æ–‡ä½¿ç”¨ toy example ä¾†èªªæ˜ M-softmax loss é€ æˆçš„ç¾è±¡: A-Softmax Lossä»¥å…©é¡ä¾†èªªæ˜, M-softmax loss å°‡ $x$ åˆ†é¡æˆ class 1 çš„æ¢ä»¶ç‚º $\\cos \\theta_1 &gt; \\cos \\theta_2$, ä¹Ÿå°±æ˜¯ $\\theta_1 &lt; \\theta_2$. A-softmax loss å‰‡è®“é€™å€‹æ¢ä»¶æ›´åš´æ ¼, å®ƒå¸Œæœ› $m$ å€çš„ $\\theta_1$ éƒ½é‚„å°æ–¼ $\\theta_2$, å› æ­¤æ¢ä»¶ç‚º $\\cos m\\theta_1 &gt; \\cos \\theta_2$. è«–æ–‡ä¸­ä»¥å¹¾ä½•çš„æ–¹å¼èªªæ˜å¾ˆæ¸…æ¥š: å› æ­¤ A-softmax loss å¦‚ä¸‹: è«–æ–‡ä½¿ç”¨ toy example ä¾†èªªæ˜ A-softmax loss é€ æˆçš„ç¾è±¡: å¯ä»¥çœ‹åˆ°ç›¸æ¯”æ–¼ M-softmax loss, A-softmax loss æœƒä½¿å¾— margin å¢å¤§ é€™ç¨® within class é è¿‘, between class æ‹‰é å°±å¦‚åŒ LDA çš„æ¦‚å¿µ. A-softmax ä¹Ÿèƒ½é€ æˆé€™ç¨®æ•ˆæœä¸”æ˜¯åœ¨ angular çš„ measure ä¸‹. è€Œå¸¸è¦‹çš„æƒ…å½¢éƒ½æ˜¯é‡å° euclidean distance, ä¾‹å¦‚ä½¿ç”¨ triplet loss (æ¨è–¦é€™ç¯‡ blog èªªæ˜å…·é«”ä¸” tensorflow å¯¦ç¾éå¸¸å²å®³). åŸå‰‡ä¸Šæˆ‘å€‘å¸Œæœ›èˆ‡ class $i$ çš„å¤¾è§’ $\\theta_i$ æ„ˆå°, æ‰€ç®—å‡ºä¾†çš„ logits ä¹Ÿå°±æ˜¯ $\\cos\\theta_i$ è¦æ„ˆå¤§, æ‰€ä»¥æ”¾å¤§ $m$ å€çš„å¤¾è§’æ‰€ç®—å‡ºä¾†çš„ logits, $\\cos m\\theta_i$ å¿…é ˆè¦è®Šå°.ä½†ç”±æ–¼ $\\cos$ æ˜¯ periodic function, ä¸€æ—¦ $m\\theta_i$ è¶…é $2\\pi$ å°±åè€Œå¯èƒ½ä½¿å¾— logits è®Šå¤§, é€™å°±é©å¾—å…¶åäº†. ç²¾ç¢ºä¾†èªª $\\cos m\\theta_i &lt; \\cos\\theta_i$ åªæœƒåœ¨ $\\theta_i$ å±¬æ–¼ $[0,\\pi/m]$ å€é–“ç¯„åœå…§æˆç«‹. å› æ­¤æˆ‘å€‘å¿…é ˆå° A-softmax loss ä½œå¦‚ä¸‹æ”¹å‹•: å…¶ä¸­ $$\\begin{align} \\psi(\\theta)=(-1)^k \\cos(m\\theta)-2k\\\\ \\mbox{where }\\theta\\in[\\frac{k\\pi}{m},\\frac{(k+1)\\pi}{m}]\\mbox{ and }k\\in[0,m-1] \\end{align}$$ æˆ‘å€‘å°‡ $\\psi$ ç•«å‡ºä¾†: å…©å€‹è§€å¯Ÿ: é¦–å…ˆ $\\psi$ çš„ç¢ºæœƒéš¨è‘—è§’åº¦è®Šå¤§è€Œè®Šå°, é€™ç¬¦åˆæˆ‘å€‘è¦çš„ logits çš„è¡Œç‚º. å†ä¾†è¦è¨ˆç®—å‡ºæ­£ç¢ºçš„ $\\psi(\\theta)$ å¿…é ˆè¦å…ˆçŸ¥é“ $k$, ä¹Ÿå°±æ˜¯éœ€è¦çŸ¥é“ $\\theta$ è½åœ¨å“ªå€‹å€é–“æ‰è¡Œ. ç¬¬äºŒé»å¯èƒ½æ¯”è¼ƒæ£˜æ‰‹, æˆ‘å€‘æ€è€ƒä¸€ä¸‹æ€éº¼åœ¨ tensorflow çš„ graph ä¸­å¯¦ç¾ â€¦. hmmâ€¦. å¥½åƒæœ‰é»éº»ç…© Tensorflow Implementation A-softmax Losså…¶å¯¦ç¶²è·¯ä¸Šå°±å¾ˆå¤š tensorflow çš„å¯¦ç¾äº†, ä¸çœ‹é‚„å¥½, ä¸€çœ‹æ‰ç™¼ç¾ A-softmax loss çš„ $\\psi$ å¯¦ç¾æ­¥é©Ÿå¦‚ä¸‹: é€™ä»€éº¼æ“ä½œ?! æ€éº¼è·ŸåŸä¾†ç†è§£çš„ (3) and (4) é•·ç›¸å·®é€™éº¼å¤š! ç¶²è·¯ä¸Šå¹¾ä¹å¤§å®¶éƒ½ç›´æ¥æ‹¿ä¾†ç”¨, ä¹Ÿæ²’ä»€éº¼èªªæ˜. ä¸éæˆ‘å€‘ä»”ç´°åˆ†æä¸€ä¸‹, é‚„æ˜¯èƒ½ç™¼ç¾ç«¯å€ª.é¦–å…ˆæ³¨æ„åˆ°é€™æ¨£çš„å¯¦ç¾æ˜¯åŸºæ–¼ $m=4$ åšçš„. (è«–æ–‡çš„å¯¦é©—æœ€å¾Œåœ¨é€™å€‹è¨­å®šæœ‰ä¸éŒ¯çš„æ•ˆæœ) å› æ­¤å°‡ $m=4$ å¥—å…¥ (3)(4) å¾—: $$\\begin{align} \\psi(\\theta)=(-1)^k \\cos(\\color{red}{4}\\theta)-2k\\\\ \\mbox{where }\\theta\\in[\\frac{k\\pi}{\\color{red}{4}},\\frac{(k+1)\\pi}{\\color{red}{4}}]\\mbox{ and }k\\in[0,\\color{red}{3}] \\end{align}$$ æ¥è‘—æˆ‘å€‘ä½œå¦‚ä¸‹åˆ†æ: ç™¼ç¾ $s3=(-1)^k$ å’Œ $s4=-2k$, å› æ­¤ $$\\begin{align} \\psi(\\theta)=\\color{green}{(-1)^k} \\cos(4\\theta)\\color{blue}{-2k} = \\color{green}{s3}[1-8\\cos^2\\theta +8\\cos^4\\theta]\\color{blue}{+s4} \\end{align}$$ è€Œ $\\cos\\theta$ å‰‡å› ç‚º weights $W$ çš„ norm é™åˆ¶ç‚º 1, æ‰€ä»¥åªéœ€è¦ $Wx$ å†é™¤ä»¥ $x$ çš„ norm å³å¯. åˆ°é€™è£¡æœ€éº»ç…©çš„å¯¦ä½œå•é¡Œåˆ†æå®Œç•¢, ä¾æ¨£ç•«è‘«è˜†ä¹Ÿå¯ä»¥åšå‡º $m=2$, $m=3$. SummaryTake home messages: M-softmax loss ç®—å‡ºä¾†çš„ embeddings åœ¨ test éšæ®µå¯ä»¥ç›´æ¥ç”¨ cosine measure A-softmax loss æ›´é€²ä¸€æ­¥ä½¿å¾—å„é¡åˆ¥ä¹‹é–“çš„è§’åº¦æ‹‰æ›´é–‹, é”åˆ° large margin æ•ˆæœ A-softmax loss å¯¦ä½œä¸Šä¸å¥½è¨“ç·´, å¯ä»¥ä½¿ç”¨è«–æ–‡ä¸­æåˆ°çš„è¨“ç·´æ–¹æ³•, ä¸€é–‹å§‹åå‘åŸä¾†çš„ softmax loss, ç„¶å¾Œæ¼¸æ¼¸åå‘ A-softmax loss M-softmax loss ç°¡å–®å¯¦ç”¨, ç¶“é weight norm = 1 çš„æ¢ä»¶, è«–æ–‡ä¸­èªªæ˜èƒ½å»æ‰ prior åˆ†å¸ƒ Reference SphereFace: Deep Hypersphere Embedding for Face Recognition Blog: Triplet loss","tags":[{"name":"SphereFace","slug":"SphereFace","permalink":"http://yoursite.com/tags/SphereFace/"},{"name":"Angular softmax loss","slug":"Angular-softmax-loss","permalink":"http://yoursite.com/tags/Angular-softmax-loss/"},{"name":"Modified softmax loss","slug":"Modified-softmax-loss","permalink":"http://yoursite.com/tags/Modified-softmax-loss/"}]},{"title":"Adaptive Filters ç°¡ä»‹ (2) Fast Convolution and Frequency Domain","date":"2019-06-08T15:35:35.000Z","path":"2019/06/08/Adaptive-Filters-Notes-2/","text":"ä¸Šä¸€ç¯‡èªªæ˜äº† time domain çš„ adaptive filters, ç”±æ–¼æ˜¯ sample-by-sample è™•ç†, å› æ­¤å¤ªæ…¢äº†ä¸å¯ç”¨, çœŸæ­£å¯ç”¨çš„éƒ½æ˜¯åŸºæ–¼ frequency domain. ä¸éåœ¨æ·±å…¥ä¹‹å‰, ä¸€å®šè¦å…ˆäº†è§£ convolution åœ¨ input ç‚º block-by-block çš„æƒ…æ³ä¸‹å¦‚ä½•åŠ é€Ÿ. æœ¬æ–‡å…§å®¹ä¸»è¦åƒè€ƒ Partitioned convolution algorithms for real-time auralization by Frank Wefers (æ›¸çš„ä»‹ç´¹ååˆ†è©³ç›¡). Convolution åˆ†é¡å¦‚ä¸‹: æˆ‘å€‘å°±é‡å°æœ€å¸¸ä½¿ç”¨çš„æƒ…å½¢ä»‹ç´¹: Input (UP) and Filter (0). é€™æ˜¯å› ç‚ºå¯¦éš›æ‡‰ç”¨ input æ˜¯ infinite length, æ‰€ä»¥éœ€è¦ block-by-block çµ¦å®š, è€Œ filter é€šå¸¸éƒ½æ˜¯ finite length, å¯ä»¥é¸æ“‡ä¸ partition, æˆ– uniformly partitioned ä»¥ä¾¿å¾—åˆ°æ›´ä½çš„å»¶é²æ•ˆæœ. é‡å° block-based input çš„ convolution, æˆ‘å€‘æœ‰å…©ç¨®æ¶æ§‹: OverLap-and-Add (OLA) OverLap-and-Save (OLS) OLAOLA ç›¸å°ä¾†èªªå¾ˆå¥½ç†è§£çš„. æ¯ä¸€å€‹æ–°ä¾†çš„ data block $x_i$ (é•·åº¦ç‚º $M$), éƒ½èˆ‡ filter $h$ (é•·åº¦ç‚º $N$) åš linear convolution, ç”¢ç”Ÿçš„ output $y_i$ (é•·åº¦ç‚º $M+N-1$) é–‹é ­çš„ $N-1$ å€‹çµæœèˆ‡å‰ä¸€å€‹output block é‡ç–Šçš„éƒ¨åˆ†ç–ŠåŠ  (â€œaddâ€), æ‰€ä»¥ç¨± overlap-and-ADD. ç¤ºæ„åœ–å¦‚ä¸‹: OLSOLS å‰‡å¾ output è§’åº¦ä¾†çœ‹. æ ¹æ“šç¾åœ¨çš„ output ä¾†æ±ºå®šéœ€è¦ç”¨åˆ°é‚£äº› input åš linear convolution. èˆ‰ä¾‹ input block $x_i$ é•·åº¦ç‚º $B=3$, filter $h$ é•·åº¦ç‚º $N=4$, å‰‡ output block $y_i$ çš„çµæœå¯ä»¥å¾ä¸‹åœ–ä¾†çœ‹å‡ºä¾†: æ³¨æ„åˆ°, æˆ‘å€‘ä¸€é–‹å§‹å…ˆå°‡ $h$ å³é‚Šè£œä¸Š $B-1=2$ å€‹ $0$, è€Œ input block $x_i$ å·¦é‚Šè£œä¸Š $N-1=3$ å€‹èˆŠçš„ input data. ç›®çš„æ˜¯æŠŠ $x_i$ å’Œ $h$ éƒ½æ¹Šæˆ $B+N-1$ é€™éº¼é•·.å‰‡æˆ‘å€‘å¯ä»¥ç™¼ç¾, é‡å°å¢é•·å¾Œçš„ input and filter åš lineaer convolution, é›–ç„¶æœƒå¾—åˆ°é•·åº¦ç‚º $2*(B+N-1)-1$ çš„ output, ä½†é€™å…¶ä¸­æœ‰ $B$ å€‹çµæœæ˜¯æˆ‘å€‘è¦çš„! å› æ­¤æˆ‘å€‘åªéœ€è¦ â€œsaveâ€ éœ€è¦çš„é€™ $B$ å€‹ output, å…¶ä»–éƒ½ä¸Ÿè¼ƒå³å¯. æ‰€ä»¥ç¨± overlap-and-SAVE. å¦‚ä½•æœ‰æ•ˆç‡çš„åš linear convolution?ä¸ç®¡æ˜¯ OLA æˆ– OLS éƒ½éœ€è¦å°å…©å€‹å›ºå®šé•·åº¦ (é€šå¸¸ä½¿ç”¨ padding $0$ æˆç­‰é•·) çš„ signal åš linear convolution. æ€éº¼æœ‰æ•ˆç‡çš„åš linear convolution å°±è®Šå¾—ååˆ†é‡è¦.æˆ‘å€‘éƒ½çŸ¥é“é »åŸŸçš„ç›¸ä¹˜ç›¸ç•¶æ–¼æ™‚åŸŸçš„ circular convolution. å› æ­¤å¦‚æœèƒ½ç”¨ ciruclar convolution ä¾†åšå‡º linear convolution çš„è©±, æˆ‘å€‘å°±èƒ½è½‰åˆ°é »åŸŸä¸Šå†ç›¸ä¹˜å°±å¯ä»¥äº†.Circular convolution çš„å®šç¾©å¦‚ä¸‹[1], å…¶å¯¦æ¦‚å¿µä¹Ÿå¾ˆå®¹æ˜“: æˆ‘å€‘åªéœ€è¦é©ç•¶åœ° padding zeros, å°±å¯ä»¥ä½¿å¾— padding å¾Œçš„ signals åš circular convolution æœƒç­‰æ–¼åŸä¾†çš„ singals åš linear convolution. å¦‚ä¸‹åœ–[1] å› æ­¤ä½¿ç”¨ FFT-domain çš„ circular convolution ä¾†å¯¦ç¾ fast linear convolution æµç¨‹å¦‚ä¸‹ Fast Conv with OLAåœ¨ OLA æ¶æ§‹ä¸­ä½¿ç”¨ FFT-domain çš„ circular convolution å¦‚ä¸‹: Padding zeros ä¸ç®¡åœ¨å‰é‚„æ˜¯åœ¨å¾Œéƒ½å¯ä»¥, åªè¦æ»¿è¶³ $K=\\geq M+B-1$ é¿å… aliasing å³å¯. Fast Conv with OLSåœ¨ OLS æ¶æ§‹ä¸­ä½¿ç”¨ FFT-domain çš„ circular convolution å¦‚ä¸‹: Input signal ä¸æ˜¯ padding zeros, è€Œæ˜¯åœ¨å·¦é‚Š padding ä¹‹å‰çš„ input è¨Šè™Ÿ (åƒè€ƒæœ¬ç¯‡ä¸Šé¢çš„ OLS æ®µè½), ç”¨é€™æ¨£çš„ padding æ–¹å¼ä¾†çœ‹ circular convolution çš„è©±, æ¯ä¸€æ¬¡æˆ‘å€‘å°± â€œsaveâ€ output çš„æœ€å¾Œ $B$ å€‹çµæœå³å¯. åœ¨å¯¦ä½œä¸Šé€šå¸¸æœƒå°‡ $B=N$, ä¸¦ä¸”è¨­å®š $K=2B=2N$, é€™æ¨£æˆ‘å€‘æ¯ä¸€æ¬¡åªéœ€è¦ä¿ç•™å‰ä¸€æ¬¡çš„ input block, ä¸¦ä¸” padding çµ¦æ–°ä¾†çš„ input block. Frequncy Domain Adaptive FilterFrequency Domain Adaptive Filter (FDAF) è«‹åƒè€ƒ [2], æ•´ç†çš„éå¸¸å¥½, æ‰€ä»¥é€™è£¡å°±ä¸å¤šæè¿°, å®Œå…¨å¯ä»¥ç…§è‘—å¯¦ä½œå‡ºä¾†! æˆ‘å€‘æœƒç™¼ç¾å…¶å¯¦å®ƒæ¡ç”¨çš„æ˜¯æˆ‘å€‘ä¸Šé¢èªªéçš„ Fast Convolution with OLS æ¶æ§‹, åªæ˜¯ filter å¿…é ˆ adaptive æ›´æ–°. ä»¥ä¸‹æ˜¯ python implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# In the frequency domain methods, notations are defined as:# x: reference signal, [-1, 1]# d: desired signal, [-1, 1]# step_size: step size# alpha: the alpha filter for tracking the energy for each bin# w: the retruned filter# e: the error signal, of size (itr_num,)# ========== FDAF (Frequency Domain Adaptive Filters)def FDAF(x,d,step_size,N=512,alpha=0.9): iter_num = len(d)//N-1 assert(iter_num&gt;0) # Init W = np.zeros(2*N,dtype=complex) pow_lambda = np.ones(2*N)*np.finfo(np.float32).eps rtn_e = np.zeros((iter_num-1)*N) # Main Iteration for itridx in range(1,iter_num): x_2blocks = x[(itridx-1)*N:(itridx+1)*N] # (2N) d_block = d[itridx*N:(itridx+1)*N] # (N) X = fft(x_2blocks) # (2N) Y = np.einsum('i,i-&gt;i',X,W) y = ifft(Y) # (2N) y = y[N:] # (N), discard first half block # print (y) # e = np.real(d_block - y) # (N) e = d_block - y # (N) # print(len(rtn_e)) rtn_e[(itridx-1)*N:itridx*N] = np.real(e) e = np.concatenate([np.zeros([N]),e]) # (2N) E = fft(e) # (2N) pow_lambda = alpha*pow_lambda + (1-alpha)*(np.abs(X)**2) # scale error signal, just like NLMS E = E/pow_lambda # Set the upper bound of E, to prevent divergence m_errThreshold = 0.2 Enorm = np.abs(E) # (2N) # print(E) for eidx in range(2*N): if Enorm[eidx]&gt;m_errThreshold: E[eidx] = m_errThreshold*E[eidx]/(Enorm[eidx]+1e-10) # Constraint Part gradient = np.einsum('i,i-&gt;i',X.conj(),E) # (2N) gradient = ifft(gradient) gradient[N:] = 0 gradient = fft(gradient) # (2N) # Update Part W = W + step_size*gradient return rtn_e Summaryæˆ‘å€‘ä»‹ç´¹äº†é‡å° input æ˜¯ block-by-block çµ¦å®šæ™‚, è¨ˆç®— linear convolution çš„å…©ç¨®æ¶æ§‹: OLA, OLS. è€Œå¦‚ä½•åŠ é€Ÿ linear convolution æˆ‘å€‘å‰‡ä»‹ç´¹äº†ä½¿ç”¨ circular convolution ä¾†ç­‰åƒ¹åœ°å®Œæˆ linear convolution. Circular convolution å¯ä»¥åˆ©ç”¨é »åŸŸç›¸ä¹˜ä¾†åŠ å¿«é€Ÿåº¦ (å¾—ç›Šæ–¼ FFT çš„æ•ˆç‡). é™¤äº†å° input åˆ‡ block ä¹‹å¤–, æˆ‘å€‘ä¹Ÿé‚„å¯ä»¥å° filter $h$ åˆ‡ block, é€™æ¨£çš„å¥½è™•æ˜¯è¨ˆç®—é‡å¯ä»¥åœ¨æ›´ä½, ä¸” latency ä¹Ÿæœƒé™ä½. é€™éƒ¨åˆ†è«‹åƒè€ƒæ›¸çš„ Ch5, é™„ä¸Šä¸€å¼µæ›¸æœ¬è£¡çš„æ¶æ§‹åœ–: é€™ç¨®æ–¹å¼å…¶å¯¦å¾ˆé‡è¦, åŸå› æ˜¯ webrtc ä¸­çš„ AEC æ¡ç”¨çš„æ˜¯ Partitioned Block Frequency Domain Adaptive Filter (PBFDAF) [3], å°±æ˜¯ filter ä¹Ÿæ˜¯ uniformly partitioned. æœ€å¾Œæˆ‘å€‘åˆ©ç”¨ OLA å’Œ fast convolution, åˆ—å‡ºä¾† frequency domain AF çš„æ¶æ§‹åœ–. åŒæ™‚å¦‚æœæƒ³è¦é€²ä¸€æ­¥é™ä½ latency å‰‡éœ€ä½¿ç”¨ PBFDAF[3] (filter ä¹Ÿ partition). Reference Partitioned convolution algorithms for real-time auralization by Frank Wefers Block Adaptive Filters and Frequency Domain Adaptive Filters by Prof. Ioan Tabus On the implementation of a partitioned block frequency domain adaptive filter (PBFDAF) for long acoustic echo cancellation","tags":[{"name":"Adaptive Filters","slug":"Adaptive-Filters","permalink":"http://yoursite.com/tags/Adaptive-Filters/"},{"name":"OLA","slug":"OLA","permalink":"http://yoursite.com/tags/OLA/"},{"name":"OLS","slug":"OLS","permalink":"http://yoursite.com/tags/OLS/"},{"name":"circular convolution","slug":"circular-convolution","permalink":"http://yoursite.com/tags/circular-convolution/"},{"name":"linear convolution","slug":"linear-convolution","permalink":"http://yoursite.com/tags/linear-convolution/"}]},{"title":"Adaptive Filters ç°¡ä»‹ (1) Time Domain","date":"2019-05-14T14:03:03.000Z","path":"2019/05/14/Adaptive-Filters-Notes/","text":"ç²—ç•¥ç­†è¨˜ time domain adaptive filters, frequency domain adaptive filters æœƒåœ¨ä¸‹ä¸€ç¯‡ç­†è¨˜. æ‡‰ç”¨ä»¥ Acoustic Echo Cancellation (AEC) ä¾†èªªæ˜. Motivationç›´æ¥ä½¿ç”¨ wiki. AEC è¦è§£æ±ºçš„æ˜¯å¦‚ä¸‹çš„æƒ…å½¢ å„å€‹è¨Šè™Ÿé—œè¯å¦‚ä¸‹: $$\\begin{align} y(n)=h(n)\\ast x(n) \\\\ d(n)=y(n)+v(n) \\\\ \\hat{y}(n)=\\hat{h}(n)\\ast x(n)\\\\ e(n)=d(n)-\\hat{y}(n) \\end{align}$$ ç›®çš„æ˜¯æ‰¾åˆ° $\\hat{h}$ æ»¿è¶³ä¸‹å¼: $$\\begin{align} \\hat{y}(n)\\approx y(n)\\Rightarrow e(n)\\approx v(n) \\end{align}$$ å°æ–¼ç¬¬ $n$ å€‹ sample é»ä¾†èªª, æˆ‘å€‘é€šå¸¸ä½¿ç”¨éå»(å«è‡ªå·±) $p$ å€‹ samples. ä¸‹é¢å°å¯«ç²—é«”è¡¨ç¤º vector, å¤§å¯«ç²—é«”è¡¨ç¤º matrix. Optimal Solutionä¹Ÿå°±æ˜¯ Wiener solution. ä½†åœ¨çœŸå¯¦ä¸–ç•Œä¸­, ä¸ç®¡ reference signal ($x(n)$) or desired signal ($d(n)$) éƒ½æ˜¯ non-stationary çš„. æœ€ç›´æ¥ä¸”æš´åŠ›çš„æƒ³æ³•å°±æ˜¯æ¯éš”ä¸€æ®µæ™‚é–“é‡æ–°ç®—ä¸€æ¬¡ Wiener solution. ä¸éæƒ³ç•¶ç„¶çˆ¾é€™æ˜¯è¡Œä¸é€šçš„. å› æ­¤å°±å¿…é ˆæ¡ç”¨ Stochastic update æ–¹å¼. Stochastic Update ä¸Šé¢ç´…è‰²çš„å¼å­å°±æ˜¯å…¸å‹çš„ LMS algorithm. å¦å¤–æˆ‘å€‘çŸ¥é“ optimization é‚„å¯ä»¥ä½¿ç”¨ second-moment, ä¹Ÿå°±æ˜¯ä½¿ç”¨äºŒéšå°å‡½æ•¸ (Hessian Matrix). é€™å°±æ˜¯ Newtonâ€™s method: é‡å° $\\mathbf{R}_{xx} \\mbox{ , } \\mathbf{R}_{xd}$ ä½¿ç”¨ä¸åŒçš„ approximation æ–¹å¼å°±æœƒå¾—åˆ°ä¸åŒæ¼”ç®—æ³•, ä¾‹å¦‚: ä¸Šé¢ç´…è‰²çš„å¼å­å°±æ˜¯å…¸å‹çš„ NLMS algorithm. ç”¨é€™æ¨£çš„æ–¹å¼é‚„å¯æ¨å‡º e-NLMS, leaky-LMS, RLS ç­‰ç­‰â€¦ å¯¦ä½œä¸Š NLMS åœ¨ reference signal $x(n)$ å¾ˆå°çš„æ™‚å€™, ç”±æ–¼å…¬å¼ä¸Šåˆ†æ¯æœƒé™¤ä»¥ $x^Hx$, è€Œåˆ†å­åªæœ‰ä¸€æ¬¡æ–¹ $x$, å› æ­¤é™¤ä¸‹ä¾†æœƒå°è‡´ gradient å®¹æ˜“è®Šå¤§, æ‰€ä»¥ç™¼æ•£. é€™æ˜¯ NLMS å¯¦ä½œä¸Šè¦è€ƒæ…®çš„æƒ…å½¢. Misadjustmentæˆ‘å€‘çŸ¥é“æœ€ä½³è§£ç‚º Wiener solution, ä½†ç”±æ–¼æˆ‘å€‘æ¡ç”¨ stochastic gradient æ–¹å¼, ä¹Ÿå°±æ˜¯èªª update çš„ gradient æœ¬èº«å­˜åœ¨èª¤å·®, é€™äº› gradient çš„ variance å°±ç›´æ¥å½±éŸ¿äº†æœ€çµ‚æ”¶æ–‚çš„æ•ˆæœè·Ÿ Wiener solution çš„æ”¶æ–‚çµæœä¹‹é–“çš„å·®è·. æ­¤å·®è·æˆ‘å€‘ç¨± misadjustment æˆ–ç¨± Excess Meam Square Error ç›´æ¥æ“·å– Ali Sayed, Adaptive Filters p230 çš„å®šç¾©: ç‚ºä»€éº¼è¦èªªé€™å€‹å‘¢? æ˜¯å› ç‚ºå¯¦ä½œä¸Šæœ‰å…©å€‹å› ç´ æœƒç›´æ¥å½±éŸ¿æœ€çµ‚æ”¶æ–‚æ•ˆæœçš„å¥½å£, åˆ†åˆ¥æ˜¯ tap length å’Œ step size. å¾ç†è«–åˆ†æå’Œå¯¦ä½œç¶“é©—ä¾†èªª, tap length å¤ªå°æœƒç„¡æ³•æœ‰æ•ˆæ¨¡æ“¬ RIR, è€Œå¤ªå¤§æœƒå°è‡´ EMSE æé«˜ (æ”¶æ–‚æ•ˆæœåè€Œè®Šå·®), å› æ­¤é¸å–çš„ tap length å¿…é ˆè¦æ ¹æ“š sampling rate å’Œè¦æ¶ˆé™¤çš„ echo path ä¾†è¨ˆç®—ä¸€ä¸‹. LMS or NLMS çš„ EMSE å¯çŒœè€ƒ Ali Sayed, Adaptive Filters p249 and p253. (æ¢ä»¶å·²ç°¡åŒ–åœ¨ ref and desired signals ç‚º stationary æƒ…æ³) å¦å¤– step size è¼ƒå°æœƒæœ‰è¼ƒå¥½çš„æ”¶æ–‚æ•ˆæœ, ä½†æ˜¯æ”¶æ–‚é€Ÿåº¦æœƒæ…¢ä¸” tracking èƒ½åŠ›è¼ƒå·®. ä¸€å€‹æœ‰æ•ˆçš„æ–¹å¼ç‚ºä½¿ç”¨ Practical Variable Step Size (PVSS) æ–¹æ³•, å…·é«”å¯åƒè€ƒ å¾…è£œ å¥½äº†, time domain åˆ°é€™å°±å·®ä¸å¤šäº†, ç¼ºé»ä¹Ÿå¾ˆæ˜é¡¯, æ…¢!, å› ç‚ºæ˜¯ sample-by-sample è™•ç†. æ¥è‘—ç¨å¾®æ¢³ç†ä¸€ä¸‹ frequency domain æ–¹æ³•. Reference wiki Least mean squares filter Ali Sayed, Adaptive Filters","tags":[{"name":"Adaptive Filters","slug":"Adaptive-Filters","permalink":"http://yoursite.com/tags/Adaptive-Filters/"},{"name":"AEC","slug":"AEC","permalink":"http://yoursite.com/tags/AEC/"},{"name":"LMS","slug":"LMS","permalink":"http://yoursite.com/tags/LMS/"},{"name":"NLMS","slug":"NLMS","permalink":"http://yoursite.com/tags/NLMS/"}]},{"title":"Far Field Notes (4) How Spatial Feature Clusters","date":"2019-04-12T13:36:17.000Z","path":"2019/04/12/Far-Field-Notes-4-How-Spatial-Feature-Clusters/","text":"é€™æ˜¯ far field ç­†è¨˜ç³»åˆ—ç¬¬å››ç¯‡, å¯«é€™ç¯‡æ˜¯å› ç‚ºåš CGMM-MVDR æ™‚, å¾ˆå¥½å¥‡ç‚ºä½• spatial features èšé¡çš„çµæœå¯ä»¥å°æ‡‰ä¸åŒæ–¹å‘çš„è²æº. å› æ­¤è¨˜éŒ„ä¸‹è‡ªå·±çš„ä¸€é»æƒ³æ³•. å‡è¨­æˆ‘å€‘æœ‰ $M$ å€‹éº¥å…‹é¢¨, å‰‡åœ¨ stft (short-time fourier transform) ä¸Šä¾†èªª, $\\mathbf{f}_{\\omega,t}$ è¡¨ç¤ºä¸€å€‹é »ç‡ $\\omega$, æ™‚é–“ $t$ çš„ $M$ ç¶­å‘é‡. å°æ–¼æŸä¸€å€‹ $\\theta$ æ–¹å‘çš„ narrowband è¨Šè™Ÿ, ideally æˆ‘å€‘å¯ä»¥é€™éº¼è¡¨ç¤º $$\\begin{align} \\mathbf{f}_{\\omega,t}^{\\theta}=f(\\omega)\\mathbf{\\upsilon}(\\theta)=f(\\omega) \\left[ \\begin{array}{clr} e^{-j\\omega\\tau_0} \\\\ e^{-j\\omega\\tau_1} \\\\ \\vdots \\\\ e^{-j\\omega\\tau_{M-1}} \\end{array} \\right] \\end{align}$$ $\\tau_i$ è¡¨ç¤ºç”± $\\theta$ ç”¢ç”Ÿçš„ç¬¬ $i$ å€‹ mic çš„ time delay. å› æ­¤ spatial feature æ¯å€‹ç¶­åº¦ä¹‹é–“çš„ phase offset é—œä¿‚æ˜¯å›ºå®šçš„, ç”± $\\mathbf{\\upsilon}(\\theta)$ æ±ºå®š. æ‰€æœ‰å¦‚æœæœ‰å…©å€‹æ–¹å‘ $\\theta_1$ and $\\theta_2$ çš„è²æº, phase offset é—œä¿‚å„è‡ªæ˜¯ $\\mathbf{\\upsilon}(\\theta_1)$ å’Œ $\\mathbf{\\upsilon}(\\theta_2)$. å•é¡Œæ˜¯è¦ç”¨ä»€éº¼æ¨£çš„ cluster èƒ½å°ç›¸åŒ phase offset é—œä¿‚çš„ complex vector èšé¡åœ¨ä¸€èµ·, è€Œå°ä¸åŒ phase offset é—œä¿‚èƒ½åˆ†é–‹å‘¢? é—œéµçš„ç­”æ¡ˆå°±æ˜¯ Circularly Symmetric Gaussian Distribution Circularly Symmetric Gaussian Distributionç›´æ¥å¼•ç”¨ slides è£¡çš„ä¸€æ®µå®šç¾© A complex Gaussian random vector $Z$ is circularly symmetric if $e^{j\\phi}Z$ has the same distribution as $Z$ for all real $\\phi$. æ„æ€å°±æ˜¯å¦‚æœæˆ‘å€‘ä¹˜ä¸Šå›ºå®šçš„ phase offset $\\phi$ (è²æºæœ‰ time delay), é€™ç›¸ç•¶æ–¼ä¸æ”¹è®Šç¶­åº¦ä¹‹é–“çš„ phase offset é—œä¿‚ (ä¸æ”¹è®Šè²æºæ–¹å‘ $\\theta$), é€™æ¨£çš„è©±å®ƒå€‘æœƒæ˜¯åŒä¸€å€‹æ©Ÿç‡åˆ†ä½ˆ, è€Œé€™ç¨®ç‰¹æ€§å®Œå…¨ç¬¦åˆæˆ‘å€‘çš„éœ€æ±‚! æˆ‘å€‘ç›´æ¥æ“·å– slide ä¸­çš„ Circularly Symmetric Gaussian Distribution çš„å®šç¾©: è©³ç´°è«‹è¦‹ [1] çš„ slides. Reference Circularly Symmetric Gaussian Random Vectors","tags":[{"name":"CGMM","slug":"CGMM","permalink":"http://yoursite.com/tags/CGMM/"},{"name":"Spatial","slug":"Spatial","permalink":"http://yoursite.com/tags/Spatial/"}]},{"title":"æ‡·èˆŠç¯‡, å–®é€šé“é™å™ª, MMSE-STSA, MMSE-LSA æ–¹æ³•","date":"2019-03-20T13:04:18.000Z","path":"2019/03/20/MMSE-STSA-and-LSA/","text":"è¨˜éŒ„ä¸€ä¸‹å–®é€šé“é™å™ªçš„ä¸€å€‹ç¶“å…¸æ–¹æ³•, MMSE-STSA, MMSE-LSA, å·²ç¶“æ˜¯ 1984 å·¦å³çš„æ–‡ç« äº†. å–®é€šé“é™å™ª OMLSA ä¹Ÿå¾é€™è¡ç”Ÿå‡ºä¾†çš„. æˆ‘å€‘å…ˆå¾ MMSE-STSA èªªèµ·, å…¨åæ˜¯ minimum mean-square error short time spectral amplitude.$y(t)=x(t)+d(t),0\\leq t\\leq T$$x$, $d$, $y$ åˆ†åˆ¥æ˜¯ speech, noise, å’Œæ”¶åˆ°çš„ noisy signal, å…¶ä¸­ $x$, $d$ ç›¸äº’ç¨ç«‹. ç›¸å°æ‡‰çš„ç¬¬ $k$ å€‹ frequency bin å¦‚ä¸‹:$$X_k=A_k\\exp(j\\alpha_k) \\\\ D_k \\\\ Y_k=R_k\\exp(j\\theta_k)$$ MMSE-STSA $^{[1]}$ç›®æ¨™å‡½å¼ç‚º$$\\begin{align} \\arg\\min_{\\hat{A}_k}{\\mathbb{E}\\left[\\left(A_k-\\hat{A}_k\\right)^2\\vert y(t),0\\leq t\\leq T\\right]} \\end{align}$$ æœ€ä½³è§£ç‚º$$\\begin{align} \\hat{A}_k=\\mathbb{E}\\left[A_k\\vert y(t),0\\leq t \\leq T\\right] \\end{align}$$ ä½†é—œéµæ˜¯æˆ‘å€‘ä¸çŸ¥é“ clean speech çš„ amplitude $A_k$, é‚£è©²æ€éº¼ä¼°å‘¢? é¦–å…ˆæˆ‘å€‘å°æ¯å€‹ frequency bin çš„åˆ†å¸ƒå‡è¨­ç‚º Gaussian distribution (complex). å¼•ç”¨åŸæ–‡ â€œSince the Fourier coefficient is, after all, a weighted sum (or integral) of random variables resulting from the random process samplesâ€, åœ¨ä¸€å€‹çŸ­æ™‚çš„ frame ä¸­å¤§è‡´ä¸Šæ˜¯ stationary, å› æ­¤å¯ä»¥çœ‹ä½œæ˜¯ä¸€å€‹ WSS çš„ ramdom process, å†åŠ ä¸Š cental limit theorem, å°±ç•¶ä½œé«˜æ–¯åˆ†å¸ƒå§. å¥—ç”¨ Guassian distribution å‡è¨­, åšå¦‚ä¸‹æ¨å°$$\\begin{align} \\hat{A}_k=\\mathbb{E}\\left[A_k\\vert y(t),0\\leq t \\leq T\\right]=\\mathbb{E}\\left[A_k\\vert Y_0,Y_1,...\\right] \\\\ =\\mathbb{E}\\left[A_k\\vert Y_k\\right] \\\\ =\\int_0^{\\infty}\\int_0^{2\\pi}a_k p(a_k,\\alpha_k\\vert Y_k)d\\alpha_k d a_k = \\int_0^{\\infty}\\int_0^{2\\pi}a_k \\frac{p(a_k,\\alpha_k,Y_k)}{p(Y_k)}d\\alpha_k d a_k \\\\ =\\frac{ \\int_0^{\\infty}\\int_0^{2\\pi}a_k p(Y_k\\vert a_k,\\alpha_k) p(a_k,\\alpha_k) d\\alpha_k d a_k }{ \\int_0^{\\infty}\\int_0^{2\\pi} p(Y_k\\vert a_k,\\alpha_k) p(a_k,\\alpha_k) d\\alpha_k d a_k } \\end{align}$$ å…¶ä¸­ (3) åˆ° (4) æˆ‘å€‘å‡è¨­æ¯å€‹ frequency bin æ˜¯ç¨ç«‹çš„ç”±æ–¼æˆ‘å€‘å‡è¨­æ¯å€‹ frequency bin éƒ½æ˜¯ complex Gaussian distribution, å› æ­¤ (6) çš„æ©Ÿç‡åˆ†ä½ˆå¦‚ä¸‹å®šç¾©:$$\\begin{align} p(Y_k\\vert a_k,\\alpha_k)=\\frac{1}{\\pi\\lambda_d (k)}\\exp\\left[ -\\frac{1}{\\lambda_d (k)}\\vert Y_k - a_k e^{j\\alpha_k} \\vert^2 \\right] \\\\ p(a_k,\\alpha_k)=\\frac{1}{\\pi\\lambda_x (k)}\\exp\\left[-\\frac{a_k^2}{\\lambda_x (k)}\\right] \\end{align}$$ æ³¨æ„åˆ° (7) èƒ½é€™éº¼å¯«æ˜¯å› ç‚ºæˆ‘å€‘çŸ¥é“ $x$ and $d$ äº’ç›¸ç¨ç«‹, å› æ­¤åœ¨çµ¦å®š $x$ çš„æƒ…å½¢ä¸‹, åªæ˜¯æ”¹è®Š mean çš„ä½ç½®, å…¶ variance ä»ç”± $d$ ä¾†æ±ºå®š. å¦å¤–:$$\\begin{align} \\lambda_x (k)=\\mathbb{E}\\left[\\vert X_k \\vert ^2\\right]=A_k^2 \\\\ \\lambda_d (k)=\\mathbb{E}\\left[\\vert D_k \\vert ^2\\right] \\end{align}$$ è¡¨ç¤ºç¬¬ $k$ å€‹ bin çš„ speech and noise çš„ varianceå°‡ (7) and (8) å¸¶å…¥ (6) ä¸¦æ„Ÿè¬å‰å¤§çš„ä½œè€…æ¨å°å¾—åˆ°:$$\\begin{align} \\hat{A}_k=\\Gamma(1.5)\\frac{\\sqrt{\\upsilon_k}}{\\gamma_k}M(-0.5;1;-\\upsilon_k)R_k \\\\ \\hat{A}_k=\\Gamma(1.5)\\frac{\\sqrt{\\upsilon_k}}{\\gamma_k}\\exp\\left(-\\frac{\\upsilon_k}{2}\\right)\\left[(1+\\upsilon_k)I_0(\\frac{\\upsilon_k}{2})+\\upsilon_k I_1(\\frac{\\upsilon_k}{2})\\right]R_k \\end{align}$$ å…¶ä¸­ $\\Gamma$ è¡¨ç¤º gamma function, $\\Gamma(1.5)=\\sqrt{\\pi}/2$; $M(a;c;x)$ æ˜¯ confluent hypergeometric function (é€™æ˜¯å¤–æ˜Ÿç¬¦è™Ÿå§), $I_0$ and $I_1$ æ˜¯ modified Bessel funciton of zero and first order. ç¸½ä¹‹å°±æ˜¯èƒ½å¸¶å…¥è¨ˆç®—çš„æ±è¥¿, æœ€é‡è¦, ä¹Ÿæ˜¯éœ€è¦æˆ‘å€‘ä¼°è¨ˆçš„è®Šæ•¸å¦‚ä¸‹:$$\\begin{align} \\upsilon_k\\triangleq \\frac{\\xi_k}{1+\\xi_k}\\gamma_k \\\\ \\color{orange}{ \\xi_k\\triangleq\\frac{\\lambda_x (k)}{\\lambda_d (k)} } \\\\ \\color{orange}{ \\gamma_k\\triangleq\\frac{R_k^2}{\\lambda_d (k)} } \\\\ \\end{align}$$ $\\xi_k$ å’Œ $\\gamma_k$ åˆ†åˆ¥ç¨±ç‚º prior SNR å’Œ posterior SNR. ç¸½ä¹‹å¦‚èƒ½ä¼°å‡º $\\xi_k$ å’Œ $\\gamma_k$, æˆ‘å€‘å°±èƒ½è¨ˆç®—å‡º gain å€¼, ä¹‹å¾Œçš„æ–¹æ³•å¦‚ LSA, OMLSA ä¹Ÿéƒ½å¦‚æ­¤. æ–‡ç« å¾Œé¢æœƒä½¿ç”¨ MCRA ä¾†ä¼°ç®—é€™å…©å€‹ SNR. ç¾åœ¨å°±ç®—å‚³çµ±æ–¹æ³•ä¸€èˆ¬ä¹Ÿå¾ˆå°‘ä½¿ç”¨ MMSE-STSA, è‡³å°‘æœƒä½¿ç”¨ LSA å–ä»£. LSA æœ‰è¿‘ä¼¼çš„è¨ˆç®—æ–¹å¼, å› æ­¤æˆ‘å€‘ä¹Ÿä¸ç³¾çµ (12) åˆ°åº•æ€éº¼ç®—å‡ºä¾†. MMSE-LSA $^{[2]}$å¤§è‡´æƒ³æ³•è·Ÿæµç¨‹è·Ÿä¸Šé¢ä¸€æ¨£(åªæ˜¯æˆ‘ç®—ä¸å‡ºä¾†), åªæ˜¯ç›®æ¨™å‡½æ•¸é‡å° log å€¼ä¾†è¨ˆç®—$$\\begin{align} \\arg\\min_{\\hat{A}_k}{\\mathbb{E}\\left[\\left(\\log A_k-\\log\\hat{A}_k\\right)^2\\vert y(t),0\\leq t\\leq T\\right]} \\end{align}$$ åŒæ¨£ç¶“éä¸æ˜¯äººé¡çš„è¨ˆç®—å¾Œå¾—åˆ°:$$\\begin{align} \\hat{A}_k=\\frac{\\xi_k}{1+\\xi_k}\\exp\\left[\\frac{1}{2}\\int_{\\upsilon_k}^{\\infty}\\frac{e^{-t}}{t}dt\\right]R_k \\end{align}$$ [3] çµ¦å‡ºäº†ä¸€å€‹å¥½ç®—çš„è¿‘ä¼¼çµæœ$$\\begin{align} \\int_{\\upsilon_k}^{\\infty}\\frac{e^{-t}}{t}dt\\approx \\left\\{ \\begin{array}{rcl} -2.31\\log_{10}(\\upsilon_k)-0.6\\mbox{ for }\\upsilon_k&lt;0.1 \\\\ -1.544\\log_{10}(\\upsilon_k)+0.166\\mbox{ for }0.1\\leq\\upsilon_k\\leq 1 \\\\ 10^{-(0.52\\upsilon_k+0.26)}\\mbox{ for }\\upsilon_k&gt;1 \\\\ \\end{array}\\right. \\end{align}$$ å¦å¤–é‚„æœ‰ optimally-modified log-spectral amplitude (OMLSA) [4] æ–¹æ³•, ä½œè€…æœ‰æä¾› MATLAB codes. é€™ç®—å–®é€šé“é™å™ªæ¨™é…äº†, ä½†å¯¦é©—çµæœå°è½è¦ºæœ‰å¹«åŠ©, å° WER ä¸ä¸€å®šé™ä½. ç¸½ä¹‹ä¸ç®¡å“ªä¸€ç¨®æ–¹æ³•, éƒ½å¿…é ˆå¾ˆå¥½çš„ä¼°å‡º prior and posterior SNR. MCRA Prior/Posterior SNR ä¼°è¨ˆé‡å° STFT æ™‚é–“ $l$, frequency bin $k$ ä¾†èªª, å‡è¨­æˆ‘å€‘å·²ä¼°å‡ºä¾† speech presence probability $p(k,l)$, æˆ‘å€‘å¯ä»¥é€™éº¼ update noise çš„ variance:$$\\begin{align} \\hat{\\lambda}_d(k,l+1)=\\hat{\\lambda}_d(k,l)p(k,l)+\\left[\\alpha_d\\hat{\\lambda}_d(k,l)+(1-\\alpha_d)|Y(k,l)|^2\\right](1-p(k,l)) \\end{align}$$ é€™å¾ˆå¥½ç†è§£, å¦‚æœæœ‰ speech çš„è©±, noise variance å°±æ²¿ç”¨åŸä¾†èˆŠçš„, è€Œå¦‚æœæ²’æœ‰ speech, nosie vaiance å°±è¦ç”¨ç•¶å‰ frame é€é $\\alpha_d$ å¹³æ»‘åœ°æ›´æ–°ä¸€ä¸‹ (å°±ç¨±é€™æ¨£çš„å¹³æ»‘ç‚º $\\alpha$ å¹³æ»‘). ä¼°è¨ˆ $p(k,l)$ ä¹‹å‰, æ–‡ç« çš„åšæ³•æ˜¯éƒ½å…ˆé‡å° time and frequency åšå¹³æ»‘. frequency å¯é¸ç”¨ä¸€å€‹ window (å¯ç”¨é¡ä¼¼ Gaussian window), è€Œæ™‚é–“ä¸Šçš„å¹³æ»‘å¯ä½¿ç”¨ $\\alpha$ å¹³æ»‘. ä»¤ $S(k,l)$ ç‚ºæˆ‘å€‘å¹³æ»‘å¾Œçš„ spectrum power, ç„¶å¾Œå°æ¯å€‹ bin éƒ½ tracking ä¸€å°æ®µæ™‚é–“çš„æœ€å°å€¼, ä»¤ç‚º $Sâ€™(k,l)$. å‰‡å¾ˆæ˜é¡¯å¦‚æœ $S(k,l)&gt;\\delta Sâ€™(k,l)$, æˆ‘å€‘å°±å¯ä»¥èªç‚ºæœ‰ speech, æ©Ÿç‡ç‚º 1, å¦å‰‡ç‚º 0. é€™æ¨£çš„ speech æ©Ÿç‡éäº† $\\alpha$ å¹³æ»‘çš„çµæœå°±æ˜¯ $p(k,l)$. æ˜ç¢ºä¸€é»å¯«ä¸‹ç‚º:$$\\begin{align} p(k,l)=\\alpha_p p(k,l-1)+(1-\\alpha_p)\\mathbf{I}[S(k,l)&gt;\\delta S&apos;(k,l)] \\end{align}$$ å…¶ä¸­ $\\mathbf{I}[.]$ ç‚º indicator function MCRA æœ‰å“ªäº›èª¿æ•´çš„åƒæ•¸å¯¦éš›æƒ…å½¢æœ‰ä¸€äº›éœ€è¦èª¿æ•´çš„åƒæ•¸, åˆ—åœ¨ä¸‹é¢ $\\alpha_d$: noise variance smoothing $\\alpha_p$: speech probability smoothing STFT çš„ time and frequency smoothing åƒæ•¸ $\\delta$: åˆ¤æ–·ç•¶å‰ frame and bin æ˜¯å¦ç‚º speech çš„ threshold tracking minimal power $Sâ€™(k,l)$ çš„åƒæ•¸, è­¬å¦‚è¦ç”¨å¤šå°‘å€‹ frame ä¾†æ‰¾ minimum å¾…åšäº›å¯¦é©—æ‰æœƒçŸ¥é“æ•ˆæœâ€¦ Reference Speech Enhancement Using a Minimum Mean-Square Error Short-Time Spectral Amplitude Estimator by Yariv Ephraim and David Malah Speech Enhancement Using a Minimum Mean-Square Error Log-Spectral Amplitude Estimator by Yariv Ephraim and David Malah [A Noise Reduction Pre-processor for Mobile Voice Communication] by R. Martin â€¦ Speech enhancement for non-stationary noise environments by Israel Cohen and Baruch Berdugo","tags":[{"name":"MMSE-STSA","slug":"MMSE-STSA","permalink":"http://yoursite.com/tags/MMSE-STSA/"},{"name":"MMSE-LSA","slug":"MMSE-LSA","permalink":"http://yoursite.com/tags/MMSE-LSA/"},{"name":"OMLSA","slug":"OMLSA","permalink":"http://yoursite.com/tags/OMLSA/"},{"name":"MCRA","slug":"MCRA","permalink":"http://yoursite.com/tags/MCRA/"}]},{"title":"Far Field Notes (3) Equivalence of MWF, MaxSNR, and MVDR Filters","date":"2019-03-18T12:33:46.000Z","path":"2019/03/18/Far-Field-Notes-3-MWF-MaxSNR-MVDR-Filters/","text":"é€™æ˜¯ far field ç­†è¨˜ç³»åˆ—ç¬¬ä¸‰ç¯‡, ä¸»è¦ç‚ºè‡ªå·±å­¸ç¿’ç”¨, å¦‚æœ‰éŒ¯èª¤é‚„è«‹æŒ‡æ­£. ä¸»è¦åƒè€ƒ Microphone Array Signal Processing Ch6 å’Œ Speech Processing in Modern Communication: Challenges and Perspectives Ch9.3.4 åœ¨ narrow-band çš„æƒ…å½¢ä¸‹, Multi-channel Wiener Filter (MWF), maximum SNR (MSNR) å’Œ Minimum Variance Distortionless Response (MVDR) ä¸‰è€…æ±‚å‡ºä¾†çš„ filter è§£åªå·®åœ¨ norm å¤§å°ä¸åŒ. ä½†åæ‡‰åœ¨æœ€å¾Œçš„ full-bank è¡Œç‚ºä»ç„¶ä¸åŒ. é€™éƒ¨åˆ†å¯çœ‹æ›¸. æœ¬ç¯‡ä¸»è¦ç´€éŒ„ narrow-bank ä¸‹ä¸‰è€…ç‚ºä½• equivalent. ç®—æ˜¯æ›¸æœ¬çš„æ‘˜è¦ç­†è¨˜å§. Signal Modelåœ¨ frequency doamin ä¸‹, æˆ‘å€‘æœ‰å¦‚ä¸‹çš„é—œä¿‚ $$\\begin{align} Y_n(j\\omega)=G_n(j\\omega)S(j\\omega)+V_n(j\\omega) \\\\ =X_n(j\\omega)+V_n(j\\omega)\\mbox{, }n=1,2,...,N \\\\ \\end{align}$$ $N$ æ˜¯éº¥å…‹é¢¨æ•¸é‡, $S(j\\omega)$ æ˜¯åŸå§‹è¨Šè™Ÿ, $G_n(j\\omega)$ æ˜¯è²æºåˆ° mic $n$ çš„ impluse response, $V_n(j\\omega)$ æ˜¯ noise, è€Œ $X_n(j\\omega)$ æ˜¯ mic $n$ çš„è¨Šè™Ÿ. æˆ‘å€‘å¸Œæœ›é‚„åŸçš„æ˜¯ $X_1(j\\omega)$ è€Œä¸æ˜¯ $S(j\\omega)$.æ’æˆ vector å½¢å¼å¦‚ä¸‹$$\\begin{align} Z(j\\omega)=\\mathbf{h}^H(j\\omega)\\mathbf{y}(j\\omega) \\\\ =\\mathbf{h}^H(j\\omega)[\\mathbf{x}(j\\omega)+\\mathbf{v}(j\\omega)] \\\\ \\end{align}$$ å…¶ä¸­$$\\begin{align} \\mathbf{y}(j\\omega)=[Y_1(j\\omega),Y_2(j\\omega),...,Y_N(j\\omega)]^T \\\\ \\mathbf{x}(j\\omega)=S(j\\omega)[G_1(j\\omega),G_2(j\\omega),...,G_N(j\\omega)]^T=S(j\\omega)\\mathbf{g}(j\\omega) \\\\ \\mathbf{v}(j\\omega)=[V_1(j\\omega),V_2(j\\omega),...,V_N(j\\omega)]^T \\\\ \\mathbf{h}(j\\omega)=[H_1(j\\omega),H_2(j\\omega),...,H_N(j\\omega)]^T \\\\ \\end{align}$$ æ³¨æ„åˆ°$$\\begin{align} \\Phi_{xx}(j\\omega)=\\mathbb{E}\\left[\\mathbf{x}(j\\omega)\\mathbf{x}^H(j\\omega)\\right]=\\phi_{ss}(j\\omega)\\mathbf{g}(j\\omega)\\mathbf{g}^H(j\\omega) \\end{align}$$ MWFå°‡ error term å¯«å‡ºä¾† $$\\begin{align} \\mathcal{E}(j\\omega)=Z(j\\omega)-X_1(j\\omega) \\\\ =\\mathbf{h}^H(j\\omega)\\mathbf{v}(j\\omega)+[\\mathbf{h}(j\\omega)-\\mathbf{u}]^H\\mathbf{x}(j\\omega)\\\\ =\\color{orange}{\\mathcal{E}_v(j\\omega)}+\\color{blue}{\\mathcal{E}_x(j\\omega)} \\\\ \\end{align}$$ å…¶ä¸­ $\\mathbf{u}$ æ˜¯ä¸€å€‹ $N\\times 1$ çš„ vector, åªæœ‰ç¬¬ä¸€å€‹æ˜¯1, å…¶ä»–æ˜¯0. Error term å¯ä»¥æ‹†æˆå…©é …, åˆ†åˆ¥å°æ‡‰äº† noise reduction ç¨‹åº¦å’Œ speech distortion ç¨‹åº¦MWF çš„ç›®æ¨™å‡½å¼å¦‚ä¸‹:$$\\begin{align} J_{MWF}[\\mathbf{h}(j\\omega)]=\\mathbb{E}\\left[| \\mathcal{E}(j\\omega) |^2\\right]\\\\ = \\color{orange}{ \\mathbb{E}\\left[| \\mathcal{E}_v(j\\omega) |^2\\right] } + \\color{blue}{ \\mathbb{E}\\left[| \\mathcal{E}_x(j\\omega) |^2\\right] } \\end{align}$$ å¯ä»¥çœ‹æˆ noise reduction å’Œ speech distortion åŒç­‰é‡è¦æƒ…æ³ä¸‹å»æ±‚è§£æœ€å¥½çš„ $\\mathbf{h}$å¾®åˆ†ç­‰æ–¼é›¶æ±‚è§£å¾—åˆ°å¦‚ä¸‹:$$\\begin{align} \\Phi_{yy}(j\\omega)\\mathbf{h}_W(j\\omega)=\\Phi_{yx}(j\\omega)\\mathbf{u}=\\Phi_{xx}(j\\omega)\\mathbf{u} \\end{align}$$ä¸Šå¼æœ€å¾Œæ¨å°æ˜¯ç”±æ–¼ $x$ and $v$ æ˜¯ independent. å› æ­¤æœ€å¾Œçš„ MWF è§£ç‚º:$$\\begin{align} \\mathbf{h}_W(j\\omega)=\\Phi_{yy}^{-1}(j\\omega)\\Phi_{xx}(j\\omega)\\mathbf{u} \\\\ =\\left[ \\mathbf{I}_{N\\times N} - \\Phi_{yy}^{-1}(j\\omega)\\Phi_{vv}(j\\omega) \\right]\\mathbf{u} \\end{align}$$ MVDRMVDR è¦è§£çš„å•é¡Œå¦‚ä¸‹:$$\\min \\color{orange}{ \\mathbb{E}\\left[| \\mathcal{E}_v(j\\omega) |^2\\right] }\\\\ \\mbox{subject to } \\color{blue}{\\mathbb{E}\\left[| \\mathcal{E}_x(j\\omega) |^2\\right]}=0$$ é€™ä¹Ÿå¯ä»¥çœ‹å‡º MVDR ç‚ºä»€éº¼å« MVDR.é¦–å…ˆå…ˆå°‡ constraint æ”¹å¯«æˆ (æ”¹å¯« speech distortion error term, å®šç¾©åœ¨ (11), (12)):$\\left[\\mathbf{u}-\\mathbf{h}(j\\omega)\\right]^H\\mathbf{x}(j\\omega)=0 \\\\$ ä¸¦åˆ©ç”¨ $\\mathbf{x}(j\\omega)=S(j\\omega)\\mathbf{g}(j\\omega)$ å¯å¾—åˆ°$\\mathbf{h}^H(j\\omega)\\mathbf{g}(j\\omega)=G_1(j\\omega)$å› æ­¤ MVDR å•é¡Œçš„é€šå¸¸å¦‚ä¸‹è¡¨é”:$$\\min \\mathbf{h}^H(j\\omega) \\Phi_{vv}(j\\omega) \\mathbf{h}(j\\omega) \\\\ \\mbox{subject to } \\mathbf{h}^H(j\\omega)\\mathbf{g}(j\\omega)=G_1(j\\omega) \\\\$$ é€™å€‹æœ€ä½³åŒ–å•é¡Œæ­£å¥½å°±æ˜¯ä¸Šä¸€ç¯‡çš„ LCMV, æ‰€ä»¥èªª MVDR æ˜¯ LCMV çš„ä¸€ç¨® case.ç”¨ Lagrange multipliers æ±‚è§£å¾—åˆ°$$\\begin{align} \\mathbf{h}_{MVDR}(j\\omega)=G_1^{\\ast}(j\\omega)\\frac{\\Phi_{vv}^{-1}(j\\omega)\\mathbf{g}(j\\omega)}{\\mathbf{g}^H(j\\omega)\\Phi_{vv}^{-1}(j\\omega)\\mathbf{g}(j\\omega)} \\end{align}$$ å° (18) é€²ä¸€æ­¥æ¨å°, ç‚ºäº†ç²¾ç°¡ä»¥ä¸‹ ${j\\omega}$ çœç•¥ä¸å¯«$$\\begin{align} \\mathbf{h}_{MVDR}=\\frac{\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}G_1^{\\ast}}{tr\\left[\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}\\mathbf{g}^H\\right]}=\\frac{\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}\\mathbf{g}^H\\mathbf{u}}{tr\\left[\\Phi_{vv}^{-1}\\phi_{ss}\\mathbf{g}\\mathbf{g}^H\\right]} \\\\ =\\frac{ \\Phi_{vv}^{-1}\\Phi_{xx}\\mathbf{u} }{ tr\\left[\\Phi_{vv}^{-1}\\Phi_{xx}\\right] } \\\\ =\\frac{ \\Phi_{vv}^{-1}(\\Phi_{yy}-\\Phi_{vv})\\mathbf{u} }{ tr\\left[ \\Phi_{vv}^{-1}(\\Phi_{yy}-\\Phi_{vv}) \\right] } \\\\ =\\frac{ (\\Phi_{vv}^{-1}\\Phi_{yy}-\\mathbf{I})\\mathbf{u} }{ tr\\left[\\Phi_{vv}^{-1}\\Phi_{yy}\\right]-N } \\end{align}$$ (19) åˆ° (20) ä½¿ç”¨äº† (9).(22) çš„å½¢å¼æ›¸æœ¬èªªå¾ˆé‡è¦, å› ç‚ºé¿å…äº†å¾ˆé›£ä¼°è¨ˆçš„ $\\mathbf{g}$, å–è€Œä»£ä¹‹çš„æ˜¯æˆ‘å€‘è¦ä¼°è¨ˆå‡º $\\Phi_{vv}$ MWF èˆ‡ MVDR ç­‰åƒ¹æƒ…å½¢åŒæ¨£ç‚ºäº†ç²¾ç°¡ä»¥ä¸‹ ${j\\omega}$ çœç•¥ä¸å¯«, é¦–å…ˆæˆ‘å€‘çŸ¥é“$$\\begin{align} \\Phi_{yy}=\\Phi_{vv} + \\phi_{ss}\\mathbf{g}\\mathbf{g}^H \\end{align}$$ ä½¿ç”¨ Woodburyâ€™s identity å¯å¾—:$$\\begin{align} \\Phi_{yy}^{-1}=\\Phi_{vv}^{-1}-\\frac{ \\Phi_{vv}^{-1}\\Phi_{xx}\\Phi_{vv}^{-1} }{ 1+tr\\left[\\Phi_{vv}^{-1}\\Phi_{xx}\\right] } \\end{align}$$ å°‡ (24) å¸¶å…¥åˆ° (17) ä¸¦ç¶“éä¸€äº›ä»£æ•¸æ›¿æ›æˆ‘å€‘å¾—åˆ°$$\\begin{align} \\mathbf{h}_{W}=\\frac{ \\Phi_{vv}^{-1}\\Phi_{xx} }{ 1+tr\\left[\\Phi_{vv}^{-1}\\Phi_{xx}\\right] }\\mathbf{u} \\end{align}$$ é€™å€‹å¼å­èˆ‡ MVDR çš„ (20) æ¯”è¼ƒä¸€ä¸‹æˆ‘å€‘ç™¼ç¾$$\\begin{align} \\mathbf{h}_{W}(j\\omega)=c(\\omega)\\mathbf{h}_{MVDR}(j\\omega) \\end{align}$$ å…¶ä¸­ $c(\\omega)$ æ˜¯èˆ‡ $\\omega$ ç›¸é—œçš„ä¸€å€‹ scalar. å› æ­¤ MWF èˆ‡ MVDR è§£åªå·®åœ¨ä¸€å€‹ $\\omega$ ç›¸é—œçš„å¸¸æ•¸é … Maximum SNR (MSNR)åŒæ¨£ç‚ºäº†ç²¾ç°¡ä»¥ä¸‹ ${j\\omega}$ çœç•¥ä¸å¯«, output SNR å®šç¾©ç‚º:$$\\begin{align} \\mbox{oSNR}\\left[\\mathbf{h}\\right]=\\frac{ \\mathbf{h}^H \\Phi_{xx} \\mathbf{h} }{ \\mathbf{h}^H \\Phi_{vv} \\mathbf{h} } \\end{align}$$ é€™å€‹ç­‰åŒæ–¼ generalized eigenvalue problem.$$\\begin{align} \\Phi_{xx}\\mathbf{h}=\\lambda\\Phi_{vv}\\mathbf{h} \\end{align}$$ æ‰€ä»¥$\\mathbf{h}_{MSNR}\\mbox{ is eigenvector w.r.t max eigenvalue of matrix } \\Phi_{vv}^{-1}\\Phi_{xx}$eigenvector ä¹˜ä¸Šä¸€å€‹ scalar ä»ç„¶æ˜¯ eigenvector, å› æ­¤é€šå¸¸éƒ½æœƒå°‡ $\\mathbf{h}_{MSNR}$ çš„ norm å®šç‚º 1 æ¥è‘—æˆ‘å€‘èªªæ˜ $\\mathbf{h}$ çš„è§£å…·æœ‰ä»¥ä¸‹å½¢å¼:$$\\begin{align} \\mathbf{h}\\propto\\Phi_{vv}^{-1}\\mathbf{g} \\end{align}$$ å…ˆå‡å®š (29) ç‚ºç­‰å¼:$$\\begin{align} \\mathbf{h}=\\Phi_{vv}^{-1}\\mathbf{g} \\end{align}$$ åˆ©ç”¨ (9) å’Œ (30) å¾—åˆ°ä»¥ä¸‹çš„æ¨å°$$\\begin{align} \\Phi_{vv}^{-1}\\Phi_{xx}\\mathbf{h}=\\Phi_{vv}^{-1} \\phi_{ss}\\mathbf{g}\\mathbf{g}^H \\mathbf{h}=\\left(\\Phi_{vv}^{-1}\\mathbf{g}\\right)\\left(\\phi_{ss}\\mathbf{g}^H\\mathbf{h}\\right)=\\mathbf{h}\\lambda \\end{align}$$ æˆ‘å€‘ç™¼ç¾ $\\mathbf{h}$ å¦‚æœ‰ (30) çš„å½¢å¼, å‰‡ç‚º maximum SNR (27) çš„è§£. ç•¶ç„¶ eigenvector ä¹˜ä¸Š scalar ä»ç„¶æ˜¯ eigenvector, æ‰€ä»¥ (29) ç‚º maximum SNR çš„è§£.æœ€å¾Œç”±æ–¼ $\\Phi_{xx}$ ç‚º rank 1 æ‰€ä»¥åªæœƒæœ‰ä¸€å€‹ nonzero eigenvalue, å› æ­¤ maximum SNR æ‰€æœ‰è§£çš„å½¢å¼å¿…ç„¶ç‚º (29) çš„å½¢å¼. MVDR èˆ‡ MSNR ç­‰åƒ¹æƒ…å½¢æª¢æŸ¥ä¸‹ (18) çš„ MVDR è§£, å¾ˆå¿«å°±ç™¼ç¾æ»¿è¶³ (29) MSNR çš„è§£çš„å½¢å¼, å› æ­¤$$\\begin{align} \\mathbf{h}_{MVDR}(j\\omega)=d(\\omega)\\mathbf{h}_{MSNR}(j\\omega) \\end{align}$$ çµè«–MWF, MVDR, MSNR ä¸‰å€‹å•é¡Œçš„è§£åœ¨ narrowband ä¸Šåªå·®åœ¨ scalar. ä½†ä»¥ fullband ä¾†èªª, è¡¨ç¾é‚„æ˜¯ä¸åŒçš„. Reference Microphone Array Signal Processing by Jocab Benesty Speech Processing in Modern Communication: Challenges and Perspectives","tags":[{"name":"MWF","slug":"MWF","permalink":"http://yoursite.com/tags/MWF/"},{"name":"MSNR","slug":"MSNR","permalink":"http://yoursite.com/tags/MSNR/"},{"name":"MVDR","slug":"MVDR","permalink":"http://yoursite.com/tags/MVDR/"}]},{"title":"Far Field Notes (2) LCMV filter and Frost's algorithm","date":"2019-03-02T09:36:58.000Z","path":"2019/03/02/Far-Field-Notes-2-LCMV-and-Frost/","text":"é€™æ˜¯ far field ç­†è¨˜ç³»åˆ—ç¬¬äºŒç¯‡, ä¸»è¦ç‚ºè‡ªå·±å­¸ç¿’ç”¨, å¦‚æœ‰éŒ¯èª¤é‚„è«‹æŒ‡æ­£. ä¸»è¦åƒè€ƒ Microphone Array Signal Processing Ch4 å’Œ Frostâ€™s algorithm ä¸Šä¸€ç¯‡æœ€å¾Œé›–ç„¶ä½¿ç”¨ fixed beamformer å¾—åˆ°äº† response-invariant beamformer, ä½†é€™å€‹æ–¹æ³•é™åˆ¶æ˜¯ filter ä¸€æ—¦è¨­è¨ˆå¥½å°±å¯«æ­»äº†, æ²’è¾¦æ³•è‡ªå·± update (æ‰€ä»¥æ‰å« â€œfixedâ€ beamformer). é€™å¼•å…¥ä¸€å€‹å•é¡Œæ˜¯, å¦‚æœå‰›å¥½æœ‰ä¸€å€‹ inteference noise åœ¨è¡°æ¸›ä¸é‚£éº¼å¤§çš„è§’åº¦æ™‚, å°±ç„¡æ³•å£“å¾—å¾ˆå¥½. è€Œé€™ç¯‡è¦ä»‹ç´¹çš„ LCMV (Linear Constrained minimum variance) filter ä»¥åŠ Frostâ€™s beamformer èƒ½é‡å°çµ¦å®šçš„æ–¹å‘æŠ½å–è¨Šè™Ÿ, ä¸¦ä¸”å°å…¶ä»–æ–¹å‘çš„ inteference nosie å£“æŠ‘çš„æœ€å¥½. æ³¨æ„ sound source æ–¹å‘å¿…é ˆçµ¦å®š, LCMV æ±‚å¾—çš„ weights æœƒæƒ³è¾¦æ³•å°å…¶ä»–æ–¹å‘çš„ inteference å£“æŠ‘. å¦‚åŒ LCMV å­—é¢ä¸Šçš„æ„æ€ä¸€æ¨£. æœƒå°‡æ•´å€‹å•é¡Œè½‰æ›æˆ minimize variance subject to some linear constraints. å¦å¤–ç›¸ç•¶ç¶“å…¸çš„ Frostâ€™s beamformer (1972å¹´å‘¢!) å‰‡å°‡ filter çš„ optimal æ±‚è§£æ”¹æˆä½¿ç”¨ stochastic gradient descent æ–¹å¼, æ‰€ä»¥éå¸¸é©åˆå¯¦éš›çš„ real time ç³»çµ±, é€™äº›ä¸‹æ–‡æœƒè©³ç´°èªªæ˜. æ¶æ§‹è¨­å®šå’Œ Signal Modelæ¶æ§‹å¦‚ä¸‹åœ– (åœ–ç‰‡ä¾†æº:ref), ç¬¬ä¸€æ­¥æ˜¯ä¸€å€‹ delay stage, é€™ç›¸ç•¶æ–¼æ˜¯é‡å°ä¸€å€‹ steering direction è£œå„Ÿæ¯å€‹ mic ä¹‹é–“çš„ time delay (è¨Šè™Ÿå°é½Šå¥½). ç¬¬äºŒæ­¥æ‰æ˜¯ beamformer, æˆ‘å€‘çŸ¥é“ time domain ä½¿ç”¨ filter-and-sum æ¶æ§‹, å¦‚æœæ˜¯ frequency domain å‰‡ä½¿ç”¨æ‹†é »çš„æ¶æ§‹. å¿˜äº†å¯åƒè€ƒç¬¬ä¸€ç¯‡. æœ¬æ–‡ä»¥ filter-and-sum ä¾†ç­†è¨˜, å¦å¤– signal model ä»¥ä¸‹æ¨å°å°‡æœƒä½¿ç”¨ anechoic model, ç¬¬ä¸€ç¯‡æœ‰å®šç¾©å¯å›å»æŸ¥é–±. åŒæ™‚æœ¬æ–‡æ¥ä¸‹ä¾†çš„ notation æœƒèˆ‡åœ–ä¸­çš„ä¸åŒ. ä¸Šåœ–åªæ˜¯ç”¨ä¾†é¡¯ç¤º filter-and-sum æ¶æ§‹. Notationsä¸€äº› notations æˆ‘å€‘å…ˆå®šç¾©èµ·ä¾†. $N$ æ˜¯éº¥å…‹é¢¨æ•¸é‡, $L$ æ˜¯ filter tap æ•¸é‡, æˆ‘å€‘ aligned å¥½çš„ anechoic model å¦‚ä¸‹:$$\\begin{align} \\mathbf{y}(k)=s(k)\\mathbf{\\alpha}+\\mathbf{v}(k) \\end{align}$$å…¶ä¸­$$\\begin{align} \\mathbf{y}(k)=[y_1(k),...,y_N(k)]^T \\\\ \\mathbf{v}(k)=[v_1(k),...,v_N(k)]^T \\\\ \\mathbf{\\alpha}=[\\alpha_1,\\alpha_2,...,\\alpha_N]^T \\end{align}$$ $s(k)$ æ˜¯æ™‚é–“ $k$ çš„è²æºè¨Šè™Ÿ, $\\alpha$ æ˜¯ $N\\times 1$ çš„ attenuation factors, $\\mathbf{v}(k)$ æ˜¯æ™‚é–“ $k$ çš„ $N\\times 1$ noise è¨Šè™Ÿå‘é‡, å› æ­¤ $\\mathbf{y}(k)$ æ˜¯æ™‚é–“ $k$ çš„ $N\\times 1$ éº¥å…‹é¢¨æ”¶åˆ°çš„è¨Šè™Ÿå‘é‡. æ³¨æ„åˆ°ç”±æ–¼æˆ‘å€‘å…ˆ align å¥½ delay äº†, æ‰€ä»¥åŸå…ˆçš„ anechoic model å¯ä»¥ç°¡åŒ–æˆä¸Šé¢çš„è¡¨é”. è€ƒæ…®åˆ° filter-and-sum æ¶æ§‹, æˆ‘å€‘å°‡æ•´å€‹ $N$ å€‹ mic æ¯å€‹ mic éƒ½æœ‰ $L$ å€‹å€¼ä»¥ä¸‹åœ–(åœ–ç‰‡ä¾†æº:ref)çš„é †åºä¸²æˆä¸€å€‹ $NL$ vectorå› æ­¤æˆ‘å€‘å¾—åˆ°é€™äº›å‘é‡$$\\begin{align} \\mathbf{y}_{NL}(k)=[\\mathbf{y}^T(k), \\mathbf{y}^T(k-1), ..., \\mathbf{y}^T(k-L+1)]^T \\\\ \\mathbf{x}_{NL}(k)=[s(k)\\mathbf{\\alpha}^T, s(k-1)\\mathbf{\\alpha}^T, ..., s(k-L+1)\\mathbf{\\alpha}^T]^T \\\\ \\mathbf{v}_{NL}(k)=[\\mathbf{v}^T(k), \\mathbf{v}^T(k-1), \\mathbf{v}^T(k-L+1)]^T \\end{align}$$æ‰€ä»¥æ•´é«”çš„ signal model æ”¹å¯« (1) å¾Œå¯å¾—:$$\\begin{align} \\mathbf{y}_{NL}(k)=\\mathbf{x}_{NL}(k) + \\mathbf{v}_{NL}(k) \\end{align}$$ Filter-and-sum çš„ filter $\\mathbf{h}$ ä¹Ÿç”¨é€™å€‹é †åºå®šç¾©å¦‚ä¸‹, å› æ­¤æ˜¯ä¸€å€‹é•·åº¦ç‚º $NL$ çš„å‘é‡$$\\begin{align} \\mathbf{h}=[\\mathbf{h}_0^T, \\mathbf{h}_1^T, \\mathbf{h}_{L-1}^T]^T \\end{align}$$æœ€å¾Œæ•´å€‹ beamformer çš„è¼¸å‡º $z(k)$ å°±å¯ä»¥é€™éº¼å¯«$$\\begin{align} z(k)=\\mathbf{h}^T\\mathbf{y}_{NL}(k) = \\color{orange}{ \\mathbf{h}^T\\mathbf{x}_{NL}(k) } + \\color{blue}{ \\mathbf{h}^T\\mathbf{v}_{NL}(k) } \\end{align}$$ Problem DefinitionLCMV çš„ä¸»è¦æƒ³æ³•å°±åœç¹åœ¨ (10) çš„æ©˜è‰²å’Œè—è‰²å…©å€‹éƒ¨åˆ†ä¸Šé¢: æˆ‘å€‘å¸Œæœ›æ©˜è‰²éƒ¨åˆ†èƒ½å¤ é‚„åŸå‡ºåŸå§‹è¨Šè™Ÿ $s(k)$ ä¸”è—è‰²éƒ¨åˆ†èƒ½å¤ æ„ˆå°æ„ˆå¥½ (ä»£è¡¨è‘— noise æ„ˆå°æ„ˆå¥½). é¦–å…ˆæˆ‘å€‘å°‡æ©˜è‰²éƒ¨åˆ†ä½œå¦‚ä¸‹æ¨å°: $$\\begin{align} \\color{orange}{ \\mathbf{h}^T\\mathbf{x}_{NL}(k) } =\\mathbf{h}^T \\left[ \\begin{array}{clr} s(k)\\mathbf{\\alpha} \\\\ s(k-1)\\mathbf{\\alpha} \\\\ \\vdots \\\\ s(k-L+1)\\mathbf{\\alpha} \\end{array} \\right] = sum\\left( \\left[ \\begin{array}{clr} \\mathbf{h}_0^T\\mathbf{\\alpha}\\cdot s(k) \\\\ \\mathbf{h}_1^T\\mathbf{\\alpha}\\cdot s(k-1) \\\\ \\vdots \\\\ \\mathbf{h}_{L-1}^T\\mathbf{\\alpha}\\cdot s(k-L+1) \\end{array} \\right] \\right) \\\\ = sum\\left( \\color{red}{ \\left[ \\begin{array}{clr} u_0\\cdot s(k) \\\\ u_1\\cdot s(k-1) \\\\ \\vdots \\\\ u_{L-1}\\cdot s(k-L+1) \\end{array} \\right] } \\right) \\end{align}$$ (12) ç‚ºå¼•å…¥çš„æ¢ä»¶, è—‰ç”±é€™æ¨£çš„æ¢ä»¶ä¾†é‚„åŸåŸå§‹è¨Šè™Ÿ.$u$ ($L$é•·åº¦çš„å‘é‡) å®šç¾©äº†æˆ‘å€‘å¸Œæœ›åœ¨æ™‚é–“ $k$ çš„é‚„åŸçµæœ, æ˜¯åŸå§‹è¨Šè™Ÿçš„æ¬Šé‡å’Œå®šç¾©ä¸€å€‹ matrix (size of $NL\\times L$) å¦‚ä¸‹: $$\\begin{align} \\mathbf{C}_{\\mathbf{\\alpha}}= \\left[ \\begin{array}{clr} \\mathbf{\\alpha} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{\\alpha} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{\\alpha} \\\\ \\end{array} \\right] = \\left[ \\begin{array}{clr} \\mathbf{c}_{\\alpha,0} &amp; \\mathbf{c}_{\\alpha,1} &amp; \\cdots &amp; \\mathbf{c}_{\\alpha,L-1} \\\\ \\end{array} \\right] \\end{align}$$ è§€å¯Ÿ (11) and (12) ä¸¦åˆ©ç”¨ $\\mathbf{C_{\\alpha}}$ å¯ä»¥å°‡ constraint æ˜ç¢ºå¯«å‡ºå¦‚ä¸‹: $$\\begin{align} \\mathbf{C_{\\alpha}}^T\\mathbf{h}=\\mathbf{\\mathbf{u}} \\end{align}$$ è—è‰²éƒ¨åˆ†ä»£è¡¨æœ€å¾Œçš„ noise æˆåˆ†, å¸Œæœ›æ„ˆå°æ„ˆå¥½è¨ˆç®—è—è‰²éƒ¨åˆ†çš„èƒ½é‡ç‚º $$\\begin{align} \\mathbf{h}^T \\mathbb{E} \\left[ \\mathbf{v}_{NL}(k)\\mathbf{v}_{NL}^T(k) \\right] \\mathbf{h}=\\mathbf{h}^T\\mathbf{R}_{\\mathbf{v},\\mathbf{v}}\\mathbf{h} \\end{align}$$ ä½†é—œéµæ˜¯æˆ‘å€‘ç„¡æ³•å¾—çŸ¥å¯¦éš›çš„ noise signal, æˆ‘å€‘æœ‰çš„åªæœ‰ observation $\\mathbf{y}_{NL}(k)$, é‚£è©²æ€éº¼è¾¦å‘¢?LCMV å¾ˆå²å®³çš„ä¸€é»æ˜¯, ç”±æ–¼ä¸Šé¢å‰›æåˆ°çš„ constraints, å°è‡´æ©˜è‰²éƒ¨åˆ†çš„èƒ½é‡æ˜¯ constant, å› æ­¤ä»¥ä¸‹å…©å€‹å•é¡Œæ˜¯ç­‰åƒ¹çš„ $$\\begin{align} \\min_{\\mathbf{h}}{ \\mathbf{h}^T\\mathbf{R}_{\\mathbf{v},\\mathbf{v}}\\mathbf{h} } \\equiv \\min_{\\mathbf{h}}{ \\mathbf{h}^T\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} } \\end{align}$$ åˆ°é€™è£¡æˆ‘å€‘å¯ä»¥å¯«å‡ºå®Œæ•´çš„æœ€ä½³åŒ–å•é¡Œ$$\\begin{align} \\begin{array}{clr} \\color{blue}{ \\min_{\\mathbf{h}}{ \\mathbf{h}^T\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} } } \\\\ \\color{orange}{ \\mbox{subject to }\\mathbf{C_{\\alpha}}^T\\mathbf{h}=\\mathbf{\\mathbf{u}} } \\end{array} \\end{align}$$ Optimal Solutionè¦è§£å•é¡Œ (17), åŸºæœ¬ä¸Šä½¿ç”¨ Lagrange function æ±‚è§£å°±å¯ä»¥, è§£å¦‚ä¸‹:$$\\begin{align} \\mathbf{h}=\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}^{-1}\\mathbf{C_{\\alpha}} \\left( \\mathbf{C_{\\alpha}}^T \\mathbf{R}_{\\mathbf{y},\\mathbf{y}}^{-1} \\mathbf{C_{\\alpha}} \\right)^{-1} \\mathbf{u} \\end{align}$$ ä½†æ˜¯é‡é»ä¾†äº†, ä»¥ä¸Šé€™äº›æ¨å°å…¨éƒ¨éƒ½å‡è¨­æ˜¯ stationary, å¯¦éš›æƒ…æ³ä¸€å®šæ˜¯ non-stationary æ€éº¼è¾¦? æœ€ç›´è¦ºçš„æƒ³æ³•å°±æ˜¯, æˆ‘å€‘æ¯éš”ä¸€æ®µæ™‚é–“å°±ç”¨ (18) é‡æ–°ç®—ä¸€ä¸‹ $\\mathbf{h}$. ä½†å¾ˆæ˜é¡¯é€™éå¸¸æ²’æ•ˆç‡ (covarianceä¼°è¨ˆ, inverseé‹ç®—) æ ¹æœ¬ä¸å¯è¡Œ. å› æ­¤å¿…é ˆæ”¹æˆ iteratively update $\\mathbf{h}$ çš„æ–¹å¼.Frostâ€™s algorithm çš„ä¸€å€‹é‡è¦è²¢ç»ä¹Ÿå°±æ˜¯åœ¨é€™, ä½¿ç”¨ stochastic gradient descent æ–¹å¼ update $\\mathbf{h}$! Frostâ€™s Algorithmå•é¡Œ (17) çš„ Lagrange function å¦‚ä¸‹:$$\\begin{align} \\mathcal{L}(\\mathbf{h},\\mathbf{\\lambda}) = \\frac{1}{2} \\mathbf{h}^T\\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} + \\mathbf{\\lambda}^T(\\mathbf{C_{\\alpha}}^T\\mathbf{h}-\\mathbf{\\mathbf{u}}) \\end{align}$$å› æ­¤ gradient å¦‚ä¸‹:$$\\begin{align} \\nabla_{\\mathbf{h}}\\mathcal{L} = \\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h} + \\mathbf{C_{\\alpha}}\\mathbf{\\lambda} \\end{align}$$gradient descent update å¼å­å¦‚ä¸‹:$$\\begin{align} \\mathbf{h}_{t+1} = \\mathbf{h}_{t} - \\mu \\left( \\mathbf{R}_{\\mathbf{y},\\mathbf{y}}\\mathbf{h}_t + \\mathbf{C_{\\alpha}}\\mathbf{\\lambda}_t \\right) \\end{align}$$ç”±æ–¼æœ‰ constraint, å¿…é ˆæ»¿è¶³ update å¾Œä»ç„¶æ»¿è¶³æ¢ä»¶, å› æ­¤:$$\\begin{align} \\mathbf{u}=\\mathbf{C_{\\alpha}}^T\\mathbf{h}_{t+1} \\end{align}$$å°‡(21)å¸¶å…¥(22)æ•´ç†å¾—åˆ°$\\lambda_t$, æ¥è‘—å†å°‡$\\lambda_t$å¸¶å›(21)å¾—åˆ°çµæœå¦‚ä¸‹, ä¸¦ä¸å›°é›£åªæ˜¯ä¸€äº›ä»£æ•¸é‹ç®—:$$\\begin{align} \\mathbf{h}_{t+1} = \\mathbf{h}_{t} - \\mu \\left[ \\mathbf{I} - \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1}\\mathbf{C}^T \\right] \\mathbf{R}_{\\mathbf{y},\\mathbf{y}} \\mathbf{h}_{t} + \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1} \\left[ \\mathbf{u}-\\mathbf{C}^T\\mathbf{h}_t \\right] \\end{align}$$å®šç¾©å…©å€‹ matrix $\\mathbf{A}$, $\\mathbf{B}$ å¦‚ä¸‹ (æ³¨æ„åˆ°é€™å…©å€‹ matrix æ˜¯äº‹å…ˆè¨ˆç®—å¥½çš„):$$\\begin{align} \\mathbf{A} \\triangleq \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1}\\mathbf{u} \\\\ \\mathbf{B} \\triangleq \\mathbf{I} - \\mathbf{C}(\\mathbf{C}^T\\mathbf{C})^{-1}\\mathbf{C}^T \\end{align}$$å› æ­¤å¯ä»¥æ”¹å¯«(23)å¦‚ä¸‹:$$\\begin{align} \\mathbf{h}_{t+1} = \\mathbf{B}[\\mathbf{h}_t - \\mu \\mathbf{R}_{\\mathbf{y},\\mathbf{y}} \\mathbf{h}_t] + \\mathbf{A} \\end{align}$$ç”±æ–¼ä½¿ç”¨ stochastic æ–¹å¼, å› æ­¤ expectation ä½¿ç”¨æœ€æ–°çš„ä¸€æ¬¡ sample å³å¯:$$\\begin{align} \\mathbf{R}_{\\mathbf{y},\\mathbf{y}} = \\mathbb{E} \\left[ \\mathbf{y}_{NL}(t)\\mathbf{y}_{NL}^T(t) \\right] \\thickapprox \\color{green}{ \\mathbf{y}_{NL}(t)\\mathbf{y}_{NL}^T(t) } \\end{align}$$å°‡(27)å¸¶å…¥(26)ä¸¦ç”¨(10)æ›¿æ›ä¸€ä¸‹, æˆ‘å€‘å¾—åˆ°æœ€çµ‚çš„ update å¼å­:$$\\begin{align} \\color{red}{ \\mathbf{h}_{t+1} = \\mathbf{B}[\\mathbf{h}_t - \\mu z(t)\\mathbf{y}_{NL}(t)] + \\mathbf{A} } \\end{align}$$ç”±æ–¼ $\\mathbf{A}$ å’Œ $\\mathbf{B}$ æ˜¯å›ºå®šçš„, è·ŸåŸä¾†çš„ optimal è§£æ¯”è¼ƒ (18), å¯ä»¥æ˜é¡¯çŸ¥é“é€Ÿåº¦ä¸Šæœƒå¿«éå¸¸å¤š.å¦å¤– $\\mathbf{h}_0$ åªéœ€è¦é¸æ“‡ä¸€å€‹ trivial çš„ feasible point å³å¯:$$\\begin{align} \\mathbf{h}_{0} = \\mathbf{A} \\end{align}$$ çµè«–æœ¬ç¯‡è¨˜éŒ„äº† filter-and-sum æ¶æ§‹çš„ beamformer, LCMV çš„å•é¡Œå’Œå…¶æœ€ä½³è§£. LCMV å¯ä»¥é‡å°çµ¦å®šçš„ä¸€å€‹æ–¹å‘, æ‰¾å‡º filter $\\mathbf{h}$ ä½¿å¾—æŠ½å–çœ‹çš„æ–¹å‘çš„è¨Šè™ŸåŒæ™‚å£“æŠ‘å…¶ä»–æ–¹å‘çš„è¨Šè™Ÿ.å¯¦ä½œä¸Šç›´æ¥å¥—ç”¨æœ€ä½³è§£å¤ªæ…¢ä¸å¯è¡Œ, è€Œ Frostâ€™s algorithm æä¾›äº†ä¸€å€‹ stochastic gradeint update æ–¹æ³•æ›´æ–° $\\mathbf{h}$, é€™ä½¿å¾— real-time system è®Šå¾—å¯è¡Œ. Reference Microphone Array Signal Processing by Jocab Benesty Frostâ€™s algorithm","tags":[{"name":"MVDR","slug":"MVDR","permalink":"http://yoursite.com/tags/MVDR/"},{"name":"LCMV","slug":"LCMV","permalink":"http://yoursite.com/tags/LCMV/"},{"name":"Frost","slug":"Frost","permalink":"http://yoursite.com/tags/Frost/"}]},{"title":"Far Field Notes (1), Beampattern","date":"2019-02-26T12:22:55.000Z","path":"2019/02/26/Far-Field-Notes-1-Beampattern/","text":"é€™æ˜¯ far field ç­†è¨˜ç³»åˆ—ç¬¬ä¸€ç¯‡, ä¸»è¦ç‚ºè‡ªå·±å­¸ç¿’ç”¨, å¦‚æœ‰éŒ¯èª¤é‚„è«‹æŒ‡æ­£. ä¸»è¦åƒè€ƒ Optimum Array Processing Ch2 ä»¥åŠ Microphone Array Signal Processing Ch3. Beampattern å°±æ˜¯å¸Œæœ›èƒ½å¾—åˆ°å¦‚ä¸‹åœ– [ref Fig3.3] çš„è¡¨ç¤º, èªªæ˜ç¶“éä¸€å€‹éº¥å…‹é¢¨é™£åˆ—çš„è™•ç†å¾Œ, æ¯å€‹è§’åº¦æ‰€å¾—åˆ°çš„å¢ç›Šæƒ…å½¢. å› æ­¤å¯ä»¥çœ‹å‡ºä¸»è¦ä¿ç•™å“ªäº›æ–¹å‘çš„è¨Šè™Ÿ, ä»¥åŠæŠ‘åˆ¶å“ªäº›æ–¹å‘çš„è¨Šè™Ÿ. Geometry Settingsé å ´ä¸€èˆ¬å‡è¨­ plan wave, å’Œ narrow band. å¯¦éš›è™•ç†èªéŸ³ç­‰ broadband æ™‚æˆ‘å€‘æœƒæ¡å– fft åˆ†é ». æˆ‘å€‘å®šç¾©å¦‚ä¸‹çš„ geometry, å…¶ä¸­é‡è¦çš„å…©å€‹è§’åº¦ç‚º $\\theta$ å’Œ $\\phi$ (å¦‚åœ–ç´…åœˆ). $\\mathbf{a}$ è¡¨ç¤ºè²æºçš„å…¥å°„å–®ä½å‘é‡. ä»¥ä¸‹ç¬¦è™Ÿå¦‚æœæ˜¯ç²—é«”è¡¨ç¤ºç‚ºå‘é‡æˆ–çŸ©é™£, å¦å‰‡å°±æ˜¯ scalar. Signal Modelæˆ‘å€‘å®šç¾© $f(t)$ ç‚ºè²æºè¨Šè™Ÿ, $v_n(t)$ ç‚º nth mic çš„å™ªè²æº Anechoic Model æˆ‘å€‘ä»¥ä¸‹çš„ä»‹ç´¹éƒ½æ˜¯åŸºæ–¼ Anechoic Model, ä¸¦ä¸”å…ˆåšå¦‚ä¸‹çš„ç°¡åŒ– Reverberant Model ç›¸ç•¶æ–¼å°‡ attenuation factor $\\alpha$ æ”¹æˆ impulse response, æ‰€ä»¥ç›¸ä¹˜æ”¹æˆ convolution. Time Delayç‚ºäº†æ–¹ä¾¿æ¨å°éº¥å…‹é¢¨ä¹‹é–“çš„ time delay, æˆ‘å€‘å…ˆå°‡ Anechoic Model åšå¦‚ä¸‹ç°¡åŒ– å› æ­¤å°æ–¼ plan wave å‡è¨­å’Œè²æºçš„å…¥å°„å–®ä½å‘é‡ $a$ ä¾†èªª, æˆ‘å€‘å¾ˆå®¹æ˜“å°±å¾—åˆ° time delay å¦‚ä¸‹ Uniform Lineary Array (ULA) Circular Arrayä»¥ä¸€å€‹ 6 mic çš„ circular array ä¾†èªª, æœ‰å¦‚ä¸‹çš„ time delay Array Manifold Vectorç”±æ–¼ time delay $\\tau$ åœ¨ freqeuncy $\\omega$ åªæ˜¯ä¹˜ä¸Š $e^{-j\\omega\\tau}$, å› æ­¤æˆ‘å€‘å¯ä»¥å¾—åˆ°ä¸€å€‹ compact çš„è¡¨ç¤º é‡è¤‡ä¸€éé€™è£¡å¾—åˆ°çš„é‡è¦å¼å­ $$\\begin{align} \\color{red}{ \\mathbf{F}(\\omega,\\mathbf{p})=F(\\omega)\\mathbf{\\upsilon}_k(\\mathbf{k}) } \\end{align}$$ æˆ‘å€‘ç¨± $\\mathbf{\\upsilon}_k(\\mathbf{k})$ ç‚º Array Manifold Vector. è¦æ³¨æ„çš„æ˜¯, å…¶å¯¦ä¹Ÿå¯ä»¥ç”¨ time delay $\\tau$ ä¾†è¡¨ç¤º, é€™æ™‚æˆ‘å€‘é€™éº¼å¯« (å¦‚ä¸Šåœ–ç°è‰²çš„éƒ¨åˆ†)$$\\mathbf{\\upsilon}_\\tau (\\mathbf{\\tau})= \\left[ \\begin{array}{clr} e^{-j\\omega\\tau_0} \\\\ e^{-j\\omega\\tau_1} \\\\ \\vdots \\\\ e^{-j\\omega\\tau_{N-1}} \\end{array} \\right]$$æˆ–ç”šè‡³å…¥å°„è§’åº¦ $\\theta$ å¦‚æœå¯ä»¥å®Œå…¨è¡¨é” $\\tau$ çš„è©±, æˆ‘å€‘ä¹Ÿèƒ½é€™éº¼å¯« $\\mathbf{\\upsilon}_\\theta(\\mathbf{\\theta})$. Array Signal Processingæ—©æœŸçš„ array processing (narrow band) æ˜¯å°æ¯å€‹éº¥å…‹é¢¨æœ‰å„è‡ªçš„ weights, ç„¶å¾Œå†ç¸½åˆèµ·ä¾†, é€™ç¨®ä½œæ³•å«åš weight-and-sum. è€Œå°æ–¼ broadband è¨Šè™Ÿä¾†èªª, ç›¸ç•¶æ–¼æ‹†é »ä¹˜å¾ˆå¤š narrow band, å› æ­¤åœ¨æ¯å€‹é »å¸¶ä¸Š, éƒ½æœ‰ N å€‹éº¥å…‹é¢¨çš„ weights. é€™åœ¨æ™‚åŸŸä¸Šç­‰åƒ¹æ–¼æ¯å€‹éº¥å…‹é¢¨éƒ½æœ‰å„è‡ªçš„ filters, ç¨± filter-and-sum. ä»¥ä¸‹ä»‹ç´¹ filter-and-sum å’Œé »åŸŸçš„æ¶æ§‹. Filter-and-Sum åœ¨å¯¦ä½œä¸Šé€šå¸¸æ¡ç”¨ FIR filter, å› æ­¤æ¶æ§‹å¦‚ä¸‹:ç¬¦è™Ÿæœ‰é»ä¸åŒ, é€™æ˜¯å› ç‚ºåœ–æ˜¯æ¡ç”¨å¦ä¸€æœ¬æ›¸ Microphone Array Signal Processing Frequency Domainé‡å° filter-and-sum åš frequency transform å¾—åˆ°å¦‚ä¸‹: å¯¦éš›æ¶æ§‹åœ–å¦‚ä¸‹:ä¸€æ¨£ç¬¦è™Ÿæœ‰é»ä¸åŒ, é€™æ˜¯å› ç‚ºåœ–æ˜¯æ¡ç”¨å¦ä¸€æœ¬æ›¸ Microphone Array Signal Processing Frequency-wavenumber Response Functioné‡å° frequency domain çš„ array processing, æˆ‘å€‘å¯ä»¥å¸¶å…¥å…ˆå‰æ¨å¾—çš„ (1) å¾—åˆ°å¦‚ä¸‹: æ‰€ä»¥ $\\Upsilon(\\omega,\\mathbf{k})$ ç‰©ç†æ„ç¾©å°±æ˜¯é‡å° frequency $\\omega$ å’Œ wavenumber $\\mathbf{k}$ (æ§åˆ¶äº†è²æºå…¥å°„è§’åº¦ $\\theta$ ç­‰ç­‰çš„ç‰©ç†é‡) çš„ response. Beampatternwavenumber $\\mathbf{k}$ æ¯”è¼ƒæŠ½è±¡, å¦‚æœæˆ‘å€‘æ›æˆè§’åº¦ $\\theta$, $\\phi$ å°±æœƒç›´è§€å¾ˆå¤š, è€Œ beampattern åªæ˜¯é‡å° $\\Upsilon(\\omega,\\mathbf{k})$ æ›æˆç”¨è§’åº¦è€Œå·². æ‰€ä»¥ $B(\\omega:\\theta,\\phi)$ ç‰©ç†æ„ç¾©å°±æ˜¯é‡å° frequency $\\omega$ å’Œå…¥å°„è§’åº¦ $\\theta$, $\\phi$ çš„ response. Delay-and-sum BeampatternDelay-and-sum æƒ³æ³•å¾ˆç°¡å–®, å°±æ˜¯è£œå„Ÿæ¯å€‹ mic çš„ time delay è€Œå·². å› æ­¤æ‰€éœ€è¦çš„ filter $H(\\omega)$ å°±æ˜¯ array manifold vector çš„ conjugate å³å¯. å¦‚ä¸‹åœ–: ä½†é€™éº¼åšæœ‰å€‹ç¼ºé», å°±æ˜¯é«˜é »æ™‚é›–ç„¶é‡å°è²æºæ–¹å‘çš„ mainlobe è®Šçª„äº†, ä½†åŒæ™‚ sidelobe å»è®Šå¤šäº†. ä¹Ÿå°±æ˜¯åœ¨é«˜é »æ™‚, æŸäº›æ–¹å‘çš„è²æºæ¶ˆä¸æ‰. å¦‚ä¸‹åœ–: Fixed Beampatternç‚ºäº†ä¿®æ­£ä¸Šè¿° DS beamformer çš„å•é¡Œ, æˆ‘å€‘å¸Œæœ›å¾—åˆ° response-invariant broadband beamformer. å¸Œæœ›èƒ½æœ‰ä¸‹åœ–çš„çµæœ: ä¸­å¿ƒæ€æƒ³å¾ˆç°¡å–®, é‡å°æŸå€‹é »ç‡ $\\omega$ ä¾†æ±‚å‡ºç›¸å°æ‡‰çš„ $H$ ä½¿å¾— beampattern æœƒèˆ‡æˆ‘å€‘ desired beampattern æœ‰ least-sqaure å·®ç•°. ä»¥ä¸‹ $H(\\omega)$ æœƒçœç•¥ $\\omega$ ä¸å¯« çµè«–åˆ°é€™è£¡æˆ‘å€‘è¨è«–äº†é å ´çš„ signal model, é‡å° anechoic model æˆ‘å€‘æœ€çµ‚å°å‡ºäº† beampattern. åšç‚ºä¾‹å­æˆ‘å€‘ä½¿ç”¨ç°¡å–®çš„ delay-and-sum (DS) beamformer ä¾†çœ‹å®ƒçš„ beampattern é•·ä»€éº¼æ¨£. å¯ä»¥çœ‹åˆ°åœ¨é«˜é »æ™‚å¾ˆå¤šæ–¹å‘é‚„æ˜¯ç„¡æ³•å£“æŠ‘, å› æ­¤ä½¿ç”¨ least-square æ–¹æ³•æ‰¾å‡ºæ¯å€‹é »ç‡éœ€è¦çš„ spatial filter ä¾†é€¼è¿‘æˆ‘å€‘éœ€è¦çš„ beampattern. Reference Optimum Array Processing: Part IV of Detection, Estimation, and Modulation Theory by Harry L. Van Trees Microphone Array Signal Processing by Jocab Benesty Direction of Arrival Estimation Using the Parameterized Spatial Correlation Matrix","tags":[{"name":"array manifold vector","slug":"array-manifold-vector","permalink":"http://yoursite.com/tags/array-manifold-vector/"},{"name":"beampattern","slug":"beampattern","permalink":"http://yoursite.com/tags/beampattern/"},{"name":"anechoic model","slug":"anechoic-model","permalink":"http://yoursite.com/tags/anechoic-model/"},{"name":"wavenumber","slug":"wavenumber","permalink":"http://yoursite.com/tags/wavenumber/"}]},{"title":"Bayesian Learning Notes","date":"2018-12-20T14:39:42.000Z","path":"2018/12/20/Bayesian-Learning-Notes/","text":"æ‰è²»æˆ‘å­¸ç¿’ ML é€™éº¼ä¹…, æœ€è¿‘æ‰å®Œæ•´äº†è§£ Bayesian learning å¤§æ¶æ§‹, ä»¥åŠèˆ‡ MLE, MAP, Variational Inference, Sampling ä¹‹é–“çš„é—œè¯. é€™æ‰çµ‚æ–¼æœ‰äº†è¦‹æ¨¹åˆè¦‹æ—çš„æ¸¯è¦ºé˜¿! ç­†è¨˜æ•´ç†å¦‚ä¸‹ â€¦ åœ–ç‰‡ä¾†è‡ª wiki, æˆ‘ä¹Ÿå¥½æƒ³è¦é€™å€‹è£é£¾ç‡ˆ. å°±é€™éº¼ä¸€å€‹ Bayeâ€™s Rule, æ’èµ·äº†çµ±è¨ˆæ©Ÿå™¨å­¸ç¿’çš„åŸºçŸ³! Bayesian Learningçµ¦å®šè¨“ç·´é›† ($X,Y$) å’Œä¸€å€‹ probabilistic classifier $p(y|x,\\theta)$, åŒæ™‚å®šç¾©å¥½ prior distribution $p(\\theta)$. æ ¹æ“š Bayeâ€™s rule, Training stage å¦‚ä¸‹: $$\\begin{align} p(\\theta|X,Y)=\\frac{p(Y|X,\\theta)p(\\theta)}{\\color{red}{\\int p(Y|X,\\theta)p(\\theta)\\,d\\theta}} \\end{align}$$ Testing stage å¦‚ä¸‹: $$\\begin{align} p(y^*|x^*,X,Y)=\\color{red}{\\int p(y^*|x^*,\\theta)p(\\theta|X,Y)\\,d\\theta} \\end{align}$$ æ³¨æ„åˆ°é—œéµçš„å…©å€‹ç´…è‰²ç©åˆ†é€šå¸¸éƒ½æ˜¯ä¸å®¹æ˜“ç®—, æˆ–æ ¹æœ¬ç®—ä¸å‡ºä¾†. æ­¤æ™‚æˆ‘å€‘æœ‰å…©ç¨®é¸æ“‡: ä½¿ç”¨ Variational Inference æ‰¾å‡ºä¸€å€‹ $q(\\theta)$ ä¾†é€¼è¿‘ $p(\\theta|X,Y)$ ä½¿ç”¨ sampling æ–¹æ³•. ç†è§£ä¸€ä¸‹é€™å€‹ç©åˆ†çš„å½¢å¼, å¯ä»¥ç™¼ç¾é€™æ˜¯åœ¨ç®—æ ¹æ“šæŸå€‹æ©Ÿç‡åˆ†ä½ˆ$p(x)$è¨ˆç®—$f(x)$çš„æœŸæœ›å€¼. å› æ­¤, å¦‚æœæˆ‘å€‘ç›´æ¥æ ¹æ“š $p(x)$ sample å‡º $M$ å€‹ $x$, å°±å¯ä»¥ç”¨å¦‚ä¸‹çš„å¹³å‡ç®—å‡ºè¿‘ä¼¼å€¼äº†. $$\\begin{align} \\int p(x)f(x) \\,dx \\simeq \\frac{1}{M}\\sum_{i=1}^M f(x_i)\\mbox{, where }x_i \\sim p(x) \\end{align}$$ æˆ‘å€‘å¯èƒ½æœƒæƒ³, æ˜¯ä¸æ˜¯å¯ä»¥å°‡ Bayesian learning åšäº›ç°¡åŒ–ä¾†é¿æ‰ä¸Šè¿°ç´…è‰²ç©åˆ†? æ˜¯çš„, MLE å’Œ MAP å°±æ˜¯ç°¡åŒ–äº†å®Œæ•´çš„ Bayesian learning éç¨‹. ä¸‹é¢ä»‹ç´¹. MLE and MAPBayeâ€™s rule (å¼ (1)), åœ¨ ML ä¸­èˆ‰è¶³è¼•é‡, å¹¾ä¹æ˜¯æ‰€æœ‰çš„æ ¹æœ¬. é‡æ–°åˆ—å‡ºä¾†ä¸¦ç”¨ä¸åŒé¡è‰²åšå¼·èª¿ $$\\begin{align} \\color{orange}{p(\\theta|X,Y)}=\\frac{\\color{blue}{p(Y|X,\\theta)}\\color{green}{p(\\theta)}}{\\color{red}{\\int p(Y|X,\\theta)p(\\theta)\\,d\\theta}} \\end{align}$$ æ©˜è‰²ç¨±ç‚º posterior distribution, è—è‰²ç‚º likelihood, è€Œç¶ è‰²ç‚º prior distribution. æ³¨æ„åˆ°ç´…è‰²çš„æœŸæœ›å€¼åŸºæœ¬ç®—ä¸å‡ºä¾†, åœ¨é€™ç¨®æƒ…æ³ä¸‹, æˆ‘å€‘è¦æ€éº¼å¾—åˆ° posterior? MLEMLE (Maximum Likelihood Estimation) çš„æƒ³æ³•æ˜¯, æ—¢ç„¶ posterior ç®—ä¸å‡ºä¾†, é‚£ä¹¾è„†ç›´æ¥ç”¨ä¸€å€‹ $\\theta^*$ ä»£è¡¨æ•´å€‹ $p(\\theta|X,Y)$ åˆ†å¸ƒç®—äº†. è‡³æ–¼è¦æ‰¾å“ªä¸€é»å‘¢, å°±æ‰¾å° likelihood æœ€å¤§çš„é‚£é»å§! æ•¸å­¸é€™éº¼å¯«: $$\\begin{align} \\theta_{MLE}=\\arg\\max_\\theta p(Y|X,\\theta) \\end{align}$$ æ—¢ç„¶å·²ç¶“ç”¨ä¸€å€‹é»ä¾†ä»£è¡¨æ•´å€‹ posterior äº†, å› æ­¤åŸä¾†çš„ testing (2) å°±ä¸éœ€è¦ç©åˆ†äº†, testing stage ç›´æ¥å°±æ˜¯: $$\\begin{align} p(y^*|x^*,\\theta_{MLE}) \\end{align}$$ MAPMAP (Maximum A Posterior) estimation è·Ÿ MLE ç›¸åŒ, ä¹Ÿä½¿ç”¨ä¸€å€‹é»ä¾†ä»£è¡¨æ•´å€‹ posterior: $$\\begin{align} \\theta_{MP}=\\arg\\max_\\theta p(\\theta|X,Y) \\end{align}$$ æ„æ€æ˜¯ MAP ç›´æ¥ä½¿ç”¨ mode ä¾†ä»£è¡¨æ•´å€‹ posterior. å› æ­¤ testing stage ä¹Ÿå¦‚åŒ MLE æƒ…å½¢: $$\\begin{align} p(y^*|x^*,\\theta_{MP}) \\end{align}$$ ä¸éè°æ˜çš„è®€è€…æ‡‰è©²æœƒè¦ºå¾—å¾ˆç–‘æƒ‘, posterior ä¸æ˜¯å¾ˆé›£è¨ˆç®—, æˆ–æ ¹æœ¬ç®—ä¸å‡ºä¾†, é€™æ¨£æ€éº¼å¯èƒ½æ‰¾çš„åˆ° mode? æ˜¯çš„, ä¸€èˆ¬æƒ…å½¢ä¸‹æ˜¯æ‰¾ä¸å‡ºä¾†, ä½†æœ‰ä¸€å€‹ç‰¹æ®Šæƒ…æ³å«åš conjugate prior. conjugate prior æŒ‡çš„æ˜¯ prior èˆ‡ posterior å±¬æ–¼åŒä¸€å€‹ distribution family, ç­‰æ–¼æ˜¯å‘Šè¨´æˆ‘å€‘ posterior æ˜¯ä»€éº¼æ¨£çš„ distribution, å› æ­¤ç®—ä¸å‡ºä¾†çš„ç´…è‰²æœŸæœ›å€¼(å¼(4))ä¹Ÿæ ¹æœ¬æ²’å¿…è¦å»è¨ˆç®—, åªä¸éæ˜¯å€‹ normalization constant. å› æ­¤æ˜ç¢ºçŸ¥é“ posterior æ˜¯ä»€éº¼æ¨£çš„ distribution, æ‰¾ mode å°±å®¹æ˜“å¤šäº†. æ‰€ä»¥å°æ–¼ MAP ä¾†èªªæœ‰å“ªäº› distribution æ˜¯äº’ç‚º conjugate è®Šå¾—å¾ˆé‡è¦. æˆ‘å€‘å¯ä»¥å¾ wiki ä¸ŠæŸ¥åˆ°æ˜ç¢ºè³‡æ–™. åŸºæœ¬ä¸Š exponential family éƒ½æ˜¯. å®Œå…¨é¿æ‰ç´…è‰²ç©åˆ†é …äº†å—?å¾ˆå¤šæ¨¡å‹éƒ½å…·æœ‰ latent variable (ä¸€èˆ¬éƒ½ç”¨ $z$ è¡¨ç¤º) çš„å½¢å¼ ç¨å¾®èªªæ˜ä¸‹, ä¸€èˆ¬èªªçš„ latent variable æœƒéš¨è‘— data è®Šå¤§è€Œè®Šå¤§, è€Œ parameter $\\theta$ ä¸æœƒ. ä»¥ GMM ç‚ºä¾‹å­, latent variable æŒ‡æ¯ä¸€å€‹ observation æ˜¯å“ªä¸€å€‹ Gaussian ç”¢ç”Ÿå‡ºä¾†çš„é‚£å€‹ index, è€Œ parameter æ˜¯ Gaussian components çš„ mean, var, å’Œ mixture weights é›†åˆ. å¯ä»¥ä½¿ç”¨ EM algorithm ä¾†æ‰¾å‡º MLE æˆ– MAP . å…¶ä¸­ E-step ç‚º â€œä»¤ $q(z)$ ç­‰æ–¼ $p(z|x,\\theta^{odd})$â€, é€™åˆå›åˆ°å¦‚åŒå¼ (1) æ±‚ posterior æœƒé‡åˆ°åˆ†æ¯ç©åˆ†é …çš„å•é¡Œ. å¦‚æœæˆ‘å€‘çš„ $z$ çš„å€¼æœ‰é™å€‹çš„ (å¦‚ GMM, $z$ çš„å€¼å°±æ˜¯ component çš„ index), $p(z|x,\\theta^{odd})$ å¯ä»¥ç›´æ¥ç®—å‡ºä¾†. ä½†è¤‡é›œä¸€é»å°±ä¸è¡Œäº†, æ‰€ä»¥æƒ…æ³åˆè®Šå¾—è·ŸåŸä¾†çš„ Bayesian learning ä¸€æ¨£, å…©ç¨®é¸æ“‡: ä½¿ç”¨ Variational Inference, é€™æ™‚ç¨±ç‚º Variational EM. ä½¿ç”¨ sampling æ–¹æ³•. Sampling é€šå¸¸æ¡ç”¨ MCMC æ–¹å¼, é€™æ™‚ç¨±ç‚º MCMC EM. Summaryæ“·å–è‡ª Coursera çš„ Bayesian Methods for Machine Learning èª²ç¨‹æŠ•å½±ç‰‡å¦‚ä¸‹: (åœ–ä¸­çš„ $T$ æŒ‡çš„æ˜¯ latent variable)","tags":[{"name":"Bayesian Learning","slug":"Bayesian-Learning","permalink":"http://yoursite.com/tags/Bayesian-Learning/"},{"name":"Conjugate Prior","slug":"Conjugate-Prior","permalink":"http://yoursite.com/tags/Conjugate-Prior/"},{"name":"MLE","slug":"MLE","permalink":"http://yoursite.com/tags/MLE/"},{"name":"MAP","slug":"MAP","permalink":"http://yoursite.com/tags/MAP/"}]},{"title":"Gaussian Process used in Bayesian Optimization","date":"2018-12-09T10:46:36.000Z","path":"2018/12/09/Gaussian-Process-used-in-Bayesian-Optimization/","text":"ä¸Šäº† Coursera çš„ Bayesian Methods for Machine Learning, å…¶ä¸­æœ€å¾Œä¸€é€±çš„èª²ç¨‹ä»‹ç´¹äº† Gaussian processes &amp; Bayesian optimization è¦ºå¾—å¾ˆæœ‰æ”¶ç©«, å› ç‚ºåš ML æœ€ç—›è‹¦çš„å°±æ˜¯ hyper-parameter tuning, å¸¸è¦‹çš„æ–¹æ³•å°±æ˜¯æ‰‹å‹•èª¿, grid search or random search. ç¾åœ¨å¯ä»¥æœ‰ä¸€å€‹è¼ƒ â€œæ¨¡å‹â€ çš„ä½œæ³•: Bayesian optimization. ç‚ºäº†ç­è§£é€™å€‹éç¨‹, æˆ‘å€‘æœƒä»‹ç´¹å¦‚ä¸‹å…§å®¹ä¸¦åŒæ™‚ä½¿ç”¨ GPy and GPyOpt åšäº› toy example: Random Process and Gaussian Process Stationary and Wide-Sense Stationary (WSS) GP for regression GP for bayesian optimization è®“æˆ‘å€‘é€²å…¥ GP çš„é ˜åŸŸå§ Random Process (RP) and Gaussian Process (GP)Random process (RP) æˆ–ç¨± stochastic process å®šç¾©ç‚º [Def]: For any $x\\in\\mathbb{R}^d$ assign random variable $f(x)$ ä¾‹å¦‚ $d=1$ ä¸”æ˜¯é›¢æ•£çš„æƒ…å½¢, ($x\\in\\mathbb{N}$) å®šç¾©èªªæ˜å°æ–¼æ¯ä¸€å€‹ $x$, $f[x]$ éƒ½æ˜¯ä¸€å€‹ r.v. æ‰€ä»¥ $f[1]$, $f[2]$, â€¦ éƒ½æ˜¯ r.v.s. æ­¤ case é€šå¸¸æŠŠ $x$ ç•¶ä½œæ™‚é–“ $t$ ä¾†çœ‹. è€Œ Gaussian Process å®šç¾©ç‚º [Def]: Random process $f$ is Gaussian, if for any finite number points, their joint distribution is normal. Stationary and Wide-Sense Stationary (WSS)Stationaryä¸€å€‹ RP æ˜¯ stationary å®šç¾©å¦‚ä¸‹: [Def]: Random process is stationary if its finite-dimensional distributions depend only on relative position of the points ç°¡å–®èˆ‰ä¾‹: å–ä¸‰å€‹ r.v.s $(x_1,x_2,x_3)$ ä»–å€‘çš„ joint pdf æœƒè·Ÿ $(x_1+t,x_2+t,x_3+t)$ ä¸€æ¨¡ä¸€æ¨£$$\\begin{align} p(f(x_1),f(x_2),f(x_3))=p(f(x_1+t),f(x_2+t),f(x_3+t)) \\end{align}$$æ‰€ä»¥ pdf åªèˆ‡ç›¸å°ä½ç½®æœ‰é—œ, ç™½è©±è¬›å°±æ˜¯æˆ‘å€‘è§€å¯Ÿ joint pdf å¯ä»¥ä¸ç”¨åœ¨æ„çœ‹çš„æ˜¯å“ªå€‹å€æ®µçš„ä¿¡è™Ÿ, å› ç‚ºéƒ½æœƒä¸€æ¨£. é€²ä¸€æ­¥åœ°, å¦‚æœé€™å€‹ RP æ˜¯ GP çš„è©±, æˆ‘å€‘çŸ¥é“ joint pdf æ˜¯ normal, è€Œ normal åªç”± mean and variance totally æ±ºå®š, å› æ­¤ä¸€å€‹ GP æ˜¯ stationary åªè¦ mean and variance åªè·Ÿç›¸å°ä½ç½®æœ‰é—œå°±æœƒæ˜¯ stationary. åŸºæ–¼é€™æ¨£çš„æ¢ä»¶æˆ‘å€‘å¯ä»¥å¯«å‡ºä¸€å€‹ stationary GP çš„å®šç¾©: [Def]: Covariance matrix or Kernel åªè·Ÿç›¸å°ä½ç½®æœ‰é—œ, ä»¥ä¸‹ç‚ºä¸‰ç¨®å¸¸è¦‹çš„å®šç¾©æ–¹å¼ä¸ç®¡æ€æ¨£, é€šå¸¸ç›¸å°ä½ç½®è¿‘çš„ r.v. éƒ½æœƒå‡è¨­æ¯”è¼ƒç›¸é—œ, (é€™ä¹Ÿç¬¦åˆå¯¦éš›ç‹€æ³, è­¬å¦‚è²éŸ³è¨Šè™Ÿæ™‚é–“é»ç›¸è¿‘çš„ sample æœƒè¼ƒç›¸é—œ), ä¹Ÿå› æ­¤ kernel éƒ½æœƒé•·é¡ä¼¼ä¸‹é¢çš„æ¨£å­ æ”¯ç·š Wide-Sense Stationary (WSS)æœ¬æ®µå¯è·³é, ä¸»è¦æ˜¯ç‚ºäº†æ›´æ·±åœ°ç†è§£ stationary åšçš„è£œå…….WSS å®šç¾©ç‚º [Def]: Random Process is WSS if its finite-dimensional distributionâ€™s mean and variance depend only on relative position of the points æ³¨æ„ WSS èˆ‡ Stationary çš„å®šç¾©å·®ç•°. æº–ç¢ºä¾†èªª Stationary è¦æ±‚æ‰€æœ‰çš„ moments éƒ½åªèˆ‡ç›¸å°ä½ç½®æœ‰é—œ, ä½† WSS åªè¦æ±‚åˆ° first and second order moments. èªªæ˜äº† WSS å°‡ stationary çš„æ¢ä»¶æ”¾å¯¬. æ³¨æ„åˆ° WSS ä¸ä¸€å®šæ˜¯ stationary çš„ GP, é€™æ˜¯å› ç‚º WSS æ²’æœ‰è¦æ±‚ distribution å¿…é ˆæ˜¯ Normal.WSS, Stationary GP, Stationary RP ä¹‹é–“çš„é—œä¿‚å¯ä»¥é€™éº¼æè¿°:$$\\begin{align} \\mbox{Stationary GP}\\subset\\mbox{Stationary RP}\\subset\\mbox{WSS} \\end{align}$$ å…¶å¯¦ WSS èˆ‡æœ¬ç¯‡ä¸»è¦è¨è«–çš„ Bayesian Optimization æ²’æœ‰ç›´æ¥é—œä¿‚, æœƒæƒ³ä»‹ç´¹æ˜¯å› ç‚ºæ»¿è¶³ WSS çš„è©±, èƒ½ä½¿æˆ‘å€‘æ›´ç›´è¦ºçš„ â€œçœ‹å‡ºâ€ ä¸€å€‹è¨Šè™Ÿæ˜¯å¦å¯èƒ½æ˜¯ stationary. (å¦å¤– WSS åœ¨ Adaptive Filtering éå¸¸é‡è¦, ç›¸ç•¶æ–¼åŸºçŸ³çš„å­˜åœ¨)é¦–å…ˆä½¿ç”¨èª²ç¨‹çš„ stationary ç¯„ä¾‹: ä¸­é–“çš„åœ–æ˜é¡¯ä¸æ˜¯ stationary å› ç‚º mean éš¨è‘—ä½ç½®æ”¹è®Šä¸æ˜¯ constant, ä½†å·¦é‚Šå’Œå³é‚Šå°±çœŸçš„ä¸æ˜¯é‚£éº¼å®¹æ˜“çœ‹å‡ºä¾†äº†. é‚£éº¼ç©¶ç«Ÿæœ‰ä»€éº¼æ–¹æ³•è¼”åŠ©æˆ‘å€‘åˆ¤æ–· stationary å‘¢?WSS çš„ power-spectral density property èªªæ˜äº†ä¸€å€‹ signal å¦‚æœæ˜¯ WSS, å‰‡å®ƒçš„ Covariance matrix or Kernel (è¨Šè™Ÿè™•ç†é€šå¸¸ç¨± auto-correlation) çš„ DTFT æ­£å¥½ä»£è¡¨çš„ç‰©ç†æ„ç¾©å°±æ˜¯ power spectral density, è€Œå› ç‚º kernel ä¸æœƒå› ä½ç½®æ”¹è®Š, é€™å°è‡´äº†ä¸ç®¡æˆ‘å€‘åœ¨å“ªå€‹å€æ®µå–å‡ºä¸€å€‹ window çš„è¨Šè™Ÿ, å®ƒå€‘çš„ power spectral density éƒ½æœƒé•·ä¸€æ¨£. é€™å€‹æ€§è³ªå¯ä»¥è®“æˆ‘å€‘å¾ˆæ–¹ä¾¿çš„ â€œçœ‹å‡ºâ€ æ˜¯å¦æ˜¯ stationary. (ç°¡å–®è¬›å°±æ˜¯çœ‹ signal çš„ frequency domain æ˜¯å¦å› ç‚ºéš¨è‘—æ™‚é–“è€Œè®ŠåŒ–, è®Šçš„è©±å°±ä¸€å®šä¸æ˜¯ stationary) å¥½äº†, æ¥è‘—å›åˆ°ä¸»ç·šå». GP for regressionç›´æ¥ç¯€éŒ„èª²ç¨‹ slides, å› ç‚º stationary GP çš„ mean æ˜¯ const, å› æ­¤æˆ‘å€‘æ‰£æ‰ offset è®“å…¶ç‚º 0, ä¹‹å¾Œå†è£œå›å³å¯.åš Regression çš„ç›®çš„å°±æ˜¯ given $x$ å¦‚ä½•é æ¸¬ $f(x)$, è€Œæˆ‘å€‘æœ‰çš„ training data ç‚º $x_1, â€¦, x_n$ ä»¥åŠå®ƒå€‘ç›¸å°çš„ $f(x_1),â€¦,f(x_n)$. GP å°±å¯ä»¥å¾ˆæ¼‚äº®åœ°åˆ©ç”¨ conditional probability é æ¸¬ $f(x)$ ç”±æ–¼æ˜¯ GP, å°è‡´ä¸Šé¢è—è‰²éƒ¨åˆ†çµæœä»æ˜¯ Gaussian, å› æ­¤æˆ‘å€‘å¾—åˆ°Regression å…¬å¼: æœ‰æ™‚å€™æˆ‘å€‘çš„ observation $f(x)$ æ˜¯ noisy çš„, æ­¤æ™‚ç°¡å–®åœ°å° $f(x)$ åŠ ä¸Šä¸€å€‹ random Gaussain noise æœƒä½¿å¾—æˆ‘å€‘çš„ model robust äº›. ä¸Šé¢å…¬å¼éƒ½ä¸ç”¨æ”¹, åªè¦é‡å° kernel ä½œå¦‚ä¸‹æ›´å‹•å³å¯ GPy toolkit exampleæˆ‘å€‘ä½¿ç”¨ Gaussian process regression tutorial çš„ç¯„ä¾‹, ä½¿ç”¨ä¸Šç®—æ˜¯å¾ˆç›´è¦º, è®“æˆ‘å€‘ç°¡å–®å¯¦é©—ä¸€ä¸‹ 12345678910111213import numpy as npimport GPyimport matplotlib.pyplot as plt# Generate data points, where Y is noisyX = np.random.uniform(-3.,3.,(20,1))Y = np.sin(X) + np.random.randn(20,1)*0.2# Define kernel, we use RBFkernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)# Define GP regression modelm = GPy.models.GPRegression(X,Y,kernel,noise_var=1.)# See plotfig = m.plot()plt.show() Codes çš„çµæœå¦‚ä¸‹ [Note]: ä¸Šåœ–çš„ Mean å’Œ Confidence æŒ‡çš„æ˜¯ Regression å…¬å¼çš„ $\\mu$ and $\\sigma^2$ (å‰å¹¾å¼µæœ‰ç´…æ¡†çš„åœ–), å¦å¤–ä½¿ç”¨ GPy ç®— regression çµæœçš„è©±é€™éº¼ä½¿ç”¨1mu, sigma2 = m.predict(np.array([[1.0]])) åŸºæœ¬ä¸Š input æ˜¯ä¸€å€‹ shape=(batch_size,in_dim) çš„ array å¯ä»¥çœ‹åˆ°å°±ç®—æ˜¯ data point é™„è¿‘, æ‰€é¡¯ç¤ºçš„ y é‚„æ˜¯æœ‰éå¸¸å¤§çš„ uncertainty, é€™æ˜¯å› ç‚º observation noise çš„ variance å¯èƒ½å¤ªå¤§äº†, æˆ‘å€‘å¾ 1.0 æ”¹æˆ 0.04 (æ­£ç¢ºç­”æ¡ˆ) çœ‹çœ‹ å¯ä»¥çœ‹åˆ°æœ‰ data point çš„åœ°æ–¹ä¸ç¢ºå®šæ€§é™ä½å¾ˆå¤š, ä¸”ä¸ç¢ºå®šæ€§çœ‹èµ·ä¾†å¾ˆåˆç† (ç•¶ç„¶, å› ç‚ºæˆ‘å€‘ç”¨æ­£ç¢ºç­”æ¡ˆçš„ noise var)æ¥è‘—æˆ‘å€‘æ”¹ kernel çš„ lengthscale (æ§åˆ¶å¹³æ»‘ç¨‹åº¦) å¾ 1 ç¸®å°æˆ 0.5 æ‡‰è©²å¯ä»¥é æœŸ regression çš„ mean æœƒæ‰­æ›²æ¯”è¼ƒå¤§, çµæœå¦‚ä¸‹ çœ‹çœ‹ä¸€å€‹æ¥µç«¯æƒ…æ³, å°‡ kernel çš„ lengthscale é™åˆ°éå¸¸å°, é€™æœƒå°è‡´ kernel é€€åŒ–æˆ delta function, ä¹Ÿå°±æ˜¯é™¤äº†è‡ªå·±å¤§å®¶äº’ä¸ç›¸é—œ. æŸ¥çœ‹ regression å…¬å¼ (å‰å¹¾å¼µæœ‰ç´…æ¡†çš„åœ–), ç”±æ–¼ kernel é€€åŒ–æˆ delta function, k å‘é‡è¶¨è¿‘æ–¼ 0, æ‰€ä»¥ regression å…¬å¼çš„ mean è¶¨è¿‘æ–¼ 0, variance è¶¨è¿‘æ–¼ prior K(0). æˆ‘å€‘å°‡ lengthscale èª¿æ•´æˆ 0.02 å¾—åˆ°å¦‚ä¸‹çš„åœ– å¯ä»¥çœ‹åˆ° x ç¨å¾®é›¢é–‹ data point çš„åœ°æ–¹, åŸºæœ¬ mean å°±å›åˆ° 0, ä¸” variance å›åˆ° prior K(0) å¾é€™ç°¡å–®çš„å¯¦é©—æˆ‘å€‘ç™¼ç¾, é€™ä¸‰å€‹æ§åˆ¶åƒæ•¸:-RBF variance-RBF lengthscale-GPRegression noise_var è¨­ç½®ä¸å¥½, åŸºæœ¬å¾ˆæ‚²åŠ‡, é‚£æ€éº¼æ‰æ˜¯å°çš„? ä¸‹ä¸€æ®µæˆ‘å€‘ä»‹ç´¹ä½¿ç”¨æœ€ä½³åŒ–æ–¹å¼æ‰¾å‡ºæœ€å¥½çš„åƒæ•¸. Learning the kernel parametersç”±æ–¼éƒ½æ˜¯ normal, å› æ­¤ MLE ç›®æ¨™å‡½å¼å¯å¾®, å¯å¾®å°±ä½¿ç”¨ gradient ascent ä¾†æ±‚åƒæ•¸. èª²ç¨‹ slide å¦‚ä¸‹ GPy çš„ä½¿ç”¨å°±é€™éº¼ä¸€è¡Œ m.optimize(messages=True) çµæœå¦‚ä¸‹ GP for bayesian optimizationå•é¡Œå†æè¿°ä¸€ä¸‹, å°±æ˜¯èªªæ¨¡å‹æœ‰ä¸€å¤§å †åƒæ•¸ (ç¨± $x$) è¦èª¿æ•´, é¸æ“‡ä¸€çµ„åƒæ•¸å¯ä»¥å¾—åˆ°ä¸€å€‹ validation accuracy (ç¨± $f(x)$), æˆ‘å€‘å¸Œæœ›æ‰¾åˆ°ä¸€çµ„åƒæ•¸ä½¿å¾— $f(x)$ æœ€å¤§. é€™å€‹éº»ç…©ä¹‹è™•å°±åœ¨æ–¼æˆ‘å€‘å° $f(x)$ çš„è¡¨ç¤ºä¸€ç„¡æ‰€çŸ¥, åªèƒ½é€éæ¡æ¨£å¾—åˆ° $f(x)$, è€Œæ¯æ¬¡è¦å¾—åˆ°ä¸€å€‹æ¡æ¨£çš„ $f(x)$ éƒ½è¦ç¶“éæ¼«é•·çš„è¨“ç·´æ‰èƒ½å¾—åˆ°. åœ¨é€™ç¨®æƒ…å½¢ä¸‹, æ€éº¼é€éæœ€å°‘æ¡æ¨£é»å¾—åˆ°è¼ƒå¤§çš„ $f(x)$ å€¼å‘¢? å¤§çµ•å°±æ˜¯ç”¨ GP ä¾† approximate $f(x)$ åˆç†å—? æˆ‘å€‘é€™éº¼æƒ³, ç”±æ–¼ kernel ä¸€èˆ¬çš„å®šç¾©æœƒä½¿å¾—ç›¸è¿‘çš„æ¡æ¨£é»æœ‰è¼ƒé«˜çš„ç›¸é—œå€¼, ä¹Ÿå°±æ˜¯é¡ä¼¼çš„åƒæ•¸æœƒå¾—åˆ°è¼ƒç›¸é—œçš„ validation accuracy. é€™éº¼æƒ³çš„è©±å¤šå°‘æœ‰äº›åˆç†. å¦ä¸€å€‹å¥½è™•æ˜¯ä½¿ç”¨ GP å¯ä»¥å¸¶çµ¦æˆ‘å€‘æ©Ÿç‡åˆ†å¸ƒ, é€™ä½¿å¾—æˆ‘å€‘å¯ä»¥ä½¿ç”¨å„ç¨®è€ƒé‡ä¾†æ±ºå®šä¸‹ä¸€å€‹æ¡æ¨£é». ä¾‹å¦‚: æˆ‘å€‘å¯ä»¥è€ƒæ…®åœ¨é‚£äº›ä¸ç¢ºå®šæ€§è¼ƒå¤§çš„åœ°æ–¹è©¦è©¦çœ‹, å› ç‚ºèªªä¸å®šæœ‰æ›´é«˜çš„$f(x)$, æˆ–æ˜¯åœ¨å·²çŸ¥ç›®å‰ä¼°æ¸¬çš„ GP model ä¸‹æœ‰è¼ƒé«˜çš„ mean å€¼é‚£è£æ¡æ¨£. é€™å…©ç¨®æ–¹å¼ç¨±ç‚º â€œExplorationâ€ and â€œExploitationâ€ æ‰€ä»¥ Bayesian optimization ä¸»è¦å°±å…©å€‹éƒ¨åˆ†, Surrogate model (å¯ä»¥ä½¿ç”¨ GP) å’Œ Acquisition function: æ¼”ç®—æ³•å°±å¾ˆç›´è¦ºäº†, æ ¹æ“š Acquisition function å¾—åˆ°çš„æ¡æ¨£é» $x$ è¨ˆç®—å‡ºä¾† $f(x)$ å¾Œ, é‡æ–°æ›´æ–° GP (ä½¿ç”¨ MLE æ‰¾å‡ºæœ€å¥½çš„ GP åƒæ•¸æ›´æ–°), æ›´æ–°å¾Œç¹¼çºŒè¨ˆç®—ä¸‹å€‹æ¡æ¨£é». æˆ‘å€‘é‚„æœªèªªæ˜ Acquisition function æ€éº¼å®šç¾©, å¸¸ç”¨çš„æ–¹æ³•æœ‰ä¸‹é¢ä¸‰ç¨®: Maximum probability of improvement (MPI) Upper confidence bound (UCB) Expected improvement (EI), ç¶²è·¯ä¸Šèªªæœ€å¸¸è¢«ç”¨ Expected improvement (EI) å¯ä»¥åƒè€ƒé€™ç¯‡ blog æ­é…é€™å€‹ implementation, é€™è£¡å°±ä¸é‡è¤‡äº†. å€¼å¾—ä¸€æçš„æ˜¯ Acquisition function é€šå¸¸æœ‰å€‹åƒæ•¸ $\\xi$ å¯ä»¥æ§åˆ¶ Exploration å’Œ Exploitation çš„ tradeoff. æ¥è‘—æˆ‘å€‘ä½¿ç”¨ GPyOpt ç·´ç¿’ä¸€å€‹ toy example. GPyOpt toy exampleé¡ä¼¼ä¸Šé¢çš„ GP for regression çš„ç¯„ä¾‹, $x$ and $f(x)$ æˆ‘å€‘ç°¡å–®å®šç¾©å¦‚ä¸‹: 1234sample_num = 20offset = 10def probeY(x): # i.e. f(x) return np.sin(x) + np.random.randn()*0.2 + offset é€™æ¬¡æˆ‘å€‘å°‡ $f(x)$ æ•…æ„åŠ äº†ä¸€å€‹ offset, é›–ç„¶å°æ–¼ GP regression çš„å‡è¨­æ˜¯ mean=0, ä¸é GPyOpt é è¨­æœƒå° $f(x)$ å»æ‰ offset, æ‰€ä»¥å…¶å¯¦æˆ‘å€‘å¯ä»¥å¾ˆå®‰å…¨çš„ä½¿ç”¨ API. æ¥è‘—é—œéµç¨‹å¼ç¢¼å¦‚ä¸‹ 12345678bounds = [&#123;'name': 'var_1', 'type': 'continuous', 'domain': (-3.0,3.0)&#125;] # domain definitionmyBopt = GPyOpt.methods.BayesianOptimization(f=probeY, # function to optimize domain=bounds, # box-constraints of the problem normalize_Y=True, # normalize Y to mean = 0 (default) acquisition_type='EI', # acquisition function type (default) maximize=False, # do maixmization? (default=False) exact_feval = False)myBopt.run_optimization(max_iter) bounds æè¿°äº†æ¯ä¸€å€‹ probeY çš„ arguments. ç‰¹åˆ¥è¦èªªä¸€ä¸‹ exact_feval = False, é€™æ˜¯èªªæ˜ $f(x)$ æ˜¯ noisy çš„, æ‰€ä»¥æœƒæœ‰ noise variance å¯ä»¥ model (åƒè€ƒ 3. GP for regression çš„ regression å…¬å¼å«noiseçš„æƒ…å½¢), é€™åœ¨å¯¦éš›æƒ…æ³éå¸¸é‡è¦. æ›´å¤š BayesianOptimization åƒæ•¸æè¿° åƒè€ƒé€™ ä½¿ç”¨å¦‚ä¸‹æŒ‡ä»¤çœ‹ acquisition å’Œ regression function çš„çµæœ1myBopt.plot_acquisition() æ‰èŠ±1xå€‹æ¡æ¨£æ±‚å‡ºä¾†çš„ regression function å°±å¾ˆé€¼è¿‘çœŸå¯¦ç‹€æ³äº†, é‚„ä¸éŒ¯. å¦å¤–æ³¨æ„åˆ°é€™è£¡çš„ $f(x)$ å·²ç¶“å»æ‰ offset äº†.å†ä¾†ä½¿ç”¨å¦‚ä¸‹æŒ‡ä»¤çœ‹æ¯ä¸€æ¬¡æ¡æ¨£å€¼ä¹‹é–“çš„å·®ç•°, ä»¥åŠæ¡æ¨£é»çš„ $f(x)$1myBopt.plot_convergence() ç¬¬9æ¬¡æ¡æ¨£é–‹å§‹, æ¡æ¨£é» $x$ ä»¥åŠ $f(x)$ ä¹‹é–“åŸºæœ¬æ²’ä»€éº¼å·®ç•°äº†.è‹¥è¦æ‹¿åˆ°æ¡æ¨£éç¨‹çš„ $x$ and $f(x)$, å¯ä½¿ç”¨ myBopt.X å’Œ myBopt.Y XGBoost parameter tuningé‡å° XGBoost çš„åƒæ•¸é€²è¡Œæœ€ä½³åŒ– (å¯åƒè€ƒé€™ç¯‡ blog), é—œéµå°±åœ¨æ–¼ä¸Šé¢çš„ $probeY$ éœ€æ›¿æ›æˆè¨ˆç®—æŸå€‹æ¡æ¨£çš„ evaluation accuracy (å› æ­¤é‚„éœ€è¦è¨“ç·´). é—œéµç¨‹å¼ç¢¼å¦‚ä¸‹: 123456789101112131415161718192021222324252627282930313233343536from xgboost import XGBRegressorfrom sklearn.model_selection import cross_val_score... Some codes here# Score. Optimizer will try to find minimum, so we will add a \"-\" sign.def probeY(parameters): parameters = parameters[0] score = -cross_val_score( XGBRegressor(learning_rate=parameters[0], max_depth=int(parameters[2]), n_estimators=int(parameters[3]), gamma=int(parameters[1]), min_child_weight = parameters[4]), X, y, scoring='neg_mean_squared_error').mean() score = np.array(score) return score# Bounds (NOTE: define continuous variables first, then discrete!)bounds = [ &#123;'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)&#125;, &#123;'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)&#125;, &#123;'name': 'max_depth', 'type': 'discrete', 'domain': (1, 50)&#125;, &#123;'name': 'n_estimators', 'type': 'discrete', 'domain': (1, 300)&#125;, &#123;'name': 'min_child_weight', 'type': 'discrete', 'domain': (1, 10)&#125; ]np.random.seed(777)optimizer = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds, acquisition_type ='MPI', acquisition_par = 0.1, exact_eval=True)max_iter = 50max_time = 60optimizer.run_optimization(max_iter, max_time)optimizer.plot_convergence()optimizer.X[np.argmin(optimizer.Y)] èª²ç¨‹è®“æˆ‘å€‘æ¸¬è©¦äº† sklearn.datasets.load_diabetes() dataset, ä½¿ç”¨ GPyOpt å¯ä»¥è®“ XGBoost æ¯”é è¨­åƒæ•¸æœ‰ 9% çš„æå‡! é‚„æ˜¯å¾ˆä¸éŒ¯çš„. å†ä¾†å°±å¾ˆæœŸå¾…æ˜¯å¦èƒ½çœŸçš„åœ¨å·¥ä½œä¸Šå° DNN å¥—ç”¨äº†. Reference GPy and GPyOpt GPyOptâ€™s documentation can find APIs GPyOpt tutorial å¾ˆå¥½çš„ GPy and GPyOpt æ•¸å­¸å’Œç¯„ä¾‹ blog Acquisition function implementation WSS power-spectral density property https://www.imft.fr/IMG/pdf/psdtheory.pdf","tags":[{"name":"Gaussian Process","slug":"Gaussian-Process","permalink":"http://yoursite.com/tags/Gaussian-Process/"},{"name":"Bayesian Optimization","slug":"Bayesian-Optimization","permalink":"http://yoursite.com/tags/Bayesian-Optimization/"},{"name":"Stationary","slug":"Stationary","permalink":"http://yoursite.com/tags/Stationary/"}]},{"title":"CTC Implementation Practice","date":"2018-10-16T12:25:10.000Z","path":"2018/10/16/CTC-Implementation-Practice/","text":"Credit æ˜¯æ­¤ç¯‡ DingKe ipynb çš„, ä»–å®Œæ•´å‘ˆç¾äº† CTC loss ä»¥åŠ gradient çš„è¨ˆç®—, éå¸¸æ£’!æ­¤ç­†è¨˜åŠ å…¥è‡ªå·±çš„èªªæ˜, ä¸¦ä¸”æœ€å¾Œä½¿ç”¨ tensorflow ä¾†é©—è­‰.é€™ç¯‡å¦ä¸€å€‹ä¸»è¦ç›®çš„ç‚ºæ”¹æˆå¯ä»¥ç·´ç¿’çš„æ ¼å¼ (#TODO tag). å› ç‚ºæˆ‘ç›¸ä¿¡æœ€å¥½çš„å­¸ç¿’æ–¹å¼æ˜¯è‡ªå·±é€ ä¸€æ¬¡è¼ªå­, æ‰€ä»¥å¯ä»¥çš„è©±, è«‹è©¦è‘—æŠŠ #TODO tag çš„éƒ¨åˆ†åšå®Œå§.æˆ‘å€‘åªå°ˆæ³¨åœ¨ CTC loss çš„ forward, backwark and gradient. Decoding éƒ¨åˆ†è«‹åƒè€ƒåŸä½œè€…çš„ ipynb. æœ€å¾Œä½¿ç”¨ tf.nn.ctc_loss and tf.gradients èˆ‡æˆ‘å€‘çš„è¨ˆç®—åšå°æ¯” å®Œæˆä»¥ä¸‹æ­¥é©Ÿ å®Œæˆ CTC_Practice.ipynb #TODO tag åƒè€ƒ CTC_Practice_Answer.ipynb Reference DingKe ipynb Sequence Modeling With CTC Graves CTC","tags":[{"name":"CTC","slug":"CTC","permalink":"http://yoursite.com/tags/CTC/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"}]},{"title":"Variational Inference and VAE Notes","date":"2018-09-18T14:21:05.000Z","path":"2018/09/18/Variational-Inference-Notes/","text":"å‰ä¸€é™£å­å­¸ç¿’äº† Variational Inference, å› ç‚ºè‡ªå·±è¨˜æ€§åªæœ‰ LSTM æ²’æœ‰ L, æ‰€ä»¥è¶•å¿«è¨˜ä¸‹ç­†è¨˜. å­¸å¾—é‚„æ˜¯å¾ˆç²—æ·º, åˆæ˜¯ä¸€å€‹å¤§å‘é˜¿.ç›£ç£å­¸ç¿’ä¸å¤–ä¹å°±æ˜¯ training å’Œ testing (inference). è€Œ inference åœ¨åšçš„äº‹æƒ…å°±æ˜¯åœ¨è¨ˆç®—å¾Œé©—æ¦‚ç‡ $p(z|x)$. åœ¨ PGM ä¸­é€šå¸¸æ˜¯ intractable, æˆ–è¦æ‰¾åˆ° exact solution çš„è¨ˆç®—è¤‡é›œåº¦å¤ªé«˜, é€™æ™‚ VI å°±æ´¾ä¸Šç”¨å ´äº†. VI ç°¡å–®è¬›å°±æ˜¯ç•¶ $p(z|x)$ ä¸å®¹æ˜“å¾—åˆ°æ™‚, å¯ä»¥å¹«ä½ æ‰¾åˆ°ä¸€å€‹å¾ˆå¥½çš„è¿‘ä¼¼, $q(z)$. æ”¾ä¸Šä¸€å¼µ NIPS 2016 VI tutorial çš„åœ–, éå¸¸å½¢è±¡åœ°è¡¨ç¤º VI åšçš„äº‹æƒ…: å°‡æ‰¾ $p(z|x)$ çš„å•é¡Œè½‰åŒ–æˆä¸€å€‹æœ€ä½³åŒ–å•é¡Œ. æ€éº¼çœ‹ä½œæœ€ä½³åŒ–å•é¡Œ?æˆ‘å€‘è¦æ‰¾åˆ°ä¸€å€‹ $q(z)$ å»é€¼è¿‘ $p(z|x)$, å› æ­¤éœ€è¦è¨ˆç®—å…©å€‹æ©Ÿç‡åˆ†ä½ˆçš„è·é›¢, è€Œ KL-divergence æ˜¯å€‹å¾ˆå¥½çš„é¸æ“‡ (é›–ç„¶ä¸æ»¿è¶³æ•¸å­¸ä¸Šçš„è·é›¢å®šç¾©). æ‰€ä»¥æˆ‘å€‘çš„ç›®æ¨™å°±æ˜¯å¸Œæœ› $KL(q(z)\\Vert p(z|x))$ æ„ˆå°æ„ˆå¥½, æ¥è‘—æˆ‘å€‘å° KL å®šç¾©é‡æ–°åšå¦‚ä¸‹çš„è¡¨é”: $$\\begin{align} KL\\left(q(z)\\Vert p(z|x)\\right)=-\\sum_z q(z)\\log\\frac{p(z|x)}{q(z)}\\\\ =-\\sum_z q(z)\\left[\\log\\frac{p(x,z)}{q(z)}-\\log p(x)\\right]\\\\ =-\\sum_z q(z)\\log\\frac{p(x,z)}{q(z)}+\\log p(x) \\end{align}$$ å¾—åˆ°é€™å€‹éå¸¸é‡è¦çš„å¼å­: $$\\begin{align} \\log p(x)=KL\\left(q(z)\\Vert p(z|x)\\right)+ \\color{red}{ \\sum_z q(z)\\log\\frac{p(x,z)}{q(z)} } \\\\ =KL\\left(q(z)\\Vert p(z|x)\\right)+ \\color{red}{ \\mathcal{L}(q) } \\\\ \\end{align}$$ ç‚ºä»€éº¼åšé€™æ¨£çš„è½‰æ›å‘¢? é€™æ˜¯å› ç‚ºé€šå¸¸ $p(z|x)$ å¾ˆé›£å¾—åˆ°, ä½†æ˜¯ complete likelihood $p(z,x)$ é€šå¸¸å¾ˆå¥½æ±‚.è§€å¯Ÿ (5), æ³¨æ„åˆ°åœ¨ VI çš„è¨­å®šä¸­ $\\log p(x)$ è·Ÿæˆ‘å€‘è¦æ‰¾çš„ $q(z)$ ç„¡é—œ, ä¹Ÿå°±é€ æˆäº† $\\log p(x)$ æ˜¯å›ºå®šçš„. ç”±æ–¼ $KL\\geq 0$, è®“ $KL$ æ„ˆå°æ„ˆå¥½ç­‰åŒæ–¼è®“ $\\mathcal{L}(q)$ æ„ˆå¤§æ„ˆå¥½. å› æ­¤ VI çš„ç›®æ¨™å°±æ˜¯è—‰ç”±æœ€å¤§åŒ– $\\mathcal{L}(q)$ ä¾†è¿«ä½¿ $q(z)$ æ¥è¿‘ $p(z|x)$. $\\mathcal{L}(q)$ å¯ä»¥çœ‹å‡ºä¾†æ˜¯ marginal log likelihood $\\log p(x)$ çš„ lower bound. å› æ­¤ç¨± variational lower bound æˆ– Evidence Lower BOund (ELBO). ELBO çš„ gradientæˆ‘å€‘åšæœ€ä½³åŒ–éƒ½éœ€è¦è¨ˆç®— objective function çš„ gradient. è®“è¦æ‰¾çš„ $q$ ç”±åƒæ•¸ $\\nu$ æ§åˆ¶, i.e. $q(z;\\nu)$, æ‰€ä»¥æˆ‘å€‘è¦æ‰¾ ELBO çš„ gradient å°±æ˜¯å° $\\nu$ å¾®åˆ†. $$\\begin{align} \\mathcal{L}(\\nu)=\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\\\ \\Rightarrow \\nabla_{\\nu}\\mathcal{L}(\\nu)=\\nabla_{\\nu}\\left(\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\right)\\\\ \\mbox{Note }\\neq \\mathbb{E}_{z\\sim q}\\left(\\nabla_{\\nu}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\right)\\\\ \\end{align}$$ æ³¨æ„ (8) ä¸èƒ½å°‡ Expectation èˆ‡ derivative äº¤æ›çš„åŸå› æ˜¯å› ç‚ºè¦å¾®åˆ†çš„ $\\nu$ èˆ‡è¦è¨ˆç®—çš„ Expectation åˆ†å¸ƒ $q$ æœ‰é—œ. ä¸‹é¢æœƒæåˆ°ä¸€å€‹å¾ˆé‡è¦çš„æŠ€å·§, Reparameterization trick, å°‡ Expectation èˆ‡ derivative äº¤æ›, è€Œäº¤æ›å¾Œæœ‰ä»€éº¼å¥½è™•å‘¢? ä¸‹é¢æåˆ°çš„æ™‚å€™å†èªªæ˜. å›åˆ° (7) å±•é–‹ Expectation ç¹¼çºŒè¨ˆç®— gradient, ç›´æ¥ç”¨ NIPS slide çµæœå¦‚ä¸‹: è¨ˆç®—ä¸€å€‹æ©Ÿç‡åˆ†ä½ˆçš„ Expectation å¯ç”¨ Monte Carlo method æ¡æ¨£, ä¾‹å¦‚æ¡æ¨£ $T$ å€‹ samples$$\\begin{align} \\mathbb{E}_{z\\sim q}f(z)\\approx\\frac{1}{T}\\sum_{t=1}^Tf(z)\\mbox{, where }z\\sim q \\end{align}$$ å› æ­¤ gradient å¯ä»¥é€™éº¼å¤§è‡´æ‰¾å‡ºä¾†, ä¸éé€™æ–¹æ³•æ‰¾å‡ºä¾†çš„ gradient èˆ‡çœŸå¯¦çš„ gradient å­˜åœ¨å¾ˆå¤§çš„èª¤å·®, æ›å¥è©±èªª, é€™å€‹è¿‘ä¼¼çš„ gradient variance å¤ªå¤§äº†. åŸå› å…©å€‹ $q$ æœ¬èº«å°±é‚„åœ¨ä¼°è¨ˆ, æœ¬èº«å°±ä¸æº–ç¢ºäº† Monte Carlo method æ¡æ¨£æ‰€é€ æˆçš„èª¤å·® ä¸‹ä¸€æ®µçš„ reparameterization trick å°±å¯ä»¥å»é™¤æ‰ä¸Šé¢ç¬¬ä¸€å€‹èª¤å·®, å› æ­¤ä¼°å‡ºä¾†çš„ gradient å°±ç©©å®šå¾ˆå¤š. Reparameterization Trickæˆ‘å€‘ç”¨ Gaussian èˆ‰ä¾‹, ä»¤ $q$ æ˜¯ Gaussian, $q(z;\\mu,\\sigma)=\\mathcal{N}(\\mu,\\sigma)$, å…¶ä¸­ $\\nu=${$\\mu,\\sigma$}, è€Œæˆ‘å€‘å…¶å¯¦å¯ä»¥çŸ¥é“ $z=\\mu+\\sigma \\epsilon$, where $\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})$. å› æ­¤:$$\\begin{align} \\mathcal{L}(\\nu)=\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z)-\\log q(z;\\nu)\\right]\\\\ =\\mathbb{E}_{ \\color{red}{ \\epsilon\\sim \\mathcal{N}(0,\\mathbf{I}) } }\\left[\\log p(x, \\color{red}{ \\mu+\\sigma \\epsilon } )-\\log q( \\color{red}{ \\mu+\\sigma \\epsilon } ;\\nu)\\right] \\end{align}$$ é€™æ™‚å€™æˆ‘å€‘è¨ˆç®— ELBO çš„ gradient æ™‚, æˆ‘å€‘ç™¼ç¾ $\\nu$ èˆ‡ Expectation çš„åˆ†ä½ˆ, $\\mathcal{N}(0,\\mathbf{I})$, ç„¡é—œäº†! å› æ­¤ (7) å¥—ç”¨ä¸Šé¢çš„ trick å°±å¯ä»¥å°‡ Expectation èˆ‡ derivative äº¤æ›. çµæœå¦‚ä¸‹: $$\\begin{align} \\nabla_{\\mu}\\mathcal{L}(\\nu)=\\mathbb{E}_{\\epsilon\\sim \\mathcal{N}(0,\\mathbf{I})}\\left[\\nabla_{\\mu}\\left(\\log p(x,\\mu+\\sigma \\epsilon) - \\log q(\\mu+\\sigma \\epsilon;\\nu)\\right)\\right]\\\\ \\approx\\frac{1}{T}\\sum_{t=1}^T \\nabla_{\\mu}\\left( \\log p(x,\\mu+\\sigma \\epsilon) - \\log q(\\mu+\\sigma \\epsilon;\\nu) \\right)\\mbox{, where }\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})\\\\ \\end{align}$$ åœ¨ä¸Šä¸€æ®µè¨ˆç®— ELBO gradient æ‰€é€ æˆèª¤å·®çš„ç¬¬ä¸€é …åŸå› å°±ä¸å­˜åœ¨äº†, å› æ­¤æˆ‘å€‘ç”¨ reparameterization å¾—åˆ°çš„ gradient å…·æœ‰å¾ˆå°çš„ variance. é€™å€‹ github åšäº†å¯¦é©—, ç™¼ç¾ reperameterization çš„ç¢ºå¤§å¤§é™ä½äº†ä¼°è¨ˆçš„ gradient çš„ variance. $$\\begin{align} \\nabla_{\\mu}\\left(\\log p(x,\\mu+\\sigma \\epsilon) - \\log q(\\mu+\\sigma \\epsilon;\\nu)\\right) \\end{align}$$ æ€éº¼è¨ˆç®—å‘¢? æˆ‘å€‘å¯ä»¥ä½¿ç”¨ Tensorflow å°‡è¦è¨ˆç®— gradient çš„ function å¯«å‡ºä¾†, tf.gradients å°±èƒ½ç®— VAEVariational Inference æ€éº¼è·Ÿ Neural Network æ‰¯ä¸Šé—œä¿‚çš„? é€™å¯¦åœ¨å¾ˆç¥å¥‡.æˆ‘å€‘å…ˆä¾†çœ‹çœ‹ ELBO é™¤äº† (6) çš„å¯«æ³•, é‚„å¯ä»¥é€™éº¼è¡¨ç¤º: $$\\begin{align} \\mathcal{L}(\\nu)=\\mathbb{E}_{z\\sim q}\\left[\\log p(x,z) - \\log q(z;\\nu)\\right]\\\\ =\\mathbb{E}_{z\\sim q}\\left[ \\log p(x|z) + \\log p(z) - log q(z;\\nu) \\right]\\\\ =\\mathbb{E}_{z\\sim q}\\left[ \\log p(x|z)\\right] + \\mathbb{E}_{z\\sim q}\\left[ \\log \\frac{p(z)}{q(z;\\nu)}\\right]\\\\ =\\mathbb{E}_{z\\sim q}\\left[ \\log p(x|z)\\right] - KL(q(z;\\nu)\\|p(z))\\\\ \\end{align}$$ æˆ‘å€‘è®“ $p(x|z)$ è¢«åƒæ•¸ $\\theta$ æ‰€æ§åˆ¶, æ‰€ä»¥æœ€å¾Œ ELBO å¦‚ä¸‹:$$\\begin{align} \\mathcal{L}(\\nu,\\theta)=\\mathbb{E}_{z\\sim q}\\left[ \\log \\color{orange}{ p(x|z,\\theta) } \\right] - KL( \\color{blue}{ q(z;\\nu) } \\|p(z))\\\\ \\end{align}$$ è®“æˆ‘å€‘ç”¨åŠ›çœ‹ (19) ä¸€åˆ†é˜æ¥è‘—åœ¨ç”¨åŠ›çœ‹ (19) ä¸€åˆ†é˜æœ€å¾Œåœ¨ç”¨åŠ›çœ‹ (19) ä¸€åˆ†é˜ æœ‰çœ‹å‡ºä»€éº¼å—? â€¦ å¦‚æœæ²’æœ‰, è©¦è‘—å°ç…§ä¸‹é¢é€™å¼µåœ– Encoder å’Œ Decoder éƒ½åŒæ™‚ç”¨ NN ä¾†å­¸ç¿’, é€™è£¡ $\\nu$ å’Œ $\\theta$ åˆ†åˆ¥è¡¨ç¤º NN çš„åƒæ•¸, è€Œä½¿ç”¨ Reparameterization trick ä¾†è¨ˆç®— ELBO çš„ gradient (14) å°±ç›¸ç•¶æ–¼åœ¨åšé€™å…©å€‹ NN çš„ backprop. ä½†æ˜¯ä¸Šåœ–çš„ Encoder ç”¢ç”Ÿçš„æ˜¯ä¸€å€‹ pdf, è€Œçµ¦ Decoder çš„æ˜¯ä¸€å€‹ sample $z$, é€™è©²æ€éº¼ä¸²ä¸€èµ·? VAE çš„åšæ³•å°±æ˜¯å°‡ $q(z)$ è¨­å®šç‚º diagonal Gaussian, ç„¶å¾Œåœ¨é€™å€‹ diagonal Gaussian æ¡æ¨£å‡º $T$ å€‹ $z$ å°±å¯ä»¥ä¸Ÿçµ¦ Decoder. ä½¿ç”¨ diagonal Gaussian æœ‰å…©å€‹å¥½è™•: æˆ‘å€‘å¯ä»¥ç”¨ reparameterization trick, å› æ­¤æ¡æ¨£åªåœ¨æ¨™æº–é«˜æ–¯ä¸Šæ¡æ¨£, è‡ªç„¶åœ° Encoder çš„ output å°±æ˜¯ $\\mu$ å’Œ $\\sigma$ äº†. (19)çš„ KL é …ç›´æ¥å°±æœ‰ closed form solution, å…æ‰ç®— expectation (å‡è¨­$p(z)$ä¹Ÿæ˜¯Gaussiançš„è©±) æ ¹æ“š1, æ¶æ§‹æ”¹å‹•å¦‚ä¸‹: å°‡åŸä¾†çš„ ELBO (10) è½‰æˆ (19) ä¾†çœ‹çš„è©±, é‚„å¯ä»¥çœ‹å‡ºä¸€äº›è³‡è¨Š.ç•¶æœ€å¤§åŒ– (19) çš„æ™‚å€™ RHS ç¬¬ä¸€é …è¦æ„ˆå¤§æ„ˆå¥½ (likelihood æ„ˆå¤§æ„ˆå¥½), å› æ­¤é€™ä¸€é …ä»£è¡¨ reconstruct error æ„ˆå°æ„ˆå¥½. RHS ç¬¬äºŒé …, ä¹Ÿå°±æ˜¯ $KL(q(z;\\nu)\\Vert p(z))$ å‰‡è¦æ„ˆå°æ„ˆå¥½. å› æ­¤æœƒå‚¾å‘æ–¼è®“ $q(z;\\nu)$ æ„ˆæ¥è¿‘ $p(z)$ æ„ˆå¥½. é€™å¯ä»¥çœ‹åš regularization. ä½†æ˜¯åˆ¥å¿˜äº†ä¸€é–‹å§‹èªª VI çš„åšæ³•å°±æ˜¯è—‰ç”±æœ€å¤§åŒ– ELBO ä¾†è¿«ä½¿ $q(z;\\nu)$ æ¥è¿‘ $p(z|x)$, è€Œä¸Šé¢æ‰èªªæœ€å¤§åŒ– ELBO æœƒå‚¾å‘æ–¼è®“ $q(z;\\nu)$ æ¥è¿‘ $p(z)$.é€™ä¸²èµ·ä¾†å°±èªª $q(z;\\nu)$ æ¥è¿‘ $p(z|x)$ æ¥è¿‘ $p(z)$. åœ¨ VAE è«–æ–‡è£¡å°±å°‡ $p(z)$ ç›´æ¥è¨­å®šç‚º $\\mathcal{N}(0,\\mathbf{I})$. å› æ­¤æ•´å€‹ VAE è¨“ç·´å®Œçš„ Encoder çš„ $z$ åˆ†å¸ƒæœƒæœ‰é«˜æ–¯åˆ†å¸ƒçš„æƒ…å½¢. Conditional VAE (CVAE)åŸä¾†çš„ VAE ç„¡æ³•æ§åˆ¶è¦ç”ŸæˆæŸäº›é¡åˆ¥çš„åœ–åƒ, ä¹Ÿå°±æ˜¯éš¨æ©Ÿç”¢ç”Ÿ $z$ ä¸çŸ¥é“é€™æœƒå°æ‡‰åˆ°å“ªå€‹é¡åˆ¥. CVAE å¯ä»¥æ ¹æ“šæ¢ä»¶ä¾†ç”¢ç”Ÿåœ–åƒ, ä¹Ÿå°±æ˜¯é™¤äº†çµ¦ $z$ ä¹‹å¤–éœ€è¦å†çµ¦ $c$ (é¡åˆ¥) è³‡è¨Šä¾†ç”Ÿæˆåœ–åƒ. æ€éº¼è¾¦åˆ°çš„å‘¢? æ–¹æ³•ç°¡å–®åˆ°æˆ‘åš‡ä¸€è·³, çœ‹åŸæœ¬è«–æ–‡æœ‰é»è¿·è¿·ç³Šç³Š, ä½†é€™ç¯‡æ–‡ç« è§£é‡‹å¾—å¾ˆæ¸…æ¥š! ç°¡å–®ä¾†èªªå°‡åŸä¾†çš„æ¨å€’å…¨éƒ¨åŠ ä¸Š condition on $c$ çš„æ¢ä»¶. å¾ (4) å‡ºç™¼ä¿®æ”¹å¦‚ä¸‹: $$\\begin{align} \\log p(x \\color{red}{ | c } ) =KL\\left(q(z \\color{red}{ | c } )\\Vert p(z|x, \\color{red}{ c } )\\right)+ \\sum_z q(z \\color{red}{ | c } )\\log\\frac{p(x,z \\color{red}{ | c } )}{q(z \\color{red}{ | c } )} \\\\ \\end{align}$$ ç”¨æ¨å° VAE ä¸€æ¨¡ä¸€æ¨£çš„æµç¨‹, å…¶å¯¦ä»€éº¼éƒ½æ²’åš, åªæ˜¯å…¨éƒ¨ conditioned on $c$ å¾—åˆ° (19) çš„ condition ç‰ˆæœ¬ $$\\begin{align} \\mathcal{L}(\\nu,\\theta \\color{red}{ | c } )=\\mathbb{E}_{z\\sim q}\\left[ \\log \\color{orange}{ p(x|z,\\theta, \\color{red}{ c } ) } \\right] - KL( \\color{blue}{ q(z;\\nu \\color{red}{ | c } ) } \\|p(z))\\\\ \\end{align}$$ é€™èªªæ˜äº†æˆ‘å€‘åœ¨å­¸ Encoder å’Œ Decoder çš„ NN æ™‚å¿…é ˆåŠ å…¥ conditioned on $c$ é€™å€‹æ¢ä»¶! NN æ€éº¼åšåˆ°é€™é»å‘¢? å¾ˆæš´åŠ›, ç›´æ¥å°‡ class çš„ one-hot è·ŸåŸä¾†çš„ input concate èµ·ä¾†å°±ç•¶æˆæ˜¯ condition äº†. å› æ­¤ CVAE çš„æ¶æ§‹å¦‚ä¸‹: å¯¦ä½œç´°ç¯€å°±ä¸å¤šèªªäº†, ç›´æ¥åƒè€ƒ codes ç”±æ–¼æˆ‘å€‘çš„ condition æ˜¯ one-hot, å¦‚æœåŒæ™‚å°‡å…©å€‹ label è¨­å®šç‚º 1, æ˜¯ä¸æ˜¯å°±èƒ½ conditioned on two classes å‘¢? å¯¦é©—å¦‚ä¸‹ conditioned on â€˜0â€™ and â€˜4â€™ conditioned on â€˜1â€™ and â€˜3â€™ å¦å¤–, å¦‚æœçµ¦çš„ condition å€¼æ¯”è¼ƒå°, æ˜¯ä¸æ˜¯å°±å¯ä»¥ç”¢ç”Ÿæ¯”è¼ƒä¸æ˜¯é‚£éº¼ç¢ºå®šçš„ image å‘¢? æˆ‘å€‘å˜—è©¦ conditioned on â€˜4â€™ ä¸”å€¼å¾ 0.1 (weak) åˆ° 1.0 (strong), çµæœå¦‚ä¸‹: é€™å€‹ condition å€¼å¤§å°é‚„çœŸæœ‰åæ‡‰å¼·åº¦å‘¢! Neural network çœŸçš„å¾ˆç¥å¥‡é˜¿~ Mean Field VIè®“æˆ‘å€‘æ‹‰å› VI. Mean Field é€²ä¸€æ­¥é™åˆ¶äº† $q$ çš„ç¯„åœ, å®ƒå‡è¨­æ‰€æœ‰æ§åˆ¶ $q$ çš„åƒæ•¸ {$\\nu_i$} éƒ½æ˜¯äº’ç›¸ç¨ç«‹çš„, é€™æ¨£æ‰€å½¢æˆçš„å‡½æ•¸ç©ºé–“ç¨±ç‚º mean-field family. æ¥è‘—æ¡å– coordinate ascent æ–¹å¼, é‡å°æ¯å€‹ $\\nu_i$ ç¨ç«‹ update. é€™ç¨® fatorized çš„ $q$ ä¸€å€‹å•é¡Œæ˜¯ estimate å‡ºä¾†çš„åˆ†å¸ƒæœƒå¤ª compact, åŸå› æ˜¯æˆ‘å€‘ä½¿ç”¨çš„æŒ‡æ¨™æ˜¯ $KL(q|p)$, è©³ç´°åƒè€ƒ PRML Fig 10.2. æ”¾ä¸Š NIPS 2016 slides, ç¬¦è™Ÿæœƒè·Ÿæœ¬æ–‡æœ‰äº›ä¸åŒ, ä¸éç¸½çµå¾—å¾ˆå¥½: å¦å¤–æƒ³äº†è§£æ›´å¤š Mean Field VI æˆ–æ˜¯é€éä¾‹å­äº†è§£, æ¨è–¦ä»¥ä»¥ä¸‹å…©å€‹è³‡æ–™: Variational Inference tutorial series by Chieh Wu Variational Coin Toss by BjÃ¶rn Smedman Reference Variational Inference tutorial series by Chieh Wu Variational Inference: Foundations and Modern Methods (NIPS 2016 tutorial) Reparameterization Trick Goker Erdogan æœ‰å¾ˆå¥½çš„ VAE, VI æ–‡ç«  Conditional VAE åŸè«–æ–‡ Conditional VAE å¥½æ–‡ç«  Variational Coin Toss by BjÃ¶rn Smedman My CVAE TF Practice Appendix: EM è·Ÿ VI å¾ˆåƒé˜¿åœ¨ä¸€èˆ¬ EM çš„è¨­å®šä¸Š, æˆ‘å€‘æ˜¯å¸Œæœ›æ‰¾åˆ°ä¸€çµ„åƒæ•¸ $\\tilde{\\theta}$ å¯ä»¥è®“ marginal likelihood $\\log p(x|\\theta)$ æœ€å¤§, formally speaking: $$\\begin{align} \\tilde{\\theta}=\\arg\\max_\\theta \\log p(x|\\theta) \\end{align}$$ å¦‚åŒ (4) å’Œ (5), æ­¤æ™‚è¦æ±‚çš„è®Šæ•¸ä¸å†æ˜¯ $q$, è€Œæ˜¯ $\\theta$: $$\\begin{align} \\log p(x|\\theta)=KL\\left(q(z)\\Vert p(z|x,\\theta)\\right)+\\sum_z q(z)\\log\\frac{p(x,z|\\theta)}{q(z)}\\\\ =KL\\left(q(z)\\Vert p(z|x,\\theta)\\right)+ \\color{orange}{ \\mathcal{L}(q,\\theta) } \\\\ \\end{align}$$ æ­¤æ™‚çš„ $\\log p(x|\\theta)$ ä¸å†æ˜¯å›ºå®šçš„ (VIæ˜¯), è€Œæ˜¯æˆ‘å€‘å¸Œæœ›æ„ˆå¤§æ„ˆå¥½. è€Œæˆ‘å€‘çŸ¥é“ $\\mathcal{L}(q,\\theta)$ æ˜¯å®ƒçš„ lower bound é€™é»ä¸è®Š, å› æ­¤å¦‚æœ lower bound æ„ˆå¤§, å‰‡æˆ‘å€‘çš„ $\\log p(x|\\theta)$ å°±ç•¶ç„¶å¯èƒ½æ„ˆå¤§. é¦–å…ˆæ³¨æ„åˆ° (23) å’Œ (24) é‡å°ä»»ä½•çš„ $q$ å’Œ $\\theta$ ç­‰å¼éƒ½æˆç«‹, æˆ‘å€‘å…ˆå°‡ $\\theta$ ç”¨ $\\theta^{old}$ ä»¥åŠ $q(z)$ ç”¨ $p(z|x,\\theta^{old})$ ä»£å…¥å¾—åˆ°: $$\\begin{align} \\log p(x|\\theta^{old})= KL\\left(p(z|x,\\theta^{old})\\Vert p(z|x,\\theta^{old})\\right)+\\mathcal{L}(p(z|x,\\theta^{old}),\\theta^{old})\\\\ =0+\\mathcal{L}(p(z|x,\\theta^{old}),\\theta^{old})\\\\ \\leq\\max_{\\theta}\\mathcal{L}(p(z|x,\\theta^{old}),\\theta)\\\\ \\end{align}$$ æ¥è‘—æ±‚$$\\begin{align} \\theta^{new}=\\arg\\max_{\\theta} \\mathcal{L}(p(z|x,\\theta^{old}),\\theta) \\end{align}$$ å¦‚æ­¤ lower bound å°±è¢«æˆ‘å€‘æé«˜äº†.(28) å°±æ˜¯ EM çš„ M-step, è©³ç´°è«‹çœ‹ PRML Ch9.4 æˆ–åƒè€ƒä¸‹åœ–ç†è§£ â€œ$q(z)$ ç”¨ $p(z|x,\\theta^{old})$ ä»£å…¥â€ é€™å¥è©±å…¶å¯¦æœ‰å•é¡Œ, å› ç‚ºé—œéµä¸å°±æ˜¯ $p(z|x,\\theta)$ å¾ˆé›£æ±‚å—? é€™ä¼¼ä¹è®Šæˆäº†ä¸€å€‹é›ç”Ÿè›‹è›‹ç”Ÿé›çš„æƒ…æ³. (å°±æˆ‘ç›®å‰çš„ç†è§£) æ‰€ä»¥é€šå¸¸ EM è™•ç†çš„æ˜¯ discrete çš„ $z$, ç„¶å¾Œåˆ©ç”¨ $\\sum_z p(x,z|\\theta)$ ç®—å‡º $p(x|\\theta)$, æ¥è‘—å¾—åˆ°æˆ‘å€‘è¦çš„ $p(z|x,\\theta)$. ç­‰æ–¼æ˜¯ç›´æ¥ç°¡åŒ–äº†, ä½† VI ç„¡æ­¤é™åˆ¶.","tags":[{"name":"Variational Inference","slug":"Variational-Inference","permalink":"http://yoursite.com/tags/Variational-Inference/"},{"name":"ELBO","slug":"ELBO","permalink":"http://yoursite.com/tags/ELBO/"},{"name":"Variational Auto Encoder (VAE)","slug":"Variational-Auto-Encoder-VAE","permalink":"http://yoursite.com/tags/Variational-Auto-Encoder-VAE/"}]},{"title":"Ensemble Algorithm Summary Notes","date":"2018-09-03T13:45:08.000Z","path":"2018/09/03/Ensemble-Algorithm-Summary-Notes/","text":"é€™æ˜¯ç”¨è‡ªå·±ç†è§£çš„æ–¹å¼æ•´ç†äº†æ—è»’ç”°è€å¸« ML èª²ç¨‹. å…¶ä¸­ Decision tree and Random Forest æ²’ç´€éŒ„. ä»¥å‰ç¬¬ä¸€æ¬¡æ¥è§¸åˆ° Adaboost çš„æ™‚å€™å°±è¢«å®ƒæ·±æ·±è‘—è¿·äº†, ç•¶æ™‚ face detection å¯å•†ç”¨ç®—æ³•ç„¡ä¸æ¡ç”¨ç¶“å…¸çš„ Viola and Jones adaboost method. åœ¨ç¾åœ¨ DNN æˆä¸»æµçš„æ™‚å€™, é›–ç„¶ adaboost å…‰ç’°å·²é€€å», ä½†åœ¨ data mining, data science é ˜åŸŸ boosting æ–¹æ³•ä»æ˜¯æœ€æˆåŠŸçš„ç®—æ³•ä¹‹ä¸€. åŸºæœ¬ä¸Šåœ¨ Kaggle æ¯”è³½å¯ä»¥çœ‹åˆ°ä¸»è¦å…©å¤§æ–¹æ³•, èˆ‰å‡¡è²éŸ³å½±åƒæ–‡å­—ç­‰ç­‰çš„è¾¨è­˜å°±æ˜¯ DNN, å…¶ä»–å‡¡æ˜¯ data mining ç›¸é—œçš„å°±å±¬ boosting (xgboost).æœ‰è¶£çš„æ˜¯, è¿‘å¹´ä¹Ÿæœ‰ç ”ç©¶äººå“¡ç”¨ ensemble çš„è§’åº¦çœ‹å¾… DNN, å¾é€™è§’åº¦å°±èƒ½ç†è§£ç‚ºä½•ä¸€è·¯å¾ highway network â€“&gt; skip layer resent â€“&gt; resnext çš„æ¶æ§‹æ¼”è®Š, ä»¥åŠç‚ºä½•æ•ˆæœé€™éº¼å¥½. å¯ä»¥åƒè€ƒ â€œæ·±åº¦ç¥ç»ç½‘ç»œä¸­æ·±åº¦ç©¶ç«Ÿå¸¦æ¥äº†ä»€ä¹ˆï¼Ÿâ€ å¾ˆç²¾å½©çš„è§£é‡‹, æˆ–æ˜¯ MSR 2017 é€™ç¯‡è«–æ–‡ Deep Convolutional Neural Networks with Merge-and-Run Mappings ç­†è¨˜å…§å®¹å¦‚ä¸‹: Bagging (or bootstrap) Adaboost æ¼”ç®—æ³•2.1 Adaboost large margin è§£é‡‹2.2 Adaboost exponential error è§£é‡‹ Additive Model (a framework) Gradient Boosting Adaboost as an additive model Gradient Boost Decision Tree (GBDT) å¾…ç ”ç©¶: XGBoost (Kaggle æ¯”è³½ç¥å™¨) Bagging (or bootstrap)é‚„è¨˜å¾—æˆ‘å€‘åœ¨ Why-Aggregation-Work é€™ç¯‡æåˆ°, ç•¶æˆ‘å€‘æœ‰å¾ˆå¤š weak learner ${g_t}$ æ™‚, è¦å¾—åˆ°ä¸€å€‹ strong learner $G$ æœ€ç°¡å–®çš„æ–¹æ³•å°±æ˜¯æŠ•ç¥¨(æˆ–å¹³å‡). æ‰€ä»¥ä¸€å€‹é—œéµå•é¡Œæ˜¯è¦æ€éº¼ç”¢ç”Ÿå¾ˆå¤šçš„ $g_t$?Bagging (or bootstrap) æä¾›äº†ä¸€å€‹ç°¡å–®çš„æ–¹æ³•: å‡è¨­ dataset $D$ æœ‰ $N$ ç­†è³‡æ–™, bagging å°±æ˜¯å¾ $D$ ä¸­é‡è¤‡æ¡æ¨£å‡º $Nâ€™$ ç­†, æˆ‘å€‘ç¨± $Dâ€™$, ç„¶å¾Œ $g_t$ å°±å¯ä»¥ç”¨ $Dâ€™$ è¨“ç·´å‡ºä¾†.æ—¢ç„¶ç¾åœ¨å¯ä»¥æ–¹ä¾¿åœ°ç”¢ç”Ÿå¾ˆå¤š ${g_t}$, ç„¶å¾Œå°± $G$ å°±æ¡ç”¨å¹³å‡æ–¹å¼, ensemble algorithm å°±çµæŸäº†?! ç•¶ç„¶æ²’æœ‰, åˆ¥å¿˜äº†æœ‰ä¸€å€‹å¾ˆé—œéµçš„ç‰¹æ€§æ˜¯, ç•¶ ${g_t}$ æ„è¦‹æ„ˆåˆ†æ­§æ™‚ç”¢ç”Ÿå‡ºä¾†çš„ $G$ æ•ˆæœæ„ˆå¥½!é‚£æˆ‘å€‘å°±å•äº†, bagging ä¸å°±æ¡æ¨£å—? æˆ‘æ€éº¼çŸ¥é“é€™æ¬¡æ¡æ¨£å‡ºä¾†çš„ $Dâ€™$ æ‰€è¨“ç·´å‡ºä¾†çš„ $g_t$ æœƒè·Ÿä¹‹å‰ä¸€æ¬¡çš„æ„è¦‹åˆ†æ­§?æˆ‘å€‘å°±æ˜¯èƒ½çŸ¥é“! (ç¥å¥‡å§) è¦äº†è§£ç‚ºä»€éº¼, æˆ‘å€‘å¿…é ˆå…ˆå°‡ bagging æ“´å±•ä¸€ä¸‹, æƒ³æˆæ˜¯å° weighted $D$ æ¡æ¨£, å…¶ä¸­æ¯ä¸€ç­†è³‡æ–™ $x_n$ çš„ weight $u_n$ ä»£è¡¨æŠ½ä¸­çš„æ©Ÿç‡. å¦‚æœ bagging æ˜¯å° weighted $D$ æ¡æ¨£çš„è©±, åœ¨ç¬¬ t è¼ªçš„ $g_t$ å¾—åˆ°æ–¹å¼å¦‚ä¸‹: $$\\begin{align} g_t=\\arg\\min_{h\\in \\mathcal{H}}\\left(\\sum_{n=1}^N u_n^{(t)} \\mathbb{I}[y_n\\neq h(x_n)] \\right) \\end{align}$$ å…¶ä¸­ $\\mathbb{I}[â€¦]$ è¡¨ç¤º indicator function, æ¢ä»¶ç‚º true å‰‡ return 1, otherwise return 0.æƒ³æ³•å°±æ˜¯æˆ‘å€‘è¦è¨­è¨ˆä¸€çµ„æ–°çš„æ¬Šé‡, è®“æ–°çš„æ¬Šé‡å°æ–¼ $g_t$ ä¾†èªªç›¸ç•¶æ–¼äº‚çŒœ, é€™æ¨£ç”¨æ–°æ¬Šé‡æ‰¾å‡ºçš„ $g_t+1$ å°±æœƒè·Ÿä¹‹å‰çš„æ„è¦‹åˆ†æ­§äº†. å…·é«”ä¾†èªª, æ–°æ¬Šé‡è¦æœ‰ä»¥ä¸‹çš„æ•ˆæœ: $$\\begin{align} \\frac{\\sum_{n=1}^N{u_n^{(t+1)} \\mathbb{I}[y_n\\neq g_t(x_n)]}}{\\sum_{n=1}^N{u_n^{(t+1)}}}=\\frac{1}{2} \\end{align}$$ ç‰©ç†æ„ç¾©å°±æ˜¯å°æ–¼ $g_t$ ä¾†èªª$$\\begin{align} \\mbox{for weak learner }g_t\\mbox{: }\\left(\\mbox{total }u_n^{(t+1)}\\mbox{ of incorrect}\\right)= \\left(\\mbox{total }u_n^{(t+1)}\\mbox{ of correct}\\right) \\end{align}$$ æ‰€ä»¥æ–°çš„æ¬Šé‡èª¿æ•´æ–¹å¼å…¶å¯¦å¾ˆç°¡å–®, ç”¨ä¸€å€‹ä¾‹å­è§£é‡‹. å‡å¦‚ $u_n^t$ incorrect åˆæ˜¯ 300, $u_n^t$ correct åˆæ˜¯ 500. æˆ‘å€‘åªè¦æŠŠä¹‹å‰çš„ $u_n^t$ incorrectéƒ¨åˆ†éƒ½ä¹˜ 500, è€Œ correct éƒ¨åˆ†ä¹˜ 300å°±å¯ä»¥äº†.æˆ–è€…æˆ‘å€‘é€™éº¼å¯«, å®šç¾© $\\epsilon_t=300/(300+500)$, å‰‡$$\\begin{align} u_n^{(t+1)}=u_n^{(t)}(1-\\epsilon_t) \\mbox{, if } y_n\\neq g_t(x_n)\\\\ u_n^{(t+1)}=u_n^{(t)}\\epsilon_t \\mbox{, if } y_n = g_t(x_n)\\\\ \\end{align}$$ æˆ–é€šå¸¸ä¹Ÿå¯ä»¥é€™éº¼è¨ˆç®— æ‰€ä»¥ç›®å‰ç‚ºæ­¢, æˆ‘å€‘å¯ä»¥ç”¨ bagging çš„æ–¹å¼ (å° weighted data) ç”¢ç”Ÿå‡ºçœ‹ä¼¼ç›¸ç•¶æ„è¦‹ä¸åŒçš„ $g_t$, é‚£æœ€å¾Œçš„ $G$ ç”¨å¹³å‡å°±å¯ä»¥äº†å—? å¯èƒ½ä¸å¤§å¥½, å› ç‚º $g_t$ æ˜¯é‡å°æŸä¸€ç¨®æ¬Šé‡çš„ dataset å¥½, ä¸ä»£è¡¨å°åŸä¾†æ²’æœ‰æ¬Šé‡ (æˆ–uniformæ¬Šé‡) çš„ dataset æ˜¯å¥½çš„.æ—¢ç„¶ç›´æ¥å¹³å‡å¯èƒ½ä¸å¤ å¥½, ä¸å¦‚å°±ç”¨ linear combination æ–¹å¼çµ„åˆ $g_t$ å§, ä¸éçµ„åˆçš„ coefficients æ˜¯éœ€è¦å·§æ€è¨­è¨ˆçš„. è€Œ Adaboost å°±è¨­è¨ˆå‡ºäº†ä¸€ç¨®çµ„åˆæ–¹å¼, èƒ½è­‰æ˜é€™ç¨®çµ„åˆæ–¹å¼æœƒä½¿å¾— training error æ”¶æ–‚è‡³0. (å¦ä¸€ç¨®ç”¨ additive model çš„è§£é‡‹æ–¹å¼ç‚ºé€™æ¨£çš„ coefficient è¨­è¨ˆæ–¹å¼ç›¸ç•¶æ–¼ç”¨ steepest descent ä¸¦é¸æ“‡æœ€ä½³çš„æ­¥é•·). é€™äº›æœƒåœ¨æ–‡ç« ä¸‹é¢èªªæ˜. Adaboost æ¼”ç®—æ³• Adaboost large margin è§£é‡‹ä¸€èˆ¬ä¾†èªª, model æ„ˆè¤‡é›œæ„ˆå®¹æ˜“ overfit, ä¸éå¾ˆç‰¹åˆ¥çš„æ˜¯ adaboost éš¨è‘— iteration çµåˆæ„ˆå¤š weak learners åè€Œä¸æœƒæœ‰å®¹æ˜“ overfit çš„ç¾è±¡. å…¶ä¸­ä¸€ç¨®è§£é‡‹æ–¹å¼æ˜¯ adaboost å…·æœ‰é¡ä¼¼ SVM çš„ large margin æ•ˆæœ.æˆ‘å€‘é¦–å…ˆåˆ†æä¸€ä¸‹ç¬¬ t+1 æ¬¡ iteration, dataset çš„ weights$$\\begin{align} u_n^{(t+1)}=u_n^{(t)}\\diamond_t^{-y_n g_t(x_n)}\\\\ =u_n^{(t)}\\exp (-y_n \\alpha_t g_t(x_n)) \\end{align}$$ æˆ‘å€‘é€™è£¡ä½¿ç”¨ binary classification ä¾†èªªæ˜, å…¶ä¸­ $y_n,g_t(x_n)\\in${-1,+1}, å¼ (6) å¯ä»¥å¾ä¸Šä¸€æ®µ â€œAdaboost æ¼”ç®—æ³•â€ çš„åœ–ä¸­æ­¥é©Ÿ2çš„ update å¼å­çœ‹å‡º. è€Œå¼ (7) å¾ $\\diamond_t$ å®šç¾©å¾—åˆ°.ä¸Šå¼å¯ä»¥ä¸€è·¯å±•é–‹åˆ°é–‹é ­ (iteration 1), å¦‚ä¸‹: $$\\begin{align} u_n^{(T+1)}=u_n^{(1)}\\prod_{t=1}^T \\exp (-y_n \\alpha_t g_t(x_n)) \\\\ =\\frac{1}{N}\\exp\\left(-y_n \\color{orange}{ \\sum_{t=1}^T \\alpha_t g_t(x_n) } \\right) \\end{align}$$ æœ‰ç™¼ç¾å—? æ©˜è‰²çš„éƒ¨åˆ†å…¶å¯¦å°±æ˜¯æˆ‘å€‘çš„ $G$ $$\\begin{align} G(x_n)=sign\\left( \\color{orange}{ \\sum_{t=1}^T \\alpha_t g_t(x_n) } \\right) \\end{align}$$ è€Œå¦‚æœå°‡ $\\alpha_t$ çœ‹æˆæ˜¯ t-th coefficient, $g_t(x_n)$ çœ‹æˆæ˜¯ t-th ç¶­åº¦çš„ç‰¹å¾µ, æ©˜è‰²éƒ¨åˆ†å°±ç­‰åŒæ–¼ unnormalized margin. (é™¤ä»¥ coefficients çš„ norm å°±æ˜¯ marginäº†)Adaboost å¯ä»¥è­‰æ˜ (with exponential decay) $$\\begin{align} \\sum_{n=1}^N u_n^{(t)}\\rightarrow 0\\mbox{, for }t\\rightarrow 0 \\end{align}$$ é€™æ„å‘³è‘—ä»€éº¼? èªªæ˜äº†éš¨è‘— iteration å¢åŠ , æ©˜è‰²çš„å€¼æœƒæ„ˆå¤§, ç­‰åŒæ–¼æˆ‘å€‘çš„ $G$ å°æ–¼è³‡æ–™çš„ margin æœƒæ„ˆå¤§.è­‰æ˜å¯åƒè€ƒ æèˆª çµ±è¨ˆå­¸ç¿’æ–¹æ³• p142 Adaboost exponential error è§£é‡‹å…¶å¯¦å–®çœ‹å¼ (9) æˆ‘å€‘å®Œå…¨å¯ä»¥æŠŠå®ƒç•¶æˆ error function. é‡å¯«ä¸€ä¸‹: $$\\begin{align} u_n^{(T+1)}=\\frac{1}{N}\\exp\\left(-y_n \\color{orange}{ \\sum_{t=1}^T \\alpha_t g_t(x_n) } \\right)\\\\ =\\frac{1}{N}\\exp\\left(-y_n \\color{orange}{ f_T(x_n) } \\right) \\end{align}$$ æ€éº¼èªªå‘¢? å…¶å¯¦æ©˜è‰²éƒ¨åˆ†æˆ‘å€‘å¯æƒ³æˆæ˜¯è©²ç­†è³‡æ–™ $x_n$ çš„åˆ†æ•¸, è¨˜åš $f_T(x_n)$, ç•¶ $y_n=+1$ æ™‚, å¦‚æœ $f_T(x_n)$ å¾ˆå°å‰‡æœƒå°è‡´ $\\exp(-y_n f_T(x_n))$ æœƒå¾ˆå¤§, åŒç†ç•¶ $y_n=-1$ æ™‚, å¦‚æœ $f_T(x_n)$ å¾ˆå¤§å‰‡æœƒå°è‡´ $\\exp(-y_n f_T(x_n))$ æœƒå¾ˆå¤§. å› æ­¤ $\\exp(-y_n f_T(x_n))$ å¯ä»¥ç•¶æˆ error function ä¾† minimize.è€Œå®ƒè·Ÿ 0-1 error function æœ‰å¦‚ä¸‹çš„é—œä¿‚: è€Œæˆ‘å€‘çŸ¥é“ Adaboost æ»¿è¶³å¼ (11), ç­‰åŒæ–¼èªªæ˜ exponential error æ”¶æ–‚. ç”±æ–¼ upper bound çš„é—œä¿‚ä¹Ÿå°è‡´äº† 0-1 error æ”¶æ–‚.è½åˆ°æœ‰å€‹æ–¹æ³•å¯ä»¥ä½¿ error è¿…é€Ÿæ”¶æ–‚åˆ° 0, é€™ä¸æ˜¯å¤ªå®Œç¾äº†å—? åˆ¥é«˜èˆˆå¾—å¤ªæ—©, å› ç‚ºé€™å€‹ error æ˜¯ inside error. æœ‰å­¸é ML çš„ç«¥é‹å°±æ‡‰è©²æœƒè­¦è¦ºåˆ°ç•¶ inside error ç‚º 0, æ„å‘³è‘—éå¸¸å®¹æ˜“ overfit! å¥½åœ¨å¯¦ä½œä¸Š Adaboost å»ä¸æ˜¯é‚£éº¼å®¹æ˜“ (åŸå› åœ¨ä¸Šä¸€æ®µ large margin çš„è§£é‡‹), é€™å°±å¸¶ä¾†äº†ä¸€å€‹å¥½è™•, å°±æ˜¯åœ¨ä½¿ç”¨ Adaboost çš„æ™‚å€™, æˆ‘å€‘å¯ä»¥å¾ˆæ”¾å¿ƒçš„ç›´æ¥è¨“ç·´å¤šæ¬¡ iteration, ç”šè‡³åˆ° inside error æ¥è¿‘ 0, æœ€å¾Œçš„ outside test ä¹Ÿä¸æœƒå£æ‰. é€™ç‰¹æ€§å€’æ˜¯æŒºæ–¹ä¾¿çš„. AdaBoost å°çµè«– æˆ‘å€‘å¸Œæœ›è—‰ç”±èåˆå¾ˆå¤š {$g_t$} ä¾†å¾—åˆ°å¼·å¤§çš„ $G$, åŒæ™‚æˆ‘å€‘çŸ¥é“ {$g_t$} ä¹‹é–“æ„è¦‹æ„ˆåˆ†æ­§æ„ˆå¥½.æ¯ä¸€å€‹ $g_t$ éƒ½æ˜¯æ ¹æ“šç•¶å‰ weighted dataset å¾—åˆ°çš„. åˆ©ç”¨èª¿æ•´è³‡æ–™æ¬Šé‡çš„æ–¹å¼ä¾†è®“ä¸Šä¸€æ¬¡çš„ $g_t$ è¡¨ç¾å¾ˆå·®, é€™æ¨£æ–°æ¬Šé‡çš„ dataset è¨“ç·´å‡ºä¾†çš„ $g$ å°±æœƒè·Ÿä¹‹å‰çš„çœ‹æ³•åˆ†æ­§.Adaboost å†åˆ©ç”¨ä¸€ç¨®é —ç‚ºå·§æ€çš„ç·šæ€§çµ„åˆæ–¹å¼ä¾†èåˆ {$g_t$}, æœ€çµ‚å¾—åˆ°å¼·å¤§çš„ $G$ Additive Model (a framework)é€™æ˜¯éå¸¸é‡è¦çš„ä¸€å€‹æ¡†æ¶, Adaboost åœ¨é€™æ¡†æ¶ä¸‹å¯è¦–ç‚ºå®ƒçš„ä¸€å€‹ special case, åŒæ™‚è‘—åçš„ Gradient Boost Decision Tree (GBDT) ä¹Ÿæ˜¯åŸºæ–¼æ­¤æ¡†æ¶ä¸‹çš„æ¼”ç®—æ³•. é€šå¸¸ supervised learning å°±æ˜¯åœ¨å­¸ç¿’ input and output ä¹‹é–“çš„ mapping function $f$, ç°¡å–®è¬›, ç›´æ¥å­¸ä¸€å€‹å¥½çš„ $f$ å¯èƒ½å¾ˆå›°é›£, æ‰€ä»¥ä¸å¦‚ä½¿ç”¨ greedy æ–¹å¼, å°±æ˜¯å¾ç›®å‰çš„ $f_t$ å‡ºç™¼, è€ƒæ…®æ€éº¼ä¿®æ­£ç¾åœ¨çš„ $f_t$ ä¾†ä½¿å¾— error æ›´å°. åš´è¬¹ä¸€é»æ•¸å­¸æè¿°å¦‚ä¸‹: è€ƒæ…® additive model$$\\begin{align} f_T(x)=\\sum_{t=1}^T \\alpha_t g_t(x) \\end{align}$$ å…¶ä¸­, $g_t(x)$ ç‚ºç¬¬ t æ¬¡å­¸åˆ°çš„ base learner, $\\alpha_t$ ç‚ºå®ƒçš„æ¬Šé‡.å®šç¾© $L(y,f(x))$ ç‚º loss (or error) function, æ‰€ä»¥æˆ‘å€‘è¦æ‰¾çš„ä¿®æ­£çš„ mapping function å¦‚ä¸‹: $$\\begin{align} (\\alpha_T,g_T)=\\arg\\min_{\\eta,h}\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta h(x_n)) \\end{align}$$ ç”¨ä¸Šå¼çš„æ–¹æ³•æ‰¾åˆ°è¦ä¿®æ­£çš„ mapping function å› æ­¤ mapping function æ›´æ–°å¦‚ä¸‹: $$\\begin{align} f_T(x)=f_{T-1}(x)+\\alpha_T g_T(x) \\end{align}$$ æˆ‘å€‘å¯ä»¥æƒ³æˆæ˜¯åœ¨å‡½æ•¸ç©ºé–“åš gradient descent. æ¯ä¸€æ¬¡å°±æ˜¯æ‰¾ä¸€å€‹ descent direction, åœ¨é€™è£¡å°±æ˜¯ $h$, ç„¶å¾Œè¨­å®šåˆé©çš„æ­¥é•· $\\eta$, é€™éº¼æƒ³å°±æ˜¯æœ€ä½³åŒ–çš„ gradient descent äº†. Gradient BoostingAdditive model framework å¾ˆç°¡å–®, é›£çš„åœ°æ–¹åœ¨é‚£å€‹ $\\arg\\min$ å¼ (15). è€Œ Gradient Boosting å¯ä»¥èªªæ˜¯ä¸€ç¨®æ˜ç¢ºå¯¦ç¾ Additive model çš„æ–¹å¼, æˆ‘å€‘å¯ä»¥å°‡ $\\eta$ å’Œ $h$ åˆ†é–‹æ‰¾, ä¾‹å¦‚å…ˆæ‰¾ $h$: $$\\begin{align} &amp;\\min_h\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta h(x_n))\\\\ &amp;\\mbox{by Taylor: }\\simeq \\min_h\\sum_{n=1}^N\\left(L(y_n,f_{T-1}(x_n))+\\eta h(x_n) \\color{red}{ \\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right) _{f=f_{T-1}} } \\right)\\\\ \\end{align}$$ Taylor å±•é–‹å¼é‚£é‚Šå¯ä»¥é€™éº¼æƒ³ $$\\begin{align} &amp;\\mbox{å°‡ }L(y_n, \\color{green}{ f_{T-1}(x_n) }+ \\color{blue}{ \\eta h(x_n) } )\\mbox{ çœ‹ä½œ }\\hat{L}( \\color{green}{ \\tilde{x} }+ \\color{blue}{ \\delta } )\\\\ &amp;\\mbox{å› æ­¤ by Taylor } \\simeq \\hat{L}( \\color{green}{\\tilde{x}} )+ \\color{blue}{\\delta} \\left(\\frac{\\partial \\hat{L}(x) }{\\partial x}\\right)_{x= \\color{green}{\\tilde{x}} } \\end{align}$$ ä¸Šé¢ç´…è‰²éƒ¨åˆ†åœ¨è¨ˆç®—çš„æ™‚å€™æ˜¯ä¸€å€‹å›ºå®šå€¼, æˆ‘å€‘å…ˆä»¤ç‚º $$\\begin{align} \\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right) _{f=f_{T-1}}= \\color{red}{-\\tilde{y}_n} \\end{align}$$ æ‰€ä»¥ (18) è®Šæˆ $$\\begin{align} &amp;= \\min_h\\sum_{n=1}^N\\left(L(y_n,f_{T-1}(x_n)) \\color{red}{-} \\eta h(x_n) \\color{red}{ \\tilde{y_n} } \\right)\\\\ &amp;\\mbox{å»æ‰èˆ‡}h\\mbox{ç„¡é—œé …ä¸¦è£œä¸Š}2=\\min_h \\sum_{n=1}^N \\left(-2h(x_n)\\tilde{y}_n\\right) \\end{align}$$ å¾ˆæ˜é¡¯, å¦‚æœ $h$ ç„¡é™åˆ¶, å‰‡è§£ç‚º $h=\\infty$, é€™é¡¯ç„¶ä¸æ˜¯æˆ‘å€‘è¦çš„, åœ¨ optimization çš„æ™‚å€™, æˆ‘å€‘éœ€è¦çš„åªæ˜¯ gradient çš„æ–¹å‘, è€Œä¸æ˜¯å¤§å°, å¤§å°å¯ä»¥ç”± stepsize æ§åˆ¶. ä¸éå¦‚æœåŠ ä¸Š $norm(h)=1$ æ¢ä»¶ä¸¦ä½¿ç”¨ Lagrange Multipliers æœƒè¼ƒè¤‡é›œ, å¯¦ä½œä¸Šæˆ‘å€‘å°±ç›´æ¥å°‡ $norm(h)$ ç•¶ä½œä¸€å€‹ penality åŠ åœ¨ loss è£¡å°±å¯ä»¥. å› æ­¤ (23) ä¿®æ”¹å¦‚ä¸‹: $$\\begin{align} =\\min_h \\sum_{n=1}^N \\left(-2h(x_n)\\tilde{y}_n+(h(x_n))^2\\right) \\end{align}$$ æ¹Šé½Šå¹³æ–¹é …æœƒè®Šæˆ (ä¹‹å‰åŠ çš„2æ˜¯ç‚ºäº†é€™è£¡æ¹Šå¹³æ–¹é …) $$\\begin{align} =\\min_h \\sum_{n=1}^N \\left( \\mbox{const}+\\left(h(x_n)-\\tilde{y}_n\\right)^2 \\right) \\end{align}$$ OK! åˆ°é€™è£¡æˆ‘å€‘ç™¼ç¾äº†ä¸€å€‹é‡è¦çš„è§£é‡‹, $h$ çš„æ‰¾æ³•å°±æ˜¯å° $\\tilde{y}_n$ åš sqaure error regression! å¾—åˆ° $g_T=h$ å¾Œ, é‚£éº¼æ­¥é•· $\\eta$ å‘¢? $$\\begin{align} \\alpha_T=\\min_{\\eta}\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta g_T(x_n))\\\\ \\end{align}$$ é€™å€‹è§£é€šå¸¸å¾ˆå¥½ç®—, ä»¤ $L$ å¾®åˆ†ç‚º 0 å³å¯, æ˜¯å€‹å–®è®Šé‡æ±‚è§£. åˆ°ç›®å‰ç‚ºæ­¢, æˆ‘å€‘å¯ä»¥å°‡æ•´å€‹ Gradient Boost æ¼”ç®—æ³•åˆ—å‡ºä¾†äº†: $$\\begin{align} &amp;\\mbox{1. Init }g_0(x)\\\\ &amp;\\mbox{2. For }t=1~T\\mbox{ do:}\\\\ &amp;\\mbox{3. }\\tilde{y}_n=-\\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right)_{f=f_{t-1}}\\mbox{, n=1~N}\\\\ &amp;\\mbox{4. }g_t=\\arg\\min_h\\left(h(x_n)-\\tilde{y}_n\\right)^2\\\\ &amp;\\mbox{5. }\\alpha_T=\\arg\\min_{\\eta}\\sum_{n=1}^N L\\left(y_n,f_{t-1}(x_n)+\\eta g_t(x_n)\\right)\\\\ &amp;\\mbox{6. }f_t(x)=f_{t-1}(x)+\\alpha_t g_t(x) \\end{align}$$ Adaboost as an additive modelå°‡ Adaboost å¥—ç”¨ additive model framework æ™‚æœƒæ˜¯ä»€éº¼æƒ…æ³?é¦–å…ˆ loss æ˜¯ exponential loss, ç„¶å¾Œä¸€æ¨£ç”¨ binary classification ä¾†èªªæ˜, å…¶ä¸­ $y_n,g_t(x_n)\\in${-1,+1}, å‰‡æˆ‘å€‘è¦æ‰¾çš„ $h$ å¦‚ä¸‹ (å°ç…§ (12) and (13) ä¸¦ä½¿ç”¨ additive model (14) çš„æ¶æ§‹): $$\\begin{align} g_T=\\min_h\\sum_{n=1}^N\\exp\\left(-y_n\\left(f_{T-1}(x_n)+\\eta h(x_n)\\right)\\right)\\\\ =\\min_h\\sum_{n=1}^N u_n^{(T)}\\exp(-y_n\\eta h(x_n))\\\\ \\simeq\\min_h\\sum_{n=1}^N u_n^{(T)}(1-y_n\\eta h(x_n))\\\\ =\\min_h\\sum_{n=1}^N u_n^{(T)}(-y_n h(x_n))\\\\ \\end{align}$$ (33) åˆ° (34) ä½¿ç”¨ $u_n^{(T)}$ çš„å®šç¾©, åƒè€ƒ (13). è€Œæœ€å¾Œçš„ (36) è¡¨æ˜äº†å¯¦éš›ä¸Šå°±æ˜¯é¸æ“‡è®“ training data åœ¨æ–°çš„ weighted dataset ä¸‹è¡¨ç¾æœ€å¥½çš„é‚£å€‹ $h$, å…·é«”åŸå› çœ‹ä¸‹åœ–.é€™ä¸æ­£æ˜¯ Adaboost é¸æ“‡ weak learner çš„æ–¹å¼å—? æœ€å¾Œåˆ¥å¿˜äº† stepsize, å°‡ (34) æ›ä¸€ä¸‹è®Šæ•¸, $h$ è®Š $\\eta$: $$\\begin{align} \\alpha_T=\\arg\\min_{\\eta}\\sum_{n=1}^N u_n^{(T)}\\exp(-y_n \\eta g_t(x_n))\\\\ \\end{align}$$ å…©ç¨®æƒ…æ³:$$\\begin{align} \\mbox{1. }y_n=g_t(x_n)\\mbox{: }u_n^{(T)}\\exp(-\\eta)\\\\ \\mbox{2. }y_n\\neq g_t(x_n)\\mbox{: }u_n^{(T)}\\exp(+\\eta)\\\\ \\end{align}$$ æ‰€ä»¥$$\\begin{align} \\alpha_T=\\arg\\min_{\\eta}\\left(\\sum_{n=1}^N u_n^{(T)}\\right) \\cdot \\left(\\left(1-\\epsilon_T\\right)\\exp\\left(-\\eta\\right)+\\epsilon_T\\exp\\left(+\\eta\\right)\\right) \\end{align}$$ ä»¤å¾®åˆ†ç‚º 0, æˆ‘å€‘å¯ä»¥å¾ˆå®¹æ˜“å¾—åˆ° $$\\begin{align} \\alpha_T = \\ln\\sqrt{\\frac{1-\\epsilon_T}{\\epsilon_T}} \\end{align}$$ é€™æ­£å¥½ä¹Ÿå°±æ˜¯ adaboost æ‰€è¨ˆç®—çš„æ–¹å¼! ç¸½çµä¸€ä¸‹, Adaboost åœ¨ additive model æ¡†æ¶ä¸‹, ç›¸ç•¶æ–¼ä½¿ç”¨ steepest gradient descent æ–¹å¼åœ¨å‡½æ•¸ç©ºé–“æ‰¾ weaker learner, ä¸¦ä¸”å°‡ stepsize æŒ‡å®šç‚ºæœ€ä½³æ­¥é•·. Gradient Boost Decision Tree (GBDT)Gradient Boost å¾ˆæ£’çš„ä¸€å€‹ç‰¹æ€§æ˜¯ error function æ²’é™å®š, ä¾‹å¦‚ä½¿ç”¨ exponential error å°±æ˜¯ adaboost, è€Œå¦ä¸€å€‹å¸¸ç”¨çš„æ˜¯ sqaure error.ç•¶ä½¿ç”¨ square error æ™‚, $\\tilde{y}_n$ å°±æœƒè®Šæˆ $(y_n-x_n)$ ä¹Ÿå°±æ˜¯ residual. å°ç…§ GradientBoost (27)~(32) ä¾†çœ‹, æˆ‘å€‘ç™¼ç¾æ•´å€‹æ¼”ç®—æ³•è®Šæˆå°æ¯ä¸€æ¬¡ iteration çš„ residual åš regression.å¦å¤–åœ¨å¯¦å‹™ä¸Š base learner å¸¸å¸¸ä½¿ç”¨ Decision Tree (å› ç‚º decision tree æœ‰å¾ˆå¤šå¥½è™•: å¯è§£é‡‹æ€§ã€è¨“ç·´å¿«ã€å¯è™•ç†ç¼ºå¤±è³‡æ–™â€¦), ä¸éé€™å°±è¦ç‰¹åˆ¥æ³¨æ„äº†, å› ç‚ºå¦‚æœé•·æˆ fully growed tree å°±ç›´æ¥æŠŠ residual regression åˆ° 0 äº†. å› æ­¤, decision tree éœ€è¦ regularization, è€Œå¯¦å‹™ä¸Šæ¡ç”¨ pruned tree. æ•´å€‹ GBDT ç¯€è‡ªèª²ç¨‹ slide å¦‚ä¸‹: XGBoosté€™ç¯‡æ–‡ç«  XGBoostçš„åŸç† ä»‹ç´¹å¾—å¾ˆå¥½ å¹¾å€‹é‡é»æ•´ç†, XGBoost åŸºæœ¬ä¸Šä¹Ÿæ˜¯ gradient boost çš„ä¸€ç¨®, æ¯”è¼ƒç‰¹åˆ¥çš„æ˜¯æ³°å‹’å±•å±•é–‹ (18) ä½¿ç”¨åˆ°äºŒéšå°å‡½æ•¸: $$\\begin{align} &amp;\\min_h\\sum_{n=1}^N L(y_n,f_{T-1}(x_n)+\\eta h(x_n))\\\\ &amp;\\simeq \\min_h\\sum_{n=1}^N\\left(L(y_n,f_{T-1}(x_n))+\\eta h(x_n) \\left(\\frac{\\partial L(y_n,f)}{\\partial f}\\right) _{f=f_{T-1}}\\\\ \\color{red} { +\\eta^2h^2(x_n)\\left(\\frac{\\partial^2 L(y_n,f)}{\\partial^2 f}\\right)_{f=f_{T-1}} } \\right)\\\\ &amp;=\\min_h\\sum_{n=1}^N \\left( L(y_n,f_{T-1}(x_n)) + \\eta h(x_n)\\mbox{Gradient}_n + \\frac{\\eta^2h^2(x_n)}{2}\\mbox{Hessian}_n \\right)\\\\ &amp;=\\min_h\\sum_{n=1}^N \\left( \\eta h(x_n)\\mbox{Gradient}_n + \\frac{\\eta^2h^2(x_n)}{2}\\mbox{Hessian}_n \\right)\\\\ \\end{align}$$ æœ€å¾Œå†åŠ ä¸Šä¸€å€‹ regularization term $$\\begin{align} &amp;=\\min_h\\sum_{n=1}^N \\left( \\eta h(x_n)\\mbox{Gradient}_n + \\frac{\\eta^2h^2(x_n)}{2}\\mbox{Hessian}_n \\right) + \\Omega(h)\\\\ \\end{align}$$ é‡å° (46) è¦æ‰¾åˆ°æœ€å¥½çš„ $h$, å¦‚æœä½¿ç”¨ Decision Tree, $\\Omega(h)$ å¯ä»¥ä½¿ç”¨æ¨¹çš„æ·±åº¦ã€è‘‰å­æ•¸é‡ã€è‘‰å­å€¼çš„å¤§å°ç­‰ç­‰è¨ˆç®—. ä½†é—œéµæ˜¯å¦‚ä½•æœ‰æ•ˆç‡åœ°æ‰¾åˆ°å¾ˆå¥½çš„ $h$, è€Œåœ¨ Decision Tree æ­¤å•é¡Œç›¸ç•¶æ–¼å¦‚ä½•æœ‰æ•ˆç‡çš„å° Tree åš splitting. XGBoost æ–‡ç« ä½¿ç”¨éå¸¸æœ‰æ•ˆç‡çš„è¿‘ä¼¼æ–¹æ³•, ä¸¦ä¸”è©²æ–¹æ³•å¯ä»¥å¾ˆå¥½çš„ä¸¦è¡ŒåŠ é€Ÿ. å°æ–¼ xgboost å°±åªç²—æ·ºçš„äº†è§£åˆ°é€™äº†, ä¹Ÿé‚„æ²’æœ‰çœŸçš„æœ‰ä»€éº¼èª¿æ•´çš„ç¶“é©—, å°±æŠŠé€™å€‹èª²é¡Œæ”¾åœ¨ todo list å§. Reference æ—è»’ç”°è€å¸« ML èª²ç¨‹ æèˆª çµ±è¨ˆå­¸ç¿’æ–¹æ³• Why-Aggregation-Work ä»¥å‰ Adaboost and face detection paper survey å…¶ä¸­Rapid object detection using a boosted cascade of simple features, 2001, cited 17597 æ·±åº¦ç¥ç»ç½‘ç»œä¸­æ·±åº¦ç©¶ç«Ÿå¸¦æ¥äº†ä»€ä¹ˆï¼Ÿ Deep Convolutional Neural Networks with Merge-and-Run Mappings XGBoostçš„åŸç† XGBoost: A Scalable Tree Boosting System","tags":[{"name":"bagging","slug":"bagging","permalink":"http://yoursite.com/tags/bagging/"},{"name":"Adaboost","slug":"Adaboost","permalink":"http://yoursite.com/tags/Adaboost/"},{"name":"Gradient Boost","slug":"Gradient-Boost","permalink":"http://yoursite.com/tags/Gradient-Boost/"}]},{"title":"TF Notes (5), GRU in Tensorflow","date":"2018-07-30T15:29:01.000Z","path":"2018/07/30/TF-Notes-GRU-in-Tensorflow/","text":"å°ç­†è¨˜. Tensorflow è£¡å¯¦ä½œçš„ GRU è·Ÿ Colahâ€™s blog æè¿°çš„ GRU æœ‰äº›ä¸å¤ªä¸€æ¨£. æ‰€ä»¥åšäº†ä¸€ä¸‹ TF çš„ GRU çµæ§‹. åœ–æ¯”è¼ƒé†œ, æˆ‘ç›¡åŠ›äº†â€¦ XD TF çš„ GRU çµæ§‹ u å¯ä»¥æƒ³æˆæ˜¯åŸä¾† LSTM çš„ forget gate, è€Œ c è¡¨ç¤ºè¦åœ¨ memory cell ä¸­éœ€è¦è¨˜ä½çš„å…§å®¹. é€™å€‹è¦è¨˜ä½çš„å…§å®¹ç°¡å–®è¬›æ˜¯ç”¨ä¸€å€‹ gate (r) ä¾†æ§åˆ¶ä¹‹å‰çš„ state æœ‰å¤šå°‘æ¯”ä¾‹ä¿ç•™, concate input å¾Œåš activation transform å¾Œå¾—åˆ°. å¯ä»¥å°ç…§ä¸‹é¢ tf source codes. TF Source Codesrnn_cell_impl.py 12345678910111213141516171819def call(self, inputs, state): \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\" gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._gate_kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias) value = math_ops.sigmoid(gate_inputs) r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1) r_state = r * state candidate = math_ops.matmul( array_ops.concat([inputs, r_state], 1), self._candidate_kernel) candidate = nn_ops.bias_add(candidate, self._candidate_bias) c = self._activation(candidate) new_h = u * state + (1 - u) * c return new_h, new_h","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"GRU","slug":"GRU","permalink":"http://yoursite.com/tags/GRU/"}]},{"title":"(what is) Probabilistic Graphical Models","date":"2018-06-16T02:27:30.000Z","path":"2018/06/16/what-is-Probabilistic-Graphical-Model/","text":"æœ¬ç¯‡ä¸»è¦ä»‹ç´¹ä»€éº¼æ˜¯ PGM, ä»¥åŠä¸€å€‹å¾ˆé‡è¦çš„æ‡‰ç”¨ Part-of-Speech tagging. PGM çš„éƒ¨åˆ†ä¸»è¦åœç¹åœ¨ â€œå®ƒæ˜¯ä»€éº¼?â€ ä¹Ÿå°±æ˜¯ Koller èª²ç¨‹çš„ Representation. Inference ä¸è¨è«–, å› ç‚ºè‡ªå·±ä¹Ÿæ²’è®€å¾ˆæ·±å…¥ (æ±—), è€Œ Learning å°±ç›¸ç•¶æ–¼ ML è£¡çš„ training, æœƒåœ¨ä»‹ç´¹ POS æ™‚æ¨å°ä¸€ä¸‹. æ–‡ç« çµæ§‹å¦‚ä¸‹: What is Probabilistic Graphical Model (PGM)? What is Bayesian Network (BN)? What is Markov Network (MN)? (or Markov Random Field) What is Conditional Random Field (CRF)? Part-of-Speech (POS) Tagging References æ–‡é•·â€¦ What is Probabilistic Graphical Model (PGM)?å®ƒæ˜¯æè¿° pdf çš„ä¸€ç¨®æ–¹å¼, ä¸åŒçš„æè¿°æ–¹å¼å¦‚ directed/undirected graphical model, or Factor Graph æ‰€èƒ½æè¿°çš„ pdf ç¯„åœæ˜¯ä¸åŒçš„. (ref: PRML) å…¶ä¸­ P ä»£è¡¨æ‰€æœ‰ distributions çš„é›†åˆ, U å’Œ D åˆ†åˆ¥è¡¨ç¤º undirected å’Œ directed graphical models. ä»¥æœ‰å‘ç„¡å‘åœ–ä¾†åˆ†å¦‚ä¸‹: Directed Acyclic Graph (DAG): Bayesian Network Undirected Graph: Markov Network æ³¨æ„ BN é™¤äº† directed ä¹‹å¤–, é‚„éœ€è¦ acyclic. æœ‰äº†åœ–ä¹‹å¾Œ, æ€éº¼è·Ÿ distribution ç”¢ç”Ÿé€£çµçš„? ä¸‹é¢æˆ‘å€‘åˆ†åˆ¥ä»‹ç´¹ BN and MN. What is Bayesian Network (BN)?å°æ–¼ä¸€å€‹ä»»æ„çš„ distibution over variables $x_1,â€¦,x_V$ æˆ‘å€‘å¯ä»¥ç”¨åŸºæœ¬çš„ chain rule æ‹†è§£å¦‚ä¸‹: $$\\begin{align} p(x_{1:V})=p(x_1)p(x_2|x_1)p(x_3|x_{1:2})...p(x_V|x_{1:V-1}) \\end{align}$$ è®Šæ•¸çš„ order å¯ä»¥ä»»æ„æ’åˆ—, èˆ‰ä¾‹ä¾†èªª $$\\begin{align} p(x,y,z)\\\\ =p(x)p(y|x)p(z|x,y)\\\\ =p(x)p(z|x)p(y|x,z) \\end{align}$$ åŸºæ–¼é€™ç¨®æ‹†è§£æˆ‘å€‘å¯ä»¥å¾ˆè‡ªç„¶åœ°æƒ³åˆ°, ä¸å¦‚æŠŠæ¯å€‹è®Šæ•¸éƒ½ç•¶æˆ nodes, å†å°‡ conditioning çš„é—œä¿‚ç”¨ edges é€£èµ·ä¾†. å› æ­¤åŸºæœ¬å¯ä»¥é€™éº¼è¡¨é”: $$\\begin{align} p(x_{1:V}|G)=\\prod_{t=1}^{V}p(x_t|pa(x_t)) \\end{align}$$ å…¶ä¸­ $pa(x_t)$ è¡¨ç¤º node $x_t$ çš„ parent nodes. æˆ‘å€‘ç”¨å¼ (3) å’Œ (4) ç•¶ä¾‹å­å°±å¯ä»¥ç•«å‡ºå¦‚ä¸‹çš„åœ–: å¯ä»¥ç™¼ç¾åŒä¸€å€‹ pdf å¯ä»¥ç•«å‡ºå¤šå€‹ BN, å› æ­¤è¡¨é”æ–¹å¼ä¸æ˜¯å”¯ä¸€. Conditioning V.S. Independencyæ¥è‘—æˆ‘å€‘å¯èƒ½æœƒæƒ³, å¦‚æœæˆ‘å€‘å¸Œæœ›å° pdf åŠ ä¸€äº›ç¨ç«‹æ¢ä»¶å‘¢, è­¬å¦‚å¦‚æœå¸Œæœ› $x \\perp y$, æ˜¯ä¸æ˜¯å¯ä»¥ç›´æ¥å°‡åœ–ä¸­çš„ $x$ å’Œ $y$ çš„ edge æ‹”æ‰å°±å¯ä»¥äº†å‘¢? å…ˆç ´é¡Œ, ç­”æ¡ˆæ˜¯ä¸è¡Œ. åŒæ¨£ä»¥ä¸Šé¢çš„ä¾‹å­è§£é‡‹, å¦‚æœæˆ‘å€‘ç”¨æ‹”æ‰ edge çš„è©±, åœ–è®Šæˆ: äº‹å¯¦ä¸Šé€™å…©å€‹åœ–å·²ç¶“å„è‡ªè¡¨ç¤ºä¸åŒçš„ distribution äº†. ç‰¹åˆ¥è¦æ³¨æ„åœ¨å³åœ–ä¸­æ‹”æ‰ $x$ and $y$ çš„ edge æ²’æœ‰é€ æˆ $x \\perp y$. è§£é‡‹å¦‚ä¸‹: é‚£ç©¶ç«Ÿè©²å¦‚ä½•å¾ä¸€å€‹åœ–ç›´æ¥çœ‹å‡ºè®Šæ•¸ä¹‹é–“æ˜¯å¦ç¨ç«‹? ç‚ºäº†è§£ç­”é€™å€‹å•é¡Œ, æˆ‘å€‘å…ˆå¾ç°¡å–®çš„ä¸‰å€‹ nodes é–‹å§‹ Flow of Influenceä¸‰å€‹ nodes çš„ DAG åœ–æœ¬è³ªä¸Šå°±åˆ†ä»¥ä¸‹ä¸‰é¡, å…¶ä¸­ given çš„è®Šæ•¸æˆ‘å€‘é€šå¸¸ä»¥å¯¦å¿ƒåœ“è¡¨ç¤º æˆ‘å€‘å°±å€‹åˆ¥è¨è«– éœ€è¦ç‰¹åˆ¥æ³¨æ„çš„æ˜¯ case 3 çš„ v-structure, è¡Œç‚ºè·Ÿå…¶ä»–å…©ç¨®ç›¸å. ä¸€ç¨®å¥½è¨˜çš„æ–¹å¼æ˜¯, æˆ‘å€‘å‡è¨­ given çš„è®Šé‡æ˜¯ä¸€å€‹çŸ³é ­, è€Œ edges å¯ä»¥æƒ³æˆæ˜¯æ°´æµ, æ‰€ä»¥ given è®Šé‡å°±æŠŠæ°´æµæ“‹ä½, å› æ­¤æœƒé€ æˆç¨ç«‹. å”¯ä¸€å€‹ä¾‹å¤–å°±æ˜¯ v-structure, è¡Œç‚ºå‰›å¥½ç›¸å. Active Trail in BNæˆ‘å€‘å¯ä»¥å¾ˆå®¹æ˜“å°‡ä¸‰å€‹ nodes çš„ trail æ“´å±•æˆ $V$ å€‹ nodes çš„ trail. å› æ­¤å¯ä»¥å¾ˆæ–¹ä¾¿çš„è§€å¯ŸæŸæ¢ trail èµ·å§‹çš„ node èƒ½å¦å½±éŸ¿åˆ°æœ€å¾Œçš„ node. d-separationç¹¼çºŒæ“´å±•! æˆ‘å€‘å‡è¨­åœ¨ BN $G$ ä¸Š node $x$ and $y$ æœ‰ $N$ æ¢ trails. æˆ‘å€‘å‰‡å¯ä»¥è—‰ç”±æª¢æŸ¥æ¯æ¢ trail æ˜¯å¦ active æœ€çµ‚å°±æœƒçŸ¥é“ $x$ èƒ½å¦å½±éŸ¿åˆ° $y$. éœ€è¦æ³¨æ„çš„æ˜¯, é€™äº› d-separation æ¢ä»¶æˆ‘å€‘éƒ½å¯ä»¥ç›´æ¥å¾çµ¦å®šçš„ $G$ ä¸Šç›´æ¥è®€å‡ºä¾† (å°æ–¼ distribution æ²’æœ‰ä»»ä½•å‡è¨­), ç‚ºäº†æ–¹ä¾¿æˆ‘å€‘å®šç¾©ä»¥ä¸‹å…©å€‹ terms $$\\begin{align} CI(G)=\\{\\textbf{d-sep}(x,y|z)|x,y,z\\textbf{ in }G\\}\\\\ CI(p)=\\{(x \\perp y|z)|x,y,z\\textbf{ in }G\\}\\\\ \\end{align}$$ $CI(G)$ æ‰€åˆ—å‡ºçš„ statements æ˜¯ç”± d-sep æ‰€æä¾›, ä¹Ÿå°±æ˜¯èªªå¾ $G$ ç›´æ¥è®€å‡ºä¾†çš„, è€Œ $CI(p)$ æ‰æ˜¯çœŸçš„å°æ–¼ distribution $p$ ä¾†èªªæ‰€æœ‰æ¢ä»¶ç¨ç«‹çš„ statements. OK, åˆ°ç›®å‰ç‚ºæ­¢, çµ¦å®šä¸€å€‹ BN $G$, å’Œä¸€å€‹ distribution $p$ (æ³¨æ„ $p$ ä¸ä¸€å®šå¯ä»¥è¢« $G$ æ‰€è¡¨ç¤º), ä»–å€‘ä¹‹é–“çš„é—œä¿‚åˆ°åº•æ˜¯ä»€éº¼? ä¸‹é¢å°±è¦å¼•å‡ºéå¸¸é‡è¦çš„å®šç† Factorization and Independent çš„é—œä¿‚ä¾†èªªæ˜ Factorization and Independent ç™½è©±æ–‡: å‡è¨­ $p$ å‰›å¥½å¯ä»¥å¯«æˆ $G$ çš„ factorization å‹å¼ (å¼ (5)), å‰‡æ‰€æœ‰ $G$ æŒ‡å‡ºéœ€è¦ $\\perp$ çš„ statements (æ ¹æ“š d-sep æ‰€åˆ—), $p$ éƒ½æ»¿è¶³ ç™½è©±æ–‡: å‡è¨­æ‰€æœ‰ $G$ æŒ‡å‡ºéœ€è¦ $\\perp$ çš„ statements (æ ¹æ“š d-sep æ‰€åˆ—), $p$ éƒ½æ»¿è¶³, å‰‡ $p$ å¯ä»¥å¯«æˆ $G$ çš„ factorization å‹å¼ (å¼ (5)) æˆ‘å€‘ç”¨ PRML book è£¡ä¸€å€‹å…·é«”çš„æè¿°ä¾†èªªæ˜ Thm1 and Thm2 ä¹‹é–“çš„é—œä¿‚ çµ¦å®šä¸€å€‹ $G$, å°±å¥½åƒä¸€å€‹ç¯©å­ä¸€æ¨£, æ ¹æ“šå…©ç¨®æ–¹å¼ç¯©é¸ distribution $p$ å‰›å¥½å¯ä»¥å¯«æˆ $G$ çš„ factorization å‹å¼ (å¼ (5)) $G$ æŒ‡å‡ºéœ€è¦ $\\perp$ çš„ statements (æ ¹æ“š d-sep æ‰€åˆ—), å‰›å¥½ $p$ éƒ½æ»¿è¶³ ç”¨ä¸Šé¢å…©ç¨®ç¯©é¸æ–¹å¼æœ€å¾Œç¯©å‡ºä¾†çš„ distributions åˆ†åˆ¥ç¨±ç‚º $DF1$ and $DF2$ å…©å€‹ sets. å®šç†å‘Šè¨´æˆ‘å€‘å®ƒå€‘å¼åŒä¸€å€‹é›†åˆ! ExampleæŠŠä¸‹åœ–çš„ joint pdf å¯«å‡ºä¾†: ä½¿ç”¨å¼ (5) çš„æ–¹å¼å¯«ä¸€ä¸‹, è®€è€…å¾ˆå¿«å°±ç™¼ç¾, é€™ä¸å°±æ˜¯ HMM å—? What is Markov Network (MN)?Factorizationåœ¨è§£é‡‹ MN ä¹‹å‰, å…ˆäº†è§£ä¸€ä¸‹ä»€éº¼æ˜¯ (maximal) clique. å› æ­¤, æˆ‘å€‘å¯ä»¥ç”¨ maximal cliques ä¾†å®šç¾©ä¸€å€‹ MN. $$\\begin{align} p(x)=\\frac{1}{Z}\\prod_{c\\in\\mathcal{C}}\\psi_c(x_c) \\end{align}$$ $\\mathcal{C}$ æ˜¯ maximal cliques çš„é›†åˆ. ç„¶å¾Œ $Z$ æ˜¯ä¸€å€‹ normalization term, ç›®çš„ç‚ºä½¿ä¹‹æˆç‚º distribution. $$\\begin{align} Z=\\sum_x\\prod_{c\\in\\mathcal{C}}\\psi_c(x_c) \\end{align}$$ èˆ‰å€‹ä¾‹å­: ç”¨ç„¡å‘åœ–çš„æ–¹å¼ä¾†è¡¨é” distribution æœ‰ä¸€å€‹å¾ˆå¤§çš„å¥½è™•å°±æ˜¯åˆ¤æ–· Active Trail å’Œ separation è®Šå¾—éå¸¸éå¸¸ç°¡å–®! ç›´æ¥çœ‹ä¸‹åœ–çš„èªªæ˜ å¦‚åŒåœ¨ BN æ™‚çš„è¨è«–, çµ¦å®šä¸€å€‹ MN $H$, å’Œä¸€å€‹ distribution $p$ (æ³¨æ„ $p$ ä¸ä¸€å®šå¯ä»¥è¢« $H$ æ‰€è¡¨ç¤º), ä»–å€‘ä¹‹é–“çš„é—œä¿‚å¯ä»¥ç”± Factorization and Independent çš„å®šç†ä¾†èªªæ˜ Factorization and Independentæˆ‘å€‘ç›´æ¥æ“·å– Kevin Murphy æ›¸æ‰€åˆ—çš„å®šç†, Hammersley-Clifford å®šç† è·Ÿ BN ä¸€æ¨£, factorization iff independence, ä½†æœ‰ä¸€å€‹é‡è¦çš„ assumption, å°±æ˜¯ distribution å¿…é ˆ strictly positive (å¦‚ä¸Šåœ–ç´…è‰²æ¡†çš„éƒ¨åˆ†). æˆ‘å€‘ä¸€æ¨£ç”¨ PRML ç¯©å­çš„è§€å¿µä¾†å…·é«”åŒ–: æè¿°å°±è·³éäº†. Exampleç”±æ–¼æœ‰ $p(x)&gt;0$ çš„å‡è¨­åœ¨, å› æ­¤å¦‚æœå°‡ factor functions $\\psi(x_c)$ éƒ½ä½¿ç”¨ $exp$ ä¾†å®šç¾©çš„è©±, æ•´å€‹ product ç›¸ä¹˜å¾Œçš„ distribution å¿…å®šæ»¿è¶³ strictly positive. å› æ­¤ $exp$ å°±ä¸å¤±ç‚ºä¸€ç¨®æ–¹ä¾¿çš„ modeling æ–¹å¼äº† å–˜å£æ°£çš„çµè«–åˆ°é€™è£¡, æˆ‘å€‘å¯ä»¥ ç”¨ graph ç°¡å–®çš„è¡¨ç¤ºå‡º joint pdf (ç”¨ factorization). ä¹Ÿå¯ä»¥å¾ graph ä¸­çœ‹å‡º conditional independence (ç”¨ active tail, separation) å› æ­¤æˆ‘å€‘å¯ä»¥é‡å°è¦ model çš„å•é¡Œåˆ©ç”¨ graph ä¾†æè¿° joint pdf äº†. ä½†æ˜¯å…‰æè¿°å¥½ model æ²’ç”¨, æˆ‘å€‘é‚„éœ€è¦ inference (test) and learning (train). Inference éå¸¸æ¨è–¦çœ‹ PRML ch8, è¬›å¦‚ä½•å° tree graph åš sum-product algorithm (belief propagation) éå¸¸ç²¾å½©. æ¥è‘—å¦‚ä½•æ¨å»£åˆ°ä¸€èˆ¬ general graph å‰‡å¯ä»¥ä½¿ç”¨ junction tree algorithm (æ¨è–¦çœ‹é€™ç¯‡æ–‡ç« , è§£é‡‹éå¸¸æ£’!). ä¸Šè¿°å…©ç¨®æ–¹å¼éƒ½å±¬æ–¼ exact inference, å°æ–¼ä¸€äº›æƒ…å½¢ä»æœƒéœ€è¦ exponential time è¨ˆç®—, å› æ­¤æˆ‘å€‘éœ€è¦ variational inference æˆ– sampling çš„æ–¹å¼ç®— approximation. æœ€å¾Œæœ‰é—œ learning æˆ‘å€‘ä½¿ç”¨æ¥ä¸‹ä¾†çš„ POS tagging ç•¶ç¯„ä¾‹æ¨å°ä¸€ä¸‹. ä½†åˆ¥æ€¥, åœ¨è¬› POS ä¹‹å‰æˆ‘å€‘å¾—å…ˆè«‡ä¸€å€‹é‡è¦çš„æ±è¥¿, Conditional Random Field. What is Conditional Random Field (CRF)? å¦‚åŒä¸Šåœ–çš„èªªæ˜, åŸºæœ¬ä¸Š CRF ä»èˆŠæ˜¯ä¸€å€‹ MN, æœ€å¤§çš„å·®åˆ¥æ˜¯ normalization term å¦‚ä»Šä¸å†æ˜¯ä¸€å€‹ constant, è€Œæ˜¯ depends on conditioning çš„è®Šæ•¸ $x$. ä¸€å€‹åœ¨ sequence labeling å¸¸ç”¨çš„ CRF æ¨¡å‹æ˜¯ Linear-Chain CRF æœ‰äº†é€™äº›æ¦‚å¿µå¾Œæˆ‘å€‘å°±å¯ä»¥èªªèªª POS äº† Part-of-Speech (POS) Taggingæ“·å–è‡ªæå®æ¯…æ•™æˆä¸Šèª²æŠ•å½±ç‰‡ åŸºæœ¬ä¸Šå°±æ˜¯çµ¦å®šä¸€å€‹ word sequence $x$, æˆ‘å€‘å¸Œæœ›æ‰¾å‡ºå“ªä¸€å€‹è©æ€§æ¨™è¨»çš„ sequence $y$ æœƒä½¿å¾—æ©Ÿç‡æœ€å¤§. æ©Ÿç‡æœ€å¤§çš„é‚£å€‹ $y$ å°±æ˜¯æˆ‘å€‘è¦çš„è©æ€§æ¨™è¨»åºåˆ—. ä½¿ç”¨ç¾å­¸ç¾è³£çš„ PGM modeling çŸ¥è­˜, æˆ‘å€‘å¯ä»¥ä½¿ç”¨ BN or MN çš„æ–¹å¼æè¿°æ¨¡å‹ BN: Hidden Markov Model (HMM) MN: Linear chain CRF with log-linear model æœ‰å‘åœ– HMM æ–¹æ³•ä¸€æ¨£æ“·å–è‡ªæå®æ¯…æ•™æˆä¸Šèª²æŠ•å½±ç‰‡ é‚„è¨˜å¾—æœ¬æ–‡å‰é¢è¬› BN æ™‚çš„ HMM example å—? $y$ å°±æ˜¯è©æ€§, $x$ å°±æ˜¯å­—. HMM æ˜¯åœ¨ model çµ¦å®šè©æ€§åºåˆ—æƒ…å½¢ä¸‹çš„å­—åºåˆ— distribution. äº†è§£èªéŸ³è¾¨è­˜çš„ç«¥é‹é–€æ‡‰è©²å†ç†Ÿæ‚‰ä¸éäº†, åªä¸éé€™è£¡å•é¡Œæ¯”è¼ƒç°¡å–®, åœ¨èªéŸ³è¾¨è­˜è£¡, æˆ‘å€‘ä¸æœƒé‡å°æ¯å€‹ frame å»æ¨™è¨»å®ƒæ˜¯å±¬æ–¼å“ªä¸€å€‹ç™¼éŸ³çš„ state, å› æ­¤æ¨™è¨»å…¶å¯¦æ˜¯ hidden çš„. ä½†åœ¨é€™è£¡æ¯å€‹ word éƒ½æœƒæœ‰ä¸€å€‹å°æ‡‰æ­£ç¢ºç­”æ¡ˆçš„è©æ€§æ¨™è¨», æ²’æœ‰ hidden è³‡è¨Š, å› æ­¤ä¹Ÿä¸éœ€è¦ EM algorithm, ç°¡å–®çš„ counting å³å¯åšå®Œè¨“ç·´. that all â€¦ ç„¡å‘åœ– CRF æ–¹æ³•ç²¾ç¢ºèªªæ˜¯ Linear chain CRF with log-linear model æˆ‘å€‘æŠŠ log-linear model çš„ factor å¸¶å…¥ linear chain CRF ä¸­, æ³¨æ„å…¶ä¸­ $\\phi$ æ˜¯éœ€è¦å®šç¾©çš„ç‰¹å¾µå‡½æ•¸, æˆ‘å€‘é€™è£¡å…ˆå‡è¨­å¯ä»¥æŠ½å–å‡º $K$ ç¶­. å› æ­¤å¯ä»¥æ¨å°å¦‚ä¸‹ å¯¦ä½œä¸Šæˆ‘å€‘æœƒé‡å°æ™‚é–“ share weights, é€™æ˜¯å› ç‚ºå¥å­éƒ½æ˜¯é•·çŸ­ä¸ä¸€çš„, å¦ä¸€æ–¹é¢é€™æ¨£åšä¹Ÿå¯ä»¥å¤§é‡æ¸›å°‘åƒæ•¸é‡. æ‰€ä»¥æœ€å¾Œå¯ä»¥ç°¡åŒ–æˆä¸€å€‹ weigth vector $w$ å’Œæˆ‘å€‘åˆä½µçš„ç‰¹å¾µå‘é‡ $f(x,y)$ çš„ log-linear model. Learning ç›®æ¨™å‡½æ•¸å°±æ˜¯åœ¨æœ€å¤§åŒ– CRF çš„ likelihood. æ¡ç”¨ gradient method. è€Œ gradient çš„æ¨å°äº‹å¯¦ä¸Šä¹Ÿä¸å›°é›£, åªè¦èŠ±é»è€å¿ƒå³å¯äº†è§£ ä½†æ˜¯å…¶å¯¦æˆ‘èªªä¸å›°é›£åªèªªå°äº†ä¸€åŠ, ç´…è‰²çš„åœ°æ–¹äº‹å¯¦ä¸Šéœ€è¦è·‘ inference æ‰å¯ä»¥å¾—åˆ°, å¥½åœ¨ linear-chain æ¶æ§‹ä¸‹æ­£å¥½å¯ä»¥ç”¨ Viterbi åšå‰å‘å¾Œç®—è¨ˆç®—, é€™éƒ¨åˆ†çš„å¼å­å¯ä»¥è·Ÿ â€œæèˆª çµ±è¨ˆå­¸ç¿’æ–¹æ³•â€œ é€™æœ¬æ›¸çš„ p201 å¼ (11.34) éŠœæ¥ä¸Š, è©²å¼å¯«å‡ºäº†å‰å‘å¾Œå‘è¨ˆç®—. ToolCRF++ åšç‚ºèªéŸ³è¾¨è­˜çš„å¾Œè™•ç†ååˆ†å¥½ç”¨çš„å·¥å…·, in c++. ReferencesPGM åšå¤§ç²¾æ·±, é€™å€‹æ¡†æ¶å¾ˆå®Œæ•´ä¸”åš´è¬¹, å€¼å¾—æˆ‘å¾ŒçºŒèŠ±æ™‚é–“ç ”è®€, æœ‰æ©Ÿæœƒçœ‹èƒ½å¦å°‡ Koller çš„èª²ç¨‹ä¸Šéä¸€æ¬¡çœ‹çœ‹. é€šå¸¸é€™éº¼èªªå°±è¡¨ç¤º â€¦. hmmâ€¦ä½ æ‡‚å¾— Bishop PRML book Kevin Murphy book Junction Tree Algorithm æèˆª çµ±è¨ˆå­¸ç¿’æ–¹æ³• æå®æ¯…è€å¸« ML èª²ç¨‹","tags":[{"name":"Probabilistic Graphical Models","slug":"Probabilistic-Graphical-Models","permalink":"http://yoursite.com/tags/Probabilistic-Graphical-Models/"},{"name":"Bayesian Network","slug":"Bayesian-Network","permalink":"http://yoursite.com/tags/Bayesian-Network/"},{"name":"Markov Network","slug":"Markov-Network","permalink":"http://yoursite.com/tags/Markov-Network/"},{"name":"Conditional Random Field","slug":"Conditional-Random-Field","permalink":"http://yoursite.com/tags/Conditional-Random-Field/"},{"name":"POS tagging","slug":"POS-tagging","permalink":"http://yoursite.com/tags/POS-tagging/"}]},{"title":"Kaldi Notes (1), I/O in C++ Level","date":"2018-05-31T15:32:43.000Z","path":"2018/05/31/Kaldi-Notes-IO-in-C-Level/","text":"Kaldi I/O C++ Level ç­†è¨˜, ä¸»è¦ä»‹ç´¹ä»¥ä¸‹å¹¾é», ä»¥åŠå®ƒå€‘åœ¨ Kaldi c++ è£¡å¦‚ä½•é—œè¯: æ¨™æº– low-level I/O for Kaldi Object XXXHolderé¡åˆ¥: ä¸€å€‹ç¬¦åˆæ¨™æº– low-level I/O çš„é¡åˆ¥ Kaldi Table Object: &lt;key,value&gt; pairs çµ„æˆçš„ Kaldi æ ¼å¼æª”æ¡ˆ (scp, ark), å…¶ä¸­ value ç‚º XXXHolder é¡åˆ¥ æ¨™æº– low-level I/O for Kaldi ObjectKaldi Object æœ‰è‡ªå·±çš„æ¨™æº– I/O ä»‹é¢:12345class SomeKaldiClass &#123; public: void Read(std::istream &amp;is, bool binary); void Write(std::ostream &amp;os, bool binary) const; &#125;; å› æ­¤å®šç¾©äº†è©² Kaldi Class å¦‚ä½•é‡å° istream è®€å– (ostream å¯«å…¥). åœ¨ Kaldi ä¸­, istream/ostream ä¸€èˆ¬æ˜¯ç”± Input/Output(åœ¨ util/kaldi-io.h è£¡å®šç¾©) é€™å€‹ class ä¾†é–‹å•Ÿçš„. é‚£ç‚ºä½•ä¸ç”¨ä¸€èˆ¬çš„ c++ iostream é–‹å•Ÿä¸€å€‹æª”æ¡ˆå‘¢? é€™æ˜¯å› ç‚º Kaldi æƒ³è¦æ”¯æ´æ›´å¤šæ¨£çš„æª”æ¡ˆé–‹å•Ÿæ–¹å¼, ç¨±ç‚º â€œExtended filenames: rxfilenames and wxfilenamesâ€œ. ä¾‹å¦‚å¯ä»¥å¾ stdin/stdout, pipe, file å’Œ file with offset è®€å–å¯«å…¥, è©³ç´°è«‹çœ‹æ–‡æª”çš„ â€œExtended filenames: rxfilenames and wxfilenamesâ€ éƒ¨åˆ†. æ‰€ä»¥ Input/Ouput Class æœƒè‡ªå‹•è§£æ rxfilenames/wxfilenames ç„¶å¾Œé–‹å•Ÿ istream/ostream. é–‹å•Ÿå¾Œ, Kaldi Object å°±å¯ä»¥é€éæ¨™æº–çš„ I/O ä»‹é¢å‘¼å« Read/Write æ–¹æ³•äº†. å®˜ç¶²ç¯„ä¾‹å¦‚ä¸‹:123456789101112&#123; // input. bool binary_in; Input ki(some_rxfilename, &amp;binary_in); my_object.Read(ki.Stream(), binary_in); // you can have more than one object in a file: my_other_object.Read(ki.Stream(), binary_in);&#125;// output. note, \"binary\" is probably a command-line option.&#123; Output ko(some_wxfilename, binary); my_object.Write(ko.Stream(), binary);&#125; æœ‰æ™‚å€™æœƒçœ‹åˆ°æ›´ç²¾ç°¡çš„å¯«æ³•å¦‚ä¸‹12345678int main(int argc, char *argv[]) &#123; ... std::string rxfilenames = po.GetArg(1); std::string wxfilenames = po.GetArg(2); SomeKaldiClass my_object; ReadKaldiObject(rxfilenames, &amp;my_object); WriteKaldiObject(my_object, wxfilenames, binary);&#125; å…¶ä¸­ ReadKaldiObject and WriteKaldiObject (defined in util/kaldi-io.h) çš„ä½œç”¨åªæ˜¯å°‡ Input/Output é–‹å•Ÿ xfilenames ç‚º iostream, ä¸¦å‚³çµ¦ my_object çš„æ¨™æº– I/O ä»‹é¢åŒ…è£èµ· ä¾†è€Œå·². æ“·å– define ç‰‡æ®µå¦‚ä¸‹: 12345678910111213141516171819template &lt;class C&gt; void ReadKaldiObject(const std::string &amp;filename, C *c) &#123; bool binary_in; Input ki(filename, &amp;binary_in); c-&gt;Read(ki.Stream(), binary_in);&#125;// Specialize the template for reading matrices, because we want to be able to// support reading 'ranges' (row and column ranges), like foo.mat[10:20].// ä¸Šé¢çš„ class C å¦‚æœæ˜¯ Matrix&lt;float&gt; or Matrix&lt;double&gt; çš„è©±, ä½¿ç”¨ä¸‹é¢å…©å€‹å®šç¾©// Note: é€™ç¨®æ–¹å¼æ˜¯ template çš„ specialization, åŒæ¨£åç¨±çš„ template function or class å¯ä»¥é‡è¤‡å‡ºç¾ï¼Œåªé‡å°æŸäº› type å®¢è£½åŒ–template &lt;&gt; void ReadKaldiObject(const std::string &amp;filename, Matrix&lt;float&gt; *m);template &lt;&gt; void ReadKaldiObject(const std::string &amp;filename, Matrix&lt;double&gt; *m);template &lt;class C&gt; inline void WriteKaldiObject(const C &amp;c, const std::string &amp;filename, bool binary) &#123; Output ko(filename, binary); c.Write(ko.Stream(), binary);&#125; Kaldi Table ObjectTable Object ä¸ç›´æ¥é€éæ¨™æº–çš„ Read/Write æ“ä½œ, æ˜¯å› ç‚º Table object çš„æ§‹æˆæ˜¯ç”± &lt;key,value&gt; pairs çµ„æˆçš„, è€Œ value æ‰æœƒæ˜¯ä¸€å€‹ç¬¦åˆæ¨™æº– Read/Write æ“ä½œçš„ object. é€™ç¨® table æ‰€éœ€è¦çš„è®€å¯«å¯èƒ½æœ‰å¾ˆå¤šæ–¹å¼, è­¬å¦‚ sequential access, random access ç­‰ç­‰, å› æ­¤å–®ç´”çš„ Read/Write æ¯”è¼ƒä¸èƒ½æ»¿è¶³éœ€æ±‚, æ›´éœ€è¦çš„æ˜¯è¦æœ‰ Next, Done, Key, Value ç­‰ç­‰çš„æ“ä½œæ–¹å¼. ä¾‹å¦‚ä»¥ä¸‹ç¯„ä¾‹: 12345678910111213141516std::string feature_rspecifier = \"scp:/tmp/my_orig_features.scp\", transform_rspecifier = \"ark:/tmp/transforms.ark\", feature_wspecifier = \"ark,t:/tmp/new_features.ark\";// there are actually more convenient typedefs for the types below,// e.g. BaseFloatMatrixWriter, SequentialBaseFloatMatrixReader, etc.TableWriter&lt;BaseFloatMatrixHolder&gt; feature_writer(feature_wspecifier);SequentialTableReader&lt;BaseFloatMatrixHolder&gt; feature_reader(feature_rspecifier);RandomAccessTableReader&lt;BaseFloatMatrixHolder&gt; transform_reader(transform_rspecifier);for(; !feature_reader.Done(); feature_reader.Next()) &#123; std::string utt = feature_reader.Key(); if(transform_reader.HasKey(utt)) &#123; Matrix&lt;BaseFloat&gt; new_feats(feature_reader.Value()); ApplyFmllrTransform(new_feats, transform_reader.Value(utt)); feature_writer.Write(utt, new_feats); &#125;&#125; ä¸»è¦æœ‰å¹¾ç¨® table classes:TableWriter, SequentialTableReader, RandomAccessTableReader ç­‰ç­‰, éƒ½å®šç¾©åœ¨ util/kaldi-table.h. æˆ‘å€‘å°±ä»¥ SequentialTableReader ä¾†èˆ‰ä¾‹. ä¸Šé¢çš„ç¯„ä¾‹ feature_reader å°±æ˜¯ä¸€å€‹ SequentialTableReader, ä»–çš„ &lt;key,value&gt; pairs ä¸­çš„ value å®šç¾©ç‚º BaseFloatMatrixHolder é¡åˆ¥ (ä¸€å€‹ç¬¦åˆæ¨™æº– low-level I/O çš„ Kaldi Class, ç­‰æ–¼æ˜¯å¤šä¸€å±¤åŒ…è£). XXXHolder (å¦‚ KaldiObjectHolder, BasicHolder, BasicVectorHolder, BasicVectorVectorHolder, â€¦) æŒ‡çš„æ˜¯ç¬¦åˆæ¨™æº– low-level I/O çš„ Kaldi Object, å› æ­¤é€™äº› XXXHolder éƒ½å¯ä»¥çµ±ä¸€é€é Read/Write ä¾†å‘¼å«. é€™äº› Holder çš„å®šç¾©åœ¨ util/kaldi-holder.h.å¦å¤– kaldi-holder.h æœ€å¾Œä¸€è¡Œæœƒ include kaldi-holder-inl.h. â€œ-inlâ€ æ„æ€æ˜¯ inline, é€šå¸¸æœƒæ”¾åœ¨ç›¸å°æ‡‰æ²’æœ‰ -inl çš„ .h æœ€å¾Œé¢, ç”¨ä¾†ç•¶ä½œæ˜¯ inline implementation ç”¨. SequentialTableReader çš„å®šç¾©åœ¨ â€œutil/kaldi-table.hâ€, æ“·å–è¦ä»‹ç´¹çš„ç‰‡æ®µ:1234567891011template&lt;class Holder&gt;class SequentialTableReader &#123; public: typedef typename Holder::T T; inline bool Done(); inline std::string Key(); T &amp;Value(); void Next(); private: SequentialTableReaderImplBase&lt;Holder&gt; *impl_;&#125; Done(), Next(), Key(), and Value() éƒ½å¯ä»¥å¾ feature_reader çœ‹åˆ°å¦‚ä½•ä½¿ç”¨, æ‡‰è©²å¾ˆç›´è¦º, è€Œ Holder çš„è§£é‡‹ä¸Šé¢èªªäº†. å‰©ä¸‹è¦èªªæ˜çš„æ˜¯é€™è¡Œ SequentialTableReaderImplBase&lt;Holder&gt; *impl_;. åœ¨å‘¼å« SequentialTableReader çš„ Next() æ™‚, ä»–å¯¦éš›ä¸Šå‘¼å«çš„æ˜¯ impl_ çš„ Next(). å®šç¾©åœ¨ util/kaldi-table-inl.h ç‰‡æ®µ: 12345template&lt;class Holder&gt;void SequentialTableReader&lt;Holder&gt;::Next() &#123; CheckImpl(); impl_-&gt;Next();&#125; impl_ çš„ class å®£å‘Šæ˜¯ â€œSequentialTableReaderImplBaseâ€, è©²é¡åˆ¥çš„è§’è‰²æ˜¯æä¾›ä¸€å€‹çˆ¶é¡åˆ¥, å¯¦éš›ä¸Šæœƒæ ¹æ“š impl_ çœŸæ­£çš„é¡åˆ¥å‘¼å«å…¶å°æ‡‰çš„ Next(), å°±æ˜¯å¤šå‹çš„ä½¿ç”¨. ç¾åœ¨å‡è¨­ impl_ çœŸæ­£çš„é¡åˆ¥æ˜¯ SequentialTableReaderArchiveImpl. æˆ‘å€‘å¯ä»¥åœ¨ util/kaldi-table-inl.h çœ‹åˆ°ä»–çš„ Next (line 531) å¯¦ä½œå¦‚ä¸‹:123456789virtual void Next() &#123; ... if (holder_.Read(is)) &#123; state_ = kHaveObject; return; &#125; else &#123; ... &#125;&#125; åˆ°é€™æ‰çœŸæ­£çœ‹åˆ°é€é XXXHolder ä½¿ç”¨ low-level I/O çš„ Read()! Kaldi Codes å“è³ªå¾ˆé«˜é˜¿, è¦èŠ±ä¸å°‘æ™‚é–“è®€, æœç„¶ c++ åº•å­é‚„æ˜¯å¤ªå·®äº†. References Kaldi Project","tags":[{"name":"Kaldi","slug":"Kaldi","permalink":"http://yoursite.com/tags/Kaldi/"}]},{"title":"TF Notes (4), Deconvolution","date":"2018-05-09T11:59:12.000Z","path":"2018/05/09/TF-Notes-deconvolution/","text":"é€™ç¯‡æ˜¯å€‹å°ç·´ç¿’, å°±å…©é»: äº†è§£ä»€éº¼æ˜¯ deconvolution, ä¸¦åœ¨ tensorflow ä¸­æ€éº¼ç”¨ å¯¦ä½œä¸€å€‹ CNN AutoEncoder, Encoder ç”¨ conv2d, Decoder ç”¨ conv2d_transpose What is deconvolution?ç ´é¡Œ: Deconvolution çš„æ“ä½œå°±æ˜¯ kernel tranpose å¾Œçš„ convolution. ä½¿ç”¨æå®æ¯…è€å¸«çš„ä¸Šèª²å…§å®¹, å¦‚ä¸‹åœ–: å…¶å¯¦åœ–å·²ç¶“ååˆ†æ˜ç¢ºäº†, å› æ­¤ä¸å¤šè§£é‡‹. å¦å¤–åœ¨ tensorflow ä¸­, å‡è¨­æˆ‘å€‘çš„ kernel $W$ ç‚º W.shape = (img_h, img_w, dim1, dim2). å‰‡ tf.nn.conv2d(in_tensor,W,stride,padding) æœƒå°‡ (dim1,dim2) çœ‹æˆ (in_dim, out_dim). è€Œ tf.nn.conv2d_transpose(in_tensor,W,output_shape,stride) æœƒå°‡ (dim1,dim2) çœ‹æˆ (out_dim, in_dim), æ³¨æ„æ˜¯åéä¾†çš„. æœ‰å…©é»å¤šåšèªªæ˜: tf.nn.conv2d_transpose æœƒè‡ªå‹•å° $W$ åš transpose ä¹‹å¾Œå† convolution, å› æ­¤æˆ‘å€‘ä¸éœ€è¦è‡ªå·±åš transpose. tf.nn.conv2d_transpose éœ€è¦é¡å¤–æŒ‡å®š output_shape. æ›´å¤š conv/transpose_conv/dilated_conv with stride/padding æœ‰å€‹ éå¸¸æ£’çš„å¯è¦–åŒ– çµæœåƒè€ƒæ­¤ github CNN AutoEncoderçµæ§‹å¦‚ä¸‹åœ– ç›´æ¥å°‡ embedding å£“åˆ° 2 ç¶­, æ¯å€‹é¡åˆ¥çš„åˆ†å¸ƒæƒ…å½¢å¦‚ä¸‹: embedding æ˜¯ 128 ç¶­, ä¸¦ä½¿ç”¨ tSNE æŠ•å½±åˆ° 2 ç¶­ç•«åœ–å¦‚ä¸‹: Encoder å¦‚ä¸‹: 1234567891011121314151617181920def Encoder(x): print('Input x got shape=',x.shape) # (None,28,28,1) # Layer 1 encode: Input = (batch_num, img_height, img_width, cNum). Output = (batch_num, img_height/2, img_width/2, layer_dim['conv1']) layer1_en = tf.nn.relu(tf.nn.conv2d(x, weights['conv1'], strides=[1, 1, 1, 1], padding='SAME')) # Avg Pooling layer1_en = tf.nn.avg_pool(layer1_en, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') print('After Layer 1, got shape=',layer1_en.shape) # (None,14,14,32) # Layer 2 encode: Input = (batch_num, img_height/2, img_width/2, layer_dim['conv1']). Output = (batch_num, img_height/4, img_width/4, layer_dim['conv2']) layer2_en = tf.nn.relu(tf.nn.conv2d(layer1_en, weights['conv2'], strides=[1, 1, 1, 1], padding='SAME')) # Avg Pooling layer2_en = tf.nn.avg_pool(layer2_en, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') print('After Layer 2, got shape=',layer2_en.shape) # (None,7,7,64) # Layer embedded: Input = (batch_num, img_height/4 * img_width/4 * layer_dim['conv2']). Output = (batch_num, layer_dim['embedded']) flatten_in = flatten(layer2_en) embedded = tf.matmul(flatten_in,weights['embedded']) print('embedded has shape=',embedded.shape) return embedded Decoder å¦‚ä¸‹: 12345678910111213141516171819202122def Decoder(embedded): # API: tf.nn.conv2d_transpose = (value, filter, output_shape, strides, padding='SAME', ...) bsize = tf.shape(embedded)[0] # Layer embedded decode: Input = (batch_num, layer_dim['embedded']). Output = (batch_num, in_dim_for_embedded) embedded_t = tf.matmul(embedded,weights['embedded'],transpose_b=True) embedded_t = tf.reshape(embedded_t,[-1, 7, 7, layer_dim['conv2']]) print('embedded_t has shape=',embedded_t.shape) # Layer 2 decode: Input = (batch_num, 7, 7, layer_dim['conv2']). Output = (batch_num, 14, 14, layer_dim['conv1']) layer2_t = tf.nn.relu(tf.nn.conv2d_transpose(embedded_t,weights['conv2t'],[bsize, 14, 14, layer_dim['conv1']], [1, 2, 2, 1])) print('layer2_t has shape=',layer2_t.shape) # Layer 1 decode: Input = (batch_num, 14, 14, layer_dim['conv1']). Output = (batch_num, 28, 28, cNum) layer1_t = tf.nn.relu(tf.nn.conv2d_transpose(layer2_t,weights['conv1t'],[bsize, 28, 28, cNum], [1, 2, 2, 1])) print('layer1_t has shape=',layer1_t.shape) # Layer reconstruct: Input = batch_num x layer_dim['layer1']. Output = batch_num x img_dim. reconstruct = tf.nn.relu(tf.nn.conv2d(layer1_t, weights['reconstruct'], strides=[1, 1, 1, 1], padding='SAME')) - 0.5 print('reconstruct has shape=',reconstruct.shape) return reconstruct AutoEncoder ä¸²èµ·ä¾†å¾ˆå®¹æ˜“: 12345def AutoEncoder(x): embedded = Encoder(x) reconstruct = Decoder(embedded) return [embedded, reconstruct] å®Œæ•´ source codes åƒè€ƒä¸‹é¢ reference Reference æå®æ¯… deconvolution è§£é‡‹ tf.nn.conv2d_transpose èªªæ˜ conv/transpose_conv/dilated_conv with stride/padding å¯è¦–åŒ–: github æœ¬ç¯‡å®Œæ•´ source codes","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"Computational Graph","slug":"Computational-Graph","permalink":"http://yoursite.com/tags/Computational-Graph/"}]},{"title":"ROS in Self-driving Car system","date":"2018-04-15T11:05:29.000Z","path":"2018/04/15/ROS-in-Self-driving-Car-system/","text":"é€™æ˜¯ç¶“æ­·äº†æ¼«é•·çš„æ™‚é–“, æœ€å¾Œçš„ä¸€å“©è·¯äº†â€¦.å¾2016å¹´12æœˆé–‹å§‹, åˆ°2018å¹´4æœˆä¸­, èŠ±äº†æ•´æ•´ä¸€å¹´äº”å€‹æœˆ. å…¶å¯¦æˆ‘åŸå…ˆæ‰“ç®—åŠå¹´å‰å°±ç•¢æ¥­çš„, ä½†æ˜¯ä¸­é€”æœ‰ç‹€æ³, æ‰€ä»¥åªå¥½ term2 å®Œæˆå¾Œåœäº†åŠå¹´æ‰é–‹å§‹ term3, ä¹Ÿå› æ­¤åˆ°æ˜¨å¤©æ‰å‰›ç¢ºå®šç•¢æ¥­! è€Œæ˜¨å¤©å‰›å¥½ä¹ŸåƒåŠ äº† Udacity åœ¨ä¸­åœ‹å…©å‘¨å¹´çš„æœƒ, è¦‹åˆ°äº† David Sliver æœ¬äºº, ç®—æ˜¯ç•¢æ¥­çš„ä¸€å€‹å°ç´€å¿µ! æœ€å¾Œçš„ project æ¯”è¼ƒæœ‰åˆ¥æ–¼ä»¥å¾€, æ¡ç”¨ team work çš„æ–¹å¼. æˆ‘å€‘çš„ team å…±äº”äºº, team lead Franz Pucher å¾·åœ‹, Theodore King ç¾åœ‹, å’Œæˆ‘. ç–‘? å¦å¤–å…©å€‹å‘¢? å°æ–¼ project å®Œå…¨æ²’è²¢ç»â€¦æˆ‘ä¸æƒ³èªªäº†â€¦.= = ROS ç°¡ä»‹é—œæ–¼æ©Ÿå™¨äººæ§åˆ¶å’Œè‡ªå‹•è»Šéƒ½æœƒä½¿ç”¨ ROS (Robot Operating System), ROS ä¸€å®šè¦åƒè€ƒ ROS wiki. æœ¬æ¬¡ä½œæ¥­çš„ ROS ç³»çµ±æ“·å–èª²ç¨‹åœ–ç‰‡å¦‚ä¸‹: çœ‹ä¸æ‡‚æ²’é—œä¿‚, äº†è§£ ROS ä¸»è¦ä¸‰å€‹æ¦‚å¿µ: Node, Topic, Msg å°±æ¸…æ¥šä¸Šé¢çš„åœ–åœ¨å¹¹å˜›äº†. Node ç°¡å–®è¬›é¡ä¼¼æ–¼ class, å¯ä»¥è¨‚é–±æŸäº› Topic, å’Œç™¼é€ Msg åˆ°æŒ‡å®šçš„ Topic. èˆ‰ä¾‹ä¾†èªªç•¶æœ‰æŸå€‹ Node A ç™¼é€ä¸€å€‹ msg M åˆ°ä¸€å€‹ topic T æ™‚, å¦‚æœ Node B æœ‰è¨‚é–± topic T, å‰‡ Node B æœƒæ”¶åˆ° msg M, ä¸¦ä¸”åŸ·è¡Œé å…ˆè¨­å®šå¥½çš„ call back function. ç”¨ä»¥ä¸‹çš„ç¨‹å¼ç¯„ä¾‹èˆ‰ä¾‹: 1234567891011class TrafficLightDetector(object): def __init__(self): rospy.init_node('tl_detector') # è¦åœ¨é–‹é ­å°±å…ˆ init å¥½é€™æ˜¯ ros node ... # è¨‚é–±äº†ä¸€å€‹ topic '/current_pose', ä¸¦ä¸”å¦‚æœæœ‰ msg ç™¼é€åˆ°æ­¤ topic, æ­¤ node æœƒæ”¶åˆ°ä¸¦ä¸”å‘¼å« call back function self.pose_cb sub = rospy.Subscriber('/current_pose', PoseStamped, self.pose_cb, queue_size=1) # æ­¤ node æœƒç™¼é€ msg åˆ° topic '/traffic_waypoint' self.upcoming_red_light_pub = rospy.Publisher('/traffic_waypoint', Int32, queue_size=1) def pose_cb(self, msg): self.pose = msg.pose è¦æ³¨æ„çš„æ˜¯, ç”±æ–¼ topic é‹ä½œæ–¹å¼ç‚ºä¸€æ—¦æœ‰å…¶ä»– node ç™¼é€ msg åˆ°æ­¤ topic, æœ‰è¨‚é–±æ­¤ topic çš„ node çš„ call back function éƒ½æœƒè¢«å‘¼å«. é€™å°±æ„è¬‚è‘— topic å¦‚æœç™¼é€ msg å¤ªé »ç¹, å°è‡´è¨‚é–±çš„ node ç„¡æ³•åŠæ™‚æ¶ˆåŒ–, å‰‡ msg æœƒæ‰åŒ…. ä¸€ç¨®è§£æ±ºæ–¹å¼ç‚ºä½¿ç”¨ rospy.Rate æ§åˆ¶ç™¼é€çš„é »ç‡. ä½†æ˜¯å…¶å¯¦é‚„æœ‰å¦ä¸€ç¨®å‚³é€ msg çš„æ–¹å¼: Service ç°¡å–®è¬› Service çš„æ¦‚å¿µå°±æ˜¯ request and response, ä¸åŒæ–¼ topic, service æœƒå°‡å…©å€‹ node ç›´æ¥é€£æ¥èµ·ä¾†, ä¸€å€‹ç™¼èµ· request å¾Œ, æœƒç­‰å¦ä¸€å€‹ node response æ‰æœƒæ¥è‘—åšä¸‹å». ä¸€å€‹ç°¡å–®çš„èˆ‰ä¾‹å¦‚ä¸‹: 12345678910111213141516171819# éœ€æ³¨æ„ ServiceClassName è¦å…ˆåœ¨ package è£¡çš„ srv folder å®šç¾©å¥½class NodeA(object):... # æ‹¿åˆ°è©² service service = rospy.ServiceProxy('service_name',ServiceClassName) # æ‹¿åˆ°å¯ä»¥ request çš„ msg instance msg = ServiceClassNameRequest() # ä¿®æ”¹ msg æˆéœ€è¦çš„ç‹€æ…‹ ... # ç™¼èµ· request ä¸¦å¾—åˆ° response response = service(msg) class NodeB(object):... rospy.Service('service_name',ServiceClassName, self.handler_func) ... def handler_func(self, msg): # æ”¶åˆ° request çš„ msg, åœ¨æ­¤ handler function è² è²¬è™•ç†å¦‚ä½• response ... ä¸Šé¢çš„ç¯„ä¾‹ä½¿ç”¨äº†å…©å€‹ nodes, node A è² è²¬ç™¼èµ· request, è€Œ node B è² è²¬ response. å¦å¤–ç­†è¨˜ä¸€äº› ros å¸¸ç”¨çš„æŒ‡ä»¤å’ŒåŠŸèƒ½12345678910111213141516171819202122232425262728293031323334353637383940&gt;&gt; roscore # start ROS master# rosrun å¯ä»¥æŒ‡å®šè² è²¬è¦è·‘å“ªå€‹ node&gt;&gt; rosrun package_name node_name# node ä¸€å¤š, å¯ä»¥ä½¿ç”¨ roslauch ä¸€æ¬¡åŸ·è¡Œå¤šå€‹ nodes, ä½†æ˜¯è¦å¯«å¥½ launch file&gt;&gt; roslaunch launch/launchfile# åˆ—å‡º active çš„ nodes&gt;&gt; rosnode list# åˆ—å‡º active çš„ topics&gt;&gt; rostopic list# æŸ¥çœ‹æŸå€‹ topic&gt;&gt; rostopic info topic_name# å°‡ publish åˆ°æ­¤ topic çš„ msgs éƒ½å³æ™‚é¡¯ç¤ºåœ¨ terminal ä¸Š&gt;&gt; rostopic echo topic_name# ä¸€èˆ¬ä¾†èªª rospy.loginfo('info msg') æœƒé¡¯ç¤ºåœ¨ /rosout é€™å€‹ topic, å› æ­¤é©åˆ debug&gt;&gt; rostopic echo /rosout# æŸ¥çœ‹æŸå€‹ msg&gt;&gt; rosmsg info msg_name# build è‡ªå®šç¾©çš„ ros package&gt;&gt; cd ~/catkin_ws; catkin_make# æª¢æŸ¥ package çš„ dependency&gt;&gt; rosdep install -i package_name# å¦‚æœå°‡æŸå€‹ package åŠ å…¥åˆ°è‡ªå·±çš„ catkin_ws æ™‚, éœ€åŠ åˆ° catkin_ws/src è³‡æ–™å¤¾ä¸‹, ä¸¦ä¸”é‡æ–° make&gt;&gt; cd ~/catkin_ws/src&gt;&gt; git clone 'some packages'&gt;&gt; cd ~/catkin_ws&gt;&gt; catkin_make# Build å®Œå¾Œ, éœ€è¦ source æ‰å¯ä»¥å°‡ catkin_ws/src ä¸‹çš„æ‰€æœ‰ packages éƒ½åŠ åˆ° ros ä¸­&gt;&gt; source ~/catkin_ws/devel/setup.bash Debug çš„è©± rospy.loginfo, rospy.logwarn, rospy.logerr, rospy.logfatal å¾ˆå¥½ç”¨, å®ƒå€‘åˆ†åˆ¥æœƒè¢«è¨˜éŒ„åœ¨ä»¥ä¸‹å¹¾å€‹åœ°æ–¹: Self-Driving Car ROS Nodeså› æ­¤é€™æœ€å¾Œçš„ project ä¸»è¦å°±åˆ†æˆä¸‰å€‹éƒ¨åˆ† Perception:é€™éƒ¨åˆ†è² è²¬æ”¶åˆ° /image_color é€™å€‹ topic çš„å½±åƒå¾Œ, ä¾†æ‰¾å‡º traffic sign åœ¨å“ªè£¡ä¸¦ä¸”æ˜¯å“ªç¨®ç‡ˆè™Ÿ. ç›¸ç•¶æ–¼ term1 çš„ Vehicle Tracking, æˆ‘ä¸»è¦è² è²¬æ­¤éƒ¨åˆ†, ä½†æ˜¯æ²’æœ‰ä½¿ç”¨ç•¶æ™‚åš project çš„ sliding window + svm æ–¹æ³•. ä¸‹é¢æœƒè©³ç´°ä»‹ç´¹. Planning:è² è²¬æ ¹æ“šç›®å‰è»Šå­çš„ä½ç½®ä»¥åŠå¦‚æœæœ‰ç´…ç‡ˆçš„è©±, å¿…é ˆè¦åŠƒå¥½æ–°çš„è·¯å¾‘, ä¸¦å°‡æœŸæœ›çš„é€Ÿåº¦ä¸€ä½µç™¼é€çµ¦ Control. ç›¸ç•¶æ–¼ term3 çš„ Path Planning Control:æ ¹æ“šè¦ç•«çš„è·¯å¾‘å’Œé€Ÿåº¦, æ‰¾å‡ºå¯ä»¥å¯¦éš›æ“æ§çš„åƒæ•¸ (throttle, brake, steering). ç›¸ç•¶æ–¼ term2 çš„ Model Predictive Control. ä½†æˆ‘å€‘åœ˜éšŠæ²’æœ‰ç”¨ MPC, è€Œæ˜¯ä½¿ç”¨ PID control. Perception Traffic Lightç”±æ–¼å°å¼Ÿæˆ‘ä¸æ˜¯åš CV çš„, æ²’é€™éº¼å¤šå²å®³çš„èƒ½åŠ›, å› æ­¤ä¸€é–‹å§‹æˆ‘ä¹Ÿæ²’æ‰“ç®—è¨“å€‹ YOLO ä¹‹é¡çš„æ–¹æ³•. é‡é ­é–‹å§‹è¨“ç·´çš„è©±æˆ‘åªèƒ½å…ˆæƒ³åˆ°ä¸å¦‚ç”¨ä¸Šæ¬¡ project çš„ semantic segmantation æ–¹æ³•, å°‡èªç‚ºæ˜¯ traffic sign çš„éƒ¨åˆ†æ‰¾å‡ºä¾†, æ¥è‘—ç”¨ç°¡å–®çš„é¡è‰²å€åˆ†ä¸€ä¸‹å¥½äº†. training set æˆ‘ä½¿ç”¨ Bosch Traffic Light Dataset, å…±æœ‰ 5093 å¼µ images. å¾ˆå¤šå¼µå½±åƒå®Œå…¨æ²’æœ‰ traffic sign, å› æ­¤æˆ‘å°±å¿½ç•¥, ä¸¦ä¸”æœ‰äº› traffic sign å¯¦åœ¨å¤ªå°, é‚£ç¨®æƒ…æ³ä¹Ÿå¿½ç•¥, æœ€å¾Œç¯©é¸å‡º 548 å¼µæœ‰ traffic signs çš„å½±åƒä¸¦ä¸” resize æˆ 600x800, èˆ‰å€‹ä¾‹å¦‚ä¸‹: æ³¨æ„åˆ°ç”¨çš„ semantic segmentation æ–¹æ³•æ˜¯ pixel level çš„, ä¹Ÿå°±æ˜¯èªªæ¯å€‹ pixel éƒ½æœƒå»åˆ¤åˆ¥ yes/no traffic sign. è€Œæˆ‘å€‘çœ‹åˆ°å°±ç®—æ˜¯éƒ½æœ‰ traffic sign çš„å½±åƒäº†, å¯¦éš›ä¸Š pixel æ˜¯ traffic sign æ‰€å çš„æ¯”ä¾‹é‚„æ˜¯åä½, é€™è®“æˆ‘é–‹å§‹æœ‰é»æ‡·ç–‘æ˜¯å¦ DNN æœ‰èƒ½åŠ›åˆ†è¾¨å‡ºä¾†. ä½†æ˜¯â€¦.é‚„çœŸçš„å¯ä»¥! ç¾åœ¨æœ‰ç¨®æ„Ÿè¦º, æœ‰æ™‚å€™é‡å°è³‡æ–™ä¸å¹³å‡åšäº†ä¸€äº›æ–¹å¼è®“æ¯å€‹ class å¹³å‡ä¸€äº›, ä½†æ˜¯ DNN çš„æ•ˆæœå…¶å¯¦éƒ½æ²’å•¥æå‡, æ„Ÿè¦º DNN å°è³‡æ–™ä¸å¹³å‡çš„å•é¡Œè¼ƒä¸æ•æ„Ÿ ä¸éç”±æ–¼æ¨¡æ“¬å™¨çš„ traffic sign è·Ÿ Bosch çš„å·®å¤ªå¤š, å› æ­¤æ•ˆæœä¸å¤§å¥½. æˆ‘åªå¥½åŠ å…¥äº†ä¸€äº›æ¨¡å™¨å™¨ä¸‹çš„å½±åƒå»è¨“ç·´, çµæœå°±å¥½å¾ˆå¤šäº†. ä½†é‚„æ˜¯é‡åˆ°ä¸€å€‹å•é¡Œ, æˆ‘çš„ macbook æ²’æœ‰ GPU, è·‘ä¸€å¼µå½±åƒèŠ±äº† 120 secs, è€Œä¸€ç§’é˜ camera æœƒå‚³ä¾† 8 å¼µå½±åƒ! æ ¹æœ¬è™•ç†ä¸äº†, é—œéµæ˜¯ä¹Ÿä¸çŸ¥é“ Udacity å®ƒå€‘ç”¨è‡ªå·± GPU è·‘èµ·ä¾†æœƒå¤šå¿«. æ‰€ä»¥æˆ‘å°±å°‡å½±åƒé•·å¯¬å„ç¸®å°ä¸€åŠ, ç¸½é«”é€Ÿåº¦æœƒé™åˆ°åŸä¾†çš„ 1/4. å°±ç®—å¦‚æ­¤é‚„æ˜¯ç„¡æ³•é©—è­‰æ˜¯å¦å¤ å¿«. æˆ‘å€‘åœ˜éšŠå¡åœ¨é€™å€‹ç„¡æ³•é©—è­‰çš„ç‹€æ³å¾ˆä¹…, å°è‡´å¯èƒ½éœ€è¦ç”¨åˆ°å»¶é•·å››å‘¨çš„æƒ…å½¢. æœ€å¾Œåœ¨ teammate Theodore King çš„å¹«åŠ©ä¸‹, æˆ‘å€‘ä½¿ç”¨äº† tf çš„ object detection API, ä½¿ç”¨ mobilenet é€Ÿåº¦å¿«åˆ°é åŒ—é£›èµ·ä¾†. é€£ CPU è™•ç†ä¸€å¼µå½±åƒéƒ½åªéœ€è¦ä¸åˆ°1ç§’çš„æ™‚é–“! ä½•æ³ä½¿ç”¨ GPU. æœ€çµ‚ç¸½ç®—æœ‰é©šç„¡éšªéé—œäº†. æˆ‘ä¹‹å‰åšé‚£éº¼è¾›è‹¦å¹¹å˜› é–’èŠå…¶å¯¦ Udacity è¦åŠƒç›¸ç•¶æ£’äº†, ä¸»è¦å¹¾å€‹éƒ¨åˆ†éƒ½æœ‰åˆ†åˆ¥çš„å¯¦ä½œé, æœ€å¾Œä¾†å€‹å¤§ä¸€çµ±, çœŸçš„å¾ˆæœ‰æ„æ€. ä½†æˆ‘ä»è¦åæ§½çš„æ˜¯, æç’°å¢ƒå¤ªéº»ç…©äº†! æ¨¡æ“¬å™¨è·‘åœ¨ virtualbox ä¸Š, è€Œæˆ‘çš„ virtualbox window æ²’æ³•è£å¥½, åªèƒ½è£åœ¨ macbook, ä½† macbook åˆæ²’æœ‰ GPU, å°è‡´ä½¿ç”¨ deep learning çš„æ–¹æ³•å®Œå…¨ä¸çŸ¥å¤ ä¸å¤ å¿«! å¦å¤–, VM çš„ç’°å¢ƒæˆ‘é‚„æä¸å®šæ€éº¼è·Ÿ host share data, æå¾—æˆ‘åªå¥½ä¸Šå‚³é›²ç«¯å†ä¸‹è¼‰, æœ€å¾Œè¡°äº‹æ¥è¸µè€Œä¾†, VM ä¹Ÿæä¸å®šç¿»ç‰† (å°, æˆ‘åœ¨ç¶²è·¯é•·åŸçš„ç‰†å…§)â€¦..80%éƒ½åœ¨æç’°å¢ƒâ€¦.çœŸçš„å¾ˆç—›è‹¦ æ©, çµ‚æ–¼ç•¢æ¥­äº†â€¦ çµæŸäº†é€™æ¼«é•·çš„æ—…ç¨‹. åŸä»¥ç‚ºæˆ‘æœƒèˆˆå¥®å¾—ä¸å¾—äº†, ä¸éå¯èƒ½æ˜¯å› ç‚ºæœ€å¾Œ project æç’°å¢ƒå¤ªç—›è‹¦, åŠ ä¸Šé€™æ¨£å­çš„åœ˜éšŠåˆä½œå…¶å¯¦æ²’æœ‰ç´„æŸåŠ› (æœ‰å…©å€‹å®Œå…¨çš„å£Ÿå“¡), åè€Œè§£è„«æ„Ÿå£“éäº†é«˜èˆˆ. ä½†ç¸½çµä¾†èªª, é‚„æ˜¯å¾ˆæ„Ÿè¬ Udacity é™ªä¼´äº†æˆ‘ä¸€å¹´å¤š, ä¸¦ä¸”æœ‰äº†é€™éº¼æœ‰è¶£çš„ç¶“é©—! æœ‰æ©Ÿæœƒçš„è©±, æˆ‘é‚„æ˜¯æœƒç¹¼çºŒä¸Š Udacity å…¶ä»–èª²ç¨‹çš„. Reference Our github ROS wiki TrafficLight_Detection-TensorFlowAPI Semantic Segmantation Path Planning Model Predictive Control","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"ROS","slug":"ROS","permalink":"http://yoursite.com/tags/ROS/"}]},{"title":"Udacity-Semantic-Segmentation","date":"2018-03-06T11:59:13.000Z","path":"2018/03/06/Udacity-Semantic-Segmentation/","text":"Udacity SDC term 3 ç¬¬äºŒå€‹ Project åšçš„æ˜¯ä½¿ç”¨ Deep Learning å­¸ç¿’è­˜åˆ¥ pixel ç­‰ç´šçš„è·¯é¢å€åŸŸ. ç°¡å–®è¬›å°±æ˜¯æœ‰å¦‚ä¸‹çš„ ground truth data, æ¨™ç¤ºå‡ºå“ªé‚Šæ˜¯æ­£ç¢ºçš„è·¯é¢, ç„¶å¾Œç”¨ Fully Convolutional Network å»å°æ¯å€‹ pixel åšè­˜åˆ¥. Fully-Convolutional-Network (FCN)ä¸»è¦æ˜¯å¯¦ä½œé€™ç¯‡è«–æ–‡ â€œFully Convolutional Networks for Semantic Segmentationâ€œ æ›è¨€ä¹‹, å…¨éƒ¨éƒ½æ˜¯ convolution layers, åŒ…å«ä½¿ç”¨ 1x1 convolution æ›¿æ›æ‰åŸä¾† Convnet çš„ fully-connected-layer, å’Œä½¿ç”¨ deconvolution åš upsampling. æ¶æ§‹åœ–å¦‚ä¸‹: åˆ†æˆ Encoder å’Œ Decoder éƒ¨åˆ†. Encoder ä½¿ç”¨ pre-trained å¥½çš„ VGG16 network, è² è²¬åšç‰¹å¾µæŠ½å–. æŠ½å–å‡ºä¾†çš„ç‰¹å¾µå¾Œ, æ¥ä¸Š deconvolution layers (éœ€è¨“ç·´) æ­å»ºè€Œæˆçš„ decoder part. æœ‰ä¸€å€‹ç‰¹åˆ¥ä¹‹è™•æ˜¯ä½¿ç”¨äº† skip æ–¹æ³•. é€™å€‹æ–¹æ³•æ˜¯åœ¨ decoder åš upsampling æ™‚, æœƒåŠ ä¸Šç•¶åˆç›¸å°æ‡‰å¤§å°çš„ Encoder layer è³‡è¨Š. é€™æ¨£åšè«–æ–‡è£¡æåˆ°æœƒå¢åŠ æ•´å€‹è­˜åˆ¥æ•ˆæœ. Resultsæ•ˆæœæœ‰é»è®“æˆ‘å°é©šè±”, å› ç‚ºåªä½¿ç”¨å°‘å°‘çš„ 289 å¼µåœ–ç‰‡å»è¨“ç·´è€Œå·². è·‘å‡ºä¾†çš„æ¸¬è©¦çµæœå¦‚ä¸‹: å¦å¤–é‚„æœ‰ä¸€é»æ˜¯ç¸±ä½¿å·²ç¶“æœ‰ dropout äº†, å¦‚æœæ²’æœ‰åŠ ä¸Š l2 regularization çš„è©±, æœƒ train ä¸å¥½! (l2 regularization çœŸè®“æˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°æœ‰é€™éº¼é‡è¦), åŒæ¨£è¨­å®šä¸‹, æœ‰å’Œæ²’æœ‰ l2 regularization çš„å·®åˆ¥: Reference Fully Convolutional Networks for Semantic Segmentation Source code github","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"}]},{"title":"Mixtures of Factor Analyzers","date":"2018-02-11T15:23:24.000Z","path":"2018/02/11/Mixtures-of-Factor-Analyzers/","text":"é€™ç¯‡ä½¿ç”¨ Bishop PRML çš„ notations, åŒä½¿åƒè€ƒ Zoubin Ghahramani and Geoffrey E. Hinton (æ²’éŒ¯, å°±æ˜¯é‚£ä½ Hiton, å¦å¤–, ç¬¬ä¸€ä½œè€…ä¹Ÿæ˜¯ç¥äººç´šåˆ¥, åŠæ©‹æ•™æˆ, Uber é¦–å¸­ç§‘å­¸å®¶) 1997 å¹´çš„è«–æ–‡ â€œThe EM Algorithm for Mixtures of Factor Analyzersâ€œ, å¯¦ä½œäº† Mixtures of Factor Analyzers, è‡¥æ§½! éƒ½20å¹´å»äº†! My python implementation, github. é—œæ–¼ EM çš„éƒ¨åˆ†æœƒæ¯”è¼ƒç²¾ç°¡, æƒ³çœ‹æ›´å¤šæè¿°æ¨è–¦ç›´æ¥çœ‹ PRML book. æ–‡ç« ä¸»è¦åˆ†ä¸‰å€‹éƒ¨åˆ† ä»€éº¼æ˜¯ Factor Analysis, ä»¥åŠå®ƒçš„ EM è§£ æ¨å»£åˆ° mixtures models èªè€…è­˜åˆ¥ä¸­å¾ˆé—œéµçš„ ivector ç©¶ç«Ÿè·Ÿ FA æœ‰ä»€éº¼é—œè¯? ç›´æ¥é€²å…¥æ­£é¡Œå§~ Factor Analysisä¸€è¨€ä»¥è”½ä¹‹, sub-space é™ç¶­. å‡è¨­æˆ‘å€‘éƒ½æ´»åœ¨é™°é­‚ä¸æ•£çš„ Gauss ä¸–ç•Œä¸­, æ‰€æœ‰ model éƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒ. æˆ‘å€‘è§€å¯Ÿçš„è³‡æ–™ $x$ éƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒ, ä¸”éƒ½æ˜¯é«˜ç¶­åº¦. ä½†å¯¦éš›ä¸Š $x$ é€šå¸¸åªç”±å°‘æ•¸å¹¾å€‹çœ‹ä¸åˆ°çš„è®Šæ•¸æ§åˆ¶, ä¸€èˆ¬ç¨±é€™äº›çœ‹ä¸åˆ°çš„è®Šæ•¸ç‚º latent variable $z$. å¦‚ä¸‹åœ–èˆ‰ä¾‹: æ‰€ä»¥æˆ‘å€‘çš„ä¸»è¦å•é¡Œå°±æ˜¯, æ€éº¼å° Gaussian distribution å»ºç«‹ sub-space æ¨¡å‹? ç­”æ¡ˆå°±æ˜¯ä½¿ç”¨ Linear Gaussian Model. ä¸€äº› notations å®šç¾©: $x$ è¡¨ç¤ºæˆ‘å€‘çš„ observation, ç¶­åº¦æ˜¯ $D$. $z$ æ˜¯æˆ‘å€‘çš„ latent variable, ç¶­åº¦æ˜¯ $K$. æˆ‘å€‘ä¸€èˆ¬éƒ½æœŸæœ› $K \\ll D$. $x$ and $z$ follow linear-Gaussian framework, æœ‰å¦‚ä¸‹çš„é—œä¿‚: $$\\begin{align} p(z)=N(z|0,I) \\\\ p(x|z)=N(x|Wz+\\mu,\\Psi) \\\\ \\end{align}$$ $W$ æ˜¯ä¸€å€‹ç·šæ€§è½‰æ›, å°‡ä½ç¶­åº¦çš„ latent space è½‰æ›åˆ°é«˜ç¶­åº¦çš„ observation space, å¦å¤– $\\Psi$ å¿…é ˆæ˜¯å°è§’çŸ©é™£. ç”±æ–¼æ˜¯å°è§’çš„é—œä¿‚, å› æ­¤ $\\Psi$ æ•æ‰äº† obaservation ç¶­åº¦çš„å„è‡ªè®Šç•°é‡, å› æ­¤ç¨±ç‚º uniquenesses, è€Œ $W$ å°±æ˜¯è² è²¬æ•æ‰å…±åŒé …, ç¨±ç‚º factor loading. æ›¸è£¡æœ‰ä¸€å€‹ç°¡å–®æ˜ç­çš„åœ–è§£é‡‹ä¸Šè¿°çš„æ¨¡å‹, æˆ‘å°±ä¸å¤šèªªäº†, è‡ªè¡Œçœ‹åœ–: å› ç‚ºæ˜¯ linear-Gaussian model, æ‰€ä»¥ marginal distribution ä¹Ÿæ˜¯ Gaussian: $$\\begin{align} p(x)=N(x|\\mu,C) \\\\ \\mbox{where } C=WW^T+\\Psi \\end{align}$$ åŒæ™‚, äº‹å¾Œæ©Ÿç‡ä¹Ÿæ˜¯ Gaussian $$\\begin{align} p(z|x)=N(z|GW^T\\Psi^{-1}(x-\\bar{x}),G^{-1}) \\\\ \\mbox{where } G=(I+W^T\\Psi^{-1}W)^{-1} \\end{align}$$ å®Œæ•´çš„ lineaer-Gaussian model å…¬å¼, from PRML book: æœ‰äº† $p(x)$ (å¼ 3) åŸºæœ¬ä¸Šæˆ‘å€‘å°±å¯ä»¥æ ¹æ“š training data ç®—å‡º likelihood, ç„¶å¾Œæ‰¾å‡ºä»€éº¼æ¨£çš„åƒæ•¸å¯ä»¥æœ€å¤§åŒ–å®ƒ. ä½†æ˜¯é€™è£¡çš„å•é¡Œæ˜¯å«æœ‰æœªçŸ¥è®Šæ•¸ $z$, é€™å€‹åœ¨ training data çœ‹ä¸åˆ°, å› ç‚ºæˆ‘å€‘åªçœ‹çš„åˆ° $x$. ä¸éåˆ¥æ“”å¿ƒ, EM æ¼”ç®—æ³•å¯ä»¥è™•ç†å«æœ‰æœªçŸ¥è®Šæ•¸æƒ…æ³ä¸‹çš„ maximal likelihood estimation. å¿˜äº†ä»€éº¼æ˜¯ EM, å¯ä»¥åƒè€ƒä¸€ä¸‹é€™. å¾ˆç²¾ç°¡çš„è¬›ä¸€ä¸‹å°±æ˜¯, æ‰¾åˆ°ä¸€å€‹è¼”åŠ©å‡½æ•¸ $Q$, è©²è¼”åŠ©å‡½æ•¸ä¸€å®šå°æ–¼åŸä¾†çš„ likelihood å‡½æ•¸, å› æ­¤åªè¦æ‰¾åˆ°ä¸€çµ„åƒæ•¸å¯ä»¥å°è¼”åŠ©å‡½æ•¸æœ€å¤§åŒ–, é‚£éº¼å°æ–¼åŸä¾†çš„ likelihood å‡½æ•¸ä¹Ÿæœƒæœ‰æå‡, é‡è¤‡ä¸‹å»å°±å¯ä»¥æŒçºŒæå‡, ç›´åˆ° local maximum.å¦å¤–è¼”åŠ©å‡½æ•¸å°±æ˜¯ â€œcomplete-data log likelihood and take its expectation with respect to the posterior distribution of the latent distribution evaluated using â€˜oldâ€™ parameter valuesâ€, æˆ‘çŸ¥é“å¾ˆç²—ç•¥, é‚„è«‹è‡ªè¡Œçœ‹ç­†è¨˜æˆ–æ˜¯ PRML Ch9. E-StepE-Step ä¸»è¦ç®—å‡ºåŸºæ–¼èˆŠåƒæ•¸ä¸‹çš„äº‹å¾Œæ©Ÿç‡çš„ä¸€éšäºŒéšçµ±è¨ˆé‡ é¦–å…ˆå°‡ç¬¦è™Ÿåšç°¡åŒ–, æ–¹ä¾¿å¾Œé¢çš„å¼å­æ›´ç°¡æ½” ($n$ æ˜¯è¨“ç·´è³‡æ–™çš„ index): $$\\mathbb{E}[z_n]\\equiv\\mathbb{E}_{z_n|x_n}[z_n] \\\\ \\mathbb{E}[z_nz_n^T]\\equiv\\mathbb{E}_{z_n|x_n}[z_nz_n^T] \\\\$$ äº‹å¾Œæ©Ÿç‡çš„ä¸€éšäºŒéšçµ±è¨ˆé‡å¦‚ä¸‹: $$\\begin{align} \\mathbb{E}[z_n] = GW^T\\Psi^{-1}(x_n-\\mu) \\\\ \\mathbb{E}[z_nz_n^T] = G + \\mathbb{E}[z_n] \\mathbb{E}[z_n]^T \\\\ \\mbox{where } G=(I+W^T\\Psi^{-1}W)^{-1} \\end{align}$$ å› ç‚ºäº‹å¾Œæ©Ÿç‡æ˜¯ Gaussian, æ‰€ä»¥ç”±å¼ (5) å¯ä»¥æ¨å¾—å¼ (7) å’Œ å¼ (8). M-Stepé€™ä¸€æ­¥å°±æ˜¯æœ€å¤§åŒ–è¼”åŠ©å‡½æ•¸ $Q$, å…¶ä¸­ $\\mu$ ç­‰æ–¼ sample mean, å¯ä»¥ç›´æ¥å¯«æ­»ä¸éœ€è¦ iteration. å¦å¤–å…©å€‹åƒæ•¸ update å¦‚ä¸‹: $$\\begin{align} W^{new}=\\left[\\sum_{n=1}^N (x_n-\\mu)\\mathbb{E}[z_n]^T\\right]\\left[\\sum_{n=1}^N \\mathbb{E}[z_nz_n^T]\\right]^{-1} \\\\ \\Psi^{new}=\\mbox{diag}\\left[S-W^{new}\\frac{1}{N}\\sum_{n=1}^N \\mathbb{E}[z_n](x_n-\\mu)^T\\right] \\end{align}$$ $S$ æ˜¯ sample covariance matrix (é™¤ N çš„é‚£å€‹ biased) Toy Exampleé»‘è‰²é‚£æ¢ç·šæ˜¯çœŸæ­£ç”¢ç”Ÿè³‡æ–™æ™‚çš„ $W$, å¯ä»¥ç•¶æˆæ­£ç¢ºç­”æ¡ˆ. ç´…è‰²çš„æ˜¯ FA ä¼°è¨ˆå‡ºä¾†çš„ $W$ å’Œ $p(x)$. å¯ä»¥ç™¼ç¾ $W$ æ²’æœ‰è·Ÿæ­£ç¢ºç­”æ¡ˆä¸€æ¨£, é€™æ˜¯å› ç‚ºæˆ‘å€‘åœ¨åš maximum likelihood çš„æ™‚å€™, åªé—œå¿ƒ $p(x)$, å› æ­¤å¯ä»¥æœ‰ä¸åŒçš„ latent space ç”¢ç”Ÿç›¸åŒçš„ $p(x)$. ç¯„ä¾‹ä¹Ÿä¸€ä½µæŠŠ probabilistic PCA åšå‡ºä¾†äº†, å¯ä»¥ç™¼ç¾ PPCA ç®—çš„ $W$ è·Ÿæ­£ç¢ºç­”æ¡ˆå¾ˆæ¥è¿‘, é€™æ˜¯å› ç‚ºæ­¤ç¯„ä¾‹çš„è³‡æ–™å…¶å¯¦æ˜¯æ ¹æ“š PPCA çš„æ¨¡å‹ç”¢ç”Ÿçš„, æ‰€ä»¥ PPCA è¼ƒæ¥è¿‘æ˜¯æ­£å¸¸. åŒæ™‚æˆ‘å€‘çœ‹åˆ° PPCA ä¼°è¨ˆå‡ºä¾†çš„ $p(x)$ å…¶å¯¦ä¹Ÿè·Ÿ FA ä¸€æ¨£, å†åº¦ä½è­‰ FA å…¶å¯¦ä¹Ÿæ²’ç®—éŒ¯, åªæ˜¯ä¸åŒçš„è¡¨é”æ–¹å¼. Mixtures of Factor Analyzerså°‡ FA å‡è¨­æœ‰å¤šå€‹ components çµ„æˆå°±è®Šæˆ MFA äº†, å…¶å¯¦å°±è·Ÿ GMM ä¸€æ¨£, å·®åˆ¥åœ¨æ–¼æˆ‘å€‘ç”¨äº† latent space å»å„åˆ¥ model æ¯å€‹ Gaussian Components è€Œå·²! è¦æ³¨æ„çš„æ˜¯, é€™æ™‚å€™çš„ latent variables ä¸åªæœ‰ $z$, é‚„æœ‰ $m$ (=1~M è¡¨ç¤ºæœ‰ $M$ å€‹ components), æˆ‘å€‘ç”¨ä¸‹æ¨™ $j$ è¡¨ç¤º component çš„ index. å¦å¤–, æ¯ä¸€å€‹ component, æœƒæœ‰å„è‡ªçš„ latent space, å› æ­¤æœ‰å„è‡ªçš„ $W_j$ å’Œ $\\mu_j$, ä½†æ˜¯å…¨éƒ¨çš„ components å…±ç”¨ä¸€å€‹ uniquenesses $\\Psi$. $$\\begin{align} p(x|z,m=j)=N(x|W_j z+\\mu_j,\\Psi) \\end{align}$$ å’Œ GMM ä¸€æ¨£, æ¯ä¸€å€‹ component éƒ½æœ‰ä¸€å€‹ weights, $\\pi_j$, åˆèµ·ä¾†æ©Ÿç‡æ˜¯1 E-Stepä¸€éšå’ŒäºŒéšçµ±è¨ˆé‡å¦‚ä¸‹: $$\\begin{align} \\color{red}{\\mathbb{E}[z_n|m=j]} = G_j W_j^T \\Psi^{-1}(x_n-\\mu_j) \\\\ \\color{red}{\\mathbb{E}[z_nz_n^T|m=j]} = G_j + \\mathbb{E}[z_n|m=j] \\mathbb{E}[z_n|m=j]^T \\\\ \\mbox{where } G_j=(I+W_j^T\\Psi^{-1}W_j)^{-1} \\end{align}$$ è€ŒçœŸæ­£çš„äº‹å¾Œæ©Ÿç‡ç‚º: $$\\begin{align} \\mathbb{E}[m=j,z_n] = h_{nj}\\mathbb{E}[z_n|m=j] \\\\ \\mathbb{E}[m=j,z_nz_n^T] = h_{nj}\\mathbb{E}[z_nz_n^T|m=j] \\\\ \\mbox{where } \\color{red}{h_{nj}}=\\mathbb{E}[m=j|x_n]\\propto p(x_n,m=j) \\end{align}$$ å°‡ (18) è§£é‡‹æ¸…æ¥šä¸€ä¸‹, åŸºæœ¬ä¸Šå°±æ˜¯è¨ˆç®—çµ¦å®šä¸€å€‹ $x_n$, å®ƒæ˜¯ç”± component $j$ æ‰€ç”¢ç”Ÿçš„æ©Ÿç‡æ˜¯å¤šå°‘. æˆ‘å€‘å¯ä»¥é€²ä¸€æ­¥æ¨å°å¦‚ä¸‹: $$\\begin{align} p(x_n,m=j)=p(m=j)p(x_n)\\\\ =\\pi_j N(x_n|\\mu_j,C_j=W_jW_j^T+\\Psi) \\end{align}$$ (19) åˆ° (20) çš„éƒ¨åˆ†å¯ä»¥ç”± (3) å’Œ (4) æ‰€çŸ¥é“çš„ marginal distribution $p(x)$ å¾—åˆ° åˆ°é€™è£¡, æ‰€æœ‰éœ€è¦çš„çµ±è¨ˆé‡, ç´…è‰²éƒ¨åˆ†, æˆ‘å€‘éƒ½å¯ä»¥ç®—å¾—äº†. M-Stepé€šé€šå¾®åˆ†ç­‰æ–¼é›¶, é€šé€šå¾®åˆ†ç­‰æ–¼é›¶, é€šé€šå¾®åˆ†ç­‰æ–¼é›¶ â€¦ å¾—åˆ°: $$\\begin{align} \\pi_j^{new}=\\frac{1}{N}\\sum_{n=1}^N h_{nj} \\\\ \\mu_j^{new}=\\frac{\\sum_{n=1}^N h_{nj}x_n}{\\sum_{n=1}^N h_{nj}} \\\\ W_j^{new}=\\left[\\sum_{n=1}^N h_{nj}(x_n-\\mu_j)\\mathbb{E}[z_n|m=j]^T\\right]\\left[\\sum_{n=1}^N h_{nj}\\mathbb{E}[z_nz_n^T|m=j]\\right]^{-1} \\\\ \\Psi^{new}=\\frac{1}{N}\\mbox{diag}\\left[ \\sum_{nj} h_{nj} \\left( (x_n-\\mu_j) - W_j^{new}\\mathbb{E}[z_n|m=j] \\right)(x_n-\\mu_j)^T \\right] \\end{align}$$ Toy Example åœ–æ‡‰è©²å¾ˆæ¸…æ¥šäº†, æœ‰æ­£ç¢º model åˆ° data é€™å€‹ MFA é‚„çœŸçš„ä¸å®¹æ˜“å¯¦ä½œ, å¯«èµ·ä¾†å¾ˆå¤šè¦æ³¨æ„çš„åœ°æ–¹, å¾ˆç‡’è…¦é˜¿! ä¸éåšå®Œäº†ä¹‹å¾Œé —æœ‰æˆå°±æ„Ÿ~ i-vectorå…¶å¯¦æœƒæƒ³å¯«é€™ç¯‡ä¸»è¦æ˜¯å› ç‚ºèªè€…è­˜åˆ¥ä¸­çš„ ivector, è€Œ ivector åŸºæœ¬ä¸Šå°±æ˜¯ä¸€å€‹ FA. åœ¨è¨ˆç®— ivector æ™‚, æˆ‘å€‘æœƒå…ˆä¼°è¨ˆ Universal Background Model (UBM), å…¶å¯¦å°±æ˜¯æ‰€æœ‰èªè€…çš„æ‰€æœ‰èªéŸ³ç‰¹å¾µç®—å‡ºä¾†çš„ GMM. ä»¥ä¸‹åœ–ç‚ºä¾‹, UBM æœ‰ä¸‰å€‹ mixtures, ç”¨æ·¡è—è‰²è¡¨ç¤º. è€Œé‡å°æŸä¸€ä½ speaker, å…¶ GMM ç‚ºæ©˜è‰². å‚³çµ±ä¸Šæˆ‘å€‘å°‡æ‰€æœ‰ mixture çš„ mean ä¸²æ¥æˆä¸€å€‹é•·çš„å‘é‡, å‰‡è©²å‘é‡å°±å¯ä»¥ç•¶ä½œæ˜¯è©² GMM æ¨¡å‹çš„ä¸€å€‹ä»£è¡¨, ä¸¦ç¨±ç‚º supervector ä¸ä¸€èµ·ä¸²æ¥ covariance matrix å—? weight å‘¢? ç•¶ç„¶ä¹Ÿå¯ä»¥å…¨éƒ¨éƒ½ä¸²æˆä¸€å€‹éå¸¸é•·çš„å‘é‡, ä½†ç ”ç©¶è¡¨æ˜ mean å‘é‡å°±è¶³å¤ äº† supervector ç¶­åº¦ç‚º mfcc-dim x mixtureæ•¸, å¾ˆå®¹æ˜“æœ‰ 40x1024 é€™éº¼é«˜ç¶­! å› æ­¤ ivector å°±æ˜¯åˆ©ç”¨ FA çš„æ–¹æ³•å°‡ supervector é™ç¶­. é‚£å…·é«”æ€éº¼åšå‘¢? é¦–å…ˆæˆ‘å€‘è¦å…ˆç”¨ä¸€å€‹å°æŠ€å·§å°‡ â€œå¤šå€‹ Gaussiansâ€ (æ³¨æ„ä¸æ˜¯ GMM, å› ç‚ºæ²’æœ‰mixture weightçš„æ¦‚å¿µ, æ¯ä¸€å€‹ Gaussianéƒ½åŒç­‰é‡è¦) è½‰æ›æˆä¸€å€‹ Gaussain. è¦‹åœ–å¦‚ä¸‹: æˆ‘å€‘å¯ä»¥å¾ˆå®¹æ˜“é©—è­‰å…©é‚Šæ˜¯ç­‰åƒ¹çš„. è½‰æ›æˆä¸€å€‹ Gaussian å¥½è™•å°±æ˜¯æˆ‘å€‘å¯ä»¥ç›´æ¥ä½¿ç”¨ FA é™ç¶­, è€Œ ivector å°±æ˜¯è©² FA çš„ latent variable $z$. å¦‚åŒ (2) çš„å®šç¾©: $$\\begin{align} p(x|z)=N(x|Wz+\\mu,\\Sigma) \\\\ \\end{align}$$ é€™è£¡çš„ $\\mu$ æ˜¯ UBM çš„ supervector, $\\Sigma$ å‰‡å¦‚åŒä¸Šåœ–çš„å®šç¾©, æ˜¯ä¸€å€‹ block diagonal matrix, æ¯ä¸€å€‹ block å°æ‡‰ä¸€å€‹ UBM mixture çš„ covariance matrix. å› æ­¤ $\\mu$ å’Œ $\\Sigma$ éƒ½æ˜¯ä½¿ç”¨ UBM çš„åƒæ•¸. é‡å°å¼ (25) å»æ›´ä»”ç´°äº†è§£å…¶æ‰€ä»£è¡¨çš„ç‰©ç†æ„ç¾©æ˜¯å¾ˆå€¼å¾—çš„, æ‰€ä»¥æˆ‘å€‘å¤šèªªä¸€é». ç”±æ–¼æˆ‘å€‘å·²ç¶“çŸ¥é“é€™æ¨£çš„ä¸€å€‹ Gaussian å¯¦éš›ä¸Šä»£è¡¨äº†åŸä¾† mfcc space çš„å¤šå€‹ Gaussians. æ‰€ä»¥é‡å°æŸä¸€å€‹ç‰¹å®šçš„ ivector $z^*$ ç”±å¼ (25) å¾—çŸ¥, ä»–æœ‰å¯èƒ½ä»£è¡¨äº†ä¸‹åœ–æ©˜è‰²çš„ä¸‰å€‹ Gaussians (ä¹Ÿå› æ­¤å¯èƒ½ä»£è¡¨äº†æŸä¸€å€‹ speaker çš„æ¨¡å‹): åˆ°ç›®å‰ç‚ºæ­¢æ‰€æè¿°çš„ ivector å¯¦éš›ä¸Šæ˜¯æ ¹æ“šè‡ªå·±çš„ç†è§£å°‡ 2005 å¹´ â€œEigenvoice Modeling with Sparse Training Dataâ€œ è£¡çš„ Proposition 1 (p348) çš„è¨­å®šæè¿°å‡ºä¾†. å¦‚æœ‰éŒ¯èª¤é‚„è«‹ä¾†ä¿¡æŒ‡æ­£. è©²è¨­å®šä¸­, æ¯ä¸€å€‹ mfcc vector éƒ½æœƒäº‹å…ˆè¢«æ­¸é¡å¥½å±¬æ–¼å“ªä¸€å€‹ mixture, ç­‰æ–¼ç¡¬åˆ†é¡. ä½†æ˜¯å…¶å¯¦ä¸¦ä¸éœ€è¦, ä¸€å€‹æ˜é¡¯çš„æ”¹é€²æ–¹æ³•å°±æ˜¯ä½¿ç”¨å¾Œé©—æ¦‚ç‡ä¾†åšè»Ÿåˆ†é¡. ç›´æ¥çœ‹åœ–: ç›®å‰çš„ ivector è¨ˆç®—éƒ½ä½¿ç”¨é€™ç¨®æ–¹å¼, ä¾‹å¦‚ Microsoft Research çš„ MSR Identity Toolbox. è©² toolbox ä½¿ç”¨ â€œA Straightforward and Efficient Implementation of the Factor Analysis Model for Speaker Verificationâ€œ çš„å¯¦ä½œæ–¹å¼, å¯ä»¥ç”±è«–æ–‡çš„å¼ (2),(5) çœ‹å‡ºä½¿ç”¨å¾Œé©—æ¦‚ç‡çš„è¨­å®š. æœ€å¾Œå¤šèªªä¸€äº›èªè€…è­˜åˆ¥çš„äº‹æƒ…. ivector ä¸»è¦æ˜¯é‡å°åŸé«˜ç¶­ç©ºé–“ (mfcc-dim x componentæ•¸é‡) åšé™ç¶­, è€Œæ²’æœ‰å»é‡å°èªè€…çš„è¨Šæ¯. æ‰€ä»¥å‚³çµ±æµç¨‹æœƒå†ç¶“é WCCN + LDA, è€Œ LDA å°±æœƒé‡å°åŒä¸€å€‹èªè€…ç›¡é‡é è¿‘, è€Œä¸åŒèªè€…ç›¡é‡æ‹‰é–‹. ç¶“é LDA å¾Œå°±å¯ä»¥ç”¨ $cos$ è¨ˆç®—ç›¸ä¼¼åº¦é€²è¡Œèªè€…ä¹‹é–“çš„æ‰“åˆ†. ä½†äº‹å¯¦ä¸Š, æ›´å¥½çš„åšæ³•æ˜¯ç”¨ä¸€å€‹ PLDA åšæ›´å¥½çš„æ‰“åˆ†. é—œæ–¼ PLDA è«‹åƒè€ƒé€™é‚ŠåŸå§‹æ–‡ç«  â€œProbabilistic Linear Discriminant Analysis for Inferences About Identityâ€œ, è€Œ PLDA æ›´æ˜¯èˆ‡æœ¬ç¯‡çš„ FA è„«é›¢ä¸äº†é—œä¿‚! ç¸½é«”ä¾†èªª FA, MFA å°æ–¼ç›®å‰çš„èªè€…è­˜åˆ¥ç³»çµ±ä»ç„¶ååˆ†é—œéµ, ç¸±ä½¿ç›®å‰ Kaldi ä½¿ç”¨äº†æ·±åº¦å­¸ç¿’æ›¿æ›äº† ivector, ä½†å¾Œç«¯ä»ç„¶æ¥ PLDA. Reference è‡ªå·±å¯¦ä½œçš„ Python MFA (å« Toy examples) github Zoubin Ghahramani and Geoffrey E. Hinton, The EM Algorithm for Mixtures of Factor Analyzers Bishop PRML ä»¥å‰çš„ EM ç­†è¨˜ ä»¥å‰çš„ GMM EM ç­†è¨˜ i-vector åŸå§‹è«–æ–‡ PLDA åŸå§‹è«–æ–‡ Eigenvoice Modeling with Sparse Training Data A Straightforward and Efficient Implementation of the Factor Analysis Model for Speaker Verification MSR Identity Toolbox","tags":[{"name":"Factor Analysis","slug":"Factor-Analysis","permalink":"http://yoursite.com/tags/Factor-Analysis/"},{"name":"Expectation Maximization","slug":"Expectation-Maximization","permalink":"http://yoursite.com/tags/Expectation-Maximization/"},{"name":"ivector","slug":"ivector","permalink":"http://yoursite.com/tags/ivector/"}]},{"title":"Path-Planning-Udacity-Term3-Project1","date":"2018-02-06T15:38:48.000Z","path":"2018/02/06/Path-Planning-Udacity-Term3-Project1/","text":"åœäº†åŠå¹´ çš„ Udacity Self Driving Car (SDC) Program, çµ‚æ–¼åˆé–‹å§‹äº†. åšç‚º Term3 çš„ç¬¬ä¸€å€‹ Project, æˆ‘æŠ±è‘—é«˜åº¦çš„æœŸå¾…. ä¸éå®Œæˆå¾Œ, æœ‰é»å°å¤±æœ›. å¤±æœ›çš„åŸå› æ˜¯é€™å€‹ project è·Ÿèª²ç¨‹ä¸Šçš„é€£çµæ„Ÿè¦ºä¸æ˜¯é‚£éº¼æ˜é¡¯. ä¾‹å¦‚èª²ç¨‹ä¸Šæœ‰è¬›åˆ° A*, hybrid A* çš„ç®—æ³•, ä½† project æ˜¯æ¨¡æ“¬ highway drive, å› æ­¤ A* æ¯”è¼ƒä¸é©åˆ (é©åˆåœ¨ parking lot å ´æ™¯). å¦å¤–ä¹Ÿæœ‰æåˆ°æ€éº¼é™ä½ jerk (åŠ é€Ÿåº¦çš„å¾®åˆ†, ä¸»è¦æ˜¯ä¸èˆ’é©çš„ä¾†æº), ç•¶åƒè€ƒå…§å®¹æ˜¯å¾ˆå¥½, ä¸éåœ¨å¯« Project æ™‚æ„Ÿè¦ºä¹Ÿä¸å¤§éœ€è¦. é€™ç¯‡å°±æ˜¯å€‹ç´€éŒ„, æœƒå¾ˆæ°´. æ–¹æ³•å¾ˆç°¡å–®, ä¸€å¼µåœ–è§£æ±º: ego-car æ ¹æ“šè‡ªå·±æ‰€åœ¨çš„è»Šé“, æœ€å¤šå¯ä»¥æœ‰ä¸‰æ¢è·¯å¾‘é¸æ“‡, è·¯å¾‘å°±ç”¨ spline curve ç”¢ç”Ÿ, ç¢ºä¿å¤  smooth. åŒæ™‚æœ‰ sensor-fusion çš„è³‡æ–™å¯ä»¥çŸ¥é“å…¶ä»–è»Šå­çš„ç‹€æ³, ç„¶å¾Œåˆ©ç”¨ prediction model å»é æ¸¬å…¶ä»–è»Šçš„è·¯å¾‘ (æˆ‘å°±å–®ç´”ä½¿ç”¨ constant velocity ç·šæ€§è·¯å¾‘). å¦‚æœæœ‰ collision åœ¨æœªä¾†çš„ 1 or 1.5 ç§’, è©²è·¯å¾‘å°±ç„¡æ•ˆ. å¦å¤–, å¦‚æœæœ‰è»Šå­å¤ªé è¿‘, å°±æ¸›é€Ÿ. ego-car çš„æ¯å€‹è·¯å¾‘éƒ½æœƒæœ‰å„è‡ªçš„ cost, cost æ˜¯æ ¹æ“šä¸€äº›å–œå¥½, è­¬å¦‚å“ªä¸€æ¢è»Šé“å¯ä»¥è·‘å¾—æ¯”è¼ƒå¿«, å“ªä¸€æ¢è·¯å¾‘æ¯”è¼ƒä¸æœƒè·Ÿå…¶ä»–è»Šå­å¤ªæ¥è¿‘ç­‰ç­‰â€¦ é€™ Project æœ€éº»ç…©çš„å°±æ˜¯åœ¨è¨­è¨ˆ cost function, å’Œèª¿æ•´. (é‚„æœ‰ç†Ÿæ‚‰ project çš„ç¨‹å¼ç¢¼â€¦éº»ç…©é˜¿) å½±ç‰‡é€£çµ here. Project github here æ²’äº†, æ–‡ç« æ°´ä¸æ°´ ? å¥½æ°´é˜¿, çœŸå¿ƒè™›","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"}]},{"title":"Maximum Mutual Information in Speech Recognition","date":"2017-12-16T04:08:44.000Z","path":"2017/12/16/Maximum-Mutual-Information-in-Speech-Recognition/","text":"Maximum Mutual Information (MMI) åºåˆ—çš„é‘‘åˆ¥æ€§è¨“ç·´æ–¹æ³•å¾æ—©æœŸçš„ GMM-HMM, åˆ°ç¾ä»Šå°±ç®—ä½¿ç”¨äº†æ·±åº¦å­¸ç¿’åŒæ¨£ååˆ†æœ‰ç”¨, å¦‚ Kaldi chain model åœ¨ DNN-HMM çš„åŸºç¤ä¸ŠåŠ ä¸Šåºåˆ—é‘‘åˆ¥è¨“ç·´, æ€§èƒ½æœƒå†é€²ä¸€æ­¥æå‡. å‰ä¸€é™£å­è®€äº†ä¿æ£Ÿã€é„§åŠ›çš„é€™æœ¬ èªéŸ³è­˜åˆ¥å¯¦è¸, å°æˆ‘ä¾†èªªæ•´ç†å¾—æ»¿å¥½çš„, å°±æ˜¯æ•¸å­¸éƒ¨åˆ†çš„æ¨å°æœ‰é»ç°¡æ½”äº†äº›, æ‰€ä»¥é€™ç¯‡å°±åŸºæ–¼è©²æ›¸çš„æ¨å°, è£œé½Šäº†è¼ƒè©³ç´°çš„æ­¥é©Ÿ, ä¸¦ä¸”å˜—è©¦ä½¿ç”¨ Computational graph çš„æ–¹å¼ç†è§£ MMI çš„è¨“ç·´. é‚£éº¼å°±é–‹å§‹å§! ç”¨è‡ªå·±ç•«çš„ MMI çš„è¨ˆç®—åœ–è­œç•¶å°é¢å§ :) MMI æ•¸å­¸å®šç¾©å®šç¾©$o^m=o_1^m,...,o_t^m,...,o_{T_m}^m$æ˜¯è¨“ç·´æ¨£æœ¬è£¡ç¬¬ m å¥è©±çš„ observation (MFCC,fbank,â€¦) sequence, è©² sequence æœ‰ $T_m$ å€‹ observation vector. è€Œ$w^m=w_1^m,...,w_t^m,...,w_{N_m}^m$å‰‡æ˜¯è©²å¥è©±çš„æ­£ç¢º transcription, æœ‰ $N_m$ å€‹å­—. é€šé forced-alignment å¯ä»¥å¾—åˆ°ç›¸å°æ‡‰çš„ state sequence$s^m=s_1^m,...,s_t^m,...,s_{T_m}^m$MMI ç›®çš„å°±æ˜¯å¸Œæœ›æ¨¡å‹ç®—å‡ºçš„æ­£ç¢ºç­”æ¡ˆ sequence æ©Ÿç‡æ„ˆå¤§æ„ˆå¥½, åŒæ™‚éæ­£ç¢ºç­”æ¡ˆ (èˆ‡ä¹‹ç«¶çˆ­çš„å…¶ä»– sequences) çš„æ©Ÿç‡è¦æ„ˆå°æ„ˆå¥½, æ‰€ä»¥æ­£ç¢ºç­”æ¡ˆæ”¾åˆ†å­, éæ­£ç¢ºæ”¾åˆ†æ¯, æ•´é«”è¦æ„ˆå¤§æ„ˆå¥½. ç”±æ–¼è€ƒæ…®äº†ç«¶çˆ­ sequences çš„æœ€å°åŒ–, æ‰€ä»¥æ˜¯é‘‘åˆ¥æ€§è¨“ç·´. åˆæ­¤ç¨®æ–¹å§‹æ˜¯åŸºæ–¼æ•´å¥çš„ sequence è€ƒé‡, å› æ­¤æ˜¯åºåˆ—é‘‘åˆ¥æ€§è¨“ç·´. æ•¸å­¸å¯«ä¸‹ä¾†å¦‚ä¸‹:$$J_{MMI}(\\theta;S)=\\sum_{m=1}^M J_{MMI}(\\theta\\|o^m,w^m) \\\\ =\\sum_{m=1}^M \\log \\frac{ p(o^m\\|s^m,\\theta)^KP(w^m) }{ \\sum_w p(o^m\\|s^w,\\theta)^K P(w) }$$ ç‚ºäº†ç°¡å–®åŒ–, æˆ‘å€‘å‡è¨­åªæœ‰ä¸€æ¢è¨“ç·´èªéŸ³, æ‰€ä»¥å»æ‰ $m$ çš„æ¨™è¨˜, ç„¶å¾Œ $\\sum_m$ çœç•¥: $$\\begin{align} J_{MMI}(\\theta\\|o,w) =\\log \\frac{ p(o\\|s,\\theta)^KP(w) }{ \\sum_w p(o\\|s^w,\\theta)^K P(w) } \\end{align}$$ æ¥è‘—æˆ‘å€‘è¦ç®—é‡å° $\\theta$ çš„å¾®åˆ†, æ‰å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•: $$\\begin{align} \\triangledown_\\theta J_{MMI}(\\theta\\|o,w) =\\sum_t \\triangledown_{z_t^L}J_{MMI}(\\theta\\|o,w)\\frac{\\partial z_t^L}{\\partial\\theta} \\\\ =\\sum_t e_t^L\\frac{\\partial z_t^L}{\\partial\\theta} \\end{align}$$ å…¶ä¸­å®šç¾©$e_t^L=\\triangledown_{z_t^L}J_{MMI}(\\theta\\|o,w)$ èªéŸ³è²å­¸æ¨¡å‹ (AM) å‚³çµ±ä¸Šä½¿ç”¨ GMM ä¾† model, è€Œç¾åœ¨éƒ½æ˜¯åŸºæ–¼ DNN, å…¶ä¸­æœ€å¾Œçš„ output layer å‡è¨­ç‚ºç¬¬ $L$ å±¤: $z_t^L$, éäº† softmax ä¹‹å¾Œæˆ‘å€‘å®šç¾©ç‚º $v_t^L$, è€Œå…¶ index $r$, $v_t^L(r)=P(r|o_t)$ å°±æ˜¯çµ¦å®šæŸä¸€å€‹æ™‚é–“ $t$ çš„ observation $o_t$ æ˜¯ state $r$ çš„æ©Ÿç‡. è®€è€…åˆ¥ç·Šå¼µ, æˆ‘å€‘ç”¨ Computational graph çš„æ–¹å¼å°‡ä¸Šå¼ç›´æ¥ç•«å‡ºä¾†: MMI Computational Graph è¡¨é” ä¸Šåœ–ç”¨ computational graph æ¸…æ¥šçš„è¡¨é”äº†å¼ (3) çš„è¨ˆç®—, å› ç‚ºæ‰€æœ‰åƒæ•¸ $\\theta$ åœ¨æ‰€æœ‰çš„æ™‚é–“ $t$ ä¸Šæ˜¯å…±äº«çš„, å› æ­¤è¦ $\\sum_t$, ä¹Ÿå°±æ˜¯è¦ç´¯åŠ ä¸Šåœ–æ‰€æœ‰ç´…è‰²çš„ gradient path. è¨ˆç®— $\\partial z_t^L / \\partial\\theta$ å¾ˆå®¹æ˜“, å°±æ˜¯ DNN çš„è¨ˆç®—åœ–è­œçš„ gradient, å› æ­¤é‡é»å°±åœ¨å¦‚ä½•è¨ˆç®— $e_t^L$, è€Œæ•´å€‹ MMI æœ€æ ¸å¿ƒçš„åœ°æ–¹å°±æ˜¯åœ¨è¨ˆç®—é€™å€‹äº†! MMI æ•¸å­¸æ¨å°æˆ‘å€‘æŠŠ $e_t^L(i)$ (å°±æ˜¯$e_t^L$é€™å€‹å‘é‡çš„ç¬¬$i$å€‹element)è¨ˆç®—å¦‚ä¸‹: $$\\begin{align} e_t^L(i)=\\triangledown_{z_t^L(i)}J_{MMI}(\\theta\\|o,w) \\\\ =\\sum_r \\frac{\\partial J_{MMI}(\\theta\\|o,w)}{\\partial\\log p(o_t|r)}\\frac{\\partial\\log p(o_t|r)}{\\partial z_t^L(i)} \\end{align}$$ å…ˆè§£é‡‹ä¸€ä¸‹ $\\log p(o_t|r)$ é€™å€‹ term, å¯ä»¥é‡å¯«æˆ$$\\begin{align} \\log p(o_t|r)=\\log \\color{red}{p(r|o_t)} + \\log p(o_t) - \\log p(r) = \\log \\color{red}{v_t^L(r)} + \\log p(o_t) - \\log p(r) \\end{align}$$æ‰€ä»¥é€™å€‹ term æ˜¯è·Ÿ $v_t^L(r)$ ç›¸é—œçš„, è€Œç”±æ–¼ $v_t^L$ æ˜¯ $z_t^L$ ç¶“é softmax å¾—åˆ°, å› æ­¤å¼(5)æ‰æœƒæœ‰ $\\sum_r$.æ ¹æ“šå¼ (6), æˆ‘å€‘å¯ä»¥å¾ˆå¿«ç®—å¾—å¼ (5) çš„ç¬¬äºŒå€‹åˆ†å­åˆ†æ¯é …å¦‚ä¸‹:$$\\begin{align} \\frac{\\partial\\left[\\log v_t^L(r) + \\log p(o_t) - \\log p(r)\\right]}{\\partial z_t^L(i)}=\\frac{\\partial \\log v_t^L(r)}{\\partial z_t^L(i)} \\end{align}$$å¾ˆæ˜é¡¯å› ç‚º $\\log p(o_t)$ å’Œ $\\log p(r)$ éƒ½è·Ÿ $z_t^L(i)$ ç„¡é—œæ‰€ä»¥å»æ‰.ç‚ºäº†è¨ˆç®—å¼ (5) çš„ç¬¬ä¸€å€‹åˆ†å­åˆ†æ¯é …, æˆ‘å€‘æŠŠå…ˆæŠŠå¼ (1) çš„ log é …æ‹†é–‹:$$\\begin{align} J_{MMI}(\\theta\\|o,w)= K\\color{green}{\\log p(o\\|s,\\theta)}+\\color{blue}{\\log p(w)} - \\color{orange}{\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]} \\end{align}$$æ‰€ä»¥$$\\begin{align} \\frac{\\partial J_{MMI}(\\theta\\|o,w)}{\\partial \\log p(o_t|r)}= K\\color{green}{ \\frac{\\partial\\log p(o\\|s,\\theta)}{\\partial \\log p(o_t|r)} } + \\color{blue}{ \\frac{\\partial\\log p(w)}{\\partial \\log p(o_t|r)} } - \\color{orange}{ \\frac{\\partial\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]}{\\partial \\log p(o_t|r)} } \\end{align}$$ ç¶ è‰²éƒ¨åˆ†æ³¨æ„åˆ° $\\log p(o|s,\\theta)$ åœ¨ HMM çš„æƒ…æ³ä¸‹, æ˜¯çµ¦å®š state sequence çš„è§€æ¸¬æ©Ÿç‡å€¼, å› æ­¤åªæ˜¯æ¯å€‹ state æ™‚é–“é»çš„ emission probability, æ‰€ä»¥$$\\begin{align} \\log p(o\\|s,\\theta)= \\sum_{t&apos;} \\log p(o_{t&apos;}\\|s_{t&apos;},\\theta) \\end{align}$$è€Œåªæœ‰ $tâ€™=t$ æ™‚èˆ‡å¾®åˆ†é …æœ‰é—œ, å› æ­¤è®Šæˆ$$\\begin{align} \\frac{\\partial\\log p(o\\|s,\\theta)}{\\partial \\log p(o_t\\|r)}= \\frac{\\partial \\log p(o_t\\|s_t,\\theta)}{\\partial \\log p(o_t\\|r)}=\\delta(r=s_t) \\end{align}$$ è—è‰²éƒ¨åˆ†èˆ‡å¾®åˆ†é …ç„¡é—œï¼Œå› æ­¤$$\\begin{align} \\frac{\\partial\\log p(w)}{\\partial \\log p(o_t|r)}=0 \\end{align}$$ æ©˜è‰²éƒ¨åˆ†$$\\begin{align} \\frac{\\partial\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]}{\\partial \\log p(o_t|r)}= \\frac{1}{\\sum_w p(o\\|s^w,\\theta)^K p(w)}\\times\\frac{\\partial \\sum_w \\color{red}{p(o\\|s^w,\\theta)}^K p(w)}{\\partial \\log p(o_t|r)} \\end{align}$$ ç´…è‰²çš„éƒ¨åˆ†å¦‚åŒä¸Šé¢ç¶ è‰²é …çš„è¨è«–, åªæœ‰æ™‚é–“é» $t$ æ‰è·Ÿå¾®åˆ†é …æœ‰é—œ, ä¸åŒçš„æ˜¯é€™æ¬¡æ²’æœ‰ $\\log$ å› æ­¤æ˜¯é€£ä¹˜, å¦‚æœ $s_t\\neq r$ æ•´æ¢ sequence çš„æ©Ÿç‡èˆ‡å¾®åˆ†é …ç„¡é—œ, å› æ­¤åªæœƒä¿ç•™ $s_t=r$ çš„é‚£äº› $w$ sequences.å¦å¤–,$\\frac{\\partial p(o_t\\|r)^K}{\\partial\\log p(o_t\\|r)} \\mbox{ å¯æƒ³æˆ } \\frac{\\partial e^{Kx}}{\\partial x} = Ke^{Kx}$ç¶œåˆä»¥ä¸Šè¨è«–æ©˜è‰²éƒ¨åˆ†ç‚º$$\\begin{align} \\frac{\\partial\\log\\left[\\sum_w p(o\\|s^w,\\theta)^K p(w)\\right]}{\\partial \\log p(o_t|r)}= K\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)} \\end{align}$$ å…¨éƒ¨å¸¶å…¥ä¸¦æ•´ç† $e_t^L(i)$å°‡ (11),(12),(14) ä»£å›åˆ° (9) æˆ‘å€‘å¾—åˆ°$$\\begin{align} \\frac{\\partial J_{MMI}(\\theta\\|o,w)}{\\partial \\log p(o_t|r)}= K\\left(\\delta(r=s_t)-\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)}\\right) \\end{align}$$ç¹¼çºŒå°‡ (15),(7) ä»£å›åˆ° (5) æˆ‘å€‘çµ‚æ–¼å¯ä»¥å¾—åˆ° $e_t^L(i)$ çš„çµæœäº†!$$\\begin{align} e_t^L(i)=\\sum_r K\\left(\\delta(r=s_t)-\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)}\\right) \\times \\frac{\\partial \\log v_t^L(r)}{\\partial z_t^L(i)} \\\\ = \\sum_r K\\left(\\delta(r=s_t)-\\color{red}{\\gamma_t^{DEN}(r)}\\right) \\times \\frac{\\partial \\log v_t^L(r)}{\\partial z_t^L(i)} \\\\ =K\\left(\\delta(i=s_t)-\\gamma_t^{DEN}(i)\\right) \\end{align}$$å…¶ä¸­ä¸€å€‹å¾ˆé‡è¦çš„å®šç¾©$$\\begin{align} \\gamma_t^{DEN}(r)=\\frac{\\sum_{w:s_t=r}p(o\\|s,\\theta)^K p(w)}{\\sum_w p(o\\|s^w,\\theta)^K p(w)} \\end{align}$$ç‰©ç†æ„ç¾©å°±æ˜¯æ™‚é–“$t$åœ¨ç‹€æ…‹$r$çš„æ©Ÿç‡! ç†è«–ä¸Šä¾†èªªæˆ‘å€‘è¦å–éæ‰€æœ‰å¯èƒ½çš„ word sequence $w$ ä¸¦æ±‚å’Œè¨ˆç®—, ä½†å¯¦éš›ä¸Šåªæœƒåœ¨ decoding æ™‚çš„ lattice ä¸Šè¨ˆç®—, ä»¥ç¯€çœæ™‚é–“. åˆ°ç›®å‰ç‚ºæ­¢æˆ‘å€‘ç®—å®Œäº† MMI æœ€å›°é›£çš„éƒ¨åˆ†äº†, å¾—åˆ° $e_t^L(i)$ å¾Œ (å¼(18))ï¼Œå‰©ä¸‹çš„å°±åªæ˜¯ follow ä¸Šåœ–çš„ MMI computational graph å»åš. æœ‰è®€è€…ä¾†ä¿¡è©¢å•å¼ (17) å¦‚ä½•æ¨å°è‡³ (18), éç¨‹å¦‚ä¸‹åœ–: (æŠ±æ­‰å·æ‡¶ä¸æ‰“ Latex äº†) çµè«–é‚„æœ‰ä¸€äº›å…¶ä»–è®Šç¨®å¦‚ boost MMI (bMMI)ã€MPEã€MCEç­‰ç­‰, å·®åˆ¥åªæ˜¯åœ¨æœ€å°åŒ–ä¸åŒçš„æ¨™è¨»ç²¾ç´°åº¦, æœ€é‡è¦çš„é‚„æ˜¯è¦å…ˆäº†è§£ MMI å°±å¯ä»¥å®¹æ˜“æ¨å»£äº†. é€™äº›éƒ½æœ‰ä¸€å€‹çµ±ä¸€çš„è¡¨é”æ³•å¦‚ä¸‹:$$\\begin{align} e_t^L(i)=K\\left(\\gamma_t^{DEN}(i)-\\gamma_t^{NUM}(i)\\right) \\end{align}$$æ³¨æ„åˆ°æ­£è² è™Ÿè·Ÿ (18) ç›¸å, å› ç‚ºåªæ˜¯ä¸€å€‹æœ€å¤§åŒ–æ”¹æˆæœ€å°åŒ–è¡¨ç¤ºè€Œå·². ä¸¦ä¸”å¤šäº†ä¸€å€‹åˆ†å­çš„ lattice è¨ˆç®—. Reference ä¿æ£Ÿã€é„§åŠ›: èªéŸ³è­˜åˆ¥å¯¦è¸ Ch8 Kaldi chain model","tags":[{"name":"Speech Recognition","slug":"Speech-Recognition","permalink":"http://yoursite.com/tags/Speech-Recognition/"},{"name":"Maximum Mutual Information","slug":"Maximum-Mutual-Information","permalink":"http://yoursite.com/tags/Maximum-Mutual-Information/"},{"name":"Computational Graph","slug":"Computational-Graph","permalink":"http://yoursite.com/tags/Computational-Graph/"}]},{"title":"TF Notes (3), Computational Graph in Tensorflow","date":"2017-11-29T12:36:59.000Z","path":"2017/11/29/TF-Notes-Computational-Graph-in-Tensorflow/","text":"é€™ç¯‡ä»‹ç´¹ computational graph (è¨ˆç®—åœ–è­œ), ä¸»è¦ä¾†æºåƒè€ƒè‡ªæå®æ¯…æ•™æˆçš„èª²ç¨‹å…§å®¹. ä¸¦ä¸”æˆ‘å€‘ä½¿ç”¨ tensorflow çš„æ±‚å°å‡½æ•¸ tf.gradients ä¾†é©—è­‰ computational graph. æœ€å¾Œæˆ‘å€‘åœ¨ MNIST ä¸Šé©—è­‰æ•´å€‹ DNN/CNN çš„ backpropagation å¯ä»¥åˆ©ç”¨ computational graph çš„è¨ˆç®—æ–¹å¼è¨“ç·´. Computational Graphä¸»è¦å°±æˆªåœ–ææ•™æˆçš„æŠ•å½±ç‰‡, ä¸€å€‹ computational graph çš„ node å’Œ edge å¯ä»¥å®šç¾©å¦‚ä¸‹ å°æ–¼ chain rule çš„è©±, æˆ‘å€‘çš„è¨ˆç®—åœ–è­œå¯ä»¥é€™éº¼ç•« å…¶å¯¦å°±æ˜¯ chain rule ç”¨é€™æ¨£çš„åœ–å½¢è¡¨ç¤º. æ¯”è¼ƒè¦æ³¨æ„çš„å°±æ˜¯ case 2 çš„æƒ…æ³, ç”±æ–¼ $x$ and $y$ éƒ½æœƒè¢« $s$ å½±éŸ¿, å› æ­¤è¨ˆç®— gradients æ™‚è¦ç´¯åŠ å…©æ¢è·¯å¾‘. å†ä¾†å¦ä¸€é …è¦æ³¨æ„çš„æ˜¯å¦‚æœæœ‰ share variables æˆ‘å€‘çš„è¨ˆç®—åœ–è­œè©²æ€éº¼è¡¨ç¤ºå‘¢ ? èˆ‰ä¾‹ä¾†èªªå¦‚ä¸‹çš„å‡½å¼$y=x\\cdot e^{x^2}$ è¨ˆç®—åœ–è­œç•«å‡ºä¾†é•·é€™æ¨£ ç°¡å–®ä¾†èªªæŠŠç›¸åŒè®Šæ•¸çš„ nodes ä¸Šæ‰€æœ‰çš„è·¯å¾‘éƒ½ç›¸åŠ èµ·ä¾†. ä¸Šé¢çš„ç¯„ä¾‹å°±æ˜¯è¨ˆç®— $\\frac{\\partial y}{\\partial x}$ æ™‚, æœ‰ä¸‰æ¢è·¯å¾‘æ˜¯å¾ node $x$ å‡ºç™¼æœ€çµ‚æœƒå½±éŸ¿åˆ° node $y$ çš„, è®€è€…æ‡‰è©²å¯ä»¥å¾ˆå®¹æ˜“çœ‹å‡ºä¾†. å¦å¤–å¦‚æœåˆ†åˆ¥è¨ˆç®—é€™ä¸‰æ¢è·¯å¾‘, å…¶å¯¦å¾ˆå¤š edges çš„æ±‚å°çµæœæœƒé‡è¤‡, å› æ­¤å¾ $x$ å‡ºç™¼è¨ˆç®—åˆ° $y$ æœƒå¾ˆæ²’æœ‰æ•ˆç‡, æ‰€ä»¥åéä¾† (Reverse mode) å¾ root ($y$) å‡ºç™¼, åå‘æ‰¾å‡ºè¦æ±‚çš„ nodes ($x$) å°±å¯ä»¥é¿å…å¾ˆå¤šé‡è¤‡é‹ç®—. Verify with Tensorflowè€ƒæ…®ä»¥ä¸‹ç¯„ä¾‹, å…¶ä¸­ $&lt;,&gt;$ è¡¨ç¤ºå…§ç©, $a$, $b$, å’Œ $1$ éƒ½æ˜¯å±¬æ–¼ $R^3$ çš„å‘é‡. å®ƒçš„è¨ˆç®—åœ–è­œå¦‚ä¸‹: åœ¨ Tensorflow ä¸­, tf.gradients å¯ä»¥å¹«åŠ©è¨ˆç®— gradients. èˆ‰ä¾‹ä¾†èªªå¦‚æœæˆ‘å€‘è¦è¨ˆç®— $\\frac{\\partial e}{\\partial c}$, æˆ‘å€‘åªè¦é€™æ¨£å‘¼å«å³å¯ ge_c=tf.gradients(ys=e,xs=c). ç‚ºäº†æ–¹ä¾¿, æˆ‘å€‘å°‡ $\\frac{\\partial y}{\\partial x}$ åœ¨ç¨‹å¼è£¡å‘½åç‚º gy_x. ä¸‹é¢é€™æ®µ codes è¨ˆç®—å‡ºä¸Šåœ– 5 å€‹ edges çš„ gradients: 123456789101112131415161718import tensorflow as tfimport numpy as npa = tf.placeholder(tf.float32, shape=(1,3))b = tf.placeholder(tf.float32, shape=(1,3))c = a + bd = b + 1e = tf.matmul(c,d,transpose_b=True)ge_c, ge_d = tf.gradients(ys=e,xs=[c,d])gc_a, gc_b = tf.gradients(ys=c,xs=[a,b])gd_b = tf.gradients(ys=d,xs=b)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) gec, ged, gca, gcb, gdb = sess.run([ge_c, ge_d, gc_a, gc_b, gd_b],feed_dict=&#123;a:[[2,1,0]],b:[[1,2,3]]&#125;) print('ge_c=&#123;&#125;\\nge_d=&#123;&#125;\\ngc_a=&#123;&#125;\\ngc_b=&#123;&#125;\\ngd_b=&#123;&#125;'.format(gec, ged, gca, gcb, gdb)) è¨ˆç®—çµæœç‚º 12345ge_c=[[ 2. 3. 4.]]ge_d=[[ 3. 3. 3.]]gc_a=[[ 1. 1. 1.]]gc_b=[[ 1. 1. 1.]]gd_b=[array([[ 1., 1., 1.]], dtype=float32)] å¯ä»¥è‡ªå·±æ‰‹ç®—é©—è­‰ä¸€ä¸‹, çµæœç•¶ç„¶æ˜¯å°çš„ (ä½¿ç”¨ Jacobian matrix è¨ˆç®—) æ‰€ä»¥æˆ‘å€‘å¦‚æœè¦å¾—åˆ° $\\frac{\\partial e}{\\partial b}$, æˆ‘å€‘åªè¦ç®— ge_c*gc_b + ge_d*gd_b å°±å¯ä»¥äº†. ä¸éé€™æ¨£è‡ªå·±æŠŠç›¸åŒè·¯å¾‘åšç›¸ä¹˜ï¼Œä¸åŒè·¯å¾‘åšç›¸åŠ , å¤ªéº»ç…©äº†! å…¶å¯¦æœ‰æ›´å¥½çš„æ–¹æ³•. å°æ–¼åŒä¸€æ¢è·¯å¾‘åšç›¸ä¹˜, tf.gradients æœ‰ä¸€å€‹ arguments æ˜¯ grad_ys å°±å¯ä»¥å¾ˆå®¹æ˜“åšåˆ°. tf.gradients(ys=,xs=,grad_ys=) ä»¥ä¸‹åœ–ä¾†èªªæ˜ ä½†äº‹å¯¦ä¸Šæ ¹æœ¬ä¹Ÿä¸ç”¨é€™éº¼éº»ç…©, é™¤éæ˜¯é‡åˆ°å¾ˆç‰¹æ®Šçš„ç‹€æ³, å¦å‰‡æˆ‘å€‘ç›´æ¥å‘¼å« tf.gradients(c,a), tensorflow å°±æœƒç›´æ¥å¹«æˆ‘å€‘æŠŠåŒæ¨£è·¯å¾‘çš„ gradients åšç›¸ä¹˜, ä¸åŒè·¯å¾‘çš„ gradients çµæœåšç›¸åŠ äº†! æ‰€ä»¥ä¸Šé¢å°±ç›´æ¥å‘¼å« tf.gradients(c,a) å…¶å¯¦ä¹Ÿå°±ç­‰æ–¼ gb_a2 äº†. æœ€å¾Œå›åˆ°é–‹å§‹çš„ e=&lt;a+b,b+1&gt; çš„ç¯„ä¾‹, å¦‚æœè¦è¨ˆç®— $\\frac{\\partial e}{\\partial b}$, ç…§åŸæœ¬æä¾›çš„ codes éœ€è¦è¨ˆç®—ä¸‰æ¢è·¯å¾‘å„è‡ªç›¸ä¹˜å¾Œå†ç›¸åŠ , å…¶å¯¦åªè¦ç›´æ¥å‘¼å« ge_b=tf.gradients(e,b) å°±å®Œæˆé€™ä»¶äº‹äº† (ge_c*gc_b + ge_d*gd_b) MNIST ç”¨è¨ˆç®—åœ–è­œè¨ˆç®— back propagationDNN çš„è¨ˆç®—åœ–è­œç‚ºäº†æ¸…æ¥šäº†è§£ Neural network çš„ backpropagation å¦‚ä½•ç”¨ computational graph ä¾†è¨ˆç®— gradients ä¸¦é€²è€Œ update åƒæ•¸, æˆ‘å€‘ä¸ä½¿ç”¨ tf.optimizer å¹«æˆ‘å€‘è‡ªå‹•è¨ˆç®—. ä¸€å€‹ 3 layers fo MLP-DNN çš„è¨ˆç®—åœ–è­œæˆ‘å€‘å¯ä»¥é€™æ¨£è¡¨ç¤º: åœ–è£¡çš„åƒæ•¸ç›´æ¥å°æ‡‰äº†ç¨‹å¼ç¢¼è£¡çš„å‘½å, å› æ­¤å¯ä»¥å¾ˆæ–¹ä¾¿å°ç…§. å…¶ä¸­ tensorflow è£¡åƒæ•¸çš„åå­—è·Ÿæ•¸å­¸ä¸Šçš„å°æ‡‰å¦‚ä¸‹: $$\\begin{align} gll=\\frac{\\partial \\mbox{loss}}{\\partial \\mbox{logits}} \\\\ gly2=\\frac{\\partial \\mbox{logits}}{\\partial y2}gll \\\\ gy2y1=\\frac{\\partial y2}{\\partial y1}gly2 \\\\ gy1y0=\\frac{\\partial y1}{\\partial y0}gy2y1 \\\\ (glw3,glb3)=(\\frac{\\partial \\mbox{logits}}{\\partial w3}gll,\\frac{\\partial \\mbox{logits}}{\\partial b3}gll) \\\\ (gy2w2,gy2b2)=(\\frac{\\partial y2}{\\partial w2}gly2,\\frac{\\partial y2}{\\partial b2}gly2) \\\\ (gy1w1,gy1b1)=(\\frac{\\partial y1}{\\partial w1}gy2y1,\\frac{\\partial y1}{\\partial b1}gy2y1) \\\\ (gy0w0,gy0b0)=(\\frac{\\partial y0}{\\partial w0}gy1y0,\\frac{\\partial y0}{\\partial b0}gy1y0) \\end{align}$$ ç›¸å°æ‡‰çš„ tensorflow ä»£ç¢¼å¦‚ä¸‹:12345678910111213141516gll = tf.gradients(ys=loss,xs=logits)gly2 = tf.gradients(ys=logits,xs=y2,grad_ys=gll)gy2y1 = tf.gradients(ys=y2,xs=y1,grad_ys=gly2)gy1y0 = tf.gradients(ys=y1,xs=y0,grad_ys=gy2y1)glw3 = tf.gradients(ys=logits,xs=w3,grad_ys=gll)glb3 = tf.gradients(ys=logits,xs=b3,grad_ys=gll)gy2w2 = tf.gradients(ys=y2,xs=w2,grad_ys=gly2)gy2b2 = tf.gradients(ys=y2,xs=b2,grad_ys=gly2)gy1w1 = tf.gradients(ys=y1,xs=w1,grad_ys=gy2y1)gy1b1 = tf.gradients(ys=y1,xs=b1,grad_ys=gy2y1)gy0w0 = tf.gradients(ys=y0,xs=w0,grad_ys=gy1y0)gy0b0 = tf.gradients(ys=y0,xs=b0,grad_ys=gy1y0) ç”¨åœ–ä¾†è¡¨ç¤ºç‚º: Tensorflow ç¨‹å¼ç¢¼ç”¨ä¸Šé¢ä¸€æ®µçš„æ–¹å¼è¨ˆç®—å‡ºæ‰€éœ€åƒæ•¸çš„ gradients å¾Œ, update ä½¿ç”¨æœ€å–®ç´”çš„ steepest descent, é€™éƒ¨åˆ†ç¨‹å¼ç¢¼å¦‚ä¸‹ 12345678910111213update_w3 = tf.assign_add(w3,-rate*glw3[0])update_b3 = tf.assign_add(b3,-rate*glb3[0])update_w2 = tf.assign_add(w2,-rate*gy2w2[0])update_b2 = tf.assign_add(b2,-rate*gy2b2[0])update_w1 = tf.assign_add(w1,-rate*gy1w1[0])update_b1 = tf.assign_add(b1,-rate*gy1b1[0])update_w0 = tf.assign_add(w0,-rate*gy0w0[0])update_b0 = tf.assign_add(b0,-rate*gy0b0[0])training_operation = [update_w3, update_b3, update_w2, update_b2, update_w1, update_b1, update_w0, update_b0] å®Œæ•´ç¨‹å¼ç¢¼å¦‚ä¸‹ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171import osimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.layers import flattenfrom tensorflow.examples.tutorials.mnist import input_datafrom sklearn.utils import shuffle\"\"\"Data Loading\"\"\"dataPath='../dataset/MNIST_data/'mnist = input_data.read_data_sets(dataPath, one_hot=True)# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]img_width = 28img_height = 28images = mnist.train.imagesimg_num, _ = images.shapeimages = np.reshape(images,(img_num,img_height,img_width))images = images[...,np.newaxis]print('(Input to CNN) Images with shape &#123;&#125;'.format(images.shape))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# load the validation setimages_valid = mnist.validation.imagesimg_num_valid = len(images_valid)images_valid = np.reshape(images_valid,(img_num_valid,img_height,img_width))images_valid = images_valid[...,np.newaxis]labels1Hot_valid = mnist.validation.labelsprint('Having %d number of validation images' % img_num_valid)# plotting sample imagesplt.figure(figsize=(15,5))for i in np.arange(2*7): random_idx = np.random.randint(0,img_num) plt.subplot(2,7,i+1) plt.imshow(images[random_idx][...,0],cmap='gray') plt.title(labels[random_idx][0])\"\"\"First define the hyper-parameters\"\"\"# Hyper-parametersEPOCHS = 30BATCH_SIZE = 512rate = 0.01depth_list = [512, 256, 128]cNum = 1\"\"\"Define the input output tensors\"\"\"# using one-hot decodingx = tf.placeholder(tf.float32, (None, img_height, img_width, cNum))one_hot_y = tf.placeholder(tf.int32, (None, n_classes))#one_hot_y = tf.one_hot(y, n_classes)\"\"\"Define the graph and construct it\"\"\"z0 = flatten(x)w0 = tf.get_variable('w0', shape=[img_width*img_height, depth_list[0]], initializer=tf.random_uniform_initializer(-0.1,0.1))b0 = tf.get_variable('b0', [depth_list[0]], initializer=tf.zeros_initializer)y0 = tf.nn.xw_plus_b(z0, w0, b0)z1 = tf.nn.relu(y0)w1 = tf.get_variable('w1', shape=[depth_list[0], depth_list[1]], initializer=tf.random_uniform_initializer(-0.1,0.1))b1 = tf.get_variable('b1', [depth_list[1]], initializer=tf.zeros_initializer)y1 = tf.nn.xw_plus_b(z1, w1, b1)z2 = tf.nn.relu(y1)w2 = tf.get_variable('w2', shape=[depth_list[1], depth_list[2]], initializer=tf.random_uniform_initializer(-0.1,0.1))b2 = tf.get_variable('b2', [depth_list[2]], initializer=tf.zeros_initializer)y2 = tf.nn.xw_plus_b(z2, w2, b2)z3 = tf.nn.relu(y2)w3 = tf.get_variable('w3', shape=[depth_list[2], n_classes], initializer=tf.random_uniform_initializer(-0.1,0.1))b3 = tf.get_variable('b3', [n_classes], initializer=tf.zeros_initializer)logits = tf.nn.xw_plus_b(z3, w3, b3)cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits)loss = tf.reduce_mean(cross_entropy)\"\"\"Define gradients\"\"\"gll = tf.gradients(ys=loss,xs=logits)gly2 = tf.gradients(ys=logits,xs=y2,grad_ys=gll)gy2y1 = tf.gradients(ys=y2,xs=y1,grad_ys=gly2)gy1y0 = tf.gradients(ys=y1,xs=y0,grad_ys=gy2y1)glw3 = tf.gradients(ys=logits,xs=w3,grad_ys=gll)glb3 = tf.gradients(ys=logits,xs=b3,grad_ys=gll)gy2w2 = tf.gradients(ys=y2,xs=w2,grad_ys=gly2)gy2b2 = tf.gradients(ys=y2,xs=b2,grad_ys=gly2)gy1w1 = tf.gradients(ys=y1,xs=w1,grad_ys=gy2y1)gy1b1 = tf.gradients(ys=y1,xs=b1,grad_ys=gy2y1)gy0w0 = tf.gradients(ys=y0,xs=w0,grad_ys=gy1y0)gy0b0 = tf.gradients(ys=y0,xs=b0,grad_ys=gy1y0)update_w3 = tf.assign_add(w3,-rate*glw3[0])update_b3 = tf.assign_add(b3,-rate*glb3[0])update_w2 = tf.assign_add(w2,-rate*gy2w2[0])update_b2 = tf.assign_add(b2,-rate*gy2b2[0])update_w1 = tf.assign_add(w1,-rate*gy1w1[0])update_b1 = tf.assign_add(b1,-rate*gy1b1[0])update_w0 = tf.assign_add(w0,-rate*gy0w0[0])update_b0 = tf.assign_add(b0,-rate*gy0b0[0])training_operation = [update_w3, update_b3, update_w2, update_b2, update_w1, update_b1, update_w0, update_b0]\"\"\"Define accuracy evaluation\"\"\"# calculate the average accuracy by calling evaluate(X_data, y_data)correct_prediction = tf.equal(tf.argmax(logits, axis=1), tf.argmax(one_hot_y, axis=1))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))def evaluate(X_data, y_data): num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] accuracy = sess.run(accuracy_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y&#125;) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples \"\"\"Run Session\"\"\"### Train your model here.import timeif not os.path.isdir('./models'): os.makedirs('./models')#saver = tf.train.Saver()accumulate_time = 0.0with tf.Session() as sess: sess.run(tf.global_variables_initializer()) num_examples = img_num print(\"Training...\") print() train_accuracy = np.zeros(EPOCHS) validation_accuracy = np.zeros(EPOCHS) for i in range(EPOCHS): stime = time.time() acc_train_accuracy = 0 X_train, y_train = shuffle(images, labels1Hot) for offset in range(0, num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] sess.run(training_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y&#125;) etime = time.time() accumulate_time += etime - stime validation_accuracy[i] = evaluate(images_valid, labels1Hot_valid) print(\"EPOCH &#123;&#125; ...\".format(i+1)) print(\"Validation Accuracy = &#123;:.3f&#125;\".format(validation_accuracy[i])) print() print('Cost time: ' + str(accumulate_time) + ' sec.') è¨“ç·´çµæœå¦‚ä¸‹: 1234567891011121314151617EPOCH 1 ...Validation Accuracy = 0.553EPOCH 2 ...Validation Accuracy = 0.698EPOCH 3 ...Validation Accuracy = 0.771... ç•¥EPOCH 28 ...Validation Accuracy = 0.936EPOCH 29 ...Validation Accuracy = 0.936EPOCH 30 ...Validation Accuracy = 0.936 é€™é€Ÿåº¦æœç„¶æ˜é¡¯æ¯”ç”¨ Adam æ…¢å¾ˆå¤š, ä½†è‡³å°‘èªªæ˜äº†æˆ‘å€‘çš„ç¢ºä½¿ç”¨ Computational graph çš„è¨ˆç®—æ–¹å¼å®Œæˆäº† back propagation! çµè«–Tensorflow ä½¿ç”¨è¨ˆç®—åœ–è­œçš„æ¡†æ¶ä¾†è¨ˆç®—å‡½æ•¸çš„ gradients, ä¸€æ—¦é€™æ¨£åš, ç¥ç¶“ç¶²è·¯çš„ backprop å¾ˆè‡ªç„¶äº†. äº‹å¯¦ä¸Š, æ‰€æœ‰æµè¡Œçš„æ¡†æ¶éƒ½é€™éº¼åš, å°±é€£ Kaldi åŸå…ˆåœ¨ nnet2 ä¸æ˜¯, ä½†åˆ° nnet3 ä¹Ÿæ”¹ç”¨è¨ˆç®—åœ–è­œä¾†å¯¦ä½œ. Reference æå®æ¯… Computational Graph tf.gradientsèªªæ˜ Colahâ€™s Blog","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"Computational Graph","slug":"Computational-Graph","permalink":"http://yoursite.com/tags/Computational-Graph/"}]},{"title":"Notes for KKT Conditions","date":"2017-11-14T13:36:40.000Z","path":"2017/11/14/Notes-for-KKT-Conditions/","text":"2011å¹´è‡ªå·±åšçš„ç­†è¨˜, æ”¾ä¸Šä¾†ä»¥å…æª”æ¡ˆä¸Ÿå¤±, ä¹Ÿæ–¹ä¾¿éš¨æ™‚åƒè€ƒ. åƒè€ƒè‡ª â€œNumerical Optimizationâ€ by Jorge Nocedal and Stephen J. Wright. ä½†æ˜¯æ‰“ç®—åªç”¨ Lagrange Multiplier Theorem ç†è§£ KKT. :) å°±åƒæ˜¯ä¸€èˆ¬å¾®ç©åˆ†è£¡å­¸åˆ°çš„ä¸€æ¨£, å°æ–¼ä¸€å€‹å‡½å¼ $f(x)$ è‹¥ $x^\\ast$ ç‚ºä¸€ minimal/maximum point, å‰‡å¿…è¦æ¢ä»¶ç‚º $fâ€™(x^\\ast)=0$. è€Œåœ¨ constraint optimization ç‰ˆæœ¬å¿…è¦æ¢ä»¶è®Šæˆ KKT conditions. èªªæ›´æ¸…æ¥šä¸€é»å°±æ˜¯, è‹¥ $x^\\ast$ ç‚ºä¸€ minimal/maximum point (+æ»¿è¶³æŸäº›ç¥ç§˜æ¢ä»¶) , å‰‡å¿…è¦æ¢ä»¶ç‚ºåœ¨ $x^\\ast$ æ»¿è¶³ KKT Conditions. ç¥ç§˜æ¢ä»¶ç¨±ç‚º Constraint Qualifications, å¸¸è¦‹çš„ç‚º LICQ, åœ¨ Convex opt è£¡ç‚º Slaterâ€™s condition. wiki KKT å…·é«”ä¾†èªªï¼Œæˆ‘å€‘è¦æ¢è¨çš„æ˜¯å°æ–¼ä»¥ä¸‹çš„å•é¡Œï¼Œå¦‚æœ $x^\\ast$ ç‚ºä¸€ minimal point ä¸”æ»¿è¶³å¼ (2) çš„æ¢ä»¶, å‰‡æœƒç™¼ç”Ÿä»€éº¼äº‹æƒ… (æˆ‘å€‘æ‰¾çš„æ˜¯å¿…è¦æ¢ä»¶) $$\\begin{align} \\min f(x) \\\\ \\mbox{subject to } \\begin{array}{rcl} c_i(x)=0,i \\in \\mathbf{E} \\\\ c_i(x)\\geq 0, i \\in \\mathbf{I} \\\\ \\end{array} \\end{align}$$ Descent Directionä¸€èˆ¬é€™æ¨£çš„å•é¡Œä¸æœƒæœ‰ closed form solution, å› æ­¤æœƒä½¿ç”¨æ•¸å€¼æœ€ä½³åŒ–çš„æ–¹å¼, æ‰¾å‡ºä¸€å€‹ sequence $(x_k)$ ä¾†é€¼è¿‘ $x^\\ast$. å•é¡Œæ˜¯å¦‚ä½•è®“é€™æ¨£çš„ sequence é€¼è¿‘ä¸€å€‹ (local) minimum? ä¸€å€‹å˜—è©¦æ˜¯è‡³å°‘å…ˆè®“æ‰¾åˆ°çš„æ¯ä¸€å€‹ $x_k$ éƒ½æ¯”å‰ä¸€æ­¥æ›´å¥½. æ›å¥è©±èªªå°±æ˜¯è¦ä¿è­‰æ‰¾åˆ°çš„ $x_k$ æ»¿è¶³$f(x_k)&lt;f(x_{k-1})$ æœ‰äº†é€™å€‹æƒ³æ³•, å†ä¾†å°±æ˜¯è©²æ€éº¼æ‰¾ä¸‹ä¸€å€‹é», æˆ–æ˜¯èªª, åŸºæ–¼ç¾åœ¨çš„é» $x_k$, è©²å¾€å“ªå€‹æ–¹å‘ $d$, èµ°å¤šé  $t$? ä¹Ÿå› æ­¤ï¼Œæˆ‘å€‘ä¹Ÿå°±è¡ç”Ÿäº†ä¸€å€‹å•é¡Œ, å¾€å“ªå€‹æ–¹å‘èµ°å‡½æ•¸å€¼æœƒä¿è­‰ä¸‹é™ (descent direction)? Descent direction ä¿è­‰äº†åœ¨è©²æ–¹å‘ä¸Šåªè¦ä¸è¦èµ°å¤ªå¤§æ­¥, ç›®æ¨™å‡½æ•¸å€¼ä¸€å®šæœƒä¸‹é™, åéä¾†èªª, å¦‚æœ $x^\\ast$ å·²ç¶“æ˜¯ (local) minimum äº†, åœ¨è©²é»ä¸Šä¸æ‡‰è©²å­˜åœ¨ descent direction. åŸºæ–¼ä¸Šè¿°çš„è¨è«–, æˆ‘å€‘æœ‰å¿…è¦äº†è§£æ¸…æ¥š descent direction. å®šç¾©å¦‚ä¸‹ [Def]:ä»¤ $f \\in C^1(\\mathbb{R}^n,\\mathbb{R})$, æˆ‘å€‘ç¨± $d$ ç‚º $x_0$ çš„ä¸€å€‹ descent direction å¦‚æœæ»¿è¶³: $\\exists t&gt;0$, such that $f(x_0+sd)&lt;f(x_0),\\forall s \\leq t$ å…¶å¯¦å¾ˆå®¹æ˜“è­‰å¾—: åªè¦è©²æ–¹å‘ $d$ è·Ÿ gradient $\\triangledown f(x_0)$ æ–¹å‘ç›¸å, å°±æœƒæ˜¯ $x_0$ çš„ä¸€å€‹ descent direction [Thm1]:ä»¤ $f \\in C^1(\\mathbb{R}^n,\\mathbb{R})$, å¦‚æœæ»¿è¶³ $\\triangledown f(x_0)^Td&lt;0$ å‰‡ $d$ å°±æœƒæ˜¯ $x_0$ çš„ä¸€å€‹ descent direction [Pf]: ç”±å¾®åˆ†çš„å®šç¾©å‡ºç™¼$$\\lim_{t\\rightarrow 0^+}\\frac{f(x_0+td)-f(x_0)-\\triangledown f(x_0)^Ttd}{\\parallel td \\parallel}=0 \\\\ \\Rightarrow \\lim_{t\\rightarrow 0^+}\\frac{f(x_0+td)-f(x_0)}{t \\parallel d \\parallel}=\\triangledown f(x_0)^T\\frac{d}{\\parallel d \\parallel} \\\\ \\Rightarrow \\lim_{t\\rightarrow 0^+}\\frac{f(x_0+td)-f(x_0)}{t}=\\triangledown f(x_0)^Td&lt;0 \\\\ \\Rightarrow \\exists t&gt;0,s.t.,f(x_0+sd)-f(x_0)&lt;0,for\\forall s \\leq t \\\\ \\Rightarrow \\exists t&gt;0,s.t.,f(x_0+sd)&lt;f(x_0),for\\forall s \\leq t$$ å¾ˆæ˜é¡¯ steepest descent $d=-\\triangledown f(x_0)$ æ˜¯ descent direction. å…¶å¯¦åªè¦æ»¿è¶³é€™ç¨®å½¢å¼ $d=-B\\triangledown f(x_0)$ ç•¶ $B$ æ˜¯æ­£å®šï¼Œå°±æœƒæ˜¯ descent direction. è€Œç•¶ B å®šç¾©ç‚º $\\triangledown ^2 f(x_0)$ (Hessian Matrix æ˜¯åŠæ­£å®š, é€šå¸¸æ˜¯ full rank å°±æœƒæ­£å®š), é€™ç¨®å½¢å¼å°±æ˜¯ç‰›é “æ³• $d=âˆ’\\triangledown ^2 f(x_0)\\triangledown f (x_0)$ ä¸éæˆ‘å€‘ä»Šå¤©è¦è™•ç†çš„æ˜¯ constrained opt, æœƒæœ‰ç­‰å¼æˆ–ä¸ç­‰å¼çš„æ¢ä»¶, å› æ­¤æˆ‘å€‘çš„æœå°‹ç©ºé–“åªèƒ½åœ¨æ»¿è¶³é€™äº›æ¢ä»¶ä¸‹å»æœå°‹, ç¨±è©²ç©ºé–“ç‚º feasible set = {x|xæ»¿è¶³æ‰€æœ‰(2)å¼çš„æ¢ä»¶}. å¯ä»¥æƒ³åƒ, åœ¨ feasible set çš„é™åˆ¶ä¸‹, èƒ½æœå°‹çš„ direction æœƒè¢«é™åˆ¶. å› æ­¤ â€œNumerical Optimizationâ€ é€™æœ¬æ›¸å°±å±•é–‹äº†ä¸€ç³»åˆ—çš„è¨è«–å’Œè­‰æ˜, å¯ä»¥å¾—åˆ°åœ¨é€™å€‹ feasible set ä¸‹, é€™äº› èƒ½æœå°‹çš„æ–¹å‘(æˆ‘å€‘ç¨±ç‚º limiting direction)æ‰€æ§‹æˆçš„é›†åˆ ç©¶ç«Ÿé•·ä»€éº¼æ¨£. ä¸”ç™¼ç”Ÿåœ¨æœ€ä½³è§£ä¸Šçš„ limiting directions éƒ½ä¸æœƒæ˜¯ descent direction (åˆç†, ä¸ç„¶å°±æ‰¾åˆ°æ›´ä½³çš„è§£äº†). æ­¤å¤–, çœ‹èª²æœ¬çš„è©±, æœƒç¹æ›´å¤§ä¸€åœˆæ‰æœƒçŸ¥é“ä»€éº¼æ˜¯ KKT Conditions (ä½†æ˜¯ç›¸ç•¶åš´è¬¹ä¸”è±å¯Œ). ç‚ºäº†æ¸…æ¥šäº†è§£ KKT, æˆ‘å€‘ç¹éèª²æœ¬çš„æ–¹æ³•, å®Œå…¨æ¡ç”¨å¾®ç©åˆ†å­¸éçš„ Lagrange Multiplier Theorem ä¾†èªªæ˜. äº†è§£ KKT Conditionsé™åˆ¶æ¢ä»¶ç‚ºç­‰å¼å…¶å¯¦ KKT çš„è¡¨é”å…¨éƒ¨åœç¹åœ¨ Lagrange Multiplier Theorem ä¸Š. ä¸€èˆ¬èª²æœ¬ä¸Šè¬›çš„éƒ½æ˜¯ç­‰å¼æ¢ä»¶, æˆ‘å€‘åˆ—å‡ºé«˜ç¶­èª²æœ¬è£é ­çš„å®šç†:ä¸æƒ³æ‰“ Latex äº† &gt;&lt;, è²¼åœ–å¥½äº† [Thm2]: Lagrange Multiplier Theorem è€ƒæ…®ä»¥ä¸‹å•é¡Œ $$\\min f(x) \\\\ \\mbox{subject to } \\begin{array}{rcl} c(x)=0 \\\\ \\end{array}$$ æˆ‘å€‘å¯ä»¥å¾—åˆ°, è‹¥ $x^\\ast$ ç‚ºä¸€å€‹ local minimum, ç”± Thm2 çŸ¥é“, æ»¿è¶³ $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$, for some $\\lambda$æ­¤æ™‚çš„ $\\lambda$ æ­£è² éƒ½æœ‰å¯èƒ½, ä¹Ÿå°±èªªæ˜äº†å…©å€‹ gradients æ˜¯å¹³è¡Œçš„. ç”¨åœ–ä¾†èªªæ˜å¦‚ä¸‹: é™åˆ¶æ¢ä»¶ç‚ºä¸ç­‰å¼è€ƒæ…®ä»¥ä¸‹å•é¡Œ $$\\min f(x) \\\\ \\mbox{subject to } \\begin{array}{rcl} c(x)\\geq 0 \\\\ \\end{array}$$ ç•¶ $x^\\ast$ ç‚ºä¸€å€‹ local minimum æœƒç™¼ç”Ÿä»€éº¼äº‹? åˆ†æˆå…©ç¨®æƒ…æ³è¨è«–: $c(x^\\ast)=0$ $c(x^\\ast)&gt;0$ ç¬¬ä¸€ç¨®æƒ…æ³å°±é€€åŒ–æˆæ¢ä»¶ç‚ºç­‰å¼çš„æƒ…å½¢. å› æ­¤å­˜åœ¨ $\\lambda$ æ»¿è¶³ $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$. å¦‚æœ $\\lambda&lt;0$, å°è‡´ $\\triangledown c(x^\\ast)$ è·Ÿ $\\triangledown f(x^\\ast)$ åæ–¹å‘çš„è©±, $\\triangledown c(x^\\ast)^T\\triangledown f(x^\\ast)&lt;0$ å°è‡´ $\\triangledown c(x^\\ast)$ è®Šæˆä¸€å€‹ desent direction.å‰‡è¡¨ç¤ºæˆ‘å€‘å¯ä»¥æ‰¾åˆ°ä¸€å€‹æ–¹å‘ä½¿å¾—ç›®æ¨™å‡½æ•¸å€¼ä¸‹é™ä¸”åŒæ™‚è®“æ¢ä»¶å‡½æ•¸å€¼ä¸Šå‡(å› æ­¤ä»ç„¶æ˜¯ feasible), é‚£éº¼èˆ‡ $x^\\ast$ æ˜¯ local minimum çŸ›ç›¾. å› æ­¤å¾—åˆ°çš„çµè«–æ˜¯ $\\triangledown c(x^\\ast)$ è·Ÿ $\\triangledown f(x^\\ast)$ åŒæ–¹å‘, i.e., $\\lambda\\geq 0$. åœ–ç¤ºå¦‚ä¸‹: ç¬¬äºŒç¨®æƒ…æ³æ˜¯é€€åŒ–æˆ unconstrained opt, å› ç‚º $x^\\ast$ æ˜¯åœ¨ feasible set å…§, æ›å¥è©±èªª $x^\\ast$ æ˜¯ feasible set çš„ interior point. æ—¢ç„¶æ˜¯ unconstrained opt, ä¸” $x^\\ast$ ç‚º local minimum, å‰‡è¡¨ç¤º $\\triangledown f(x^\\ast)=0$, æ‰€ä»¥ç•¶ç„¶ä¹Ÿå¯ä»¥å¯«æˆ $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$ åªä¸éæ­¤æ™‚çš„ $\\lambda=0$ æ‰€ä»¥ä¸ç®¡æ˜¯ç¬¬ä¸€ç¨®æˆ–æ˜¯ç¬¬äºŒç¨®æƒ…å½¢, æˆ‘å€‘éƒ½å¯ä»¥å¯«æˆ å­˜åœ¨ $\\lambda\\geq 0$ æ»¿è¶³ $\\triangledown f(x^\\ast)=\\lambda\\triangledown c(x^\\ast)$ KKT Conditionsåˆ°é€™è£¡ç‚ºæ­¢, æˆ‘å€‘åŸºæœ¬ä¸Šå·²ç¶“å¯ä»¥åˆ—å‡º KKT äº†: [Thm3]: Karushâ€Kuhnâ€Tucker conditions[Pf]:Condition 1 åªæ˜¯èªªæ˜å…·æœ‰ Lagrange Multiplier çš„è¡¨é”æ–¹å¼: $\\triangledown f(x^\\ast)=\\sum_i{\\lambda_i\\triangledown c_i(x^\\ast)}$Condition 2,3 æ˜¯èªªæ˜ $x^\\ast$ æ˜¯ feasible point, é€™æ˜¯å»¢è©±Condition 4 èªªæ˜ è‹¥æ¢ä»¶ç‚ºä¸ç­‰å¼, ç›¸å°æ‡‰çš„ Lagrange Multipliers å¿…é ˆå¤§æ–¼ç­‰æ–¼0, æˆ‘å€‘åœ¨ä¸Šä¸€æ®µè¨è«–äº†Condition 5 ç¨±ç‚º complementarity slackness (æˆ‘çŸ¥é“å¾ˆé›£å¿µâ€¦), é€™éœ€è¦ç¨å¾®èªªæ˜ä¸€ä¸‹å¦‚æœ $c_i$ æ˜¯ç­‰å¼æ¢ä»¶, å‰‡ $c_i(x^\\ast)=0$, å› æ­¤æ»¿è¶³ Condition 5å¦‚æœ $c_i$ æ˜¯ä¸ç­‰å¼æ¢ä»¶, ä½†æ˜¯ $c_i(x^\\ast)=0$, åŒæ¨£ä¹Ÿæ»¿è¶³ Condition 5æœ€å¾Œä¸€ç¨®æƒ…æ³æ˜¯ $c_i$ æ˜¯ä¸ç­‰å¼æ¢ä»¶, ä¸” $c_i(x^\\ast)&gt;0$. é‚„è¨˜å¾—æˆ‘å€‘ä¸Šé¢é‡å°æ­¤ç¨®æƒ…å½¢çš„è¨è«–å—? æˆ‘å€‘æœƒä»¤ä»–çš„ $\\lambda_i=0$, æ‰€ä»¥é‚„æ˜¯æ»¿è¶³ Condition 5. é€™è£¡æ²’æœ‰æåˆ°ä¸€ä»¶äº‹æƒ…å°±æ˜¯ LICQ, å…¨å Linear Independent Constraint Qualification, å¯åƒè€ƒ wiki KKT. LICQ æ¢ä»¶ç‚º: å°æ–¼æŸä¸€é» feasible point x, æ‰€æœ‰ç­‰å¼æ¢ä»¶ (åŒ…å«é‚£äº›ä¸ç­‰å¼æ¢ä»¶ä½†å‰›å¥½åœ¨ x è®Šæˆç­‰å¼) åœ¨ x é€™é»çš„ gradients éƒ½æ˜¯ç·šæ€§ç¨ç«‹. é€™å€‹æ¢ä»¶æ­£å¥½å¯ä»¥å¾ [Thm2]: Lagrange Multiplier Theorem è£¡é¢çœ‹å‡ºä¾†, Thm2 èªªæ˜å¦‚æœç­‰å¼æ¢ä»¶çš„ gradients éƒ½ç·šæ€§ç¨ç«‹, å¯ä»¥æŠŠ $\\mu=1$, å› æ­¤å¯ä»¥å¯«æˆ Condition 1: $\\triangledown f(x^\\ast)=\\sum_i{\\lambda_i\\triangledown c_i(x^\\ast)}$, ä¹Ÿå› æ­¤å¯ä»¥æ»¿è¶³ KKT. èª²æœ¬è£¡çš„è­‰æ³•èª²æœ¬è£¡çš„è­‰æ˜å¯¦åœ¨é —è¿‚è¿´, ä½†æ˜¯æä¾›äº†å¾ˆè±å¯Œå’Œæ·±åˆ»çš„ç†è§£. é€™è£¡é‚„æ˜¯åŠªåŠ›è¨˜éŒ„ä¸‹ä¾†å§! {æ•¸å­¸å¤š, è¬¹æ…æœç”¨} æˆ‘å€‘åˆ†æˆ5å€‹æ­¥é©Ÿä¾†è¨è«–: Limiting directions Limiting direction èˆ‡ Local minimum çš„é—œè¯ Limiting directions çš„é›†åˆ, å°±ç¨±ç‚º F å§ LICQ æˆç«‹æ™‚, â€œLimiting directions éƒ½ä¸æ˜¯ descent directionâ€ èˆ‡ â€œLagrange Multipliersâ€ çš„ç­‰åƒ¹é—œä¿‚ ä¸²èµ·ä¾†è®Šæˆ KKT 1. Limiting directionsç›´è§€ä¾†èªª, å°æ–¼æŸä¸€é» $x_0$ (ç•¶ç„¶å±¬æ–¼ feasible set) ç”¨åœ¨ feasible set ä¸­çš„æŸæ¢è·¯å¾‘å»é€¼è¿‘å®ƒ, è€Œé€¼è¿‘çš„æœ€å¾Œæ–¹å‘å°±æ˜¯ limiting direction. å¦å¤–, ä¸€å€‹ sequence ${z_k}$ éƒ½å±¬æ–¼ feasible set , éƒ½ä¸ç­‰æ–¼ $x_0$, ä¸”æœ€å¾Œé€¼è¿‘ $x_0$, æˆ‘å€‘ç¨±ç‚º feasible sequence. [Def]:è‹¥æ»¿è¶³ä»¥ä¸‹æ¢ä»¶ç¨± $d$ æ˜¯ $x_0$ çš„ limiting direction. (ç•¶ç„¶ $x_0$ æ˜¯ feasible point)å­˜åœ¨ä¸€å€‹ feasible sequence $(z_k)_k$ ä½¿å¾—è©² sequence æœ‰ä¸€å€‹ subsequence$$\\exists (z_{k_j})_j \\mbox{ such that } d = \\lim\\frac{(z_{k_j}-x_0)}{\\parallel z_{k_j}-x_0\\parallel}$$ å¾å®šç¾©ä¸Šæˆ‘å€‘å¯ä»¥çŸ¥é“ limiting direction é•·åº¦ç‚º 1, å› ç‚ºæˆ‘å€‘åªåœ¨ä¹æ–¹å‘. å¦å¤–è¦ç‰¹åˆ¥èªªå­˜åœ¨ä¸€å€‹ subsequence æ˜¯å› ç‚º feasible sequence ä¸æœƒåªæœ‰ä¸€å€‹ limiting direction. ä¾‹å­å¦‚ä¸‹: 2. Limiting direction èˆ‡ Local minimum çš„é—œè¯æ–‡ç« é–‹é ­æœ‰èªªæ˜, â€œå¦‚æœ $x^\\ast$ å·²ç¶“æ˜¯ (local) minimum äº†, åœ¨è©²é»ä¸Šä¸æ‡‰è©²å­˜åœ¨ descent direction.â€ å°æ–¼ constrained opt çš„ç‰ˆæœ¬ç›¸ç•¶æ–¼ â€œå¦‚æœ $x^\\ast$ å·²ç¶“æ˜¯ (local) minimum äº†, å®ƒçš„ limiting directions éƒ½ä¸èƒ½æ˜¯ descent direction.â€ ç”¨æ•¸å­¸å¯«å‡ºä¾†å¦‚ä¸‹: [Thm4]:å·²çŸ¥ $x^\\ast$ æ˜¯ä¸€å€‹ local minimum, å‰‡å®ƒæ‰€æœ‰çš„ limiting direction $d$ éƒ½æ»¿è¶³ $\\triangledown f(x^\\ast)^Td \\geq 0$ ç›´è§€ä¸Šå¦‚æœä¸æ»¿è¶³, æˆ‘å€‘å°±å¯ä»¥æ‰¾åˆ°ä¸€å€‹ feasible sequence å¾è€Œå¾—åˆ°è©² limiting direction æœƒæ˜¯ä¸€å€‹ descent direction, å› æ­¤èˆ‡ $x^\\ast$ æ˜¯ local minium çŸ›ç›¾.æˆ‘å€‘åœ¨ç­‰ä¸‹çš„ç¬¬4å€‹æ­¥é©Ÿå¯ä»¥çœ‹åˆ°æ­¤æ¢ä»¶ â€œæ‰€æœ‰çš„ limiting direction $d$ éƒ½æ»¿è¶³ $\\triangledown f(x^\\ast)^Td \\geq 0$â€ ç­‰åƒ¹æ–¼ KKT Conditions çš„è¡¨é”æ–¹å¼. å› æ­¤ Thm4 å¯ä»¥é‡å¯«æˆ â€œå·²çŸ¥ $x^\\ast$ æ˜¯ä¸€å€‹ local minimum, å‰‡æ»¿è¶³ KKT Conditionsâ€, åœ¨æœ€å¾Œç¬¬5æ­¥æœƒä¸²èµ·ä¾†. 3. Limiting directions çš„é›†åˆ (F) [Def]: Active Setå°æ–¼æŸä¸€ feasible point $x_0$, å®ƒçš„ active set $\\mathbf{A}(x_0)$ å®šç¾©ç‚º$\\mathbf{A}(x_0) = \\mathbf{E} \\cup \\{i \\in \\mathbf{I} | c_i(x_0)=0 \\}$ [Def]: LICQå¦‚æœä»¥ä¸‹é›†åˆç‚ºç·šæ€§ç¨ç«‹é›†, å‰‡ç¨± LICQ åœ¨ $x_0$ æˆç«‹$\\{\\triangledown c_i(x_0), i \\in \\mathbf{A}(x_0) \\}$ å…¶å¯¦æˆ‘å€‘åœ¨ä¸Šé¢çš„è¨è«–éƒ½æœ‰ä½¿ç”¨é€™å…©å€‹å®šç¾©, é€™è£¡åªä¸éç”¨æ•¸å­¸è¡¨ç¤ºæ–¹ä¾¿ç­‰ä¸‹çš„è¨è«–.æŸä¸€é»å®ƒçš„æ‰€æœ‰ limiting direction çš„é›†åˆ ($F$) å¦‚ä¸‹: [Thm5]:å°æ–¼æŸä¸€ feasible point $x_0$,$$F=\\left\\{ \\begin{array}{c|r} d &amp; \\begin{array}{rcl} d^T\\triangledown c_i(x_0)=0,i \\in \\mathbf{E} \\\\ d^T\\triangledown c_i(x_0) \\geq 0, i \\in \\mathbf{A}(x_0) \\cap \\mathbf{I} \\\\ \\parallel d \\parallel = 1\\\\ \\end{array} \\end{array} \\right\\}$$ ç‚ºäº†ä¸æ¨¡ç³Šç„¦é», è­‰æ˜å°±è·³é, æƒ³çœ‹çš„ç«¥é‹é–€å°±æŸ¥ä¸€ä¸‹èˆŠçš„ç­†è¨˜ å¦å¤–, å®šç¾© $F_1=\\alpha F$, for $\\alpha\\geq 0$ (æ‰€ä»¥æ˜¯ convex cone). å› æ­¤ $F1$ åªä¸éæ˜¯æŠŠ $\\parallel d \\parallel =1$ çš„æ¢ä»¶å»èª¿. 4. LICQ æˆç«‹æ™‚, é—œéµçš„ç­‰åƒ¹é—œä¿‚æˆ‘å€‘ä»¥ä¸‹éƒ½å‡è¨­ LICQ æˆç«‹, é€™éº¼åšå°±å¯ä»¥å¾ˆæ–¹ä¾¿åœ°è®“ limiting direction çš„é›†åˆç”¨ $F1$ ä¾†è¡¨ç¤º. é‚„è¨˜å¾—åœ¨ â€œ2. Limiting direction èˆ‡ Local minimum çš„é—œè¯â€ æœ‰æåˆ°æˆ‘å€‘å¸Œæœ›æ‰¾åˆ°æŸä¸€ feasible point $x_0$ çš„ $F1$ éƒ½ä¸æ˜¯ descent direction, å› æ­¤è©²é»å°±å¾ˆæœ‰å¯èƒ½æ˜¯æˆ‘è¦æ‰¾çš„ local optimum. è€Œé€™ä¸€å€‹æ¢ä»¶ â€œæŸä¸€ feasible point $x_0$ çš„ $F1$ éƒ½ä¸æ˜¯ descent directionâ€ å…¶å¯¦èˆ‡ Lagrange Multipliers æ¯æ¯ç›¸é—œ, ä¹Ÿå› æ­¤è·Ÿ KKT conditions æœƒç”¢ç”Ÿé€£çµ. ä¸‹é¢å®šç†å¯ä»¥è­‰æ˜é€™å€‹æ¢ä»¶å¯ä»¥ç­‰åƒ¹æ–¼ KKT condition çš„è¡¨é”æ–¹å¼. [Thm6]:å°æ–¼æŸä¸€ feasible point $x_0$, Let $\\mathbf{A}(x_0)=(1â€¦m)$, $\\mathbf{A}^T=[\\triangledown c_1(x_0)â€¦\\triangledown c_m(x_0)]$ å‰‡$$\\triangledown f(x-0)^Td\\geq 0,\\forall d \\in F1 \\Leftrightarrow\\\\ \\exists \\lambda \\in \\mathbb{R}^m \\mbox{ where } \\lambda_i \\geq 0 \\forall i \\in \\mathbf{A}(x_0) \\cap \\mathbf{I} \\mbox{, such that } \\triangledown f(x_0)=\\sum_{i=1}^m \\lambda_i \\triangledown c_i(x_0)=\\mathbf{A}^T$$ åŒæ¨£è·³é, è­‰æ˜å¯æŸ¥çœ‹èˆŠçš„ç­†è¨˜ æˆ‘å€‘å¯ä»¥ä»”ç´°å°ç…§ä¸€ä¸‹ä¸Šé¢çš„ Lagrange Multiplier é‚£å€‹æ¢ä»¶, å…¶å¯¦å®ƒè·Ÿ â€œ[Thm3]: Karushâ€Kuhnâ€Tucker conditionsâ€ æ˜¯ä¸€æ¨£çš„, åªå·®åœ¨ä¸€å€‹åœ°æ–¹å°±æ˜¯ complementarity slackness æ²’æœ‰æ˜ç¢ºå¯«å‡ºä¾†, ä½†æˆ‘å€‘çŸ¥é“ä¸€å®šå­˜åœ¨ $\\lambda$ å¯ä»¥æ»¿è¶³. å› æ­¤é€™å€‹ Lagrange Multiplier çš„æ¢ä»¶ä¹Ÿå°±æ˜¯ KKT çš„è¡¨é”æ–¹å¼. 5. ä¸²èµ·ä¾†è®Šæˆ KKTè…¦è¢‹å·®ä¸å¤šéƒ½æ‰“çµäº†, ç›®å‰ç‚ºæ­¢åˆ°åº•å¾—åˆ°äº†ä»€éº¼é—œä¿‚? æˆ‘å€‘ä¾†æ•´ç†ä¸€ä¸‹ Thm4 å‘Šè¨´æˆ‘å€‘ä¸€å€‹æœ€ä½³è§£ä¸€å®šæœƒä½¿å¾—å®ƒçš„ limiting directions éƒ½ä¸æ˜¯ descent direction.Thm5 å‘Šè¨´æˆ‘å€‘ limiting directions çš„é›†åˆå…¶å¯¦å°±æ˜¯ $F1$ (or $F$).Thm6 å‘Šè¨´æˆ‘å€‘å°æ–¼ä»»ä¸€å€‹ $F1$ çš„ direction, éƒ½ä¸æ˜¯ descent direction, ç­‰åŒæ–¼æ»¿è¶³ KKT çš„è¡¨é”æ–¹å¼. å°‡ Thm4,5,6 ä¸²èµ·ä¾†è®Šæˆ: ä¸€å€‹æœ€ä½³è§£æ»¿è¶³ KKT çš„è¡¨é”æ–¹å¼. (ç•¶ç„¶å‰ææ˜¯æœ‰æ»¿è¶³ LICQ) æ‰“é€™ç¯‡ç´¯åˆ°ä¸æƒ³æœ‰çµè«–çš„çµè«–ä¸æƒ³æœ‰çµè«–äº†, ä¹¾è„†ä¾†ç¢ç¢å¿µå§. æœ€ä½³åŒ–æ˜¯æˆ‘å”¸æ›¸æœŸé–“å¾ˆæ„›çš„ä¸€é–€ç§‘ç›®, ç•¶æ™‚æ„ˆæ˜¯å”¸å®ƒ, æ„ˆæ˜¯ä¸æ‡‚. ä»¥å‰ä¹Ÿå¾ˆæ„›çœ‹ Stephen P. Boyd çš„ convex opt èª²ç¨‹, ä½†ç¾åœ¨è…¦è¢‹è£¡ä¼¼ä¹åªå‰©æ•™æˆåå­—å’Œèª²ç¨‹åå­—äº†. å–”å°äº†, æˆ‘é‚„è¨˜å¾—ä¸€ä»¶äº‹æƒ…, å°±æ˜¯åœ¨ convex å•é¡Œæ™‚, KKT condition æœƒè®Šæˆ å……è¦æ¢ä»¶. è‡³æ–¼ç´°ç¯€, æ©â€¦ [å¾…è£œå……]: æˆ‘æœ‰æ‰¾åˆ°ç•¶æ™‚çš„ç­†è¨˜é—œæ–¼ convex å•é¡Œä¸‹çš„ KKT conditions, ä»¥åŠå®ƒçš„ dual problem è¨è«–. åªèƒ½èªª: 1. å­¸ç”Ÿå¯ä»¥å¾ˆè‡ªç”±æŒæ§è‡ªå·±çš„æ™‚é–“, å·¥ä½œå¾Œçš„æ™‚é–“éƒ½æ˜¯é›¶ç¢çš„é˜¿! æ ¹æœ¬æ²’æ³•é•·æ™‚é–“æ­»å—‘æŸä¸€æ¨£å­¸ç§‘! 2. ç•¶ç¢¼è¾²å·¥ç¨‹å¸«æ•¸å­¸çœŸçš„æœƒé€€æ­¥, ç¢¼è¾²å‹ç¢¼è¾²çš„å¥½, ä½†å¸Œæœ›è‡ªå·±ä¹Ÿåˆ¥å¿˜è¨˜é‡è¦çš„æ•¸å­¸è§€å¿µäº†. Reference Numerical Optimization 2nd edition, Jorge Nocedal ç­†è¨˜ for the proof of Thm5,6","tags":[{"name":"Nonlinear Constraint Optimization","slug":"Nonlinear-Constraint-Optimization","permalink":"http://yoursite.com/tags/Nonlinear-Constraint-Optimization/"},{"name":"KKT","slug":"KKT","permalink":"http://yoursite.com/tags/KKT/"}]},{"title":"TF Notes (2), Speedup and Benchmark with Two GPU Cards","date":"2017-11-01T12:28:49.000Z","path":"2017/11/01/TF-Notes-Speedup-and-Benchmark-with-Two-GPU-Cards/","text":"é€™ç¯‡æ–‡ç« å¯¦ä½œäº†å®˜ç¶²çš„ åŒæ­¥å¼ data Parallelism æ–¹æ³• refï¼Œä¸¦ä¸”èˆ‡åŸæœ¬åªç”¨ä¸€å¼µGPUåšå€‹æ¯”è¼ƒã€‚å¯¦é©—åªä½¿ç”¨å…©å¼µå¡ï¼Œå¤šå¼µå¡æ–¹æ³•ä¸€æ¨£ã€‚ä¸»è¦æ¶æ§‹å¦‚ä¸‹åœ– by TF å®˜ç¶²: å…©å¼µå¡ç­‰æ–¼æ˜¯æŠŠä¸€æ¬¡è¦è¨ˆç®—çš„ mini-batch æ‹†æˆå…©åŠçµ¦å…©å€‹ (ç›¸åŒçš„) models å»ä¸¦è¡Œè¨ˆç®— gradientsï¼Œç„¶å¾Œå†äº¤ç”± cpu çµ±ä¸€æ›´æ–° modelã€‚è©³ç´°è«‹è‡ªè¡Œåƒè€ƒå®˜ç¶²ã€‚ä¸‹é¢ç›´æ¥ç§€ Codes å’Œçµæœã€‚ Machine Spec.GPU å¡ç‚º Tesla K40c CPU ç‚º Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz å–® GPU è·‘ MNISTç›´æ¥ä¸Š Codes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201import gzipimport osimport tempfileimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.layers import flattenfrom tensorflow.examples.tutorials.mnist import input_datafrom sklearn.utils import shuffle\"\"\"Data Loading\"\"\"dataPath='../dataset/MNIST_data/'mnist = input_data.read_data_sets(dataPath, one_hot=True)# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]img_width = 28img_height = 28images = mnist.train.imagesimg_num, _ = images.shapeimages = np.reshape(images,(img_num,img_height,img_width))images = images[...,np.newaxis]print('(Input to CNN) Images with shape &#123;&#125;'.format(images.shape))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# load the validation setimages_valid = mnist.validation.imagesimg_num_valid = len(images_valid)images_valid = np.reshape(images_valid,(img_num_valid,img_height,img_width))images_valid = images_valid[...,np.newaxis]labels1Hot_valid = mnist.validation.labelsprint('Having %d number of validation images' % img_num_valid)# plotting sample imagesplt.figure(figsize=(15,5))for i in np.arange(2*7): random_idx = np.random.randint(0,img_num) plt.subplot(2,7,i+1) plt.imshow(images[random_idx][...,0],cmap='gray') plt.title(labels[random_idx][0])\"\"\"First define the hyper-parameters\"\"\"# Hyper-parametersEPOCHS = 30BATCH_SIZE = 512rate = 0.001drop_out_keep_prob = 0.5ksize = 5cnn_depth_list = [16, 32]mlp_depth_list = [256, 128]cNum = 1\"\"\"Define the input output tensors\"\"\"# using one-hot decodingx = tf.placeholder(tf.float32, (None, img_height, img_width, cNum))one_hot_y = tf.placeholder(tf.int32, (None, n_classes))#one_hot_y = tf.one_hot(y, n_classes)keep_prob = tf.placeholder(tf.float32) # probability to keep units\"\"\"Define the graph and construct it\"\"\"class MNISTCNN: def __init__(self, ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes): self._ksize = ksize self._cnn_depth_list = cnn_depth_list self._mlp_depth_list = mlp_depth_list self._img_height = img_height self._img_width = img_width self._cNum = cNum self._n_classes = n_classes self._mu = 0 self._sigma = 0.1 def create(self,x,keep_prob): conv = self._conv(x, self._cNum, self._ksize, self._cnn_depth_list[0], 'conv1') # Pooling. Input = 24x24xlayer_depth['layer_1']. Output = 12x12xlayer_depth['layer_1']. conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') for lidx in range(1,len(self._cnn_depth_list)): conv = self._conv(conv, self._cnn_depth_list[lidx-1], self._ksize, self._cnn_depth_list[lidx], 'conv&#123;&#125;'.format(lidx+1)) conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') fc = flatten(conv) bsize, fc_in_dim = fc.shape fc = self._fc(fc,fc_in_dim,self._mlp_depth_list[0],'fc1') fc = tf.nn.dropout(fc, keep_prob) # dropout for lidx in range(1,len(self._mlp_depth_list)): fc = self._fc(fc,self._mlp_depth_list[lidx-1],self._mlp_depth_list[lidx],'fc&#123;&#125;'.format(lidx+1)) fc = tf.nn.dropout(fc, keep_prob) # dropout with tf.variable_scope('logits') as scope: logits_w = tf.get_variable('logits_w', shape=[self._mlp_depth_list[-1],self._n_classes],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) logits_b = tf.get_variable('logits_b', shape=[self._n_classes],\\ initializer=tf.zeros_initializer) logits = tf.nn.xw_plus_b(fc, logits_w, logits_b, name=scope.name) print(logits.shape) return logits def _conv(self, x, in_depth, ksize, out_depth, scope_name, relu=True): bsize,h,w,cNum = x.shape assert(h-(ksize-1)&gt;=1) assert(w-(ksize-1)&gt;=1) with tf.variable_scope(scope_name) as scope: # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=(ksize, ksize, in_depth, out_depth),\\ initializer=tf.random_normal_initializer(self._mu,self._sigma)) biases = tf.get_variable('biases', shape=(out_depth),initializer=tf.zeros_initializer) out = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding='VALID',name=scope.name) + biases if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out def _fc(self, x, num_in, num_out, scope_name, relu=True): \"\"\"Create a fully connected layer.\"\"\" with tf.variable_scope(scope_name) as scope: # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=[num_in, num_out],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) biases = tf.get_variable('biases', [num_out],\\ initializer=tf.zeros_initializer) # Matrix multiply weights and inputs and add bias out = tf.nn.xw_plus_b(x, weights, biases, name=scope.name) if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out mnistCNN = MNISTCNN(ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes)logits = mnistCNN.create(x,keep_prob)\"\"\"Define loss and optimizer\"\"\"cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits)loss_operation = tf.reduce_mean(cross_entropy)optimizer = tf.train.AdamOptimizer(learning_rate = rate)training_operation = optimizer.minimize(loss_operation)# Define accuracy evaluation# calculate the average accuracy by calling evaluate(X_data, y_data)correct_prediction = tf.equal(tf.argmax(logits, axis=1), tf.argmax(one_hot_y, axis=1))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))def evaluate(X_data, y_data): num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] accuracy = sess.run(accuracy_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y, keep_prob: 1.0&#125;) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples \"\"\"Run Session\"\"\"### Train your model here.import timeif not os.path.isdir('./models'): os.makedirs('./models')#saver = tf.train.Saver()accumulate_time = 0.0with tf.Session() as sess: sess.run(tf.global_variables_initializer()) num_examples = img_num print(\"Training...\") print() train_accuracy = np.zeros(EPOCHS) validation_accuracy = np.zeros(EPOCHS) for i in range(EPOCHS): stime = time.time() acc_train_accuracy = 0 X_train, y_train = shuffle(images, labels1Hot) for offset in range(0, num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] sess.run(training_operation, feed_dict=&#123;x: batch_x, one_hot_y: batch_y, keep_prob: drop_out_keep_prob&#125;) etime = time.time() accumulate_time += etime - stime validation_accuracy[i] = evaluate(images_valid, labels1Hot_valid) print(\"EPOCH &#123;&#125; ...\".format(i+1)) print(\"Validation Accuracy = &#123;:.3f&#125;\".format(validation_accuracy[i])) print() print('Cost time: ' + str(accumulate_time) + ' sec.') åŒæ­¥å¼ data Parallelism åœ¨å…©å¼µ GPU è·‘ MNISTä¸€æ¨£ç›´æ¥ä¸Š Codes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287import gzipimport osimport tempfileimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.layers import flattenfrom tensorflow.examples.tutorials.mnist import input_datafrom sklearn.utils import shuffle\"\"\"Data Loading\"\"\"dataPath='../dataset/MNIST_data/'mnist = input_data.read_data_sets(dataPath, one_hot=True)# read the images and reformat the image shape from [img_num,img_height,img_width] to [img_num,img_height,img_width,1]img_width = 28img_height = 28images = mnist.train.imagesimg_num, _ = images.shapeimages = np.reshape(images,(img_num,img_height,img_width))images = images[...,np.newaxis]print('(Input to CNN) Images with shape &#123;&#125;'.format(images.shape))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# read the labelslabels1Hot = mnist.train.labelsprint('(Input to CNN) labels1Hot.shape = &#123;&#125;'.format(labels1Hot.shape))labels = np.argmax(labels1Hot,axis=1)labels = labels[...,np.newaxis]print('labels.shape = &#123;&#125;'.format(labels.shape))n_classes = len(np.unique(labels))# load the validation setimages_valid = mnist.validation.imagesimg_num_valid = len(images_valid)images_valid = np.reshape(images_valid,(img_num_valid,img_height,img_width))images_valid = images_valid[...,np.newaxis]labels1Hot_valid = mnist.validation.labelsprint('Having %d number of validation images' % img_num_valid)plt.figure(figsize=(15,5))for i in np.arange(2*7): random_idx = np.random.randint(0,img_num) plt.subplot(2,7,i+1) plt.imshow(images[random_idx][...,0],cmap='gray') plt.title(labels[random_idx][0])\"\"\"First define the hyper-parameters\"\"\"# Hyper-parametersEPOCHS = 30BATCH_SIZE = 512rate = 0.001drop_out_keep_prob = 0.5ksize = 5cnn_depth_list = [16, 32]mlp_depth_list = [256, 128]cNum = 1\"\"\"Define the graph and construct it\"\"\"class MNISTCNN: def __init__(self, ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes): self._ksize = ksize self._cnn_depth_list = cnn_depth_list self._mlp_depth_list = mlp_depth_list self._img_height = img_height self._img_width = img_width self._cNum = cNum self._n_classes = n_classes self._mu = 0 self._sigma = 0.1 def create(self,x,keep_prob): conv = self._conv(x, self._cNum, self._ksize, self._cnn_depth_list[0], 'conv1') # Pooling. Input = 24x24xlayer_depth['layer_1']. Output = 12x12xlayer_depth['layer_1']. conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') for lidx in range(1,len(self._cnn_depth_list)): conv = self._conv(conv, self._cnn_depth_list[lidx-1], self._ksize, self._cnn_depth_list[lidx], 'conv&#123;&#125;'.format(lidx+1)) conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') fc = flatten(conv) bsize, fc_in_dim = fc.shape fc = self._fc(fc,fc_in_dim,self._mlp_depth_list[0],'fc1') fc = tf.nn.dropout(fc, keep_prob) # dropout for lidx in range(1,len(self._mlp_depth_list)): fc = self._fc(fc,self._mlp_depth_list[lidx-1],self._mlp_depth_list[lidx],'fc&#123;&#125;'.format(lidx+1)) fc = tf.nn.dropout(fc, keep_prob) # dropout with tf.variable_scope('logits') as scope: with tf.device('/cpu:0'): logits_w = tf.get_variable('logits_w', shape=[self._mlp_depth_list[-1],self._n_classes],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) logits_b = tf.get_variable('logits_b', shape=[self._n_classes],\\ initializer=tf.zeros_initializer) logits = tf.nn.xw_plus_b(fc, logits_w, logits_b, name=scope.name) print(logits.shape) return logits def _conv(self, x, in_depth, ksize, out_depth, scope_name, relu=True): bsize,h,w,cNum = x.shape assert(h-(ksize-1)&gt;=1) assert(w-(ksize-1)&gt;=1) with tf.variable_scope(scope_name) as scope: with tf.device('/cpu:0'): # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=(ksize, ksize, in_depth, out_depth),\\ initializer=tf.random_normal_initializer(self._mu,self._sigma)) biases = tf.get_variable('biases', shape=(out_depth),initializer=tf.zeros_initializer) out = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding='VALID',name=scope.name) + biases if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out def _fc(self, x, num_in, num_out, scope_name, relu=True): \"\"\"Create a fully connected layer.\"\"\" with tf.variable_scope(scope_name) as scope: with tf.device('/cpu:0'): # Create tf variables for the weights and biases weights = tf.get_variable('weights', shape=[num_in, num_out],\\ initializer=tf.random_uniform_initializer(-0.1,0.1)) biases = tf.get_variable('biases', [num_out],\\ initializer=tf.zeros_initializer) # Matrix multiply weights and inputs and add bias out = tf.nn.xw_plus_b(x, weights, biases, name=scope.name) if relu: # Apply ReLu non linearity relu = tf.nn.relu(out) return relu else: return out mnistCNN = MNISTCNN(ksize, cnn_depth_list, mlp_depth_list, img_height, img_width, cNum, n_classes)# Averaging gradients for all tower models on GPUdef average_gradients(tower_grads): \"\"\"Calculate the average gradient for each shared variable across all towers. Note that this function provides a synchronization point across all towers. Args: tower_grads: List of lists of (gradient, variable) tuples. The outer list is over individual gradients. The inner list is over the gradient calculation for each tower. Returns: List of pairs of (gradient, variable) where the gradient has been averaged across all towers. \"\"\" average_grads = [] for grad_and_vars in zip(*tower_grads): # Note that each grad_and_vars looks like the following: # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN)) grads = [] for g, _ in grad_and_vars: # Add 0 dimension to the gradients to represent the tower. expanded_g = tf.expand_dims(g, 0) # Append on a 'tower' dimension which we will average over below. grads.append(expanded_g) # Average over the 'tower' dimension. grad = tf.concat(axis=0, values=grads) grad = tf.reduce_mean(grad, 0) # Keep in mind that the Variables are redundant because they are shared # across towers. So .. we will just return the first tower's pointer to # the Variable. v = grad_and_vars[0][1] grad_and_var = (grad, v) average_grads.append(grad_and_var) return average_grads# Construct model for each GPU, where variables are shared/updated by CPUwith tf.device('/cpu:0'): optimizer = tf.train.AdamOptimizer(learning_rate = rate)# Calculate the gradients for each model tower.tower_grads = []logits_list = []feed_x = []feed_one_hot_y = []keep_prob = tf.placeholder(tf.float32) # probability to keep unitswith tf.variable_scope(tf.get_variable_scope()): for i in range(2): with tf.device('/gpu:%d' % i): x = tf.placeholder(tf.float32, (None, img_height, img_width, cNum)) feed_x.append(x) one_hot_y = tf.placeholder(tf.int32, (None, n_classes)) feed_one_hot_y.append(one_hot_y) logits = mnistCNN.create(x,keep_prob) logits_list.append(logits) cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits) loss_op = tf.reduce_mean(cross_entropy) tf.get_variable_scope().reuse_variables() # Calculate the gradients for each batch of data on this model tower. grads = optimizer.compute_gradients(loss_op) # Keep track of the gradients across all towers. tower_grads.append(grads)with tf.device('/cpu:0'): # We must calculate the mean of each gradient. Note that this is the # synchronization point across all towers. grads = average_gradients(tower_grads) # Apply the gradients to adjust the shared variables. apply_gradient_op = optimizer.apply_gradients(grads) training_op = apply_gradient_op \"\"\"Prediction/Inference Part\"\"\"# Define accuracy evaluation, calculate the average accuracy by calling evaluate(X_data, y_data)# Using model that in the First GPU to calculatecorrect_prediction = tf.equal(tf.argmax(logits_list[0], axis=1), tf.argmax(feed_one_hot_y[0], axis=1))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))def evaluate(X_data, y_data): num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] accuracy = sess.run(accuracy_operation, feed_dict=&#123;feed_x[0]: batch_x, feed_one_hot_y[0]: batch_y, keep_prob: 1.0&#125;) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples \"\"\"Run Session\"\"\"# &#123;feed_x[0]:batch_x_1, feed_x[1]:batch_x_2,\\# feed_one_hot_y[0]:batch_y_1, feed_one_hot_y[1]:batch_y_1, keep_prob:drop_out_keep_prob&#125;def gen_feed_dict(batch_x, batch_y, drop_out_keep_prob): assert(len(batch_x)==len(batch_y)) assert(len(batch_x)%2==0) data_num = int(len(batch_x)/2) rtn_dict = &#123;&#125; rtn_dict[feed_x[0]] = batch_x[:data_num] rtn_dict[feed_x[1]] = batch_x[data_num:] rtn_dict[feed_one_hot_y[0]] = batch_y[:data_num] rtn_dict[feed_one_hot_y[1]] = batch_y[data_num:] rtn_dict[keep_prob] = drop_out_keep_prob return rtn_dict ### Train your model here.import timeif not os.path.isdir('./models'): os.makedirs('./models')#saver = tf.train.Saver()accumulate_time = 0.0with tf.Session() as sess: sess.run(tf.global_variables_initializer()) num_examples = img_num print(\"Training...\") print() train_accuracy = np.zeros(EPOCHS) validation_accuracy = np.zeros(EPOCHS) stime = time.time() for i in range(EPOCHS): stime = time.time() acc_train_accuracy = 0 X_train, y_train = shuffle(images, labels1Hot) for offset in range(0, num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] feed_dict = gen_feed_dict(batch_x, batch_y, drop_out_keep_prob) sess.run(training_op, feed_dict=feed_dict) etime = time.time() accumulate_time += etime - stime validation_accuracy[i] = evaluate(images_valid, labels1Hot_valid) print(\"EPOCH &#123;&#125; ...\".format(i+1)) print(\"Validation Accuracy = &#123;:.3f&#125;\".format(validation_accuracy[i])) print('Cost time: ' + str(accumulate_time) + ' sec.') å¹¾å€‹æ³¨æ„è™•: è¨˜å¾—å»ºç«‹ variables (tf.get_variable) æ™‚è¦ä½¿ç”¨ with tf.device(&#39;/cpu:0&#39;): ç¢ºä¿è®Šé‡æ˜¯å­˜åœ¨ cpu å…§ å¯ä»¥è·‘ä¸€å°æ®µ code: 12with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: sess.run(tf.global_variables_initializer()) ä¾†è§€å¯Ÿè®Šé‡æ˜¯å¦æ­£ç¢ºæ”¾åœ¨ cpu ä¸Šã€‚ å»¶çºŒ 2. è‹¥ä½¿ç”¨ jupyter notebook å¯ä»¥é€™æ¨£åš jupyter notebook &gt; outputlogï¼ŒåŸ·è¡Œå®Œ 2. çš„ code æ¥è‘— cat outputlog | grep &#39;cpu&#39; è§€å¯Ÿè®Šé‡æ˜¯å¦å­˜åœ¨ã€‚ ä½¿ç”¨ collections (å¦‚ä¸‹) ä¾†ç¢ºèªè®Šæ•¸æœ‰æ­£ç¢ºåˆ†äº« (é¤Šæˆå¥½ç¿’æ…£)123trainable_collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)global_collection = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)print('Without Scope: len(trainable_collection)=&#123;&#125;; len(global_collection)=&#123;&#125;'.format(len(trainable_collection),len(global_collection))) Benchmark Results batch size = 128 æ™‚ï¼Œä½¿ç”¨å…©å¼µ GPU èŠ±çš„æ™‚é–“ç‚ºä¸€å¼µçš„ 0.78 å€ã€‚è€Œ batch size = 512 æ™‚çš„æ•ˆæœæ›´æ˜é¡¯ï¼Œç‚º 0.69 å€ã€‚ ä¸€é»å°çµè«–é€™ç¨®åŒæ­¥çš„æ¶æ§‹é©åˆåœ¨ batch size å¤§çš„æ™‚å€™ï¼Œæ•ˆæœæœƒæ›´æ˜é¡¯ã€‚å¯¦é©—èµ·ä¾†å…©å¼µå¡åœ¨ 512 batch size èŠ±çš„æ™‚é–“åœ¨ä¸€å¼µå¡çš„ 0.7 å€ã€‚ä¸éç›¸æ¯”ä½¿ç”¨å…©å¼µå¡ï¼Œä¸€å¼µå¡å…¶å¯¦æœ‰ä¸€é»å„ªå‹¢æ˜¯åœ¨è®Šé‡å…¨éƒ¨æ”¾åœ¨ GPU ä¸Šï¼Œå› æ­¤çœå»äº† CPU &lt;â€“&gt; GPU çš„å‚³è¼¸ä»£åƒ¹ã€‚é€™ä¹Ÿæ˜¯ä¸»è¦åªåˆ° 0.7 å€ï¼Œè€Œæ²’æœ‰æ¥è¿‘ 0.5 å€çš„é—œéµåŸå› ã€‚ Reference Tensorflow å®˜ç¶² åŒæ­¥å¼ data Parallelism æ–¹æ³• Tensorflow github cifar10_multi_gpu_train.py","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"}]},{"title":"A Toy Example for Teacher Student Domain Adaptation","date":"2017-10-22T02:19:54.000Z","path":"2017/10/22/A-Toy-Example-for-Teacher-Student-Domain-Adaptation/","text":"çœ‹äº†é€™ç¯‡ 2017 Microsoft AI and Research çš„æ–‡ç«  â€œLarge-Scale Domain Adaptation via Teacher-Student Learningâ€œ è¦ºå¾—æ»¿æœ‰æ„æ€çš„ï¼ŒåŠ ä¸Šå¾ˆå®¹æ˜“å¯¦ä½œï¼Œå› æ­¤å°±åˆ†æä¸€ä¸‹é€™ç¯‡çš„å¯è¡Œæ€§ã€‚ è¨­è¨ˆäº†ä¸€å€‹ MNIST Toy Example ä¾†å±•ç¤º T/S Learning çš„èƒ½åŠ›ï¼Œè‡ªå·±ä¹Ÿæƒ³çŸ¥é“é€™å€‹æ–¹æ³•æœ‰å¤šå¯é ã€‚ç›¸é—œçš„å¯¦é©— code è«‹åƒè€ƒ github TS Learning Methodsä»‹ç´¹ä¸€ä¸‹ TS Learning çš„æ–¹æ³•ã€‚ä»–è¦è§£æ±ºçš„å•é¡Œæè¿°å¦‚ä¸‹ å‡è¨­æˆ‘å€‘å·²ç¶“æœ‰äº†ä¸€å€‹è¨“ç·´å¥½çš„èªéŸ³è¾¨è­˜æ¨¡å‹ï¼Œç¾åœ¨è¦è¾¨è­˜é å ´çš„è²éŸ³ï¼ŒåŸ Domain (è¿‘å ´éŒ„è£½çš„è²éŸ³)ï¼Œå¯èƒ½æ•ˆæœå°±æœƒä¸å¥½ã€‚è¦è§£æ±ºæœ€ç›´æ¥çš„æ–¹æ³•å°±æ˜¯é‡æ–°éŒ„è£½é å ´èªæ–™ï¼ŒéŒ„è£½çš„éç¨‹ä¸­å¾ˆå®¹æ˜“å¯ä»¥å–å¾—åŒæ™‚æœ‰è¿‘å ´å’Œé å ´æœªæ¨™è¨˜çš„èªæ–™ (æ”¾å…©å€‹éº¥å…‹é¢¨ï¼Œä¸€å€‹è¿‘ä¸€å€‹é )ï¼Œä¸éé—œéµæ˜¯æ¨™è¨˜æˆæœ¬å¤ªé«˜ã€‚å› æ­¤é€™ç¯‡å°±æ˜¯æƒ³åˆ©ç”¨æœªæ¨™è¨˜çš„èªæ–™ç›´æ¥é‡å°åŸ Domain çš„æ¨¡å‹ Adapt åˆ°æ–°çš„ Domain ä¸Šã€‚ ä»¥ä¸Šé¢è«–æ–‡ä¸­çš„åœ–ä¾†èªªï¼Œå·¦é‚Š Teacher network åªèƒ½åœ¨ Source Domain æœ‰å¥½çš„è¾¨è­˜èƒ½åŠ›ï¼Œç›®æ¨™æ˜¯å¸Œæœ›å¾—åˆ°å³é‚Šçš„ Student network èƒ½åœ¨ Target Domain é‡å°åŒæ¨£å•é¡Œä¹Ÿæœ‰å¥½çš„è¾¨è­˜èƒ½åŠ›ã€‚è«–æ–‡æ–¹æ³•æ˜¯ä¸€é–‹å§‹å…ˆå°‡ Teacher network æ‹·è²ä¸€ä»½çµ¦ Student network ï¼Œæ¥è‘—å°±é–‹å§‹é¤µ parallel data çµ¦å…©å€‹ networksã€‚ æ‰€è¬‚ parallel data æ„æ€æ˜¯ç›¸åŒçš„è³‡æ–™ä¾†æºï¼Œä½†æ˜¯åœ¨ä¸åŒ domain è’é›†ï¼Œä¾‹å¦‚åŒä¸€å€‹äººè¬›åŒä¸€å¥è©±ï¼Œä¸€å€‹è¿‘å ´éº¥å…‹é¢¨è’é›†åˆ°ï¼Œå¦ä¸€å€‹é å ´è’é›†åˆ°ã€‚ç›®æ¨™å‡½å¼å°±æ˜¯å¸Œæœ›å…©å€‹ network çš„å¾Œé©—æ¦‚ç‡ç›¸åŒ (å…©è€…çš„å¾Œé©—æ¦‚ç‡å¹³æ–¹èª¤å·®ç‚º0)ï¼Œè€Œæˆ‘å€‘åªæ›´æ–° Student networkã€‚åœ¨å¯¦ä½œä¸Šä¸æœƒä½¿ç”¨å¾Œé©—æ¦‚ç‡ä¾†è¨ˆç®—å…©å€‹ network çš„èª¤å·®ï¼Œæœƒä½¿ç”¨æœªç¶“é softmax çš„é‚£å±¤ï¼Œä¹Ÿå°±æ˜¯ä¸€èˆ¬èªªçš„ logits ä¾†è¨ˆç®—ã€‚åŸå› ç°¡å–®èªªæ˜å¦‚ä¸‹: softmax æœƒå°‡åŒæ¨£æ˜¯ negative çš„é¡åˆ¥çš„æ©Ÿç‡éƒ½å£“åˆ°å¾ˆä½ï¼Œä½†æ˜¯ negative examples ä¹Ÿæœ‰åˆ†å¥½å£ï¼Œè®€è€…å¯ä»¥è©¦è©¦ [10,2,1] ç¶“é softmax å¾Œï¼Œ 2 è·Ÿ 1 ä¹‹é–“çš„å·®ç•°æœƒè¢«æŠ¹å¹³ã€‚å› æ­¤å¥½çš„åšæ³•æ˜¯ï¼Œä¸è¦ä½¿ç”¨ softmax ï¼Œè€Œæ˜¯ä½¿ç”¨ logitsã€‚ Hinton åœ¨é€™ç¯‡è«–æ–‡è£¡ä¿®æ”¹äº† softmax å‡½å¼ï¼Œå¤šäº†ä¸€å€‹ temperature $T$ï¼Œè«–æ–‡è£¡æ¨å°é€™æ¨£ä¿®æ”¹çš„ softmaxï¼Œå…¶å¯¦è·Ÿç›®æ¨™å‡½å¼ä½¿ç”¨ logits çš„å¹³æ–¹èª¤å·®æ˜¯ä¸€æ¨£çš„ (åœ¨ â€œTè·Ÿlogitså·®ç•°å¾ˆå¤§â€ ä¸” â€œlogitsçš„åˆ†å¸ƒå‡å€¼ç‚º0â€ çš„æ¢ä»¶ä¸‹) é€™æ¨£åšçš„ç‰©ç†æ„ç¾©å°±ç›¸ç•¶æ–¼ï¼Œå°‡ â€œæŸè²éŸ³çš„é å ´è¡¨ç¾åœ¨ Student network çœ¼è£¡â€ï¼Œè¦–ç‚ºè·Ÿ â€œè©²è²éŸ³çš„è¿‘å ´è¡¨ç¾åœ¨ Teacher network çœ¼è£¡â€ èªå®šç‚ºç›¸åŒä¸€ä»¶äº‹æƒ…ã€‚å› æ­¤å°±ä¸éœ€è¦é‡å° data åšæ¨™è¨˜äº†ï¼Œåªéœ€è¦æ‹¿åˆ°é€™æ¨£çš„ä¸€å¤§å † parallel data å°±å¯ä»¥ï¼Œè€Œé€™å¾ˆå®¹æ˜“ã€‚ é™„ä¸Šè«–æ–‡ä¸Šçš„æ­¥é©Ÿå¦‚ä¸‹: æ¼”ç®—æ³•å°±é€™æ¨£è€Œå·²ï¼Œå¾ˆå–®ç´”å§ã€‚ä½†æ˜¯ç©¶ç«Ÿæœ‰å¤šé æ™®? å¥½å¥‡å¿ƒä¸‹ï¼Œå°±ç”¨ MNIST è¨­è¨ˆäº† toy exampleï¼Œå°±æ˜¯æ¥ä¸‹ä¾†çš„å…§å®¹å›‰ã€‚ MNIST Toy Example for TS Learningå¯¦é©—è¨­å®š and Teacher Networké¦–å…ˆè¨­å®šå…©å€‹ Domain ç‚º: ä¸€å€‹åŸåœ– (åŸä¸–ç•Œ)ï¼Œå¦ä¸€å€‹ä¸Šä¸‹é¡›å€’çš„åœ– (ä¸Šä¸‹é¡›å€’çš„ä¸–ç•Œ)ã€‚ Teacher network æ˜¯ä¸€å€‹å¾ˆç°¡å–®çš„ â€œ6â€ å’Œ â€œ9â€ çš„è¾¨è­˜å™¨ï¼Œç•¶ç„¶æ˜¯åœ¨åŸä¸–ç•Œè¨“ç·´å¥½çš„ã€‚å¦‚æœç›´æ¥æ‹¿ teacher network å»çœ‹é¡›å€’çš„ 6ï¼ŒæœŸæœ›å®ƒèªå‡ºä¸€æ¨£æ˜¯ 6 æ˜¯è¾¨è­˜ä¸å‡ºä¾†çš„! (åŒæ¨£æœŸæœ› teacher network çœ‹å‡ºé¡›å€’çš„ 9 ä»ç„¶æ˜¯ 9 ä¹Ÿæ˜¯è¾¦ä¸åˆ°çš„) ä¹‹æ‰€ä»¥æœƒé¸ 6 å’Œ 9ï¼Œæ˜¯å› ç‚ºå°±ç®—ä¸Šä¸‹é¡›å€’ï¼Œé¡›å€’çš„ 6 å’Œæ­£å‘çš„ 9 çœ‹èµ·ä¾†ä»ç„¶æ˜¯ä¸åŒçš„! åŒæ¨£çš„ï¼Œé¡›å€’çš„ 9 å’Œæ­£å‘çš„ 6 ä¸€æ¨£çœ‹èµ·ä¾†ä¸åŒ ! æˆ‘å€‘å¾—åˆ°çš„ Teacher network è¾¨è­˜æƒ…æ³å¦‚ä¸‹: 12345678910Training...EPOCH 1 ...Train Accuracy = 0.967; Flip Accuracy = 0.098EPOCH 2 ...Train Accuracy = 0.999; Flip Accuracy = 0.151EPOCH 3 ...Train Accuracy = 0.999; Flip Accuracy = 0.110 æ˜é¡¯çœ‹åˆ°è¾¨è­˜ç‡æ¥è¿‘ 100%ï¼Œä½†æ˜¯ä¸€æ—¦ä¸Šä¸‹é¡›å€’ï¼Œè¾¨è­˜ç‡åªå‰© 10%ã€‚æœ‰æ„æ€çš„æ˜¯ï¼Œç”±æ–¼æˆ‘å€‘åªæœ‰å…©å€‹é¡åˆ¥ï¼Œå°æ–¼ä¸Šä¸‹é¡›å€’çš„è¾¨è­˜ç‡å‰©10%å¯ä»¥çœ‹åš: é¡›å€’çš„ 6ï¼Œæœƒè¢«èªæˆ 9ï¼Œè€Œé¡›å€’çš„ 9 æœƒè¢«èªç‚º 6ã€‚ä½†äº‹å¯¦ä¸Šï¼Œé¡›å€’çš„ 6 å’Œ 9 é‚„æ˜¯ä¸ä¸€æ¨£ã€‚ Student network è¨“ç·´æˆ‘å€‘å°‡ MNIST å…¶ä»–å½±åƒä¸Šä¸‹é¡›å€’ï¼Œåšå‡º parallel datasetï¼Œç„¶å¾ŒæŒ‰ç…§è«–æ–‡çš„åšæ³•åš unsupervised trainingã€‚æœ‰è¶£çš„æ˜¯å¾—åˆ°çµæœå¦‚ä¸‹: 12345678910111213141516171819EPOCH 1 ...Acc loss = 3.871242271944786Train Accuracy = 0.156; Flip Accuracy = 0.998EPOCH 2 ...Acc loss = 0.40557907682784994Train Accuracy = 0.101; Flip Accuracy = 0.999EPOCH 3 ...Acc loss = 0.3005437100890939Train Accuracy = 0.103; Flip Accuracy = 0.999EPOCH 4 ...Acc loss = 0.2651689475203995Train Accuracy = 0.097; Flip Accuracy = 0.999EPOCH 5 ...Acc loss = 0.23342516055794454Train Accuracy = 0.116; Flip Accuracy = 0.999 Student network å¯ä»¥æˆåŠŸè¾¨è­˜ é¡›å€’çš„ 6 å’Œé¡›å€’çš„ 9 äº†! æ³¨æ„ï¼Œæˆ‘å€‘å¾ä¾†æ²’æœ‰çµ¦é Student network é¡›å€’çš„ 6 å’Œé¡›å€’çš„ 9 é€™äº›è¨“ç·´è³‡æ–™! ä½†æ˜¯ç¾åœ¨å®ƒæœ‰èƒ½åŠ›è¾¨è­˜é€™å…©ç¨®åœ–äº†! ä½†æ˜¯åŒæ¨£çš„ï¼Œå¦‚æœçµ¦ student network çœ‹ä¸€å€‹æ­£å‘çš„ 6ï¼Œåœ¨ä»–çš„çœ¼å“©ï¼Œçœ‹èµ·ä¾†å°±å¦‚åŒ teacher network çœ‹åˆ° 9 ä¸€æ¨£ã€‚ ä¹Ÿå°±æ˜¯èªªï¼ŒStudent network å¤±å»äº†åŸ Domain çš„è¾¨è­˜èƒ½åŠ›ã€‚ é€™èˆ‡è«–æ–‡åŸä½œè€…çš„çµè«–ä¸å¤§ä¸€æ¨£ã€‚ ç”¨ parallel data éç›£ç£å­¸ç¿’åˆ°åº•å­¸åˆ°äº†ä»€éº¼? çµ¦ T/S ç¶²è·¯çœ‹éå¾ˆå¤šå¾ˆå¤šçš„ parallel data å¾Œï¼ŒTeacher çœ¼è£¡çš„åœ–ï¼Œåœ¨ Student çœ¼è£¡çœ‹èµ·ä¾†å°±åéä¾†ï¼Œåä¹‹äº¦ç„¶ã€‚å› æ­¤é€™æ™‚å€™å¦‚æœçµ¦ Student network çœ‹ä¸€å€‹ â€œæ­£å‘çš„6â€ï¼Œå®ƒæœƒèªç‚º: å•Š!é€™åœ¨ Teacher çœ¼è£¡çœ‹åˆ°çš„æ˜¯ä¸€å€‹é¡›å€’çš„ 6 ã€‚(è€Œ teacher network æœƒå°‡é¡›å€’çš„ 6 çœ‹åšæ˜¯ 9) å› æ­¤æˆ‘èªç‚ºï¼ŒStudent netowrk å¾ˆå®¹æ˜“å¤±å»åŸå…ˆ domain çš„è¾¨è­˜èƒ½åŠ›ï¼Œå°±åƒé€™å€‹ä¾‹å­ student network ç„¡æ³•èªå‡ºæ­£å‘çš„ 6 ä¸€æ¨£ã€‚ Summaryå¦‚ä½•è®“ä¸€å€‹ network åŒæ™‚æœ‰åŸ Domain å’Œæ–° Domain çš„è¾¨è­˜èƒ½åŠ›å‘¢ ? ä»¥ä¸Šé¢çš„ toy example ç‚ºä¾‹ï¼Œå°±æ˜¯è¾¨è­˜å…©å€‹ classes class 1: 6 and é¡›å€’çš„6class 2: 9 and é¡›å€’çš„9 æœ€ç›´è¦ºçš„åšæ³•ï¼Œå°±æ˜¯ T and S models éƒ½è·‘ä¸€æ¬¡è¾¨è­˜ï¼Œç„¶å¾Œå°‡å…©å€‹å¾Œé©—æ¦‚ç‡åŠ èµ·ä¾†å¾Œç®— argmaxã€‚ç¼ºé»å°±æ˜¯ model size ç«‹é¦¬è®Šæˆå…©å€ã€‚ æ€éº¼è®“æ¨¡å‹ size ä¸è¦è®Šæˆå…©å€å‘¢? ç°¡å–®æƒ³äº†ä¸€å€‹æ–¹å¼ï¼Œå°±æ˜¯è®“ student model æ”¹æˆé€™æ¨£çš„æ¨¡å‹: å…¶ä¸­ M model çš„éƒ¨åˆ†è² è²¬å°‡ ä¸Šä¸‹é¡›å€’çš„ domain è½‰æ›æˆåŸ domain çš„ inputï¼Œç„¶å¾Œé€™æ¨£çš„ input å°±å¯ä»¥åŸå°ä¸å‹•åœ°ç”¨ teacher model å»è¾¨è­˜ã€‚å‰›å¥½é€™å€‹å•é¡Œå…¶å¯¦ç”¨ä¸€å€‹ permuation matrix å¯ä»¥åšä¸Šä¸‹é¡›å€’ï¼Œå› æ­¤å¯¦é©—ä¸Šå°±ç›´æ¥ä½¿ç”¨ä¸€å€‹ linear layer (æ²’æœ‰ activation function)ï¼Œç•¶ç„¶ backprob ç®—å‡ºä¾†çš„ä¸æœƒæ­£å¥½æ˜¯ permutation matrix å°±æ˜¯äº†ã€‚ æ”¶æ–‚æƒ…æ³å¦‚ä¸‹: åŸºæœ¬ä¸Šæ¯”åŸå…ˆè¦æ…¢ï¼Œå› ç‚ºåŸä¾†æ˜¯æ‰€æœ‰çš„ weights éƒ½å¯ä»¥èª¿æ•´ï¼Œè€Œç¾åœ¨åªèƒ½å‹•ä¸€å€‹ linear layer 12345678910111213141516171819EPOCH 1 ...Acc loss = 9.52696829760659Train Accuracy = 0.460; Flip Accuracy = 0.806EPOCH 2 ...Acc loss = 3.6580730849143146Train Accuracy = 0.364; Flip Accuracy = 0.955EPOCH 3 ...Acc loss = 2.454553008463332Train Accuracy = 0.304; Flip Accuracy = 0.980EPOCH 4 ...Acc loss = 1.823352760733923Train Accuracy = 0.277; Flip Accuracy = 0.988EPOCH 5 ...Acc loss = 1.4707165408316494Train Accuracy = 0.235; Flip Accuracy = 0.992 é€™æ¨£åšæ³•é›–ç„¶ model size å°äº†å¾ˆå¤šï¼Œä½†æ˜¯è¦åŒæ™‚è¾¨è­˜æ­£çš„å’Œé¡›å€’çš„ä»ç„¶è¦è·‘å…©éçš„ modelã€‚ æœ‰æ²’æœ‰æ–¹æ³•çµåˆ TS learning unsupervised çš„æ–¹å¼ï¼Œä¸”åŒæ™‚å…¼é¡§å…©é‚Šçš„ domain è¾¨è­˜èƒ½åŠ›å‘¢? å°±å†æ€è€ƒçœ‹çœ‹å›‰ã€‚ Reference Large-Scale Domain Adaptation via Teacher-Student Learning Distilling the Knowledge in a Neural Network Toy Example github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Adaptation","slug":"Adaptation","permalink":"http://yoursite.com/tags/Adaptation/"}]},{"title":"Word Embeddings (Encoder-Decoder æ¶æ§‹)","date":"2017-09-07T13:22:53.000Z","path":"2017/09/07/Word-Embeddings-and-Encoder-Decoder-Neural-Net/","text":"From Sparse Vector to Embeddings with Encoderâ€“Decoder Structure æ±‚ Embeddings Encoderâ€“Decoder çµæ§‹ å­—å…¸å‘é‡è‹¥æˆ‘å€‘å­—å…¸è£¡æœ‰ $N$ å€‹ words, ç¬¬ $i$ å€‹å­— $w^i$ æ‡‰è©²æ€éº¼è¡¨ç¤ºå‘¢? é€šå¸¸ä½¿ç”¨ one-hot vector ä¾†è¡¨ç¤º: æŠŠ $w^i$ è®Šæˆä¸€å€‹é•·åº¦ $N$ çš„å‘é‡ $x^i$ã€‚ æ­å–œ! æœ‰äº† vector æˆ‘å€‘å°±å¯ä»¥å¥—ç”¨æ•¸å­¸æ¨¡å‹äº†ã€‚ å•é¡Œæ˜¯é€™æ¨£çš„å‘é‡å¤ªç¨€ç–äº†ï¼Œå°¤å…¶æ˜¯ç•¶å­—å…¸éå¸¸å¤§çš„æ™‚å€™ã€‚ ç¨€ç–å‘é‡å°æ–¼æ¨¡å‹è¨“ç·´å¾ˆæ²’æœ‰æ•ˆç‡ã€‚ æˆ‘å€‘éœ€è¦è½‰æ›åˆ°æ¯”è¼ƒç·Šå¯†çš„å‘é‡ï¼Œé€šå¸¸ç¨±ç‚º embeddingã€‚ ä¸‹åœ–èˆ‰ä¾‹å°‡ $x$ å°æ‡‰åˆ°å®ƒçš„ç·Šå¯†å‘é‡ $e$, ç·Šå¯†å‘é‡æœ‰ embed_dim ç¶­åº¦ å…ˆå‡è¨­å·²çŸ¥å¦‚ä½•å°æ‡‰åˆ°ç·Šå¯†å‘é‡å·²çŸ¥ä¸€å€‹ N * embed_dim çš„çŸ©é™£ $E$ï¼Œç¬¬ $i$ å€‹ row $e^i$ å°±æ˜¯ $w^i$ çš„ embeddingã€‚ æˆ‘å€‘å°±å¯ä»¥ä½¿ç”¨ $e$ ä¾†ä»£æ›¿åŸå…ˆçš„ç¨€ç–å‘é‡ $x$ é€²è¡Œè¨“ç·´ï¼Œè®“è¨“ç·´æ›´å¥½æ›´å®¹æ˜“ã€‚ ä»¥ä¸€å€‹èªè¨€æ¨¡å‹ä¾†èªªï¼Œä½¿ç”¨ LSTM æ¨¡å‹å¦‚ä¸‹: æ©ï¼Œé€™æ¨£å¤§åŠŸå‘Šæˆï¼Œæˆ‘å€‘çš„æ¨¡å‹å¯ä»¥é †åˆ©è¨“ç·´ â€¦. ?? ä¸å°ï¼Œ$E$ é€™å€‹ lookup table æ€éº¼æ±ºå®š? Lookup Table ä½¿ç”¨çŸ©é™£ç›¸ä¹˜ç­”æ¡ˆæ˜¯è®“æ¨¡å‹è‡ªå·±è¨“ç·´æ±ºå®šã€‚è¦æ›´äº†è§£å…§éƒ¨é‹ä½œï¼Œæˆ‘å€‘å…ˆå°‡ lookup table ä½¿ç”¨çŸ©é™£ç›¸ä¹˜çš„æ–¹å¼ä¾†çœ‹ã€‚ æ‰€ä»¥ä½¿ç”¨ lookup table LSTM çš„èªè¨€æ¨¡å‹è®Šæˆå¦‚ä¸‹ ç­‰ç­‰ï¼ŒçŸ©é™£ç›¸ä¹˜ä¸å°±è·Ÿ neural net ä¸€æ¨£å—? é€™æ¨£çœ‹èµ·ä¾†é€™å€‹ lookup table $E$ å°±æ˜¯ä¸€å±¤çš„é¡ç¥ç¶“ç¶²è·¯è€Œå·² (æ²’æœ‰ activation function)ã€‚ æˆ‘å€‘ç”¨ LL (Linear Layer) ä¾†ä»£è¡¨ï¼Œ$E$ å°±æ˜¯ LL çš„ weight matrixã€‚ è¡¨ç¤ºæˆ neural net çš„æ–¹å¼ï¼Œæˆ‘å€‘å°±ç›´æ¥å¯ä»¥ Backprob è¨“ç·´å‡º LL çš„ weight $E$ äº†ã€‚è€Œ $E$ å°±æ˜¯æˆ‘å€‘è¦æ‰¾çš„ embeddingsã€‚ Tensorflow ä¸­åšé€™æ¨£çš„ lookup table å¯ä»¥ä½¿ç”¨ tf.nn.embedding_lookup()ã€‚ Embedding çš„ä½œæ³•å¯åƒè€ƒ tf å®˜ç¶²æ­¤è™•ã€‚ LLå¾ˆå¼±æ€éº¼è¾¦?åªç”¨ä¸€å±¤ç·šæ€§çµ„åˆ (LL) å°±æƒ³æŠŠç‰¹å¾µæ“·å–åšåˆ°å¾ˆå¥½ï¼Œä¼¼ä¹æœ‰é»ç°¡åŒ–äº†ã€‚ æ²’éŒ¯ï¼Œæˆ‘å€‘éƒ½çŸ¥é“ï¼Œç‰¹å¾µæ“·å–æ˜¯ Deep neural net çš„æ‹¿æ‰‹å¥½æˆ²ï¼Œæ‰€ä»¥æˆ‘å€‘å¯ä»¥å°‡ LL æ›æˆå¼·å¤§çš„ CNNã€‚ é€™ç¨®å…ˆç¶“éä¸€å±¤ç‰¹å¾µæ“·å–ï¼Œå†åšè¾¨è­˜ï¼Œå…¶å¯¦è·Ÿ Encoder â€“ Decoder çš„æ¶æ§‹ä¸€æ¨£ã€‚ éƒ½æ˜¯å…ˆç¶“é Encoder åšå‡º embeddingsï¼Œæ¥è‘—ä½¿ç”¨ Embeddings decode å‡ºçµæœã€‚ Encoder å¦‚æœä¹Ÿæ¡ç”¨ RNN çš„è©±åŸºæœ¬ä¸Šå°±æ˜¯ sequence-to-sequence çš„æ¶æ§‹äº†ã€‚ åŸºæœ¬ä¸Šæ‹“å±•ä¸€ä¸‹ï¼Œå°åœ–æˆ–å½±åƒåš Encodeï¼Œè€Œ Decoder è² è²¬è§£ç¢¼å‡ºæè¿°çš„æ–‡å­—ã€‚æˆ–æ˜¯èªè¨€ç¿»è­¯ï¼ŒèªéŸ³è¾¨è­˜ï¼Œéƒ½å¯ä»¥é€™éº¼çœ‹å¾…ã€‚ Reference Embedding tf å®˜ç¶² link Sequence to sequence learning link Udacity lstm github colah lstm","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Embedding","slug":"Embedding","permalink":"http://yoursite.com/tags/Embedding/"}]},{"title":"AutoEncoder","date":"2017-08-26T08:38:22.000Z","path":"2017/08/26/AutoEncoder/","text":"ä½¿ç”¨ MNIST and notMNIST åšäº†ä¸€å€‹ AutoEncoder with Fully Connected DNN çš„å¯¦é©—ã€‚ ä¾åºå°‡å¯¦é©—çµæœæ ¹æ“šå¦‚ä¸‹æ­¥é©Ÿé¡¯ç¤ºå‡ºä¾†ï¼Œç¨‹å¼ç¢¼å¯ä»¥åƒè€ƒ [github] Data Loading and Plotting AutoEncoder Graph Constructiona. Define the input output tensorsb. Define the graph and construct itc. Define loss and optimizer Run Session Show some reconstructed images Plot Embeddings Do Image Generation by Decoder Data Loading and PlottingMNIST training data æœ‰ 55000 ç­†è³‡æ–™ï¼Œæ˜¯ä¸€å€‹ 28x28 çš„ imageï¼Œå€¼çš„ç¯„åœæ˜¯ [0~1]ï¼Œå› æ­¤æœƒå° input éƒ½æ¸›å» 0.5 æ­£è¦åŒ–ã€‚ è€Œ notMNIST æ•´ç†éå¾Œæœ‰ 200000 ç­†ï¼ŒåŒæ¨£ä¹Ÿæ˜¯ 28x28 çš„ imageï¼Œä½†å€¼çš„ç¯„åœå·²ç¶“æ˜¯ [-0.5~0.5]ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæ­¤è³‡æ–™é‚„åƒé›œè‘—ä¸€äº›éŒ¯èª¤ï¼Œå¦‚ä¸‹åœ–å°±å¯ç™¼ç¾ï¼Œç¬¬äºŒåˆ—çš„ç¬¬äºŒå€‹æ‡‰ç‚º Jï¼Œä½†æ˜¯æ¨™è¨˜æ˜¯ Aã€‚å› æ­¤ notMNIST ç›¸å°ä¾†èªªå¾ˆæŒ‘æˆ°ï¼Œä½†æˆ‘å€‘ä¸€æ¨£å¯ä»¥çœ‹åˆ° AutoEncoder ä¹Ÿæœƒåšå‡ºä¸€äº›åˆç†çš„å£“ç¸®ã€‚ AutoEncoder Graph ConstructionDefine the input output tensorsInput x èˆ‡ Output y éƒ½æ˜¯ä¸€æ¨£ (æ²’æœ‰è¦åš Denoise AutoEncoder)ï¼Œå…¶ä¸­ code æ˜¯å»ºç«‹ Decoder æ™‚çš„ input tensorã€‚ 1234x = tf.placeholder(tf.float32, (None, img_dim))y = tf.placeholder(tf.float32, (None, img_dim))embedding_dim = 2code = tf.placeholder(tf.float32, (None, embedding_dim)) Define the graph and construct ité‡å° Encoder å’Œ Decoder éƒ½ä½¿ç”¨åŒä¸€çµ„åƒæ•¸ï¼Œé€™æ¨£çš„å¥½è™•æ˜¯åƒæ•¸é‡ç›´æ¥å°‘ç´„ä¸€åŠï¼ŒåŒæ™‚æ¸›å°‘ overfitting çš„æ©Ÿæœƒã€‚ç•¶ç„¶æˆ‘å€‘æ²’æœ‰ç†ç”±ä¸€å®šè¦å°‡åƒæ•¸ç¶å†ä¸€èµ·ï¼Œå¯ä»¥å„è‡ªç”¨è‡ªå·±çš„æ–¹æ³• (åƒæ•¸ã€æ¨¡å‹çµæ§‹) å» Encode å’Œ Decocdeã€‚çµæ§‹å¦‚ä¸‹: Define loss and optimizeræ³¨æ„åˆ° loss çš„å®šç¾©é™¤äº†åŸä¾†çš„å½±åƒé‡å»ºèª¤å·®ä¹‹å¤–ï¼Œé‚„å¤šäº†ä¸€å€‹ embeddings çš„ l2-normã€‚é€™æ˜¯ç‚ºäº†å¸Œæœ›åœ¨ embedding space ä¸Š encode ä¹‹å¾Œéƒ½æ¥è¿‘ 0ï¼Œæ¸›å°‘é‚£ç¨®å¾ˆå¤§çš„ outliers å‡ºç¾ã€‚åƒè€ƒæå®æ¯… Deep AutoEncoder 123loss_op = tf.reduce_sum(tf.pow(tf.subtract(reconstruct_auto, y), 2.0)) + l2_weight* tf.reduce_sum(tf.pow(embedded_auto, 2.0))optimizer = tf.train.AdamOptimizer(learning_rate = rate)training_op = optimizer.minimize(loss_op) Run SessionAdam optimizer è·‘äº† 100 å€‹ epochs Show some reconstructed imageséš¨æ©Ÿé¸å¹¾å€‹ MNSIT çš„é‡å»ºåœ–: éš¨æ©Ÿé¸å¹¾å€‹ notMNSIT çš„é‡å»ºåœ–: å¯ä»¥çœ‹åˆ° notMNIST æœç„¶é›£å¤šäº†ã€‚ Plot EmbeddingsMNIST é‡å°æ‰€æœ‰ training data æ±‚å¾—çš„ 2-d embeddings å¦‚ä¸‹: notMNIST é‡å°æ‰€æœ‰ training data æ±‚å¾—çš„ 2-d embeddings å¦‚ä¸‹: å¦‚æœåªè¦åšåˆ° unsupervised dimension reduction çš„è©±ï¼Œä½¿ç”¨ t-SNE æ±‚å¾—çš„ embedding æœƒæ¯”ä¸Šåœ–éƒ½å¥½çœ‹å¾ˆå¤šã€‚ä½† t-SNE æ²’æœ‰ Decoderï¼Œç„¡æ³•çµ¦å®šä¸€å€‹ embedding å»æ±‚å›åŸå…ˆçš„ imageã€‚è€Œé€™ç¨® Encoder - Decoder çµæ§‹å°±ç›¸å°å½ˆæ€§å¾ˆå¤šã€‚ t-SNE çš„ MNIST åœ–å¦‚ä¸‹: Do Image Generation by Decoderæˆ‘å€‘é‡å° Embedding Space çš„ä¸€å€‹å€åŸŸå»ç­‰è·å–å‡ºå¾ˆå¤šé»ï¼Œç„¶å¾Œä½¿ç”¨ Decoder å» decode å‡º image ä¾†ã€‚ MNIST çš„ç¯„åœé¸æ“‡ç‚ºï¼Œ x è»¸å’Œ y è»¸ [-1~1] é–“éš” 0.2ï¼Œå…± 100 å€‹é»ã€‚(å¯åƒè€ƒä¸Šé¢ embedding space äº†è§£é¸æ“‡çš„ç¯„åœ) notMNIST çš„ç¯„åœé¸æ“‡ç‚ºï¼Œ x è»¸å’Œ y è»¸ [-2~2] é–“éš” 0.2ï¼Œå…± 400 å€‹é»ã€‚(å¯åƒè€ƒä¸Šé¢ embedding space äº†è§£é¸æ“‡çš„ç¯„åœ) å¯ä»¥ç™¼ç¾ embedding space çš„å…©å€‹ç¶­åº¦å…·æœ‰æŸäº›æ„ç¾©åœ¨! Reference æå®æ¯… Deep AutoEncoder Distill t-SNE Reducing the Dimensionality of Data with Neural Networks (Hinton 2006) æœ¬æ–‡ä¹‹ [github]","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"auto-encoder","slug":"auto-encoder","permalink":"http://yoursite.com/tags/auto-encoder/"}]},{"title":"TF Notes (1), Variable Construct Share and Collect","date":"2017-08-23T10:46:48.590Z","path":"2017/08/23/TF-Notes-Variable-Construct-Share-and-Collect/","text":"éš”äº†å¥½ä¹…æ²’æœ‰ç”¨ tensorflowï¼Œè€å¯¦èªªå·²ç¶“å¿˜å¾—å·®ä¸å¤šäº†ï¼ŒåŠ ä¸Šä¹‹å‰å…¶å¯¦æ²’æœ‰ä»”ç´°äº†è§£ tf.Variable() çš„ class å’Œ variable scope çš„æ„æ€ã€‚å› æ­¤å°±è¶é€™å€‹æ©Ÿæœƒäº†è§£ä¸€ç•ªï¼Œé †ä¾¿ç•¶æˆä½¿ç”¨ tf çš„ç†±èº«å›‰! é€™æ˜¯ TensorFlow ç­†è¨˜ç³»åˆ—çš„ç¬¬ä¸€ç¯‡æ–‡ï¼Œä¹‹å¾Œé‡åˆ°ä»€éº¼è‡ªå·±ä¸æ‡‚çš„ï¼Œå°±æœƒå†ç­†è¨˜ä¸‹å»ã€‚ å‰è¨€ä»€éº¼æ˜¯ Scope ?ç°¡å–®ä¾†èªª Tensorflow ç‚ºæ¯ä¸€å€‹ object (å« variable) éƒ½æŒ‡å®šä¸€å€‹ ç¨ä¸€ç„¡äºŒçš„ ID, é€™å€‹ ID å¯ä»¥æ˜¯ä½¿ç”¨ scope ä¾†é”æˆçš„èˆ‰ä¾‹ä¾†èªª1234with tf.variable_scope(\"foo\"): with tf.variable_scope(\"bar\"): v = tf.get_variable(\"v\", [1])assert v.name == \"foo/bar/v:0\" å¯ä»¥çœ‹åˆ°è®Šæ•¸ v çš„ ID (v.name) ç‚º &quot;foo/bar/v:0&quot; æœ‰é€™æ¨£ç¨ä¸€ç„¡äºŒçš„ ID å¾Œï¼Œå°æ–¼æ‰€æœ‰çš„è®Šæ•¸å°±éå¸¸å¥½ç®¡ç†äº†! è®Šæ•¸çš„å»ºç«‹ã€åˆ†äº«å’Œè’é›†æ³¨æ„åˆ°å»ºç«‹å’Œå–å¾—è®Šæ•¸éƒ½ä½¿ç”¨ tf.get_variable()ï¼Œè€Œ tf.get_variable() åªæœ‰åœ¨ç¬¬ä¸€æ¬¡å‘¼å«è©² ID çš„è®Šæ•¸æ™‚æ‰æœƒå»ºç«‹ï¼Œå…¶å®ƒæ¬¡å‘¼å«å¦‚æœæ²’æœ‰å°‡ ID çš„ reuse è¨­å®šç‚º true å‰‡æœƒå ±éŒ¯ (èªç‚ºå·²ç¶“æœ‰è®Šæ•¸å­˜åœ¨äº†)ï¼Œå› æ­¤å¦‚æœ reuse è¨­å®šç‚º trueï¼Œtf.get_variable() å°±æœƒæ‹¿åˆ°ä¹‹å‰å»ºç«‹å¥½çš„è®Šæ•¸ï¼Œé”åˆ° share è®Šæ•¸çš„æ•ˆæœã€‚ tf é‡å°è®Šæ•¸æä¾›äº† tf.get_variable() å’Œ tf.variable_scope() æ©Ÿåˆ¶ä¾†ç®¡ç†è®Šæ•¸ (link)ï¼Œ tf.get_variable(name, shape, initializer): Creates or returns a variable with a given name.tf.variable_scope(scope_name): Manages namespaces for names passed to tf.get_variable(). ä¸¦æä¾›äº† tf.get_collection() ä¾†å–å¾—ä¸€å€‹è®Šæ•¸çš„ listï¼Œé€™æ¨£çš„å¥½è™•æ˜¯æœ‰æ™‚å€™æˆ‘å€‘åªå¸Œæœ› update æŸäº›è®Šæ•¸ (ä¾‹å¦‚ GAN training), å› æ­¤å°±éœ€è¦å°‡é€™äº›è®Šæ•¸åŠ åˆ°ä¸€å€‹ collection ä¸­ï¼Œæˆ–æ˜¯ä½¿ç”¨ scope å‘½åä¾†ç¯©é¸ collectionã€‚ tf.get_collection(key, scope=None): Returns a list of object with the key of collection and scope name. æ­¤ç­†è¨˜æœƒå­¸åˆ°: ä½¿ç”¨ tf.get_variable() å»ºç«‹è®Šæ•¸ Share Variables Variable Scope è¦å‰‡ Variable Collections ä½¿ç”¨ tf.get_variable() å»ºç«‹è®Šæ•¸ tf.get_variable(name, shape, initializer): Creates or returns a variable with a given name. å¸¸ç”¨çš„ initializer å¯æœ‰ tf.constant_initializer(value): initializes everything to the provided value,tf.random_uniform_initializer(a, b): initializes uniformly from [a, b],tf.random_normal_initializer(mean, stddev): initializes from the normal distribution with the given mean and standard deviation. Code1123var1 = tf.get_variable('name1',[2,1],initializer=tf.zeros_initializer)print('Construct a variable with name \"var1\" with shape &#123;&#125;'.format(var1.shape))print(var1) output ç‚º12Construct a variable with name &quot;var1&quot; with shape (2, 1)&lt;tf.Variable &apos;name1:0&apos; shape=(2, 1) dtype=float32_ref&gt; Share Variableså»¶çºŒ Code1 ç¹¼çºŒåŸ·è¡Œä¸‹å» (ä»¥ä¸‹æ¯å€‹Codeç‰‡æ®µéƒ½æ˜¯ä¾æ¬¡åºåŸ·è¡Œä¸‹å»çš„çµæœ)1var1_cpy = tf.get_variable('name1') æœƒå ±éŒ¯ï¼Œå› ç‚º &#39;name1&#39; çš„è®Šæ•¸å·²ç¶“å­˜åœ¨ï¼Œå› æ­¤éœ€è¦å°‡ç›®å‰ scope ä¸‹çš„ ID éƒ½æŒ‡å®šç‚º reuse æ‰èƒ½ share &#39;name1&#39; çš„è®Šæ•¸ï¼Œç›®å‰çš„ scope æ˜¯ç©ºå­—ä¸² (â€˜â€™) Code2123456789with tf.variable_scope('',reuse=True): var1_cpy = tf.get_variable('name1')print(var1_cpy)# another way to specify reusing variables under the current scopewith tf.variable_scope(''): tf.get_variable_scope().reuse_variables() var1_cpy = tf.get_variable('name1')print(var1_cpy) output ç‚º12&lt;tf.Variable &apos;name1:0&apos; shape=(2, 1) dtype=float32_ref&gt;&lt;tf.Variable &apos;name1:0&apos; shape=(2, 1) dtype=float32_ref&gt; Variable Scope è¦å‰‡Scope å¦‚æœæŒ‡å®šç‚º reuse, å‰‡åº•ä¸‹çš„ sub-scope ç‚ºè‡ªå‹•ç¹¼æ‰¿ ç¯„ä¾‹ä¾†è‡ª tf å®˜ç¶² (link) Code31234567891011121314with tf.variable_scope(\"root\"): # At start, the scope is not reusing. assert tf.get_variable_scope().reuse == False with tf.variable_scope(\"foo\"): # Opened a sub-scope, still not reusing. assert tf.get_variable_scope().reuse == False with tf.variable_scope(\"foo\", reuse=True): # Explicitly opened a reusing scope. assert tf.get_variable_scope().reuse == True with tf.variable_scope(\"bar\"): # Now sub-scope inherits the reuse flag. assert tf.get_variable_scope().reuse == True # Exited the reusing scope, back to a non-reusing one. assert tf.get_variable_scope().reuse == False æˆ–è€…å°‡ scope å­˜èµ·ä¾†ï¼Œç›´æ¥ pass çµ¦å…¶ä»– tf.variable_scope ä½¿ç”¨ Code41234567891011with tf.variable_scope(\"foo\") as foo_scope: v = tf.get_variable(\"v\", [1])with tf.variable_scope(foo_scope): w = tf.get_variable(\"w\", [1])with tf.variable_scope(foo_scope, reuse=True): v1 = tf.get_variable(\"v\", [1]) w1 = tf.get_variable(\"w\", [1])assert v1 is vassert w1 is wprint(v1)print(w1) output ç‚º 12&lt;tf.Variable &apos;foo/v:0&apos; shape=(1,) dtype=float32_ref&gt;&lt;tf.Variable &apos;foo/w:0&apos; shape=(1,) dtype=float32_ref&gt; æ³¨æ„åˆ°å¦‚æœä¹‹å‰æœ‰å­˜èµ·ä¾†ä¸€å€‹ scope, e.g. scope_pre.name == &#39;cope1/scope2&#39;, ç„¶å¾Œå‡è¨­ç›®å‰åœ¨çš„ scope ç‚º â€˜scopeA/scopeB/scopeC/scopeDâ€™å‰‡ tf.variable_scope(scope_pre) ä¸€å®šæœƒè·³åˆ° â€˜scope1/scope2â€™ ä¸ç®¡ç¾åœ¨åœ¨å“ª When opening a variable scope using a previously existing scope we jump out of the current variable scope prefix to an entirely different one. This is fully independent of where we do it. Code51234567with tf.variable_scope(\"foo\") as foo_scope: assert foo_scope.name == \"foo\"with tf.variable_scope(\"bar\"): with tf.variable_scope(\"baz\") as other_scope: assert other_scope.name == \"bar/baz\" with tf.variable_scope(foo_scope) as foo_scope2: assert foo_scope2.name == \"foo\" # Not changed. Variable Collectionså¾—åˆ° CollectionsCollection é¡§åæ€ç¾©å°±æ˜¯å°‡è¨±å¤š objects (å¦‚ Variables, Tensors, Ops â€¦) éƒ½è’é›†æˆä¸€å€‹ listï¼Œé è¨­æƒ…æ³ä¸‹æ¯ä¸€å€‹ tf.Variable éƒ½æœƒè¢«æ”¾åˆ°å¦‚ä¸‹å…©å€‹ collections (link): tf.GraphKeys.GLOBAL_VARIABLES â€” variables that can be shared across multiple devices, tf.GraphKeys.TRAINABLE_VARIABLESâ€” variables for which TensorFlow will calculate gradients. æŠ“å–é€™äº› Collectionï¼Œå¯ä½¿ç”¨ tf.get_collection(key), ä¸¦å°‡ collection çš„ key å‚³å…¥ æ³¨æ„åˆ° Code6 ä¸­å°‡è®Šæ•¸ b çš„ trainable å±¬æ€§è¨­å®šç‚º Falseï¼Œå› æ­¤æ²’æœ‰è¢«æ”¾é€² tf.GraphKeys.TRAINABLE_VARIABLES æ­¤ collection ä¸­ã€‚ Code61234567with tf.variable_scope('rootScope') as scope: a = tf.get_variable('a',[2,3])b = tf.get_variable('b',[3,2], trainable=False)trainable_collections = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)global_collections = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)print('Collection with name \"TRAINABLE_VARIABLES\"=\\n&#123;&#125;'.format(trainable_collections))print('Collection with name \"GLOBAL_VARIABLES\"=\\n&#123;&#125;'.format(global_collections)) output ç‚º1234Collection with name &quot;TRAINABLE_VARIABLES&quot;=[&lt;tf.Variable &apos;name1:0&apos; shape=(2, 1) dtype=float32_ref&gt;, &lt;tf.Variable &apos;foo/v:0&apos; shape=(1,) dtype=float32_ref&gt;, &lt;tf.Variable &apos;foo/w:0&apos; shape=(1,) dtype=float32_ref&gt;, &lt;tf.Variable &apos;rootScope/a:0&apos; shape=(2, 3) dtype=float32_ref&gt;]Collection with name &quot;GLOBAL_VARIABLES&quot;=[&lt;tf.Variable &apos;name1:0&apos; shape=(2, 1) dtype=float32_ref&gt;, &lt;tf.Variable &apos;foo/v:0&apos; shape=(1,) dtype=float32_ref&gt;, &lt;tf.Variable &apos;foo/w:0&apos; shape=(1,) dtype=float32_ref&gt;, &lt;tf.Variable &apos;rootScope/a:0&apos; shape=(2, 3) dtype=float32_ref&gt;, &lt;tf.Variable &apos;b:0&apos; shape=(3, 2) dtype=float32_ref&gt;] åˆ©ç”¨ Scope é€²ä¸€æ­¥ç¯©é¸ Collectionsåˆ©ç”¨ tf.get_collection(key, scope) å¾—åˆ°çš„ object list, æœƒè¢« scope å†é€²ä¸€æ­¥ç¯©é¸! Code7123456with tf.variable_scope('Hello') as HelloScope: hello = tf.get_variable('helloName',shape=()) with tf.variable_scope('World') as WorldScope: world = tf.get_variable('worldName',shape=())print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=HelloScope.name))print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=WorldScope.name)) output ç‚º12[&lt;tf.Variable &apos;Hello/helloName:0&apos; shape=() dtype=float32_ref&gt;, &lt;tf.Variable &apos;Hello/World/worldName:0&apos; shape=() dtype=float32_ref&gt;][&lt;tf.Variable &apos;Hello/World/worldName:0&apos; shape=() dtype=float32_ref&gt;] å»ºç«‹è‡ªå·±çš„ collectionå¯ä»¥ä½¿ç”¨ tf.add_to_collection(&#39;my_collection_name&#39;,obj)ï¼Œç¯„ä¾‹å¦‚ä¸‹ Code8123456c = hellotf.add_to_collection('demo_used',c)d = tf.constant(np.random.randint(0,10,(5,2)),name='d',dtype='int32')tf.add_to_collection('demo_used',d)my_collection = tf.get_collection('demo_used')print('Collection with name \"demo_used\"=&#123;&#125;'.format(my_collection)) output ç‚º1Collection with name &quot;demo_used&quot;=[&lt;tf.Variable &apos;Hello/helloName:0&apos; shape=() dtype=float32_ref&gt;, &lt;tf.Tensor &apos;d:0&apos; shape=(5, 2) dtype=int32&gt;] å°‡ç¯©é¸æˆ–è‡ªå»ºçš„ Collections å‚³çµ¦ Optimizeræˆ‘å€‘å¯ä»¥å°‡æŒ‡å®šçš„ collection å‚³çµ¦ optimizer çš„ minimize() å‡½å¼, é€šéé€™ç¨®æ–¹å¼, å¯ä»¥æ˜ç¢ºæŒ‡å®šåª update é‚£äº›è®Šæ•¸ï¼Œç¯„ä¾‹å¦‚ä¸‹: 12op1 = tf.train.RMSPropOptimizer().minimize(g_loss, var_list=my_collection1)op2 = tf.train.AdamOptimizer().minimize(g_loss,var_list=my_collection2) é‡å° Optimizer.minimize() çš„ â€˜var_listâ€™ åƒæ•¸èªªæ˜ç‚º: Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES. Reference https://www.tensorflow.org/programmers_guide/variables https://www.tensorflow.org/versions/r1.2/programmers_guide/variable_scope API doc","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"}]},{"title":"Notes for Model Predictive Control","date":"2017-06-28T12:01:19.000Z","path":"2017/06/28/ModelPredictiveControl/","text":"å¾ä¸€é–‹å§‹æ±ºå®šä¸Šèª²å¾Œï¼Œç¶“éäº†åŠå¹´çµ‚æ–¼ä¾†åˆ° Udacity Term2 æœ€å¾Œä¸€å€‹ Project äº†ã€‚åªèƒ½èªªç›¡é‡è®“è‡ªå·±æŠŠæ¯ä¸€å€‹åšçš„ project éƒ½å¯«ä¸€ç¯‡ blog è¨˜éŒ„ï¼Œä½†é€™é™£å­æ™‚é–“çœŸçš„ä¸å¤ ç”¨ï¼Œæ‰€ä»¥é€™ç¯‡å°±å¾ high level çš„è§’åº¦ç€è¦½ä¸€ä¸‹å…§å®¹ã€‚ ç›®çš„æˆ‘å€‘ç”¨ä¸Šåœ–ä¾†èªªæ˜ç›®çš„ï¼ŒMPC è¦åšçš„äº‹æƒ…ï¼Œå°±æ˜¯çµ¦å®šä¸€å€‹æŒ‡å®šçš„ reference trajectory (é»ƒè‰²çš„æ›²ç·šï¼Œé€šå¸¸ç”¨ä¸€å€‹ 3rd polynomail è¡¨ç¤º)ï¼Œæˆ‘å€‘ç¶“ç”± motion model ä¾†è¨ˆç®—å‡ºæœ€ä½³çš„æ§åˆ¶ ($(\\delta,a)$åˆ†åˆ¥è¡¨ç¤ºè»Šå­è¼ªå­çš„è§’åº¦è·ŸåŠ é€Ÿåº¦)ï¼Œæœ€ä½³çš„æ„æ€å°±æ˜¯é€™æ¨£çš„æ§åˆ¶æœƒç”¢ç”Ÿå‡ºä¸€å€‹ predicted trajectory (ç¶ è‰²çš„æ›²ç·š) ä½¿å¾—è·Ÿ reference trajectory cost æœ€å°ã€‚é€™å°±ç­‰æ–¼å°‡å•é¡Œè½‰æ›æˆä¸€å€‹ nonlinear constraint optimization problem äº†ã€‚å¦å¤–å‰›æ‰æåˆ°çš„æ§åˆ¶é … $(\\delta,a)$ï¼Œå…¶ä¸­çš„ $\\delta$ ç‚ºä¸‹åœ–çš„ Wheel Orientation è§’åº¦:è€Œ $a$ è¡¨ç¤ºåŠ é€Ÿåº¦ï¼Œæ­£å€¼æ˜¯è¸©æ²¹é–€ï¼Œè² å€¼æ˜¯è¸©ç…è»Šã€‚é€™é‚Šæˆ‘å€‘ç•¶ç„¶å‡è¨­æ²¹é–€å’Œç…è»ŠåŒæ™‚åªæœƒæœ‰ä¸€å€‹å­˜åœ¨å•¦ï¼Œé–‹è»ŠæŠ€è¡“æ²’é€™éº¼å¥½ã€‚ Motion Model é€™ 6 å€‹ states $(x,y,\\psi,v,cte,e\\psi)$ åˆ†åˆ¥è¡¨ç¤º (è»Šå­xåº§æ¨™, è»Šå­xåº§æ¨™, è»Šå­headingè§’åº¦, è»Šå­é€Ÿåº¦, Cross Track Error, Error of è»Šå­è§’åº¦)CTE æˆ–ç¨± XTE æ˜¯ reference position è·Ÿ actual position ä¹‹é–“çš„èª¤å·® åŒç† $e\\psi$ å°±æ˜¯ reference çš„è§’åº¦è·Ÿå¯¦éš›è§’åº¦çš„å·®å€¼äº†ï¼Œæ³¨æ„åˆ°ï¼Œç”±æ–¼ reference trajectory å¯èƒ½æ˜¯ä¸€å€‹ 3rd polynomailï¼Œæˆ‘å€‘å¯ä»¥ç®—åˆ‡ç·šä¾†æ±‚å¾— reference çš„è§’åº¦ã€‚ Tools of Nonlinear Constraint Optå…©å€‹ä¸»è¦çš„ tool: IpoptInterior Point OPTimizationï¼Œç”¨ä¾†è§£ nonlinear constraint opt å•é¡Œã€‚ CppADåœ¨ä½¿ç”¨ Ipopt çš„æ™‚å€™ï¼Œéœ€è¦è¨ˆç®— function çš„ gradientsï¼Œè€Œ CppAD å¯ä»¥å¹«æˆ‘å€‘è‡ªå‹•è¨ˆç®—ã€‚ ä¸€å€‹å¾ˆæ£’çš„ä½¿ç”¨å…©å€‹ tools è§£ opt å•é¡Œçš„ç¯„ä¾‹: link Consider with Latencyé€šå¸¸ä¸‹äº†ä¸€é“ actuator å‘½ä»¤ (ä¾‹å¦‚åŠ é€Ÿåº¦è¦å¤šå°‘ã€è¼ªå­è§’åº¦è¦å¤šå°‘)ï¼Œåˆ°å¯¦éš›ä¸Šè»Šå­é‹ä½œæœƒæœ‰ä¸€å€‹ delayï¼Œè€Œ Udacity simulator è¨­å®šé€™å€‹ latency æ˜¯ 0.1 secondã€‚ é€™å€‹ latency åœ¨è»Šå­é€Ÿåº¦è¼ƒå¿«çš„æ™‚å€™ï¼Œå½±éŸ¿æœƒå¾ˆå¤§ï¼Œå°è‡´è»Šå­ç„¡æ³•æ­£ç¢ºé–‹å®Œã€‚ä¸€å€‹ç°¡å–®çš„è§£æ³•å°±æ˜¯æˆ‘å€‘åˆ©ç”¨ motion model å»é æ¸¬ç¶“é latency å¾Œçš„è»Šå­ statesï¼Œç„¶å¾Œå¾Œé¢æ‰€æœ‰æµç¨‹éƒ½ä¸€æ¨¡ä¸€æ¨£å³å¯ã€‚ Results [Video] With considering latency: [Video] Without considering latency: Referencemy github ç›®å‰è§£é–æˆå°±","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"Model Predictive Control","slug":"Model-Predictive-Control","permalink":"http://yoursite.com/tags/Model-Predictive-Control/"},{"name":"Nonlinear Constraint Optimization","slug":"Nonlinear-Constraint-Optimization","permalink":"http://yoursite.com/tags/Nonlinear-Constraint-Optimization/"}]},{"title":"Structure Perceptron and Structure SVM","date":"2017-05-20T01:41:27.000Z","path":"2017/05/20/Structure-Perceptron-and-Structure-SVM/","text":"è¨˜å¾—ç•¶å¹´å¿µåšçš„æ™‚å€™ï¼Œå°æ–¼SVMé —æœ‰æ„›ï¼Œä¹Ÿè¦ºå¾—æŒæ¡åº¦å¾ˆé«˜æƒ¹ï¼Œå°±æ˜¯ kernel method + convex optimization çš„å®Œç¾åˆé«”ã€‚ç›´åˆ°æŸå¤©çœ‹åˆ° structureSVMï¼Œçœ‹äº†è€åŠå¤©å¯¦åœ¨ä¸å¾—è¦é ˜ï¼Œç•¶æ™‚å°±æ”¾ä¸‹æ²’å†ç®¡äº†ã€‚å¤šå¹´å¾Œ (2015)ï¼Œå‰›å¥½å°å¤§æå®æ¯…æ•™æˆæ•™çš„èª²ç¨‹æœ€å¾Œä¸€å ‚ Project demoï¼Œæœ‰è«‹æˆ‘å€‘éƒ¨é–€ä»‹ç´¹åšçš„ä¸€äº›å…§å®¹çµ¦å­¸ç”Ÿï¼Œæ‰çœ‹åˆ°äº†å¼·å¤§çš„æè€å¸«çš„èª²ç¨‹å…§å®¹ã€‚ä»–æ‰€æ•™çš„ structure learning/svm å¯¦åœ¨æœ‰å¤ æ¸…æ¥šï¼Œåˆéå¸¸ generalï¼ŒçœŸçš„æ˜¯å¼·åˆ°çˆ†! æœ¬äººåˆå¹´è¼•ï¼Œåˆè¬™è™›ï¼Œæˆ‘çš„æ–°å¶åƒé˜¿!é™„ä¸Šä¸€å¼µæˆ‘èˆ‡æ–°å¶åƒçš„åˆç…§â€¦ XD ä»¥ä¸‹å…§å®¹ç‚ºç­†è¨˜ç”¨ï¼Œæ–¹ä¾¿æ—¥å¾Œå›æƒ³ï¼Œä¾†æº å¤§éƒ½æ˜¯æè€å¸«çš„å…§å®¹ã€‚ A General Framework (Energy-based Model)ä¸€èˆ¬ä¾†èªª ML è¦å­¸ç¿’çš„æ˜¯ $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$ é€™æ¨£çš„ä¸€å€‹ mapping functionï¼Œä½¿å¾—åœ¨å­¸ç¿’åˆ°ä¹‹å¾Œï¼Œèƒ½å¤ å°æ–¼æ–°çš„ input $x$ æ±‚å¾—é æ¸¬çš„ $y=f(x)$ã€‚ç°¡å–®çš„æƒ…æ³æ˜¯æ²’å•é¡Œï¼Œä¾‹å¦‚ binary classificationã€multi-class classification æˆ– regressionã€‚ä½†æ˜¯è¬ä¸€è¦é æ¸¬çš„æ˜¯è¤‡é›œå¾—å¤šçš„ outputï¼Œè­¬å¦‚ $\\mathcal{Y}$ æ˜¯ä¸€å€‹ treeã€bounding boxã€æˆ– sequenceï¼ŒåŸä¾†çš„æ¶æ§‹å°±å¾ˆé›£å®šç¾©äº†ã€‚ æ‰€ä»¥å°‡è¦å­¸çš„å•é¡Œæ”¹æˆå¦‚ä¸‹çš„æ¶æ§‹ã€‚ Training: $F: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ Inference: Given an object $x$, $\\tilde{y}=argmax_y F(x,y)$ $F$ å¯ä»¥æƒ³æˆç”¨ä¾†è¨ˆç®— $(x,y)$ çš„åŒ¹é…åº¦ã€‚è€Œé€™æ¨£çš„ function ä¹Ÿç¨±ç‚º Energy-based modelã€‚å¥½äº†ï¼Œå®šç¾©æˆé€™æ¨£çœ‹èµ·ä¾†æ²’ä»€éº¼ä¸åŒï¼Œè©²æœ‰çš„å•é¡Œé‚„æ˜¯åœ¨ï¼Œé‚„æ˜¯æ²’è§£æ±ºã€‚æ²’é—œä¿‚ï¼Œæˆ‘å€‘ç¹¼çºŒçœ‹ä¸‹å»ï¼Œå…ˆæŠŠä¸‰å€‹é‡è¦çš„å•é¡Œåˆ—å‡ºä¾†ã€‚ Problem 1: æ€éº¼å®šç¾© $F(x,y)$ ? Problem 2: æ€éº¼è§£æ±º $argmax_y$ ? Problem 3: æ€éº¼è¨“ç·´ $F(x,y)$ ? é€™äº›å•é¡Œåœ¨æŸç¨®æƒ…æ³æœƒè®Šå¾—å¾ˆå¥½è§£ï¼Œä»€éº¼æƒ…æ³å‘¢? è‹¥æˆ‘å€‘å°‡ $F(x,y)$ å®šç¾©æˆ Linear Model (Problem 1 ç”¨ linear å®šç¾©)ï¼Œæˆ‘å€‘ç™¼ç¾è¨“ç·´è®Šå¾—å¾ˆå®¹æ˜“ (Problem 3 å¥½è§£) !! ç–‘?! Problem 2å‘¢? å…ˆç•¶ä½œå·²è§£å§ï¼Œã„ã„ã€‚ Linear Model of $F(x,y)=w\\cdot \\phi(x,y)$æˆ‘å€‘å…ˆå‡è£ Problem 2 å·²è§£ (Problem 2 è¦èƒ½è§£ depends on å•é¡Œçš„domainï¼Œå’Œfeatureçš„å®šç¾©)ï¼Œæˆ‘å€‘ä¾†çœ‹ä¸€ä¸‹è¦æ€éº¼è¨“ç·´é€™æ¨£çš„ linear modelã€‚é¦–å…ˆç”¨æè€å¸«èª²ç¨‹çš„ç¯„ä¾‹ (è¾¨è­˜åˆéŸ³) çš„ä¾‹å­èˆ‰ä¾‹ï¼Œå…¶ä¸­ $y$ æ˜¯ä¸€å€‹ bounding box:é€™å€‹ä¾‹å­æœ‰å…©å€‹ training pairs: $(x^1,\\hat{y}^1)$ å’Œ $(x^2,\\hat{y}^2)$ï¼Œæˆ‘å€‘å¸Œæœ›æ±‚å¾—ä¸€å€‹ $w$ ä½¿å¾—ç´…è‰²çš„åœ“åœˆæŠ•å½±åˆ° $w$ ä¸Šå¾Œè¦å¤§æ–¼æ‰€æœ‰è—è‰²çš„åœ“åœˆã€‚åŒç†ï¼Œç´…è‰²çš„æ˜Ÿæ˜Ÿè¦å¤§æ–¼æ‰€æœ‰è—è‰²çš„æ˜Ÿæ˜Ÿã€‚å…¶å¯¦æˆ‘å€‘ä»”ç´°æƒ³æƒ³ï¼Œé€™å•é¡Œè·Ÿ perceptron learning éå¸¸é¡ä¼¼ï¼Œperceptron learning åœ¨åšçš„æ˜¯ binary classificationï¼Œè€Œå¦‚æœæŠŠæ¯ä¸€ç­† training data $(x^i,\\hat{y}^i)$ å’Œ $(x^i,y:y\\neq\\hat{y}^i)$ ç•¶ä½œæ˜¯ positive and negative classesï¼Œå‰›å¥½å°±æ˜¯ä¸€å€‹ bineary classification problem (é›–ç„¶ä¸åŒç­† training data æœƒæœ‰å„è‡ªçš„ positive and negative è³‡æ–™ï¼Œä½†ä¸å½±éŸ¿æ•´å€‹å•é¡Œ)æ‰€ä»¥å¦‚æœæœ‰è§£ (linear separable)ï¼Œå‰‡æˆ‘å€‘å¯ä»¥ä½¿ç”¨ Structure Perceptron åœ¨æœ‰é™æ­¥é©Ÿå…§æ±‚è§£ã€‚ Structure Perceptron è­‰æ˜çš„æ¦‚å¿µè·Ÿ perceptron ä¸€æ¨£ï¼Œå°±æ˜¯å‡è¨­æœ‰è§£ï¼Œè§£ç‚º $\\hat{w}$ï¼Œè¦æ±‚æ¯ä¸€æ¬¡çš„ update $w^k$ï¼Œæœƒè·Ÿ $\\hat{w}$ æ„ˆä¾†æ„ˆæ¥è¿‘ï¼Œä¹Ÿå°±æ˜¯ $$\\begin{align} cos\\rho_k = \\frac{\\hat{w}\\cdot w^k}{\\Vert{\\hat{w}}\\Vert\\cdot\\Vert{w^k}\\Vert} \\end{align}$$ è¦æ„ˆå¤§æ„ˆå¥½!ä½†æˆ‘å€‘ä¹ŸçŸ¥é“ $cos$ æœ€å¤§å°± 1ï¼Œå› æ­¤å°±æœ‰ upper boundï¼Œæ‰€ä»¥æœƒåœ¨æœ‰é™æ­¥é©Ÿæå®šã€‚è©³ç´°æ¨å€’æ­¥é©Ÿå¯åƒè€ƒæè€å¸«è¬›ç¾©ï¼Œæˆ–çœ‹ perceptron çš„æ”¶æ–‚è­‰æ˜ã€‚ Cost Functionç”¨ cost function çš„è§’åº¦ä¾†èªªï¼Œå…¶å¯¦ perceptron è™•ç†çš„ cost æ˜¯è¨ˆç®—éŒ¯èª¤çš„æ¬¡æ•¸ï¼Œå¦‚æœå°‡ cost çš„ function ç•«å‡ºä¾†çš„è©±ï¼Œæœƒæ˜¯ step functionï¼Œè€Œç„¡æ³•åšå¾®åˆ†æ±‚è§£ã€‚å› æ­¤é€šå¸¸æœƒå°‡ cost function æ”¹æˆå¯å¾®åˆ†çš„æ–¹å¼ï¼Œä¾‹å¦‚ linear or quadratic or what ever continuous functionã€‚æ”¹æˆå¯å¾®åˆ†å°±æœ‰å¾ˆå¤šå¥½è™•äº†ï¼Œå¯ä»¥é‡å° cost function åšå¾ˆå¤šéœ€è¦çš„ä¿®æ”¹ï¼Œé€™äº›ä¿®æ”¹åŒ…æ‹¬ 1. å°ä¸åŒçš„éŒ¯èª¤æœ‰ä¸åŒçš„æ‡²ç½° 2. åŠ å…¥ regularization term â€¦ ç­‰ç­‰ï¼Œæˆ‘å€‘ç­‰ä¸‹æœƒè«‡åˆ°ã€‚ Picture is from Duda Pattern Classification å·¦åœ–å°±æ˜¯åŸä¾†çš„ perceptron costï¼Œè€Œå³åœ–å°±æ˜¯å°‡ cost æ”¹æˆ linear costã€‚Linear cost å¯å®šç¾©å¦‚ä¸‹:$$\\begin{align} C=\\sum_{n=1}^N C^n \\\\ C^n=(max_y[w\\cdot\\phi(x^n,y)])-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$æ‰€ä»¥ï¼Œå°±gradient descentä¸‹å»å§ï¼Œè·ŸåŸä¾†çš„ perceptron learning æ”¹ cost function ä¸€æ¨£ã€‚ è®“éŒ¯èª¤çš„ç¨‹åº¦èƒ½è¡¨ç¾å‡ºä¾†é€™æ˜¯ä»€éº¼æ„æ€å‘¢? åŸä¾†çš„ cost function å°æ–¼æ¯ä¸€å€‹éŒ¯èª¤çš„æƒ…å½¢éƒ½ä¸€è¦–åŒä»ï¼Œä¹Ÿå°±æ˜¯åœ¨æ‰¾é‚£å€‹ $w$ çš„æ™‚å€™ï¼Œåªè¦éŒ¯èª¤çš„ä¾‹å­æŠ•å½±åœ¨ $w$ ä¸Šæ¯”æ­£ç¢ºçš„é‚„è¦å°å°±å¥½ï¼Œä¸åœ¨å¿½å°å¤šå°‘ï¼Œä½†äº‹å¯¦ä¸ŠéŒ¯èª¤æœƒæœ‰å¥½å£ä¹‹åˆ†ã€‚ä¸‹é¢æ˜¯ä¸€å€‹æè€å¸«çš„ä¾‹å­ï¼Œä¾‹å¦‚å³é‚Šé»ƒè‰²çš„æ¡†æ¡†é›–ç„¶è·Ÿæ­£ç¢ºç­”æ¡ˆç´…æ¡†æ¡†ä¸åŒ (æ‰€ä»¥è¢«ç•¶æˆéŒ¯èª¤çš„ä¾‹å­)ï¼Œä½†æœ‰å¤§è‡´ä¸Šéƒ½æŠ“åˆ°åˆéŸ³çš„è‡‰äº†ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥å…è¨±ä»–è·Ÿæ­£ç¢ºç­”æ¡ˆè¼ƒæ¥è¿‘ã€‚å› æ­¤ cost function å¯ä»¥ä¿®æ”¹ä¸€ä¸‹: $$\\begin{align} C=\\sum_{n=1}^N C^n \\\\ C^n=max_y[w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)]-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ $\\triangle(\\hat{y}^n,y)$ å®šç¾©äº†é€™å€‹éŒ¯èª¤çš„ä¾‹å­é¡å¤–çš„ cost (éœ€&gt;=0)ï¼Œä»¥ bounding box è€Œè¨€ï¼Œèˆ‰ä¾‹ä¾†èªªå…©å€‹ set A and Bï¼Œ$\\triangle(A,B)$ å¯å®šç¾©ç‚º $1-/frac{A \\cap B}{A \\cup B}$ã€‚ä¸ééœ€è¦ç‰¹åˆ¥ä¸€æçš„æ˜¯ï¼Œå¤šå¢åŠ é€™å€‹é¡å¤–çš„å®šç¾©ï¼Œæœ‰å¯èƒ½ä½¿å¾—åŸä¾†å®¹æ˜“è§£çš„ $argmax_y$ (Problem 2) è®Šå¾—ç„¡æ³•è§£ï¼Œæ‰€ä»¥è¦æ³¨æ„ã€‚ Minimize the upper boundå¾ˆæœ‰è¶£çš„ä¸€é»æ˜¯ï¼Œåœ¨æˆ‘å€‘å¼•å…¥äº† $\\triangle(\\hat{y}^n,y)$ (ç¨±ç‚º margin, åœ¨å¾Œé¢è¬›åˆ° structure SVM å¯ä»¥çœ‹å¾—å‡ºä¾†) å¾Œï¼Œå¯ä»¥ç”¨å¦ä¸€å€‹è§€é»ä¾†çœ‹é€™å€‹å•é¡Œã€‚ å‡è¨­æˆ‘å€‘å¸Œæœ›èƒ½å°‡ $Câ€™$ æœ€å°åŒ–: $$\\begin{align} \\tilde{y}^n=argmax_y{w\\cdot \\phi(x^n,y)} \\\\ C&apos;=\\sum_{n=1}^N{\\triangle(\\hat{y}^n,\\tilde{y}^n)} \\end{align}$$ çµæœæˆ‘å€‘ç™¼ç¾å…¶å¯¦ $\\triangle(\\hat{y}^n,\\tilde{y}^n)\\leq C^n$ï¼Œå› è€Œè®Šæˆ è€Œæˆ‘å€‘ä¸Šé¢éƒ½æ˜¯åœ¨æœ€å°åŒ– $C$ï¼Œæ‰€ä»¥å…¶å¯¦æˆ‘å€‘åœ¨åšçš„äº‹æƒ…å°±æ˜¯åœ¨æœ€å°åŒ– $Câ€™$ çš„ upper boundã€‚ä¸Šç•Œçš„è­‰æ˜å¦‚ä¸‹:é€™ç¨®è—‰ç”±æœ€ä½³åŒ– upper bound çš„æ–¹å¼ï¼Œåœ¨ adaboost ä¹Ÿè¦‹éã€‚æ™®éä¾†èªªï¼ŒåŸä¾†çš„å¼å­ä¸å®¹æ˜“æœ€ä½³åŒ–çš„æ™‚å€™ï¼Œæˆ‘å€‘è—‰ç”±å®šç¾©ä¸€å€‹å®¹æ˜“æœ€ä½³åŒ–çš„upper boundï¼Œç„¶å¾Œæœ€å°åŒ–å®ƒã€‚å¦å¤–ï¼ŒEM æ¼”ç®—æ³•ä¹Ÿæœ‰é¡ä¼¼çš„æ¦‚å¿µã€‚ Regularizationç›´æ¥åŠ å…¥ norm-2 regularization:$$\\begin{align} C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N C^n \\\\ C^n=max_y[w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)]-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ Structure SVMå…ˆè¬›çµè«–: ä¸Šé¢ Linear Model æœ€å¾Œçš„ cost function (åŒ…å«marginal and regularization terms) å°±æ˜¯ç­‰åƒ¹æ–¼ SVMã€‚åŸå…ˆå•é¡Œ P1: Find $w$ that minimize $C$$$\\begin{align} C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N C^n \\\\ C^n=max_y[w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)]-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ æ”¹å¯«å¾Œçš„å•é¡Œ P2: Find $w$ that minimize $C$$$\\begin{align} C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N C^n \\\\ For \\forall{y}: C^n\\geq w\\cdot\\phi(x^n,y)+\\triangle(\\hat{y}^n,y)-w\\cdot\\phi(x^n,\\hat{y}^n) \\end{align}$$ è§€å¯Ÿ P2ï¼Œæˆ‘å€‘æ³¨æ„åˆ°çµ¦å®šä¸€å€‹ $w$ æ™‚ï¼Œå®ƒæœ€å°çš„ $C^n$ æ‡‰è©²æœƒæ˜¯ä»€éº¼å‘¢? (æ‰¾æœ€å°æ˜¯å› ç‚ºæˆ‘å€‘è¦ minimize $C$) è­¬å¦‚æˆ‘è¦æ±‚ $x\\leq{ 5,1,2,10 }$ é€™å€‹å¼å­çš„ $x$ æœ€å°æ˜¯å¤šå°‘ï¼Œå¾ˆæ˜é¡¯å°±æ˜¯ $x=max{ 5,1,2,10 }$ã€‚å› æ­¤å¼ P2 çš„å¼ (13) å¯ä»¥å¯«æˆ P1 çš„å¼ (11)ã€‚å¯«æˆ P2 æœ‰ä»€éº¼å¥½è™•? é¦–å…ˆå°‡ $C^n$ æ”¹æˆ $\\epsilon^n$ï¼Œç„¶å¾Œå†ç¨å¾®æ”¹å¯«ä¸€ä¸‹å¾—åˆ°å¦‚ä¸‹çš„å•é¡Œ: å•é¡Œ P3: Find $w,\\epsilon^n,\\epsilon^2,â€¦,\\epsilon^N$ that minimize $C$$$C=\\frac{1}{2}\\Vert{w}\\Vert ^2+\\lambda\\sum_{n=1}^N \\epsilon^n \\\\ For \\forall{y}\\neq{\\hat{y}^n}: w\\cdot (\\phi(x^n,\\hat{y}^n)-\\phi(x^n,y))\\leq \\triangle (\\hat{y}^n,y)-\\epsilon^n,\\epsilon^n\\leq 0$$ æ³¨æ„åˆ°ï¼Œå°æ–¼ä¸€å€‹ n-th training pair $(x^n,\\hat{y}^n)$ å’Œçµ¦å®šä¸€å€‹ $y\\neq\\hat{y}^n$ ä¾†èªªï¼Œæˆ‘å€‘éƒ½æœƒå¾—åˆ°ä¸€å€‹ linear constraintã€‚å¯ä»¥å°‡ä¸Šé¢å¼å­çš„ constant ç”¨ a, bä¾†è¡¨ç¤ºè®Šæˆ:$w\\cdot a \\leq b - \\epsilon^n \\\\$ ç™¼ç¾äº†å—? å°æ–¼è®Šæ•¸ $w$ å’Œ $\\epsilon^n$ ä¾†èªªï¼Œé€™å°±æ˜¯ä¸€å€‹ linear constraintã€‚ çœ¼å°–çš„è®€è€…ï¼Œå¯èƒ½å°±æœƒè¦ºå¾— P3 å¾ˆçœ¼ç†Ÿã€‚æ²’éŒ¯!å®ƒè·Ÿ SVM é•·å¾ˆåƒ! è®“æˆ‘å€‘ä¾†è·Ÿ SVM çš„ Primal form (ä¸æ˜¯ dual form) åšå€‹æ¯”è¼ƒå§ã€‚å¯ä»¥ç™¼ç¾æœ‰å…©é»ä¸åŒï¼ŒåŸ SVM from wiki åˆ—å‡ºå¦‚ä¸‹: Margin term çš„ä¸åŒï¼ŒP3 çš„ margin æ¯”è¼ƒ generalï¼Œå¯ä»¥æ ¹æ“šæ¯å€‹ negative case éƒ½æœ‰è‡ªå·±çš„ marginï¼Œè€ŒåŸä¾† binary SVM çš„ margin æ˜¯å®šç‚º 1ã€‚ Constraint å€‹æ•¸çš„ä¸åŒï¼ŒåŸ SVM å€‹æ•¸ç‚º training data çš„å€‹æ•¸ï¼Œä½†æ˜¯ P3 çš„å€‹æ•¸ç‚ºç„¡çª®å¤šå€‹ã€‚ å‘¼! æ‰€ä»¥ P3 é€™å€‹å•é¡Œï¼Œå°±æ˜¯ SVM çš„ general ç‰ˆæœ¬ï¼Œæˆ‘å€‘ä¹Ÿç¨±ä¹‹ç‚º Structure SVMï¼Œé€™è£¡çµ‚æ–¼è·Ÿ SVM é€£çµä¸Šäº†! Cutting Plane AlgorithmåŸå…ˆ SVM æœ‰é™çš„ constraint ä¸‹ï¼Œæˆ‘å€‘ç›´æ¥ç”¨ä¸€å€‹ QP solverå¯ä»¥å¾ˆå¿«è™•ç†æ‰ã€‚ä½†åœ¨ Structure SVM æœ‰ç„¡çª®å¤šçš„ constraints ç©¶ç«Ÿè¦æ€éº¼è§£? æ˜¯å€‹å•é¡Œã€‚é¦–å…ˆè§€å¯Ÿåˆ°ï¼Œå…¶å¯¦å¾ˆå¤š constraints éƒ½æ˜¯ç„¡æ•ˆçš„ã€‚ä¾‹å¦‚:æ‰€ä»¥é€™å€‹æ¼”ç®—æ³•ç­–ç•¥å°±æ˜¯å¾ä¸€å€‹ç©ºçš„ working set $\\mathbb{A}^n$ å‡ºç™¼ï¼Œæ¯æ¬¡ iteration éƒ½æ‰¾ä¸€å€‹æœ€ violate çš„ constraint åŠ é€²å»ï¼Œç›´åˆ°ç„¡æ³•å†åŠ å…¥ä»»ä½•çš„ constraint ç‚ºæ­¢ã€‚é€™è£¡å…¶å¯¦æœ‰å…©å€‹å•é¡Œè¦è¨è«–ï¼Œç¬¬ä¸€å€‹æ˜¯ä»€éº¼æ˜¯æœ€ violate çš„ constraint? ç¬¬äºŒå€‹æ˜¯ï¼Œé€™æ¼”ç®—æ³•æœƒæ”¶æ–‚å—? é›£é“ä¸æœƒæ°¸é éƒ½æ‰¾å¾—åˆ° violate çš„ constraint ä¸€ç›´åŠ å…¥å—?æˆ‘å€‘å…ˆæŠŠæ¼”ç®—æ³•åˆ—å‡ºä¾†ï¼Œå†ä¾†è¨è«–ä¸Šé¢é€™å…©å€‹å•é¡Œã€‚ Most Violated Constraintç›´æ¥ç§€æè€å¸«çš„æŠ•å½±ç‰‡æ³¨æ„åˆ°åœ¨ Degree of Violation çš„æ¨å°ä¸­ï¼Œæ‰€æœ‰èˆ‡è®Šæ•¸ $y$ ç„¡é—œçš„éƒ¨åˆ†å¯ä»¥å»æ‰ã€‚å› æ­¤æˆ‘å€‘æœ€å¾Œå¯ä»¥å¾—åˆ°æ±‚ Most Violated Constraint å°±æ˜¯åœ¨æ±‚ Problem 2 ($argmax_y$)ã€‚æ³¨æ„åˆ°å…¶å¯¦æˆ‘å€‘ä¸€ç›´ â€œå…ˆå‡è£ Problem 2 å·²è§£â€ Convergence?è«–æ–‡ä¸­è­‰æ˜å¦‚æœè®“ violate çš„æ¢ä»¶æ˜¯å¿…é ˆè¶…éä¸€å€‹ threshold æ‰ç®— violatedï¼Œå‰‡æ¼”ç®—æ³•æœƒåœ¨æœ‰é™æ­¥é©Ÿå…§æ”¶æ–‚ã€‚åš´è¬¹çš„æ•¸å­¸è­‰æ˜è¦åƒè€ƒ paperã€‚ æœ€å¾Œçš„éº»ç…©: Problem 2 argmaxé€™ç¯‡å¯¦åœ¨æ‰“å¤ªé•·äº†ï¼Œä»¥è‡³æ–¼æˆ‘æƒ³çœç•¥é€™å€‹åœ°æ–¹äº† (æ·š)ï¼Œäº‹å¯¦ä¸Šè§£ Problem 2 å¿…é ˆçœ‹å•é¡Œæœ¬èº«æ˜¯ä»€éº¼ï¼Œä»¥ POS (Part-Of-Speech) tagging ä¾†èªªï¼ŒProblem 2 å¯ç”¨ Viterbi æ±‚è§£ã€‚è€Œé€™ä¹Ÿå°±æ˜¯ææ•™æˆä¸‹ä¸€å€‹èª²ç¨‹ Sequence Labeling Problemã€‚POS å¦‚ä½•å°æ‡‰åˆ° Structure Learning å¯¦åœ¨éå¸¸ç²¾å½©! çœŸçš„ä¸å¾—ä¸ä½©æœé€™äº›äººçš„æ™ºæ…§! æœ‰èˆˆè¶£çš„è®€è€…è«‹ä¸€å®šè¦çœ‹ææ•™æˆçš„æŠ•å½±ç‰‡å…§å®¹! ç°¡å–®ç­†è¨˜ä¸€ä¸‹: POS ä½¿ç”¨ HMM æ–¹å¼ä¾† modelï¼Œä¾‹å¦‚ä¸€å¥è©± x = â€œJohn saw the sawâ€ å°æ‡‰åˆ°è©æ€§ y = â€œPN V D Nâ€ã€‚ç„¶å¾ŒæŠŠè©æ€§ç•¶ä½œ state, word ç•¶ä½œ observationï¼Œå°±æ˜¯ä¸€å€‹å…¸å‹çš„ HMM çµæ§‹ã€‚æ¥è‘—ä½¿ç”¨ Conditional Random Field (CRF) å°‡ $log P(x,y)$ å°æ‡‰åˆ° $w\\cdot\\phi(x,y)$ çš„å½¢å¼ï¼Œåœ¨ $P(x,y)$ æ˜¯ç”± HMM å®šç¾©çš„æƒ…å½¢ä¸‹ï¼Œæˆ‘å€‘å¯ä»¥å¯«å‡ºç›¸å°æ‡‰çš„ $\\phi(x,y)$ è©²å¦‚ä½•å®šç¾©ã€‚å› æ­¤å°±è½‰æˆä¸€å€‹ structure learning çš„æ ¼å¼äº†ã€‚è©³ç´°è«‹åƒè€ƒæè€å¸«èª²ç¨‹è¬›ç¾©ã€‚ å½©è›‹ æˆ‘å¿ƒæ„›çš„æ™å¯¶è²ä¸‰æ­²ç”Ÿæ—¥å¿«æ¨‚! é€™å¹¾å¤©æœƒæœ‰ä¸€å€‹å¾ˆé‡å¤§çš„æ±ºå®šç™¼ç”Ÿ! Reference Hung-yi Lee ML courses Perceptron Learning Convergence Proof Duda Pattern Classification structureSVM åŸå§‹è«–æ–‡","tags":[{"name":"Structure SVM","slug":"Structure-SVM","permalink":"http://yoursite.com/tags/Structure-SVM/"},{"name":"Structure Perceptron","slug":"Structure-Perceptron","permalink":"http://yoursite.com/tags/Structure-Perceptron/"},{"name":"Hung-yi Lee","slug":"Hung-yi-Lee","permalink":"http://yoursite.com/tags/Hung-yi-Lee/"}]},{"title":"çµ±ä¸€çš„æ¡†æ¶ Bayes Filter","date":"2017-05-10T14:15:16.000Z","path":"2017/05/10/Bayes-Filter-for-Localization/","text":"Bayes Filter Introductionå‰å¹¾ç¯‡è¨è«–äº†å¾ˆå¤š Kalman Filter ä»¥åŠå®ƒç›¸é—œçš„è®Šå½¢ï¼Œå¦‚: EKF and UKFã€‚é€™äº›æ–¹æ³•æˆ‘å€‘éƒ½å¯ä»¥æ”¾åœ¨ Bayes Filter çš„æ¡†æ¶ä¸‹ä¾†çœ‹ï¼Œé€™éº¼åšçš„è©±ï¼ŒKF å°±åªæ˜¯å…¶ä¸­ä¸€å€‹ç‰¹ä¾‹äº† (éƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒçš„æƒ…å½¢)ã€‚è€Œå¦‚æœæˆ‘å€‘åªè€ƒæ…®å¹¾å€‹é›¢æ•£é»çš„æ©Ÿç‡ï¼Œä¸¦ç”¨è’™åœ°å¡ç¾…æ³•ä¾†æ¨¡æ“¬å–æ¨£çš„è©±ï¼Œé€™ç¨®å¯¦ä½œæ–¹å¼å°±æœƒæ˜¯ Particle Filter ã€‚æ‰€ä»¥æŒæ¡äº† Bayes Filter èƒŒå¾Œçš„é‹ä½œæ–¹å¼å°æ–¼ç†è§£é€™äº›æ–¹æ³•æ˜¯å¾ˆæœ‰å¹«åŠ©çš„ã€‚ä¸€äº›è®Šæ•¸çš„æ„ç¾©ä»ç„¶è·Ÿå‰å¹¾ç¯‡ä¸€æ¨£: z: measurementï¼Œä¹Ÿå°±æ˜¯æˆ‘å€‘å¯¦éš›ä¸Šç¶“ç”± sensor å¾—åˆ°çš„æ¸¬é‡å€¼ (æœƒæœ‰noise) x: stateï¼Œæˆ‘å€‘å¸Œæœ›ä¼°è¨ˆå‡ºä¾†çš„å€¼ï¼Œåœ¨ Localization ä¸€èˆ¬å°±æ˜¯åº§æ¨™å€¼ ç™¼ç¾äº†å—? åœ¨ä¸Šåœ–å³ KF çš„å…©å€‹æ­¥é©Ÿ: Measurement Update å’Œ State Prediction å¯¦éš›ä¸Šå°±æ˜¯ä¸Šåœ–å·¦é‚Šçš„å…©å€‹æ•¸å­¸å¼é—œä¿‚ã€‚æ­é…ä¸‹åœ–æ–‡å­—ä¸€èµ·çœ‹ï¼ŒMeasurement Update ç†è§£ç‚ºå¾—åˆ°ä¸€å€‹è§€å¯Ÿå€¼ $z$ å¾Œï¼Œæˆ‘å€‘ç”¨ Bayes Rule å¯ä»¥ä¼°æ¸¬å‡º state $x$ çš„äº‹å¾Œæ©Ÿç‡ $P(x|z)$ï¼Œè€Œè©²äº‹å¾Œæ©Ÿç‡ç¶“ç”± motion model (eg. CTRV) å¯ä»¥ä¼°æ¸¬å‡ºä¸‹ä¸€å€‹æ™‚é–“é»çš„ x æ©Ÿç‡åˆ†ä½ˆ $P(xâ€™)$ (æ­¤æ­¥é©Ÿç‚º State Prediction)ã€‚å¾—åˆ°æ–°çš„ $P(xâ€™)$ å°±å¯ä»¥ç•¶æˆä¸‹ä¸€å€‹æ™‚é–“é»çš„äº‹å‰æ©Ÿç‡ï¼Œæ‰€ä»¥ Bayes rule å°±å¯ä»¥æ¥è‘—ä¸‹å»é‡è¤‡æ­¤ loopã€‚ èˆ‡ Maximum a Posteriori (MAP) Adaptation çš„é—œä¿‚äº‹å¯¦ä¸Šï¼Œé€™æ¨£çš„æ¡†æ¶ä¹Ÿè·Ÿ MAP Adaptation æ¯æ¯ç›¸é—œ! ä¾‹å¦‚ç•¶äº‹å‰æ©Ÿç‡æ˜¯æŸäº›ç‰¹åˆ¥çš„æ©Ÿç‡åˆ†ä½ˆ (exponential family)ï¼Œç¶“ç”± Bayes rule å¾—åˆ°çš„äº‹å¾Œæ©Ÿç‡ï¼Œå®ƒçš„æ©Ÿç‡åˆ†ä½ˆæœƒè·Ÿäº‹å‰æ©Ÿç‡æ˜¯åŒä¸€é¡å‹çš„ï¼Œ(ä¾‹å¦‚éƒ½æ˜¯ Gaussian)ã€‚è€Œé€™æ¨£çš„é¸æ“‡æˆ‘å€‘ç¨±ç‚º conjugate priorã€‚ç”±æ–¼ â€œäº‹å¾Œâ€ èˆ‡ â€œäº‹å‰â€ æ©Ÿç‡æ˜¯åŒä¸€ç¨®é¡å‹çš„æ©Ÿç‡åˆ†ä½ˆï¼Œå› æ­¤æŠŠ â€œäº‹å¾Œæ©Ÿç‡â€ åœ¨ç•¶æˆä¸‹ä¸€æ¬¡è³‡æ–™ä¾†è‡¨æ™‚çš„ â€œäº‹å‰æ©Ÿç‡â€ ä¹Ÿå°±å¾ˆè‡ªç„¶äº†! é€™å°±æ˜¯ MAP Adaptation çš„æ ¸å¿ƒæ¦‚å¿µï¼Œèˆ‡ Bayes filter ä¸€æ¨¡ä¸€æ¨£é˜¿! Localization è©³ç´°å®šç¾©å¥½çš„ï¼Œæˆ‘å€‘ä¾†é‡å° Localization è©³ç´°è§£é‡‹å§ï¼Œåè©å®šç¾©å¦‚ä¸‹: è§€æ¸¬å€¼ (time 1~t)ã€æ§åˆ¶ (time 1~t)ã€å’Œåœ°åœ– $m$ éƒ½æ˜¯å‡è¨­å·²çŸ¥ï¼Œæˆ‘å€‘æ‰€ä¸çŸ¥çš„(è¦ä¼°æ¸¬çš„)æ˜¯ç›®å‰ time t çš„ç‹€æ…‹å€¼ $x$ã€‚èˆ‰ä¾‹ä¾†èªªï¼Œä¸€å€‹ä¸€ç¶­çš„åœ°åœ–å¦‚ä¸‹:è€Œè§€æ¸¬å€¼ $z_{1:t}$ å¦‚ä¸‹:å¯ä»¥çŸ¥é“æ¯ä¸€å€‹æ™‚é–“é»çš„è§€æ¸¬å€¼æ˜¯ä¸€å€‹ dimension ç‚º k çš„å‘é‡ã€‚ æ•´å€‹ Localization çš„ç›®çš„å°±æ˜¯è¦è¨ˆç®—å°æ–¼ä½ç½® $x$ æˆ‘å€‘æœ‰å¤šå°‘ä¿¡å¿ƒåº¦ï¼Œåš´è¬¹åœ°èªªï¼Œæˆ‘å€‘å°±æ˜¯è¦è¨ˆç®—å¦‚ä¸‹:$$\\begin{align} bel(x_t)=p(x_t|z_{1:t},u_{1:t},m) \\end{align}$$ æ„æ€æ˜¯åœ¨å·²çŸ¥ç›®å‰æ‰€æœ‰çš„è§€æ¸¬å€¼ã€æ§åˆ¶ã€å’Œåœ°åœ–çš„æƒ…æ³ä¸‹ï¼Œä½ç½® $x_t$ çš„æ©Ÿç‡æ˜¯å¤šå°‘ï¼Œçœ‹æ•¸å­¸å¼å­çš„è©±ï¼Œé€™ä¸å°±æ­£å¥½å°±æ˜¯ äº‹å¾Œæ©Ÿç‡ å—? æ‰€ä»¥ä¸Šé¢çš„ Bayes filter æ¶æ§‹å°±æœ‰ç™¼æ®çš„ç©ºé–“äº†ã€‚å¦å¤–ä¸€æçš„æ˜¯ï¼Œå¦‚æœå°‡åœ°åœ– $m$ ä¹Ÿç•¶æˆæœªçŸ¥çš„è©±ï¼Œå°±æ˜¯ SLAM æ¼”ç®—æ³•äº†ã€‚(é‚„æ²’æœ‰æ©Ÿæœƒå»è®€é€™å€‹æ¼”ç®—æ³•)ä¸‹åœ–æ˜¯ä¸€å€‹ä¸€ç¶­çš„ç¤ºæ„åœ–:ä½†æ˜¯è¦è¨ˆç®—é€™æ¨£çš„äº‹å¾Œæ©Ÿç‡ï¼Œå¿…é ˆè¦è€ƒæ…®å¾ä¸€é–‹å§‹åˆ°ç›®å‰æ™‚é–“é»çš„æ‰€æœ‰è§€æ¸¬å€¼å’Œæ§åˆ¶ï¼Œé€™æ¨£çš„è³‡æ–™é‡å¯¦åœ¨å¤ªå¤§ï¼Œè¨ˆç®—æœƒéå¸¸æ²’æœ‰æ•ˆç‡ã€‚å› æ­¤ï¼Œå¦‚æœèƒ½åªè€ƒæ…®ç›®å‰çš„è§€æ¸¬å€¼å’Œæ§åˆ¶ï¼Œä¸¦ç”¨ä¸Šä¸€å€‹æ™‚é–“çš„çš„äº‹å¾Œæ©Ÿç‡å°±èƒ½æ¨ç®—å‡ºä¾†çš„è©±ï¼Œå‹¢å¿…æœƒéå¸¸æœ‰æ•ˆç‡ã€‚ç°¡å–®ä¾†è¬›ï¼Œæˆ‘å€‘å¸Œæœ›ç”¨éè¿´çš„æ–¹å¼: è€ƒæ…® $bel(x_{t-1})$ å’Œç›®å‰çš„è§€æ¸¬å€¼ $z_t$ å’Œæ§åˆ¶ $u_t$ å°±èƒ½æ¨ç®— $bel(x)$ã€‚é€™å°±å¿…é ˆè¦ç°¡åŒ–ä¸Šé¢ $bel(x_t)$ åŸå§‹çš„å®šç¾©äº†ï¼Œè¦å¦‚ä½•é”åˆ°å‘¢? éœ€å€ŸåŠ© First-order Markov Assumption ã€‚ First-order Markov Assumption ç°¡åŒ– believe å‡è¨­ç›®å‰çš„æ™‚é–“é»ç‚º $t$ï¼Œæˆ‘å€‘çŸ¥é“è¦è¨ˆç®—çš„ believe $bel(x_t)$ ä»£è¡¨äº‹å¾Œæ©Ÿç‡ï¼Œå†å¥—ç”¨ Bayes rule ä¹‹å¾Œï¼Œå¯ä»¥å¾—åˆ°ä¸Šé¢çš„è¡¨ç¤ºã€‚ äº‹å¾Œæ©Ÿç‡ (Believe): ç‰¹åˆ¥æŠŠæ™‚é–“é» t çš„è§€æ¸¬å€¼å¾åŸå…ˆå®šç¾©æ‹‰å‡ºä¾†ï¼Œé€™æ˜¯è¦å¼·èª¿æˆ‘å€‘åœ¨å¾—åˆ°æœ€æ–°çš„è§€æ¸¬å€¼ $z_t$ å¾Œï¼Œå¸Œæœ›å»è¨ˆç®—æœ€æ–°çš„ believe äº‹å‰æ©Ÿç‡ (Motion Model): ç¨±ç‚º Motion Model æ˜¯å› ç‚ºå‡è¨­æˆ‘å€‘ç›®å‰åœ¨æ™‚é–“é» $t-1$ï¼Œæ¥è‘—æ‹¿åˆ°ä¸‹ä¸€æ¬¡çš„æ§åˆ¶ $u_t$ å¾Œï¼Œæˆ‘å€‘å¸Œæœ›ä¼°æ¸¬å‡ºä¸‹ä¸€æ¬¡çš„ç‹€æ…‹å€¼ $x_t$ æ˜¯ä»€éº¼ã€‚æœ‰çœ‹éå‰å¹¾ç¯‡çš„è®€è€…æ‡‰è©²é¦¬ä¸Šå°±èƒ½æƒ³åˆ°ï¼Œå¯ä»¥åˆ©ç”¨ CTRV ä¹‹é¡çš„ motion model å»è¨ˆç®—ã€‚ è§€æ¸¬å€¼æ©Ÿç‡ (Observation Model): é€™å€‹æ˜¯è¦è¨ˆç®—ç•¶ä¸‹çš„è§€æ¸¬å€¼çš„æ©Ÿç‡åˆ†ä½ˆï¼Œé€™éƒ¨åˆ†é€šå¸¸å°±æ˜¯ç¶“ç”± sensor data å¾—åˆ°å¾Œï¼Œæˆ‘å€‘å‡è¨­æ˜¯é«˜æ–¯åˆ†å¸ƒä¾†è¨ˆç®—ã€‚ Motion Model éè¿´ æˆ‘å€‘ç™¼ç¾åˆ°ï¼Œæœ€å¾Œä¸€è¡Œçš„çµæœï¼Œå°ç…§æœ¬æ–‡ç¬¬ä¸€å¼µåœ–çš„ State Prediction å¼å­æ˜¯ä¸€æ¨£çš„æ„æ€ï¼Œå·®åˆ¥åªåœ¨ä¸€å€‹æ˜¯é€£çºŒä¸€å€‹æ˜¯é›¢æ•£ã€‚å¦ä¸€å€‹å·®åˆ¥æ˜¯ï¼Œæ­¤å¼å­æ˜é¡¯å¯«å‡ºå¯ä»¥ç”¨ä¸Šä¸€æ¬¡çš„äº‹å¾Œæ©Ÿç‡åšéè¿´ï¼Œæ‰€ä»¥ç¬¬ä¸€å¼µåœ–çš„ Measurement Update è—è‰²ç®­é ­å°±é€™éº¼ä¾†çš„ã€‚ Observation Model ç°¡åŒ– Bayes Filter Summaryé‡æ–°æ•´ç†ä¸€ä¸‹ç¶“ç”± â€œMotion Model éè¿´â€ å’Œ â€œObservation Model ç°¡åŒ–â€ éå¾Œçš„äº‹å¾Œæ©Ÿç‡ $bel(x_t)$ï¼Œçµæœå¦‚ä¸‹åœ–å·¦ã€‚ (ä¸‹åœ–å³åªæ˜¯åˆ—å‡ºæœ¬æ–‡æœ€é–‹å§‹çš„ Bayes Filter å¼å­ä¾†åšå°ç…§)ã€‚çµè«–æ˜¯æˆ‘å€‘èŠ±äº†é‚£éº¼å¤§çš„åŠ›æ°£ï¼Œç”¨ä¸Šäº† 1st Markov Assumption å»è™•ç† Localization çš„éè¿´å¼å­å’Œç°¡åŒ–ï¼Œçµæœä¸æ„å¤–åœ°å°±å¦‚åŒé–‹å§‹çš„ Bayes Filter ä¸€æ¨£ã€‚ å¦å¤–ï¼Œå¯¦ä½œä¸Šå¦‚æœæ‰€æœ‰çš„ pdf éƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒçš„è©±ï¼Œçµæœå°±æ˜¯ Kalman Filterã€‚è€Œå¦‚æœé€é sampling é›¢æ•£çš„ç‹€æ…‹ä½ç½®çš„è©±ï¼Œçµæœå°±æœƒæ˜¯ Particle Filterã€‚é€™éƒ¨åˆ†å°±å…ˆä¸å¤šèªªæ˜äº†ã€‚(é™„ä¸Šèª²ç¨‹ä¸€å¼µæˆªåœ–) æœ‰é—œ Particle Filter çš„å¯¦ä½œï¼Œåœ¨ Udacity Term2 Project3 ä¸­æˆ‘å€‘å¯¦ä½œä¸€å€‹äºŒç¶­åœ°åœ–çš„ localizationã€‚ç›¸é—œ Codes å¯åœ¨ç­†è€… github ä¸­æ‰¾åˆ°ã€‚ Reference Udacity ä¸Šèª²å…§å®¹ MAP Adaptaion éƒ¨åˆ†è©³ç´°å¯åƒè€ƒ: Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains","tags":[{"name":"Bayes Filter","slug":"Bayes-Filter","permalink":"http://yoursite.com/tags/Bayes-Filter/"},{"name":"Localization","slug":"Localization","permalink":"http://yoursite.com/tags/Localization/"},{"name":"Markov Localization","slug":"Markov-Localization","permalink":"http://yoursite.com/tags/Markov-Localization/"},{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"}]},{"title":"Notes for Unscented Kalman Filter","date":"2017-04-12T12:50:16.000Z","path":"2017/04/12/Unscented-Kalman-Filter-Notes/","text":"è³‡æ–™ç‚º Udacity èª²ç¨‹å…§å®¹ã€‚äº‹å¯¦ä¸Š UKF æŒºå›‰å—¦çš„ï¼Œå–®ç´”çœ‹æœ¬æ–‡æ‡‰è©²ç„¡æ³•ç†è§£ï¼Œå¿…é ˆæ­é…å‰å…©ç¯‡ KF and EKF å’Œ CTRVã€‚ä¸»è¦æ˜¯ç­†è¨˜ç”¨ï¼Œè®“è‡ªå·±å¯ä»¥æ ¹æ“šæ–‡ç« å®Œæ•´å¯¦åšå‡ºä¾†ã€‚ ä¸€åˆ‡çš„ä¸€åˆ‡éƒ½ä¾†è‡ªæ–¼ Kalman Filter çš„ State-Space model å‡è¨­ï¼Œæˆ‘å€‘ä¾†ç¨å¾®å›é¡§ä¸€ä¸‹ã€‚ $$\\begin{align} x_k = F_kx_{k-1}+\\nu_k \\\\ z_k = H_kx_k+\\omega_k&Tab;\\\\ \\end{align}$$ å¼(1)è¡¨ç¤ºç‹€æ…‹å€¼ $x$ æ»¿è¶³ç·šæ€§çš„éè¿´é—œä¿‚å¼ï¼Œè€Œå¼(2)è¡¨ç¤ºè§€æ¸¬å€¼ $z$ æ˜¯ç•¶ä¸‹ç‹€æ…‹å€¼çš„ç·šæ€§é—œä¿‚å¼ã€‚é€™å€‹ç·šæ€§çš„é—œä¿‚å¼æ˜¯ç‚ºäº†ä½¿å¾—æˆ‘å€‘çš„é«˜æ–¯åˆ†å¸ƒåœ¨è½‰æ›å¾Œä»ç„¶æ»¿è¶³é«˜æ–¯åˆ†å¸ƒæ‰€åšçš„å‡è¨­ã€‚ä½†å¯¦éš›ä¸Šå¸¸å¸¸ä¸æ»¿è¶³ç·šæ€§çš„é—œä¿‚ï¼Œä¾‹å¦‚å‡è¨­æˆ‘å€‘çš„ $x$ åŒ…å«äº† Cartesian coordinate çš„åº§æ¨™ä½ç½®å’Œé€Ÿåº¦çš„è³‡è¨Šï¼Œä½†æ˜¯ RADAR çš„è§€æ¸¬å€¼ $z$ å»æ˜¯ç”¨ Polar coordinate ä¾†è¡¨ç¤ºï¼Œå°±æœƒæœ‰ä¸€å€‹éç·šæ€§çš„åº§æ¨™è½‰æ›ã€‚å¦ä¸€å€‹æœƒé€ æˆéç·šæ€§çš„æƒ…æ³æ˜¯ç™¼ç”Ÿåœ¨å¼(1)ï¼Œä¹Ÿå°±æ˜¯æˆ‘å€‘å¦‚æœä½¿ç”¨æ›´ç²¾ç¢ºçš„ motion modelï¼Œå¦‚ CTRVã€‚EKF è§£æ±ºçš„æ–¹æ³•æ˜¯ç”¨ Jacobian åšç·šæ€§çš„é€¼è¿‘ï¼Œä½†æ˜¯éç·šæ€§çš„é—œä¿‚å¼å¦‚æœä¸€è¤‡é›œï¼Œç®— Jacobian å°±æœƒå¤ªè¤‡é›œä¸”é€ æˆé‹ç®—é€Ÿåº¦è®Šæ…¢ã€‚å› æ­¤ï¼Œæœ¬ç¯‡è¦ä»‹ç´¹çš„ Unscented KF æœ‰ç›¸å°ç°¡å–®çš„è¾¦æ³•ï¼Œä¸¦ä¸”é‹ç®—é€Ÿåº¦å¿«ï¼Œä¸”å¯¦éš›æ•ˆæœå¥½ã€‚UKF æ¦‚å¿µä¸Šæ€éº¼åšå‘¢? æˆ‘å€‘çœ‹ä¸Šåœ–å°±å¯äº†è§£ï¼Œé¦–å…ˆåŸå§‹çš„é«˜æ–¯åˆ†å¸ƒ(ä¸Šé¢çš„ç´…è‰²æ©¢åœ“)ï¼Œç¶“ç”±éç·šæ€§è½‰æ› $f$ å¾Œå¾—åˆ°çš„ â€œå¯¦éš›åˆ†ä½ˆâ€ ç‚ºä¸‹é¢çš„é»ƒè‰²æ›²ç·šï¼Œè€Œè©²å¯¦éš›åˆ†å¸ƒçš„ mean å’Œ covariance matrix æ‰€å½¢æˆçš„çš„é«˜æ–¯åˆ†å¸ƒç‚ºä¸‹é¢çš„ç´…è‰²æ©¢åœ“ï¼Œä½†æ˜¯æˆ‘å€‘ä¸å®¹æ˜“å¾—åˆ°! é‚£éº¼æ€éº¼é€¼è¿‘ä¸‹é¢çš„ç´…è‰²æ©¢åœ“å‘¢? UKF åšæ³•å°±æ˜¯åœ¨ä¸Šåœ–é¸æ“‡ä¸€äº›ä»£è¡¨çš„é»ï¼Œç¨±ç‚º Sigma Pointsï¼Œç¶“é $f$ è½‰æ›å¾Œï¼Œå¯ä»¥å¾—åˆ°ä¸‹é¢çš„æ˜Ÿæ˜Ÿï¼Œç„¶å¾Œå°±å¯ä»¥æ ¹æ“šé€™äº›è½‰æ›å¾Œçš„æ˜Ÿæ˜Ÿå»è¨ˆç®—ä»–å€‘çš„ mean å’Œ covariance matrixï¼Œè€Œå¾—åˆ°è—è‰²çš„æ©¢åœ“ã€‚é‚£éº¼æˆ‘å€‘é¦¬ä¸Šé–‹å§‹èªªæ˜å¦‚ä½•è¨­å®š Sigma Points å§ã€‚ Sigma Points é¸æ“‡å‡è¨­ state dimension ç‚º $n_x$ï¼ŒSigma Points å°±é¸æ“‡ $2n_x+1$ å€‹é»ã€‚æˆ‘å€‘ä»¥ $n_x=2$ ä¾†èˆ‰ä¾‹èªªæ˜æœƒæ¯”è¼ƒæ¸…æ¥šï¼Œè€Œæ“´å±•åˆ°æ›´é«˜çš„ç¶­åº¦ä¹Ÿå°±éå¸¸ trivial äº†ã€‚ å¯ä»¥çŸ¥é“æˆ‘å€‘éœ€é¸æ“‡5å€‹é»($2n_x+1$)ï¼Œç¬¬ä¸€å€‹é»æ˜¯ mean vectorï¼Œæ¥è‘—é‡å°æ¯ä¸€å€‹ dimension éƒ½æ ¹æ“š mean vector å‘è©² dimension å»åšæ­£è² æ–¹å‘çš„ perturbï¼Œè€Œ $\\lambda$ è¡¨ç¤ºè¦ perturb å¤šé (ä½¿ç”¨è€…çµ¦å®šçš„å€¼)ã€‚ä½†æ˜¯è¦ç‰¹åˆ¥æ³¨æ„çš„æ˜¯ï¼Œé€™è£¡çš„ perturb dimension å¿…é ˆæ˜¯æ­£è¦åŒ–å¾Œçš„æ–¹å‘ (Whitening)ï¼Œå¦å‰‡è‹¥åŸä¾†çš„é«˜æ–¯åˆ†å¸ƒæŸä¸€å€‹æ–¹å‘ç‰¹åˆ¥å¤§(æƒ³åƒä¸€å€‹å¾ˆæ‰çš„æ©¢åœ“)ï¼Œä½¿ç”¨åŸä¾†çš„ covariance matrix å°±æœƒè¢«è©²æ–¹å‘ dominateã€‚ä¸Šä¾‹çš„ sigma points å¦‚ä¸‹: CTRV Sigma Pointsæˆ‘å€‘ä¾†çœ‹ CTRV model ä¸‹çš„ sigma points é¸æ“‡ï¼Œå…¶ä¸­ state vector and noise term åˆ†åˆ¥å®šç¾©å¦‚ä¸‹ $$\\begin{align} x= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v \\\\ \\psi \\\\ \\dot{\\psi} \\end{array} \\right) \\end{align}$$ $$\\begin{align} v_k= \\left[ \\begin{array}{center} v_{a,k} \\\\ v_{\\ddot{\\psi},k} \\end{array} \\right]\\\\ v_{a,k}\\sim N(0,\\sigma_a^2),v_{a,k}\\sim N(0,\\sigma_{\\ddot{\\psi}}^2) \\\\ Q=E[v_k,v_k^T]= \\left[ \\begin{array}{clr} \\sigma_a^2 &amp; 0 \\\\ 0 &amp; \\sigma_{\\ddot{\\psi}}^2 \\\\ \\end{array} \\right] \\end{align}$$ $v_k$ çš„ç¬¬ä¸€å€‹ term æ˜¯åŠ é€Ÿåº¦çš„ noiseï¼Œè€Œç¬¬äºŒå€‹è¡¨ç¤º yaw rate çš„è®ŠåŒ–ç‡ã€‚ç”±æ–¼åŸå§‹çš„ state recursion é‚„åƒé›œäº† $Stochastic_k$ é€™æ¨£çš„ vector (åƒè€ƒå¼(7)and(8))ï¼Œå› æ­¤è¦è¨ˆç®—ä»–å€‘çš„ covariance matrix æœƒå¤ªé›£æ! (å› ç‚ºæˆ‘å€‘éœ€è¦çŸ¥é“ covariance matrix æ‰èƒ½å°æ¯å€‹ whitening å¾Œçš„ç¶­åº¦å» perturb å–é») $$\\begin{align} x_{k+1}=x_k+Deterministic_k+ \\left[ \\begin{array}{center} \\frac{1}{2}(\\Delta{t})^2cos(\\psi_k)\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2sin(\\psi_k)\\cdot v_{a,k} \\\\ \\Delta{t}\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2\\cdot v_{\\ddot{\\psi},k} \\\\ \\Delta{t}\\cdot v_{\\ddot{\\psi},k} \\end{array} \\right]\\\\ x_{k+1}=x_k+Deterministic_k+Stochastic_k \\end{align}$$ æ¯”è¼ƒç°¡å–®çš„ä½œæ³•æ˜¯å°‡ noise term (å¼(4)) ç•¶æˆ state vector çš„å¦å¤–çš„ç¶­åº¦ï¼Œä¸»è¦çš„å¥½è™•æ˜¯ covariance matrix å°±è®Šå¾—å¾ˆå®¹æ˜“è¨ˆç®—äº†ã€‚ç„¶å¾Œä¸€æ¨£ç”¨ä¸Šè¿°çš„æ–¹å¼ç”¢ç”Ÿ Sigma Pointsã€‚å› æ­¤æ•´å€‹æµç¨‹å¦‚ä¸‹åœ–: å¯ä»¥çœ‹åˆ°åŸæœ¬ç¶­åº¦å¾5è®Šæˆ7ï¼Œå› æ­¤è¦ç”¢ç”Ÿ15é»çš„ sigma pointsï¼Œè€Œ augmentated state vector çš„ covariance matrix è®Šå¾—å¾ˆå®¹æ˜“å®šç¾©ã€‚ Sigma Points Predictionç”¢ç”Ÿäº†é€™äº› sigma points ä¹‹å¾Œï¼Œæˆ‘å€‘å°±å¯ä»¥é€éå¼(7)ï¼Œåš nonlinear recursion åˆ°ä¸‹ä¸€å€‹æ™‚é–“é»çš„ state vector (æ³¨æ„åˆ° noise term ä¹Ÿè¢« sigma points å–æ¨£äº†ï¼Œæ‰€ä»¥å¯ä»¥å¸¶å…¥å¼(7)ä¸­)! Mean and Covariance of Sigma Pointsé‚„è¨˜å¾—å—? å°‡ sigma point transform å¾Œï¼Œæˆ‘å€‘ä¸‹ä¸€æ­¥å°±æ˜¯è¦ä¼°è¨ˆå‡º mean å’Œ covarianceï¼Œå¿˜è¨˜çš„åŒé‹å€‘å¯ä»¥çœ‹ä¸€ä¸‹æœ¬æ–‡æœ€é–‹å§‹çš„åœ– (è—è‰²çš„é«˜æ–¯åˆ†å¸ƒ)ã€‚åŸºæœ¬ä¸Šæ ¹æ“šä¸€äº› data points ç®—å®ƒå€‘çš„é«˜æ–¯åˆ†å¸ƒéå¸¸ç°¡å–®ï¼Œä½†æ˜¯ç”±æ–¼æˆ‘å€‘ç•¶åˆå–çš„ sigma points å®ƒå€‘ä¹‹é–“æœ¬ä¾†çš„æ©Ÿç‡å°±ä¸åŒï¼Œå› æ­¤åœ¨è¨ˆç®—è½‰æ›å¾Œçš„é«˜æ–¯åˆ†å¸ƒå¿…é ˆè¦è€ƒæ…®æ¯å€‹é»çš„æ¬Šé‡ã€‚æ¬Šé‡çš„è¨­å®šæœ‰ä¸åŒæ–¹æ³•ï¼Œèª²ç¨‹ç›´æ¥å»ºè­°ä¸‹é¢çš„è¨­å®šï¼Œæ‰€ä»¥æ²’ç‰¹åˆ¥è¦èªªæ˜çš„ï¼Œå°±ç…§å…¬å¼è¨ˆç®—è€Œå·²: Measurement Predictionå°æ–¼ RADAR ä¾†èªªå¼(2)ä¹Ÿæ˜¯ä¸€å€‹éç·šæ€§çš„é—œä¿‚ï¼Œå› æ­¤ä¹Ÿå¯ä»¥ç”¨ sigma points çš„æ–¹æ³•ä¾†é€¼è¿‘ã€‚å‡è¨­æˆ‘å€‘åœ¨æ™‚é–“é» $k$ å–çš„ sigma points ç‚º $x$ï¼Œç¶“ééç·šæ€§ state recursion å¾Œå¾—åˆ°æ™‚é–“é» $k+1$ çš„ sigma points ç‚º $xâ€™$ï¼Œæˆ‘å€‘å¯ä»¥ç›´æ¥å°‡ $xâ€™$ ç•¶ä½œæ–°å–çš„ sigma pointsï¼Œæ‹¿ä¾†åš measurement éç·šæ€§è½‰æ› $zâ€™=h(xâ€™)+w$ï¼Œç„¶å¾Œä¸€æ¨£ç”¨ä¸Šé¢çš„å…¬å¼ç®—ä¸€ä¸‹ measurement space çš„é«˜æ–¯åˆ†å¸ƒå³å¯ã€‚RADAR çš„ $h()$ å®šç¾©å¦‚ä¸‹: $$\\begin{align} z=h(x)= \\left( \\begin{array}{clr} \\rho \\\\ \\phi \\\\ \\dot{\\rho} \\end{array} \\right) = \\left( \\begin{array}{clr} \\sqrt{p_x^2+p_y^2} \\\\ \\arctan(p_y/p_x) \\\\ \\frac{p_xcos(\\psi)v+p_ysin(\\psi)v}{\\sqrt{p_x^2+p_y^2}} \\end{array} \\right) \\end{align}$$ ç¨å¾®è¦æ³¨æ„çš„æ˜¯ï¼Œè¨ˆç®— covariance æ™‚é ˆè€ƒæ…® noise çš„ covariance (ä¸‹åœ–ç´…è‰²æ¡†èµ·ä¾†çš„åœ°æ–¹)ï¼Œé€™è·Ÿè¨ˆç®— state space ä¸­çš„é«˜æ–¯åˆ†å¸ƒä¸åŒã€‚é€™æ˜¯å› ç‚ºåœ¨ measurement space æ˜¯å…©å€‹ independent çš„é«˜æ–¯åˆ†å¸ƒç›¸åŠ  (ä¸€å€‹æ˜¯ sigma point ä¼°å‡ºä¾†çš„ï¼Œå¦ä¸€å€‹æ˜¯ noise çš„é«˜æ–¯)ï¼Œcovariance å°±æ˜¯ç›¸åŠ è€Œå·²ã€‚ å¦å¤–å°æ–¼ LIDAR ä¾†èªª measurement çš„è½‰æ›æ˜¯ç·šæ€§é—œä¿‚ï¼Œæ‰€ä»¥ä¸ä½¿ç”¨ sigma point çš„æ–¹æ³•ï¼Œå› æ­¤åœ¨è™•ç†å…©ç¨® sensor data æ™‚ï¼Œè¨˜å¾—å€åˆ†ä¸€ä¸‹ caseã€‚ Measurement Updateçµ‚æ–¼ä¾†åˆ°æœ€å¾Œçš„æ­¥é©Ÿäº†ã€‚æˆ‘å€‘è²»ç›¡åƒè¾›è¬è‹¦æ ¹æ“šæ™‚é–“é» $k$ çš„ state vector ä¼°è¨ˆå‡ºäº†æ™‚é–“é» $k+1$ çš„ measurement å€¼ï¼Œè€Œæ­¤æ™‚æˆ‘å€‘åœ¨æ™‚é–“é» $k+1$ ä¹Ÿæ”¶åˆ°äº†çœŸæ­£çš„ sensor data measurementã€‚å› æ­¤åŒæ¨£å¯ä»¥ä½¿ç”¨ KF çš„æµç¨‹å»è¨ˆç®—æ‰€æœ‰çš„ update! åŸå› æ˜¯æˆ‘å€‘å…¶å¯¦å…¨éƒ¨éƒ½é«˜æ–¯åŒ–äº† (é€é sigma points æ–¹æ³•)ã€‚ ç´…è‰²æ¡†èµ·ä¾†è™•ç‚ºè·Ÿä»¥å‰ä¸åŒçš„åœ°æ–¹ï¼Œè®Šæˆè¦è¨ˆç®— cross-correlation of â€œMeasurement Prediction é‚£å€‹ section çš„ç¬¬äºŒå¼µåœ–é‚£å…©æ’çš„ vectorsâ€ å¿ƒå¾—å…¶å¯¦æ¦‚å¿µä¸¦ä¸å›°é›£ï¼Œä½†æ˜¯é —å¤šè¨ˆç®—æµç¨‹å’Œç¬¦è™Ÿï¼ŒåŒæ™‚ä¹Ÿå¿…é ˆå…ˆäº†è§£ Kalman Filter å’Œ CTRV motion modelï¼Œä¸‹ä¸€æ­¥å°±å¯¦ä½œ Project å§! é™„ä¸Š predict çš„çµæœ:","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"Unscented Kalman Filter","slug":"Unscented-Kalman-Filter","permalink":"http://yoursite.com/tags/Unscented-Kalman-Filter/"}]},{"title":"CTRV Motion Model","date":"2017-04-11T14:15:41.000Z","path":"2017/04/11/CTRV-Motion-Model/","text":"Motion Models è³‡æ–™ç‚º Udacity èª²ç¨‹å…§å®¹ åœ¨ä¸Šä¸€ç¯‡ EKF ä¸­ï¼Œæˆ‘å€‘å…¶å¯¦å‡è¨­çš„æ˜¯ constant velocity model (CV)ï¼Œä¹Ÿå°±æ˜¯å¦‚ä¸‹çš„é—œä¿‚å¼$$\\begin{align} x_k = Fx_{k-1}+\\nu_k \\\\ x_k= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v_x \\\\ v_y \\end{array} \\right), F= \\left( \\begin{array}{clr} 1 &amp; 0 &amp; \\Delta{t} &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\Delta{t} \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right) \\end{align}$$æ­£å¥½æ»¿è¶³ Kalman Filter ä¸­ State-space model çš„å‡è¨­ï¼Œä½†é€™æ¨£çš„ motion model å¾ˆæ˜é¡¯å¤ªå–®ç´”äº†ï¼Œå› ç‚ºè»Šå­ç¸½æ˜¯åœ¨è®Šé€Ÿä¸”è½‰å½ã€‚å› æ­¤çœŸå¯¦åœ¨ä½¿ç”¨çš„æ™‚å€™ä¸æœƒç”¨ CV modelï¼Œé‚£æœƒç”¨ä»€éº¼å‘¢? ä»¥ä¸‹ç‚ºå¹¾ç¨®å¯ç”¨çš„: constant turn rate and velocity magnitude model (CTRV) constant turn rate and acceleration (CTRA) constant steering angle and velocity (CSAV) constant curvature and acceleration (CCA) Udacity åœ¨é€™æ¬¡çš„ project ä¸­è®“æˆ‘å€‘ä½¿ç”¨äº† CTRVï¼Œè€Œæ­¤ model çš„ state vector $x$ å®šç¾©å¦‚ä¸‹:$$\\begin{align} x= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v \\\\ \\psi \\\\ \\dot{\\psi} \\end{array} \\right) \\end{align}$$å…¶ä¸­ $p_x,p_y$ æ˜¯ $x,y$ åº§æ¨™ä½ç½®ï¼Œ$v$ æ˜¯é€Ÿåº¦çš„ magnitudeï¼Œ$\\psi$ æ˜¯é€Ÿåº¦çš„å‘é‡èˆ‡æ°´å¹³è»¸çš„å¤¾è§’ç¨± yaw angelï¼Œæœ€å¾Œçš„ $\\dot{\\psi}$ å‰‡æ˜¯è©²å¤¾è§’çš„è®ŠåŒ–ç‡ç¨± yaw rateã€‚è€Œ CTRV å‡è¨­çš„æ˜¯ $v$ å’Œ $\\dot{\\psi}$ æ˜¯ constantã€‚è€Œæ­¤ model å·²ä¸æ˜¯ä¸€å€‹ç·šæ€§ç³»çµ±äº†ï¼Œä¹Ÿå°±æ˜¯ç„¡æ³•ç”¨ matrix ä¾†è¡¨é”ï¼Œæ‰€ä»¥æˆ‘å€‘å°‡å¼(1)æ”¹ç‚ºå¦‚ä¸‹çš„è¡¨é”æ–¹å¼:$$\\begin{align} x_{k+1} = f(x_k,\\nu_k) \\end{align}$$å¦‚ä½•å°‡ function $f$ å¯«æˆéè¿´å¼å­å‘¢? è«‹çœ‹ä¸‹ä¸€æ®µ CTRV State Vector Recursionæˆ‘å€‘å…ˆå¿½ç•¥ noise $\\nu_k$ é€™é …ï¼Œæ™šé»å†åŠ å›ä¾†ã€‚State vector éš¨æ™‚é–“è®ŠåŒ–çš„å¼å­å¦‚ä¸‹:$$\\begin{align} x_{k+1}=x_k+\\int_{t_k}^{t_{k+1}}{ \\left[ \\begin{array} \\\\ \\dot{p}_x(t) \\\\ \\dot{p}_y(t) \\\\ \\dot{v}(t) \\\\ \\dot{\\psi}(t) \\\\ \\ddot{\\psi}(t) \\end{array} \\right] }dt\\\\ x_{k+1}=x_k+ \\left[ \\begin{array}{center} \\int_{t_k}^{t_{k+1}}{v(t)\\cdot cos(\\psi(t))}dt \\\\ \\int_{t_k}^{t_{k+1}}{v(t)\\cdot sin(\\psi(t))}dt \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] \\end{align}$$æ³¨æ„åˆ° CTRV çš„å‡è¨­ $v$ å’Œ $\\dot{\\psi}$ æ˜¯ constantï¼Œä¹Ÿå°±æœƒé€ æˆå¼(5)ä¸­ $\\dot{v}(t)=\\ddot{\\psi(t)}=0$ï¼Œä¸”å¾æ™‚é–“ $k$ åˆ° $k+1$ çš„ $\\dot{\\psi}(t)$ éƒ½ç­‰æ–¼ $\\dot{\\psi}_k$ï¼Œä¹Ÿå› æ­¤å¾—åˆ°å¼(6)ã€‚ä½†æ˜¯æˆ‘å€‘ä»ç„¶è¦è™•ç†å¼(6)å‰å…©é …çš„ç©åˆ†ï¼Œé¦–å…ˆä¸€æ¨£åŸºæ–¼CTRVå‡è¨­ $v(t)=v_k$ å°æ–¼æ™‚é–“ $k$ åˆ° $k+1$ éƒ½æ˜¯ä¸€æ¨£ï¼Œæ‰€ä»¥æåˆ°ç©åˆ†å¤–é¢ã€‚ç„¶å¾Œç”±æ–¼ yaw rate æ˜¯ constantï¼Œå› æ­¤ $\\psi(t)$ å¯ä»¥æ˜ç¢ºè¡¨ç¤ºå‡ºä¾†ï¼Œç¸½ä¹‹æ”¹å¯«å¦‚ä¸‹:$$\\begin{align} x_{k+1}=x_k+ \\left[ \\begin{array}{center} v_k\\int_{t_k}^{t_{k+1}}{cos(\\psi_k+\\dot{\\psi}_k\\cdot(t-t_k))}dt \\\\ v_k\\int_{t_k}^{t_{k+1}}{sin(\\psi_k+\\dot{\\psi}_k\\cdot(t-t_k))}dt \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] \\end{align}$$ç„¶å¾Œæ²’ä»€éº¼å¥½èªªçš„ï¼Œå°±ç©å®ƒå§:$$\\begin{align} x_{k+1}=x_k+ \\left[ \\begin{array}{center} \\frac{v_k}{\\dot{\\psi}_k}(sin(\\psi_k+\\dot{\\psi}_k\\Delta{t})-sin(\\psi_k)) \\\\ \\frac{v_k}{\\dot{\\psi}_k}(-cos(\\psi_k+\\dot{\\psi}_k\\Delta{t})+cos(\\psi_k)) \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] =x_k+Deterministic_k \\end{align}$$é€™é‚Šæœ‰ä¸€å€‹å¯¦ä½œä¸Šéœ€è¦é¿å…çš„åœ°æ–¹ï¼Œå°±æ˜¯ç•¶ $\\dot{\\psi}_k=0$ æ™‚ï¼Œä¸Šå¼çš„ç¬¬1,2é …æœƒé™¤0ã€‚ä¸éæˆ‘å€‘çŸ¥é“ç•¶ $\\dot{\\psi}_k=0$ è¡¨ç¤ºè»Šå­æ˜¯ç›´ç›´å¾€å‰é–‹ï¼Œyaw angleä¸æœƒæ”¹è®Šï¼Œå› æ­¤å¯¦éš›ä¸Šå¯ä»¥ç”¨å¦‚ä¸‹ä¾†è¨ˆç®—:$$\\begin{align} x_{k+1}=x_k+ \\left[ \\begin{array}{center} v_kcos(\\psi_k)\\Delta{t} \\\\ v_ksin(\\psi_k)\\Delta{t} \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right] =x_k+Deterministic&apos;_k \\end{align}$$ Recursion With Noise TermNoise term $v_k$ é€™è£¡æ˜¯å‡è¨­å¦‚ä¸‹:$$\\begin{align} v_k= \\left[ \\begin{array}{center} v_{a,k} \\\\ v_{\\ddot{\\psi},k} \\end{array} \\right] \\end{align}$$ ç¬¬ä¸€å€‹ term æ˜¯åŠ é€Ÿåº¦çš„ noiseï¼Œè€Œç¬¬äºŒå€‹è¡¨ç¤º yaw rate çš„è®ŠåŒ–ç‡ã€‚è€ƒæ…®å¦‚æœæœ‰é€™å…©é … noises çš„è©±ï¼Œä¸¦ä¸”å‡è¨­æ™‚é–“ $k$ åˆ° $k+1$ é€™å…©å€‹ noises çš„å€¼æ˜¯å›ºå®šçš„ï¼Œé‚£éº¼ state vector æœƒè®Šæˆå¦‚ä¸‹:$$\\begin{align} x_{k+1}=x_k+Deterministic_k+ \\left[ \\begin{array}{center} \\frac{1}{2}(\\Delta{t})^2cos(\\psi_k)\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2sin(\\psi_k)\\cdot v_{a,k} \\\\ \\Delta{t}\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2\\cdot v_{\\ddot{\\psi},k} \\\\ \\Delta{t}\\cdot v_{\\ddot{\\psi},k} \\end{array} \\right]\\\\ x_{k+1}=x_k+Deterministic_k+Stochastic_k \\end{align}$$ç¬¬ä¸‰é …æ˜¯é€Ÿåº¦ $v$ æœƒè¢«åŠ é€Ÿåº¦ $v_{a,k}$ é€™ç¨® noise æ€éº¼å½±éŸ¿ï¼Œæ‰€ä»¥å¾ˆæ˜é¡¯æ˜¯ç·šæ€§å¢åŠ ï¼ŒåŒç†ç¬¬å››å’Œç¬¬äº”é …ä¹Ÿå¾ˆå®¹æ˜“å¾—åˆ°ã€‚ç¬¬ä¸€å’Œç¬¬äºŒé …ï¼Œ$x$ and $y$ çš„ä½ç½®é€™è£¡å°±æ¯”è¼ƒéº»ç…©ï¼Œå› æ­¤æ¡ç”¨çš„æ˜¯ä¸€å€‹è¿‘ä¼¼è€Œå·²ã€‚é€™é‚Šå‡è¨­ yaw rate æ²’æœ‰å¤ªé«˜çš„æƒ…æ³ä¸‹ï¼Œä¸‹åœ–çš„å…©å€‹ç´…è‰²åœˆåœˆä½ç½®æ‡‰è©²æ˜¯å¾ˆæ¥è¿‘ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥è€ƒæ…®èµ°ç›´ç·šçš„ç´…è‰²åœˆåœˆä½ç½®ï¼Œä¹Ÿå°±å¾—åˆ°äº†(11)ç¬¬ä¸€äºŒé …çš„è¿‘ä¼¼å€¼ã€‚ Summary All CTRVçœç•¥è§£é‡‹ï¼Œå¯«å‡º state recursion çš„è¨ˆç®—ã€‚$$x= \\left( \\begin{array}{clr} p_x \\\\ p_y \\\\ v \\\\ \\psi \\\\ \\dot{\\psi} \\end{array} \\right)$$ if $\\dot{\\psi}_k\\neq0$, then$x_{k+1}=x_k+Deterministic_k+Stochastic_k$where$$Deterministic_k= \\left[ \\begin{array}{center} \\frac{v_k}{\\dot{\\psi}_k}(sin(\\psi_k+\\dot{\\psi}_k\\Delta{t})-sin(\\psi_k)) \\\\ \\frac{v_k}{\\dot{\\psi}_k}(-cos(\\psi_k+\\dot{\\psi}_k\\Delta{t})+cos(\\psi_k)) \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right]$$and$$Stochastic_k= \\left[ \\begin{array}{center} \\frac{1}{2}(\\Delta{t})^2cos(\\psi_k)\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2sin(\\psi_k)\\cdot v_{a,k} \\\\ \\Delta{t}\\cdot v_{a,k} \\\\ \\frac{1}{2}(\\Delta{t})^2\\cdot v_{\\ddot{\\psi},k} \\\\ \\Delta{t}\\cdot v_{\\ddot{\\psi},k} \\end{array} \\right]$$otherwise $\\dot{\\psi}_k=0$, then$x_{k+1}=x_k+Deterministic&apos;_k+Stochastic_k$where$$Deterministic&apos;_k= \\left[ \\begin{array}{center} v_kcos(\\psi_k)\\Delta{t} \\\\ v_ksin(\\psi_k)\\Delta{t} \\\\ 0 \\\\ \\dot{\\psi}_k\\cdot\\Delta{t} \\\\ 0 \\end{array} \\right]$$ Unscented Kalman Filter ç°¡ä»‹ç”±æ–¼ CTRV æ˜¯éç·šæ€§çš„ï¼Œæœƒç ´å£ State-space model çš„ç·šæ€§å‡è¨­ï¼Œä¾‹å¦‚ä¸‹åœ–ä¸­åŸå…ˆç´…è‰²çš„é«˜æ–¯åˆ†å¸ƒç¶“ééç·šæ€§è½‰æ›å¾Œåˆ†å¸ƒç‚ºé»ƒè‰²ã€‚ä¸éæˆ‘å€‘çŸ¥é“ EKF å¯ä»¥åˆ©ç”¨ Jaccobian matrix åšç·šæ€§é€¼è¿‘è¨ˆç®—ï¼Œæ‰€ä»¥æˆ‘å€‘åŒæ¨£å¯ä»¥è¨ˆç®—ã€‚ä½†è¦è¨ˆç®—ä¸Šè¿°éç·šæ€§ç³»çµ±çš„ Jaccobian matrix å¯¦åœ¨é¡¯å¾—æœ‰é»è¤‡é›œï¼Œå¥½åœ¨ Unscented KF å¯ä»¥å®Œå…¨é¿é–‹é€™å€‹éº»ç…©ã€‚å®ƒåˆ©ç”¨é¸æ“‡å¹¾å€‹ä»£è¡¨çš„ candidates vectorsï¼Œå«åš Sigma Pointsï¼Œå»è¨ˆç®—ç¶“ééç·šæ€§è½‰æ›å¾Œçš„å€¼ï¼Œç„¶å¾Œå°±å¯ä»¥å¾—åˆ° output domain çš„ mean å’Œ covariance matrixï¼Œä¹Ÿå°±æ˜¯ä¸Šåœ–çš„ç¶ è‰²é«˜æ–¯åˆ†å¸ƒã€‚é€™é‚Šè¦æ³¨æ„çš„æ˜¯ï¼Œoutput domain çš„çœŸå¯¦åˆ†ä½ˆä¸æ˜¯é«˜æ–¯åˆ†å¸ƒ(é»ƒè‰²)ï¼Œä½†æˆ‘å€‘ä»ç„¶å°‡å®ƒç•¶æˆæ˜¯é«˜æ–¯åˆ†å¸ƒ(ç¶ è‰²)å»è¨ˆç®— mean å’Œ covariance matrixï¼Œå› ç‚ºé€™æ¨£æ‰èƒ½ç¹¼çºŒå¥—ç”¨ Kalman filter çš„æ–¹æ³•ã€‚èªªåˆ°é€™å¯çŸ¥é“ UKF ä»ç„¶åªæ˜¯é€¼è¿‘ï¼Œä¸éæ ¹æ“š Udacity çš„èªªæ³•ï¼Œå¯¦éš›æ‡‰ç”¨ä¸Š UKF æ˜¯å¾ˆå¿« (ä¸ç”¨è¨ˆç®— Jaccobian) ä¸”å¯¦éš›ä¸Šæ•ˆæœå¾ˆå¥½!ä¸‹å›é å‘Šï¼ŒUKFå®Œæ•´ä»‹ç´¹ã€‚","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"Motion Model","slug":"Motion-Model","permalink":"http://yoursite.com/tags/Motion-Model/"},{"name":"Unscented Kalman Filter","slug":"Unscented-Kalman-Filter","permalink":"http://yoursite.com/tags/Unscented-Kalman-Filter/"}]},{"title":"Notes for Kalman Filter and Extended KF","date":"2017-04-03T08:56:13.000Z","path":"2017/04/03/Kalman-Filter-and-Extended-KF-Notes/","text":"Udacity term2 (Sensor Fusion, Localization, and Control) çš„ç¬¬ä¸€å€‹ Project å°±æ˜¯ç”¨ KF and EKF å°‡ Lidar and Radar çš„è³‡è¨Šåš fusion ä¸¦ä¸”å¯ä»¥ trackingã€‚ç”±æ–¼ KF/EKF çš„æ•¸å­¸ç¬¦è™Ÿå¾ˆå¤šï¼Œå› æ­¤æƒ³ç­†è¨˜ä¸€ä¸‹æ–¹ä¾¿æ—¥å¾Œå›æƒ³ï¼Œæ‰€ä»¥ä¸»è¦ä»¥æˆ‘è‡ªå·±çœ‹çš„è§’åº¦ï¼Œå¯èƒ½æœ‰äº›åœ°æ–¹æœƒæ²’æœ‰æ˜ç¢ºèªªæ˜ã€‚æœ¬ç¯‡çš„ç­†è¨˜ä¾†æºæ˜¯ é€™è£¡ï¼Œé€™ç¯‡çœŸçš„è¬›çš„è¶…æ£’çš„ï¼Œæ¸…æ¥šæ˜“æ‡‚! éå¸¸å»ºè­°ç›´æ¥å»çœ‹! Udacity èª²ç¨‹å…§å®¹ è‹¥è¦å¯¦ä½œæ‰€æœ‰çš„è¨ˆç®—æµç¨‹ä¸ç®¡ç†è«–çš„è©±ï¼Œå¯ç›´æ¥è·³åˆ° â€œ7. ç¸½çµ Lidar and Radar Fusionâ€ã€‚ State Space Modelé€™æ˜¯æ•´å€‹ KF/EKF çš„æ¨¡å‹å‡è¨­ï¼Œå¯«å‡ºä¾†å¦‚ä¸‹: $$\\begin{align} x_k = F_kx_{k-1}+\\nu_k \\\\ z_k = H_kx_k+\\omega_k \\end{align}$$ \\(x_k\\) æ˜¯åœ¨æ™‚é–“é» \\(t\\) çš„ statesï¼Œä¹Ÿæ˜¯æˆ‘å€‘å¸Œæœ›èƒ½å¤ ä¼°è¨ˆå‡ºä¾†çš„(ä½†æ˜¯ç„¡æ³•ç›´æ¥è§€å¯Ÿåˆ°)ã€‚è€Œ states æ»¿è¶³ ç·šæ€§çš„ä¸€æ¬¡éè¿´ é—œä¿‚ï¼Œä¹Ÿå°±æ˜¯å¼å­(1)ã€‚ \\(\\nu_k\\sim\\mathcal{N}(0,Q_k)\\) æ˜¯ state noseã€‚\\(z_k\\) æ˜¯åœ¨æ™‚é–“é» \\(t\\) çš„ observationsï¼Œé€é \\(H_k\\) å°‡ states è½‰æ›åˆ° observationsã€‚ \\(\\omega_k\\sim\\mathcal{N}(0,R_k)\\) æ˜¯ sensor noiseï¼Œè€Œ \\(R_k\\) åŸºæœ¬ä¸Šæœƒç”±è£½é€ å» å•†æä¾›ã€‚åŸºæœ¬ä¸Šå…©å€‹ noises éƒ½è·Ÿæ‰€æœ‰äººéƒ½ independentã€‚ Prediction Stageæ•´å€‹ KF/EKF éƒ½æ˜¯åŸºæ–¼ Gaussian distributionã€‚å› æ­¤å‡è¨­æˆ‘å€‘æœ‰ \\(k-1\\) æ™‚é–“é»çš„ state ä¼°è¨ˆï¼Œæ‰€ä»¥æˆ‘å€‘çŸ¥é“ \\(x_{k}\\sim\\mathcal{N}(\\hat{x}_k,P_k)\\) æœƒè®Šæˆå¦‚ä¸‹çš„ä¸€å€‹ Gaussian: $$\\begin{align} \\hat{x}_{k}=F_k\\hat{x}_{k-1} \\\\ P_k = F_kP_{k-1}F_k^T+Q_k \\end{align}$$ å¼(3)and(4)å³ç‚º Prediction Stageã€‚åˆå› ç‚ºæˆ‘å€‘çŸ¥é“ observation è·Ÿ state ä¹‹é–“çš„é—œä¿‚ç‚ºé€é \\(H_k\\) è½‰æ›ï¼Œåœ¨å®Œå…¨æ²’æœ‰ sensor noise æƒ…æ³ä¸‹ï¼Œæ‰€ä»¥å¯ä»¥å¾—çŸ¥ prediction çš„è§€å¯Ÿå€¼ç‚º: $$\\begin{align} z_{expected}\\sim\\mathcal{N}(\\mu_{expected},\\Sigma_{expected}) \\\\ \\mathcal{N}\\left( \\begin{array}{c} \\vec{\\mu}_{expected}=H_k\\hat{x}_{k}, &amp; \\Sigma_{expected} = H_kP_kH_k^T \\end{array} \\right) \\end{align}$$ Update Stageæˆ‘å€‘ä»¤å¯¦éš›ä¸Šçš„è§€å¯Ÿå€¼ç‚º \\(z_k\\sim\\mathcal{N}(\\vec{z}_k,R_k)\\)ï¼Œå°‡è§€å¯Ÿå€¼çš„ Gaussian å’Œ predict çš„ Gaussian ç•«å‡ºå¦‚ä¸‹: è€Œå°‡å…©å€‹ Gaussian pdfs ç›¸ä¹˜çš„è©±:$$\\begin{align} \\mathcal{N}(x,\\mu_0,\\Sigma_0)\\cdot\\mathcal{N}(x,\\mu_1,\\Sigma_1)=\\mathcal{N}(x,\\mu&apos;,\\Sigma&apos;) \\\\ \\end{align}$$ä»ç„¶æœƒå¾—åˆ°å¦ä¸€å€‹ Gaussian:$$\\begin{align} K=\\Sigma_0(\\Sigma_0+\\Sigma_1)^{-1} \\\\ \\mu&apos;=\\mu_0+K(\\mu_1-\\mu_0) \\\\ \\Sigma&apos;=\\Sigma_0-K\\Sigma_0 \\end{align}$$ \\(K\\) ç¨±ç‚º Kalman Gainã€‚ç”±å¼(10)å¯çŸ¥ï¼Œupdate å¾Œçš„ covariance matrix æœƒæ„ˆä¾†æ„ˆå°ï¼Œè¡¨ç¤ºæˆ‘å€‘å°æ–¼ prediction çš„è§€å¯Ÿå€¼æœƒæ„ˆä¾†æ„ˆç¢ºå®šã€‚å¦å¤–ç”±(9)å¯çŸ¥ï¼ŒKalman Gain æ§åˆ¶è‘—è¦ç›¸ä¿¡å“ªé‚Šå¤šä¸€é»ã€‚æŠŠä¼°æ¸¬çš„è§€å¯Ÿå€¼ pdf å’Œå¯¦éš›è§€å¯Ÿå€¼çš„ pdfï¼Œå³ \\(z_k\\sim\\mathcal{N}(\\vec{z}_k,R_k)\\) å’Œå¼(5)å…©å€‹ pdfs ä»£å…¥åˆ°å¼ (8)~(10) å¾—åˆ°å¦‚ä¸‹: $$\\begin{align} H_k\\hat{x}_k&apos;=H_k\\hat{x}_k+K(\\vec{z}_k-H_k\\hat{x}_k) \\\\ H_kP_k&apos;H_k^T=H_kP_kH_k^T-KH_kP_kH_k^T \\\\ K=H_kP_kH_k^T(H_kP_kH_k^T+R_k)^{-1} \\end{align}$$ æŠŠ (11)~(13) é–‹é ­çš„ \\(H_k\\) å»æ‰ï¼Œä¸¦ä¸”æŠŠ (12) and (13) çµå°¾çš„ \\(H_k^T\\) å»æ‰è®Šæˆ$$\\begin{align} \\hat{x}_k&apos;=\\hat{x}_k+K(\\vec{z}_k-H_k\\hat{x}_k) \\\\ P_k&apos;=P_k-KH_kP_k \\\\ K=P_kH_k^T(H_kP_kH_k^T+R_k)^{-1} \\end{align}$$ (14)~(16)å°±æ˜¯ KF çš„ Update Stage! æ–°çš„ states ä¼°è¨ˆå€¼å°±è¢«æˆ‘å€‘å¾—åˆ°ï¼Œç„¶å¾Œé€™å€‹å€¼å°±å¯ä»¥è¢«ç•¶æˆä¸‹ä¸€æ¬¡ loop çš„åˆå§‹å€¼ã€‚ KF Flowæ“·å–ç¶²ç«™ä¸Šçš„åœ–ç‰‡: Lidar/Radar çš„ä¸€äº›è¨­å®šstate å®šç¾©ç‚º \\(x=(p_x,p_y,v_x,v_y)\\) åˆ†åˆ¥æ˜¯ (x çš„ä½ç½®, y çš„ä½ç½®, x çš„é€Ÿåº¦, y çš„é€Ÿåº¦)ã€‚ \\(F_k\\) æœƒæ ¹æ“šå…©æ¬¡ sensor data ä¹‹é–“çš„æ™‚é–“é–“éš” \\(\\vartriangle t\\) ä¾†è¡¨ç¤º: å¦å¤–æˆ‘å€‘å°‡ åŠ é€Ÿåº¦è€ƒæ…®ç‚ºä¸€å€‹ mean = 0, covariance matrix = Q çš„ä¸€å€‹ random noise çš„è©±ï¼Œå¼ (1) and (4) å¿…é ˆåšä¿®æ”¹ã€‚å…¶ä¸­ \\(Q_v\\) ä½¿ç”¨è€…è‡ªå·±è¨­å®šèª¿æ•´ï¼Œæ‰€ä»¥ state noise çš„ covariance matrix ç‚º Lidar åªæœƒè§€å¯Ÿåˆ°ä½ç½®ï¼Œå› æ­¤ Lidar çš„ \\(H\\) ç‚º:$$H_{lidar}= \\left( \\begin{array}{clr} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\end{array} \\right)$$ Radar å°±æ¯”è¼ƒç‰¹åˆ¥äº†ï¼Œå®ƒè§€å¯Ÿåˆ°çš„æ˜¯ä»¥ polar coordinate ä¾†è¡¨ç¤ºã€‚æ‰€ä»¥å®ƒçš„ states å’Œ observation ä¹‹é–“çš„é—œä¿‚ç„¡æ³•ç”¨ä¸€å€‹ matrix \\(H\\) ä¾†ä»£è¡¨ï¼Œæ˜¯å¦‚ä¸‹çš„ non-linear å¼å­:$$\\begin{align} h(x)= \\left( \\begin{array}{clr} \\rho \\\\ \\phi \\\\ \\dot{\\rho} \\end{array} \\right) = \\left( \\begin{array}{clr} \\sqrt{p_x^2+p_y^2} \\\\ \\arctan(p_y/p_x) \\\\ \\frac{p_xv_x+p_yv_y}{\\sqrt{p_x^2+p_y^2}} \\end{array} \\right) \\end{align}$$ ç‚ºäº†è®“å®ƒç¬¦åˆ state-space model çš„ç·šæ€§å¼å­ï¼Œåªå¥½ä½¿ç”¨ Taylor å±•é–‹å¼ï¼Œåªä½¿ç”¨ Jaccobian matrix é‡å° \\(h\\) å»å±•é–‹ï¼Œè€Œé€™å€‹å°±æ˜¯ Extended KFã€‚ EKFç¨å¾®æ”¹å¯«ä¸€ä¸‹ Update Stage:$$\\begin{align} y=(\\vec{z}_k-H_k\\hat{x}_k) \\\\ S=H_kP_kH_k^T+R_k \\\\ \\hat{x}_k&apos;=\\hat{x}_k+Ky \\\\ P_k&apos;=P_k-KH_kP_k \\\\ K=P_kH_k^TS^{-1} \\end{align}$$åœ¨ EKF ä¸­ï¼Œç”±æ–¼æˆ‘å€‘ä½¿ç”¨ Taylor å±•é–‹å¼å»é€¼è¿‘ \\(h\\)ï¼Œå› æ­¤ä¸Šè¿°çš„ \\(H_k\\) å¿…é ˆä½¿ç”¨å¦‚ä¸‹å¼å­è¨ˆç®—:ä½†æ˜¯ï¼Œé€™é‚Šé‚„æœ‰ä¸€å€‹ tricky çš„åœ°æ–¹! å°±æ˜¯ å¼(18)ç›´æ¥ä½¿ç”¨å¼(17) \\(h\\) çš„ non-linear function è¨ˆç®—!å›æƒ³ä¸€ä¸‹æˆ‘å€‘å°‡ \\(h\\) åš linearlization çš„ç›®çš„: å°±æ˜¯å¼(5),(6)ä¸‹çš„é‚£å¼µåœ–çš„è½‰æ›ã€‚å¦‚æœ Gaussian pdf ç¶“é nonlinear è½‰æ›å¾Œæœƒè®Šæˆ â€œéGaussianâ€ï¼Œå› æ­¤åªå¥½åšç·šæ€§é€¼è¿‘ã€‚æ—¢ç„¶ç·šæ€§è½‰æ›çš„ pdf éƒ½å·²ç¶“æ˜¯é€¼è¿‘äº†ï¼Œä¸å¦‚å°±å°‡ mean ä½¿ç”¨æœ€ç²¾ç¢ºçš„å€¼ï¼Œå› æ­¤ \\(y\\) å°±ç›´æ¥ä½¿ç”¨å¼(17)è¨ˆç®—ã€‚æ‰€ä»¥å¼(18)è¦æ”¹æˆ:$$\\begin{align} y=(\\vec{z}_k-h(\\hat{x}_k)) \\end{align}$$ ç¸½çµ Lidar and Radar Fusion [Predict]$$\\hat{x}_{k}=F_k\\hat{x}_{k-1} \\\\ P_k = F_kP_{k-1}F_k^T+Q$$ where [Lidar Update]$$y=(\\vec{z}_k-H_{lidar}\\hat{x}_k) \\\\ S=H_{lidar}P_kH_{lidar}^T+R_k \\\\ \\hat{x}_k&apos;=\\hat{x}_k+Ky \\\\ P_k&apos;=P_k-KH_{lidar}P_k \\\\ K=P_kH_{lidar}^TS^{-1}$$ where \\(R_k\\) sensor noise covariance matrix ç”±å» å•†æä¾›, and $$H_{lidar}= \\left( \\begin{array}{clr} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\end{array} \\right)$$ [Radar Update]$$y=(\\vec{z}_k-h(\\hat{x}_k)) \\\\ S=H_kP_kH_k^T+R_k \\\\ \\hat{x}_k&apos;=\\hat{x}_k+Ky \\\\ P_k&apos;=P_k-KH_kP_k \\\\ K=P_kH_k^TS^{-1}$$ where \\(R_k\\) sensor noise covariance matrix ç”±å» å•†æä¾›, and $$h(x)= \\left( \\begin{array}{clr} \\rho \\\\ \\phi \\\\ \\dot{\\rho} \\end{array} \\right) = \\left( \\begin{array}{clr} \\sqrt{p_x^2+p_y^2} \\\\ \\arctan(p_y/p_x) \\\\ \\frac{p_xv_x+p_yv_y}{\\sqrt{p_x^2+p_y^2}} \\end{array} \\right)$$ Reference How a Kalman filter works, in pictures Udacity Term2 Lecture","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"Kalman Filter","slug":"Kalman-Filter","permalink":"http://yoursite.com/tags/Kalman-Filter/"},{"name":"Extended Kalman Filter","slug":"Extended-Kalman-Filter","permalink":"http://yoursite.com/tags/Extended-Kalman-Filter/"}]},{"title":"WGAN Part 2: ä¸»è§’ W ç™»å ´","date":"2017-03-17T13:25:12.000Z","path":"2017/03/17/WGAN-Part-2/","text":"å‰æƒ…æè¦GAN ä½œè€…è¨­è¨ˆå‡ºä¸€å€‹ Minimax gameï¼Œè®“å…©å€‹ players: ç”Ÿæˆå™¨ G å’Œ é‘‘åˆ¥å™¨ D å»å½¼æ­¤ç«¶çˆ­ï¼Œä¸¦ä¸”é”åˆ°å¹³è¡¡é»æ™‚ï¼Œæ­¤å•é¡Œé”åˆ°æœ€ä½³è§£ä¸”ç”Ÿæˆå™¨ G éŠæˆã€‚å¤§è‡´ä¸Šè¨“ç·´æµç¨‹ç‚ºå…ˆ optimize é‘‘åˆ¥å™¨ D for some iterationsï¼Œç„¶å¾Œæ› optimize ç”Ÿæˆå™¨ G (åœ¨ optimize G æ™‚ï¼Œæ­¤å•é¡Œç­‰åƒ¹æ–¼æœ€ä½³åŒ– JSD è·é›¢)ï¼Œé‡è¤‡ä¸Šè¿° loop ç›´åˆ°é”åˆ°æœ€ä½³è§£ã€‚ä½†æ˜¯ä»”ç´°çœ‹çœ‹åŸä¾†çš„æœ€ä½³åŒ–å•é¡Œä¹‹è¨­è¨ˆï¼Œæˆ‘å€‘çŸ¥é“åœ¨æœ€ä½³åŒ– G çš„æ™‚å€™ï¼Œç­‰åƒ¹æ–¼æœ€ä½³åŒ–ä¸€å€‹ JSD è·é›¢ï¼Œè€Œ JSD åœ¨é‡åˆ°çœŸå¯¦è³‡æ–™çš„æ™‚æœƒå¾ˆæ‚²åŠ‡ã€‚æ€éº¼æ‚²åŠ‡å‘¢? åŸå› æ˜¯çœŸå¯¦è³‡æ–™éƒ½å­˜åœ¨ local manifold ä¸­ï¼Œé€ æˆ training data çš„ p.d.f. å’Œ ç”Ÿæˆå™¨çš„ p.d.f. å½¼æ­¤ä¹‹é–“ç„¡äº¤é›† (æˆ–äº¤é›†çš„æ¸¬åº¦ç‚º0)ï¼Œåœ¨é€™ç¨®ç‹€æ³ JSD = log2 (constant) almost every whereã€‚ä¹Ÿå› æ­¤é€ æˆ gradients = 0ã€‚é€™æ˜¯ GAN å¾ˆé›£è¨“ç·´çš„ä¸€å€‹ä¸»å› ã€‚ ä¹Ÿå› æ­¤ WGAN çš„ä¸»è¦æ²»æœ¬æ–¹å¼å°±æ˜¯æ›æ‰ JSDï¼Œæ”¹ç”¨ Wasserstein (Earth-Mover) distanceï¼Œè€Œä¿®æ”¹éå¾Œçš„æ¼”ç®—æ³•ä¹Ÿæ˜¯ç°¡å–®å¾—é©šäºº! Wasserstein (Earth-Mover) distanceæˆ‘å€‘å…ˆçµ¦å®šç¾©å¾Œï¼Œå†ç”¨ä½œè€…è«–æ–‡ä¸Šçš„ç¯„ä¾‹è§£é‡‹å®šç¾©å¦‚ä¸‹:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_g)=\\inf_{\\gamma\\in\\prod(\\mathbb{P}_r,\\mathbb{P}_g)}E_{(x,y)\\sim \\gamma}[\\Vert x-y \\Vert] \\end{align}$$\\(\\gamma\\)æŒ‡çš„æ˜¯ real data and fake data çš„ joint distributionï¼Œå…¶ä¸­ marginal ç‚ºå„è‡ªå…©å€‹ distributionsã€‚å…ˆåˆ¥è¢«é€™äº›ç¬¦è™Ÿåš‡åˆ°ï¼Œç›´è§€çš„è§£é‡‹ç‚º: EM è·é›¢å¯ä»¥ç†è§£ç‚ºå°‡æŸå€‹æ©Ÿç‡åˆ†ä½ˆæ¬åˆ°å¦ä¸€å€‹æ©Ÿç‡åˆ†ä½ˆï¼Œæ‰€è¦èŠ±çš„æœ€å°åŠ›æ°£ã€‚ æˆ‘å€‘ç”¨ä¸‹é¢é€™å€‹ä¾‹å­æ˜ç¢ºèˆ‰ä¾‹ï¼Œå‡è¨­æˆ‘å€‘æœ‰å…©å€‹æ©Ÿç‡åˆ†ä½ˆ f1 and f2:$$\\begin{align*} f_1(a)=f_1(b)=f_1(c)=1/3 \\\\\\\\ f_1(A)=f_1(B)=f_1(C)=1/3 \\end{align*}$$é€™å…©å€‹æ©Ÿç‡åˆ†ä½ˆåœ¨ä¸€å€‹ 2 ç¶­å¹³é¢ï¼Œå¦‚ä¸‹:è€Œå…©å€‹ \\(\\gamma\\) å°æ‡‰åˆ°å…©ç¨® æ¬é‹é…å°æ³•$$\\begin{align*} \\gamma_1(a,A)=\\gamma_1(b,B)=\\gamma_1(c,C)=1/3 \\\\\\\\ \\gamma_2(a,B)=\\gamma_2(b,C)=\\gamma_2(c,A)=1/3 \\end{align*}$$å¯ä»¥å¾ˆå®¹æ˜“çŸ¥é“å®ƒå€‘çš„ marginal distributions æ­£å¥½ç¬¦åˆ f1 and f2 çš„æ©Ÿç‡åˆ†ä½ˆã€‚å‰‡é€™å…©ç¨®æ¬é‹æ³•é€ æˆçš„ EM distance åˆ†åˆ¥å¦‚ä¸‹:$$\\begin{align*} EM_{\\gamma_1}=\\gamma_1(a,A)*\\Vert a-A \\Vert + \\gamma_1(b,B)*\\Vert b-B \\Vert + \\gamma_1(c,C)*\\Vert c-C \\Vert \\\\\\\\ EM_{\\gamma_2}=\\gamma_2(a,B)*\\Vert a-B \\Vert + \\gamma_2(b,C)*\\Vert b-C \\Vert + \\gamma_2(c,A)*\\Vert c-A \\Vert \\end{align*}$$æ˜é¡¯çŸ¥é“ $\\theta=EM_{\\gamma_1}&lt;EM_{\\gamma_2}$è€Œ EM distance å°±æ˜¯åœ¨ç®—æ‰€æœ‰æ¬é‹æ³•ä¸­ï¼Œæœ€å°çš„é‚£å€‹ï¼Œä¸¦å°‡é‚£æœ€å°çš„ cost å®šç¾©ç‚ºæ­¤å…©æ©Ÿç‡åˆ†ä½ˆçš„è·é›¢ã€‚é€™å€‹è·é›¢å¦‚æœæ˜¯å…©æ¢å¹³è¡Œ 1 ç¶­çš„ç›´ç·š pdf (ä¸Šé¢çš„ä¾‹å­æ˜¯ç›´ç·šä¸Šåªæœ‰ä¸‰å€‹é›¢æ•£è³‡æ–™é»)ï¼Œæœƒæœ‰å¦‚ä¸‹çš„ cost: å°æ¯”æ­¤åœ–å’Œä¸Šä¸€ç¯‡çš„ JSD çš„çµæœï¼ŒEM èƒ½å¤ æ­£ç¢ºä¼°ç®—å…©å€‹æ²’æœ‰äº¤é›†çš„æ©Ÿç‡åˆ†ä½ˆçš„è·é›¢ï¼Œç›´æ¥çš„çµæœå°±æ˜¯ gradient é€£çºŒä¸”å¯å¾® ! ä½¿å¾— WGAN è¨“ç·´ä¸Šç©©å®šéå¸¸å¤šã€‚ ä¸€å€‹é—œéµçš„å¥½æ€§è³ª: Wasserstein (Earth-Mover) distance è™•è™•é€£çºŒå¯å¾®åŸå§‹ EM distance çš„å®šç¾© (å¼(1)) æ˜¯ intractableä¸€å€‹ç¥å¥‡çš„æ•¸å­¸å…¬å¼ (Kantorovich-Rubinstein duality) å°‡ EM distance è½‰æ›å¦‚ä¸‹:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_\\theta)=\\sup_{\\Vert f \\Vert _L \\leq 1}{ E_{x \\sim \\mathbb{P}_r}[f(x)] - E_{x \\sim \\mathbb{P}_\\theta}[f(x)] } \\end{align}$$æ³¨æ„åˆ° sup æ˜¯é‡å°æ‰€æœ‰æ»¿è¶³ 1-Lipschitz çš„ functions fï¼Œå¦‚æœæ”¹æˆæ»¿è¶³ K-Lipschitz çš„ functionsï¼Œå‰‡å€¼æœƒç›¸å·®ä¸€å€‹ scale Kã€‚ä½†æ˜¯åœ¨å¯¦ä½œä¸Šæˆ‘å€‘éƒ½ä½¿ç”¨ä¸€å€‹ family of functionsï¼Œä¾‹å¦‚ä½¿ç”¨æ‰€æœ‰äºŒæ¬¡å¼çš„ functionsï¼Œæˆ–æ˜¯ Mixture of Gaussiansï¼Œç­‰ç­‰ã€‚è€Œç¶“éè¿‘å¹¾å¹´æ·±åº¦å­¸ç¿’çš„ç™¼å±•å¾Œï¼Œæˆ‘å€‘å¯ä»¥ç›¸ä¿¡ï¼Œä½¿ç”¨ DNN ç•¶ä½œ family of functions æ˜¯å¾ˆæ´½ç•¶çš„é¸æ“‡ï¼Œå› æ­¤å‡å®šæˆ‘å€‘çš„ NN æ‰€æœ‰åƒæ•¸ç‚º \\(W\\)ï¼Œå‰‡ä¸Šå¼å¯ä»¥è¡¨é”æˆ:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\approx\\max_{w\\in W}{ E_{x \\sim \\mathbb{P}_r}[f_w(x)] - E_{z \\sim p(z)}[f_w(g_{\\theta}(z))] } \\end{align}$$é€™è£¡ä¸å†æ˜¯ç­‰å¼ï¼Œè€Œæ˜¯é€¼è¿‘ï¼Œä¸é Deep Learning å„ªç•°çš„ Regression èƒ½åŠ›æ˜¯å¯ä»¥å¾ˆå¥½åœ°é€¼è¿‘çš„ã€‚ æˆ‘å€‘é‚„æ˜¯éœ€è¦ä¿è­‰æ•´å€‹ EM distance ä¿æŒè™•è™•é€£çºŒå¯å¾®åˆ†ï¼Œé€™æ¨£å¯ä»¥ç¢ºä¿æˆ‘å€‘åš gradient-based æœ€ä½³åŒ–å¯ä»¥é †åˆ©ï¼Œé‡å°é€™é»ï¼ŒWGAN ä½œè€…å¾ˆå¼·å¤§åœ°è­‰æ˜å®Œäº†ï¼Œå¾—åˆ°çµè«–å¦‚ä¸‹: é‡å°ç”Ÿæˆå™¨ \\(g_\\theta\\)ä»»ä½• feed-forward NN çš†å¯ é‡å°é‘‘åˆ¥å™¨ \\(f_w\\)ç•¶ \\(W\\) æ˜¯ compact set æ™‚ï¼Œè©² family of functions \\(\\{f_w\\}\\) æ»¿è¶³ K-Lipschitz for some Kã€‚å…·é«”å¯¦ç¾å¾ˆå®¹æ˜“ï¼Œå› ç‚ºåœ¨ \\(R^d\\) spaceï¼Œcompact set ç­‰åƒ¹æ–¼ closed and boundedï¼Œå› æ­¤åªéœ€è¦é‡å°æ‰€æœ‰çš„åƒæ•¸å– bounding boxå³å¯!è«–æ–‡è£¡ä½¿ç”¨äº† [-0.01,0.01] é€™å€‹ç¯„åœåš clippingã€‚ èˆ‡ GAN ç¬¬ä¸€å€‹ä¸åŒé»ç‚º: é‘‘åˆ¥å™¨åƒæ•¸å– clippingã€‚ EM distance ç‚ºç›®æ¨™å‡½å¼æ‰€é€ æˆçš„ä¸åŒæˆ‘å€‘å°‡å…©è€…çš„ç›®æ¨™å‡½å¼åˆ—å‡ºä¾†åšå€‹æ¯”è¼ƒ$$\\begin{align} GAN: E_{x \\sim \\mathbb{P}_r} [\\log f_w(x)] + E_{z \\sim p(z)}[\\log (1-f_w(g_{\\theta}(z)))] \\\\ WGAN: E_{x \\sim \\mathbb{P}_r}[f_w(x)] - E_{z \\sim p(z)}[f_w(g_{\\theta}(z))] \\end{align}$$ç™¼ç¾åˆ° WGAN ä¸å– logï¼ŒåŒæ™‚å°ç”Ÿæˆå™¨çš„ç›®æ¨™å‡½å¼ä¹Ÿåšäº†ä¿®æ”¹ èˆ‡ GAN ç¬¬äºŒå€‹ä¸åŒé»ç‚º: WGAN çš„ç›®æ¨™å‡½å¼ä¸å– logï¼ŒåŒæ™‚å°ç”Ÿæˆå™¨çš„ç›®æ¨™å‡½å¼ä¹Ÿåšäº†ä¿®æ”¹ã€‚ ç¬¬ä¸‰å€‹ä¸åŒé»æ˜¯ä½œè€…å¯¦é©—çš„ç™¼ç¾ èˆ‡ GAN ç¬¬ä¸‰å€‹ä¸åŒé»ç‚º: ä½¿ç”¨ Momentum é¡çš„æ¼”ç®—æ³•ï¼Œå¦‚ Adamï¼Œæœƒä¸ç©©å®šï¼Œå› æ­¤ä½¿ç”¨ SGD or RMSPropã€‚ WGAN æ¼”ç®—æ³•ç¸½çµä¸€ä¸‹èˆ‡ GAN çš„ä¿®æ”¹è™• A. é‘‘åˆ¥å™¨åƒæ•¸å– clippingã€‚B. WGAN çš„ç›®æ¨™å‡½å¼ä¸å– logï¼ŒåŒæ™‚å°ç”Ÿæˆå™¨çš„ç›®æ¨™å‡½å¼ä¹Ÿåšäº†ä¿®æ”¹ã€‚C. ä½¿ç”¨ SGD or RMSPropã€‚ WGAN çš„å„ªé»ä¸€: ç›®æ¨™å‡½å¼èˆ‡è¨“ç·´å“è³ªé«˜åº¦ç›¸é—œåŸå§‹çš„ GAN æ²’æœ‰é€™æ¨£çš„è©•é‡æŒ‡æ¨™ï¼Œå› æ­¤æœƒåœ¨è¨“ç·´ä¸­é€”ç”¨äººçœ¼å»æª¢æŸ¥è¨“ç·´æ˜¯å¦æ•´å€‹å£æ‰äº†ã€‚ WGAN è§£æ±ºäº†é€™å€‹éº»ç…©ã€‚ä½œè€…çš„ç¯„ä¾‹å¦‚ä¸‹ï¼Œå¯ä»¥ç™¼ç¾WGANçš„ç›®æ¨™å‡½å¼ Loss æ„ˆä½ï¼Œsamplingå‡ºä¾†çš„å“è³ªæ„ˆé«˜ã€‚ äºŒ: é‘‘åˆ¥å™¨å¯ä»¥ç›´æ¥è¨“ç·´åˆ°æœ€å¥½åŸå§‹çš„ GAN éœ€è¦å°å¿ƒè¨“ç·´ï¼Œä¸èƒ½ä¸€ä¸‹å­æŠŠé‘‘åˆ¥å™¨è¨“ç·´å¤ªå¼·å°è‡´å°å‡½æ•¸å£æ‰ ä¸‰: ä¸éœ€è¦ç‰¹åˆ¥è¨­è¨ˆ NN çš„æ¶æ§‹GNN ä½¿ç”¨ MLP (Fully connected layers) é›£ä»¥è¨“ç·´ï¼Œè¼ƒæˆåŠŸçš„éƒ½æ˜¯ CNN æ¶æ§‹ï¼Œä¸¦æ­é… batch normalizationã€‚è€Œåœ¨ WGAN æ¼”ç®—æ³•ä¸‹ï¼Œ MLPæ¶æ§‹å¯èƒ½ç©©å®šè¨“ç·´ (é›–ç„¶å“è³ªæœ‰ä¸‹é™) å››: æ²’æœ‰ collapse mode (ä¿æŒç”Ÿæˆå¤šæ¨£æ€§)ä½œè€…è‡ªå·±èªªåœ¨å¤šæ¬¡å¯¦é©—çš„éç¨‹éƒ½æ²’æœ‰ç™¼ç¾é€™ç¨®ç¾è±¡ My Questions åŸå…ˆ GAN æœƒæœ‰ collapse mode çœ‹åˆ°æœ‰äººè¨è«–æ˜¯å› ç‚º KL divergence ä¸å°ç¨±çš„é—œä¿‚å°è‡´å°æ–¼ â€œç”Ÿæˆå™¨ç”Ÿå‡ºéŒ¯èª¤çš„ sampleâ€ æ¯” â€œç”Ÿæˆå™¨æ²’ç”Ÿå‡ºæ‰€æœ‰è©²å°çš„sampleâ€ é€ç½°è¦å¤§å¾ˆå¤šï¼Œä¸éé€™é‚Šè‡ªå·±é‚„æ˜¯æœ‰ç–‘å•ï¼Œå› ç‚º JSD å·²ç¶“æ˜¯å°ç¨±çš„ KL äº†ï¼Œé‚„æœƒæœ‰é€ç½°ä¸åŒå°è‡´ collapse mode çš„å•é¡Œå—? éœ€è¦å†å¤šçœ‹ä¸€ä¸‹ paper äº†è§£ã€‚ å¦‚ä½•æ§åˆ¶ sample å‡ºä¾†çš„ outputï¼Œè­¬å¦‚ mnist è¦ sampling å‡ºæŸå€‹ classã€‚å‰ææ˜¯å¸Œæœ›ä¸èƒ½å° data æœ‰ä»»ä½•æ¨™è¨˜éï¼Œä¸ç„¶å°±æ²’æœ‰ unsupervised çš„æ¢ä»¶äº†ã€‚ Conditional GAN? æœ‰ç©ºå†ç ”ç©¶ä¸€ä¸‹é€™å€‹èª²é¡Œ Tensorflow ç¯„ä¾‹æ¸¬è©¦ä¸»è¦åƒè€ƒæ­¤ githubï¼Œç”¨è‡ªå·±çš„å¯«æ³•å¯«ä¸€æ¬¡ï¼Œä¸¦åšäº›æ¸¬è©¦ ç”¨ MNIST dataset åšæ¸¬è©¦ï¼ŒåŸå§‹ input ç‚º 28x28ï¼Œå°‡å®ƒ padding æˆ 32x32ï¼Œå› æ­¤ input domain ç‚º 32x32x1 ç”Ÿæˆå™¨å¹¾å€‹é‡é»ï¼Œç¬¬ä¸€å€‹æ˜¯ç”Ÿæˆå™¨ç”¨çš„æ˜¯ conv2d_transpose (doc)ï¼Œé€™æ˜¯ç”±æ–¼åŸå…ˆçš„ conv2d ç„¡æ³•å°‡ image çš„ size è®Šå¤§ï¼Œé ‚å¤šä¸€æ¨£ã€‚å› æ­¤è¦ç”¨ conv2d_transposeï¼Œä»¥ ç¬¬ 15 è¡Œèˆ‰ä¾‹ã€‚argument wc2 çš„ shape ç‚º [3, 3, 256, 512] åˆ†åˆ¥è¡¨ç¤º [filter_h, filter_w, output_depth, input_depth]ã€‚argument [batch_size, 8, 8, 256] è¡¨ç¤º output layer çš„ shapeã€‚å¾Œé¢å…©å€‹ argument å°±å¾ˆæ˜é¡¯äº†ï¼Œåˆ†åˆ¥æ˜¯ strides [batch_stride, h_stride, w_stride, channel_stride] å’Œ paddingã€‚ç¬¬äºŒå€‹é‡é»æ˜¯æœ€å¾Œä¸€å±¤ out_sample = tf.nn.tanh(conv5)ï¼Œç”±æ–¼æˆ‘å€‘æœƒå°‡ data å…ˆ normalize åˆ° [-1,1]ï¼Œå› æ­¤ä½¿ç”¨ tanh è®“ domain ä¸€è‡´ã€‚ 123456789101112131415161718192021222324252627282930313233z_dim = 128def generator_net(z): with tf.variable_scope('generator'): # Layer 1 - 128 to 4*4*512 wd1 = tf.get_variable(\"wd1\",[z_dim, 4*4*512]) bd1 = tf.get_variable(\"bd1\",[4*4*512]) dense1 = tf.add(tf.matmul(z, wd1), bd1) dense1 = tf.nn.relu(dense1) # reshape to 4*4*512 conv1 = tf.reshape(dense1, (batch_size, 4, 4, 512)) # Layer 2 - 4*4*512 to 8*8*256 wc2 = tf.get_variable(\"wc2\",[3, 3, 256, 512]) conv2 = tf.nn.conv2d_transpose(conv1, wc2, [batch_size, 8, 8, 256], [1,2,2,1], padding='SAME') conv2 = tf.nn.relu(conv2) # Layer 3 - 8*8*256 to 16*16*128 wc3 = tf.get_variable(\"wc3\",[3, 3, 128, 256]) conv3 = tf.nn.conv2d_transpose(conv2, wc3, [batch_size, 16, 16, 128], [1,2,2,1], padding='SAME') conv3 = tf.nn.relu(conv3) # Layer 4 - 16*16*128 to 32*32*64 wc4 = tf.get_variable(\"wc4\",[3, 3, 64, 128]) conv4 = tf.nn.conv2d_transpose(conv3, wc4, [batch_size, 32, 32, 64], [1,2,2,1], padding='SAME') conv4 = tf.nn.relu(conv4) # Layer 5 - 32*32*64 to 32*32*1 wc5 = tf.get_variable(\"wc5\",[3, 3, 1, 64]) conv5 = tf.nn.conv2d_transpose(conv4, wc5, [batch_size, 32, 32, 1], [1,1,1,1], padding='SAME') out_sample = tf.nn.tanh(conv5) return out_sample é‘‘åˆ¥å™¨é€™å€‹å°±æ˜¯æœ€ä¸€èˆ¬çš„ CNNï¼Œoutput æœ€å¾Œæ˜¯ä¸€å€‹æ²’æœ‰é log çš„ scaler ä¸”ä¹Ÿæ²’æœ‰ç¶“é activation functionã€‚æ¯”è¼ƒé‡è¦çš„æ˜¯è®Šæ•¸éƒ½æ˜¯ä½¿ç”¨ get_variable å’Œ scope.reuse_variables() (è«‹åƒè€ƒ Sharing Variables)ã€‚å…·é«”çš„åŸå› æ˜¯å› ç‚ºæˆ‘å€‘æœƒå° real data å‘¼å«ä¸€æ¬¡é‘‘åˆ¥å™¨ï¼Œè€Œå°æ–¼ fake data ä¹Ÿæœƒåœ¨å‘¼å«ä¸€æ¬¡ã€‚è‹¥æ²’æœ‰ share variablesï¼Œå°±æœƒå°è‡´ç”¢ç”Ÿå…©çµ„å„è‡ªçš„ weightsã€‚tf.get_variable() è·Ÿ tf.Variable() å·®åˆ¥åœ¨æ–¼å¦‚æœå·²ç¶“æœ‰åç¨±ä¸€æ¨£çš„è®Šæ•¸æ™‚ get_variable() ä¸æœƒå†ç”¢ç”Ÿå¦ä¸€å€‹è®Šæ•¸ï¼Œè€Œæœƒ shareï¼Œä½†æ˜¯è¦çœŸçš„ share é‚„å¿…é ˆå¤šä¸€å€‹å‹•ä½œ reuse_variables ç¢ºä¿ä¸æ˜¯ä¸å°å¿ƒ share åˆ°çš„ã€‚ 12345678910111213141516171819202122232425262728293031323334353637# Construct CriticNetdef conv2d(x, W, b, strides=1): x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME') x = tf.nn.bias_add(x, b) return tf.nn.relu(x)def critic_net(x, reuse=False): with tf.variable_scope('critic') as scope: size = 64 if reuse: scope.reuse_variables() # Layer 1 - 32*32*1 to 16*16*size wc1 = tf.get_variable(\"wc1\",[3, 3, 1, size]) bc1 = tf.get_variable(\"bc1\",[size]) conv1 = conv2d(x, wc1, bc1, strides=2) # Layer 2 - 16*16*size to 8*8*size*2 wc2 = tf.get_variable(\"wc2\",[3, 3, size, size*2]) bc2 = tf.get_variable(\"bc2\",[size*2]) conv2 = conv2d(conv1, wc2, bc2, strides=2) # Layer 3 - 8*8*size*2 to 4*4*size*4 wc3 = tf.get_variable(\"wc3\",[3, 3, size*2, size*4]) bc3 = tf.get_variable(\"bc3\",[size*4]) conv3 = conv2d(conv2, wc3, bc3, strides=2) # Layer 4 - 4*4*size*4 to 2*2*size*8 wc4 = tf.get_variable(\"wc4\",[3, 3, size*4, size*8]) bc4 = tf.get_variable(\"bc4\",[size*8]) conv4 = conv2d(conv3, wc4, bc4, strides=2) # Fully connected layer - 2*2*size*8 to 1 wd5 = tf.get_variable(\"wd5\",[2*2*size*8, 1]) bd5 = tf.get_variable(\"bd5\",[1]) fc5 = tf.reshape(conv4, [-1, wd5.get_shape().as_list()[0]]) logit = tf.add(tf.matmul(fc5, wd5), bd5) return logit Graphé€™è£¡æœ‰å¹¾å€‹é‡é»ï¼Œç¬¬ä¸€å€‹æ˜¯ç”±æ–¼æˆ‘å€‘åœ¨æœ€ä½³åŒ–éç¨‹ä¸­ï¼Œæœƒ fix ä½ä¸€é‚Šçš„åƒæ•¸ï¼Œç„¶å¾Œæœ€ä½³åŒ–å¦ä¸€é‚Šï¼Œæ¥è‘—åéä¾†ã€‚æ­¤ä½œæ³•åƒè€ƒ linkç¬¬äºŒå€‹é‡é»æ˜¯ä½¿ç”¨ tf.clip_by_valueï¼Œå¯ä»¥çœ‹åˆ°æˆ‘å€‘å°æ–¼æ‰€æœ‰é€é tf.get_collection è’é›†åˆ°çš„è®Šæ•¸éƒ½å¢åŠ ä¸€å€‹ clip opã€‚ç¬¬ä¸‰å€‹é‡é»æ˜¯ä½¿ç”¨ tf.control_dependencies([opt_c]) linkï¼Œé€™å€‹å®šç¾©äº† op ä¹‹é–“çš„é—œè¯æ€§ï¼Œå®ƒæœƒç­‰åˆ° argument å…§åŸ·è¡Œå®Œç•¢å¾Œï¼Œæ‰æœƒæ¥è‘—åŸ·è¡Œä¸‹å»ã€‚æ‰€ä»¥æˆ‘å€‘å¯ä»¥ç¢ºä¿å…ˆåšå®Œ RMSPropOptimizer æ‰æ¥è‘—åš clip_by_valueã€‚å¦å¤– tf.tuple link æœƒç­‰æ‰€æœ‰çš„ input arguments éƒ½åšå®Œæ‰æœƒçœŸçš„ return å‡ºå»ï¼Œä»¥ç¢ºä¿æ¯å€‹ tensors éƒ½åšå®Œ clipping äº†ã€‚ 1234567891011121314151617181920212223242526272829# build graphdef build_graph(): z = tf.placeholder(tf.float32, shape=(batch_size, z_dim)) fake_data = generator_net(z) real_data = tf.placeholder(tf.float32, shape=(batch_size, 32, 32, 1)) # Define loss and optimizer real_logit = critic_net(real_data) fake_logit = critic_net(fake_data, reuse=True) c_loss = tf.reduce_mean(fake_logit - real_logit) g_loss = tf.reduce_mean(-fake_logit) # get the trainable variables list theta_g = tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator') theta_c = tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic') # freezing or only update designated variables opt_g = tf.train.RMSPropOptimizer(learning_rate=lr_generator).minimize(g_loss, var_list=theta_g) opt_c = tf.train.RMSPropOptimizer(learning_rate=lr_critic).minimize(c_loss, var_list=theta_c) # then pass those trainable variables to clip function clipped_var_c = [tf.assign(var, tf.clip_by_value(var, clip_lower, clip_upper)) for var in theta_c] # wait until RMSPropOptimizer is done with tf.control_dependencies([opt_c]): # fetch the clipped variables and output as op opt_c = tf.tuple(clipped_var_c) return opt_g, opt_c, z, real_data WGAN Algorithm Flowç…§ paper ä¸Šçš„æ¼”ç®—æ³• flow 123456789101112131415161718192021222324252627282930def wgan_train(): dataset = input_data.read_data_sets(\".\", one_hot=True) opt_g, opt_c, z, real_data = build_graph() saver = tf.train.Saver() config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True) config.gpu_options.allow_growth = True config.gpu_options.per_process_gpu_memory_fraction = 0.8 def next_feed_dict(): train_img = dataset.train.next_batch(batch_size)[0] train_img = 2*train_img-1 train_img = np.reshape(train_img, (-1, 28, 28)) npad = ((0, 0), (2, 2), (2, 2)) train_img = np.pad(train_img, pad_width=npad, mode='constant', constant_values=-1) train_img = np.expand_dims(train_img, -1) batch_z = np.random.normal(0, 1, [batch_size, z_dim]).astype(np.float32) feed_dict = &#123;real_data: train_img, z: batch_z&#125; return feed_dict with tf.Session(config=config) as sess: sess.run(tf.global_variables_initializer()) summary_writer = tf.summary.FileWriter(log_dir, sess.graph) for i in range(max_iter_step): print(\"itr = \",i) for j in range(c_iter): feed_dict = next_feed_dict() sess.run(opt_c, feed_dict=feed_dict) feed_dict = next_feed_dict() sess.run(opt_g, feed_dict=feed_dict) if i % 1000 == 999: saver.save(sess, os.path.join(ckpt_dir, \"model.ckpt\"), global_step=i) ä¸€é»å°çµè«–5.1. ä¸Šè¿°æ¶æ§‹æ²’æœ‰ç”¨ batch normalizationï¼Œæœ‰ç”¨çš„è©±æ•ˆæœæœƒå¥½å¾ˆå¤šï¼Œç”Ÿæˆå™¨å’Œé‘‘åˆ¥å™¨éƒ½å¯ç”¨ã€‚5.2. é‘‘åˆ¥å™¨æ›æˆå…¶ä»– CNN æ¶æ§‹ä¹Ÿå¯ä»¥ã€‚5.3. MLP æ¶æ§‹ä¹Ÿå¯ä»¥ã€‚ æ•´é«”ä¾†èªªï¼Œå°æ–¼ç†Ÿæ‚‰ tensorflow çš„äººä¾†èªªä¸é›£å¯¦ä½œ (å‰›å¥½æˆ‘ä¸æ˜¯å¾ˆç†Ÿ)ï¼Œå°¤å…¶ WGAN å¾æ ¹æœ¬ä¸Šåšçš„æ”¹é€²ï¼Œè®“æ•´å€‹ training å¾ˆå®¹æ˜“!è®“æˆ‘å€‘æœŸå¾…æ¥ä¸‹ä¾†çš„ç™¼å±•å§~ Reference GAN Wasserstein GANï¼Œä½œè€…çš„ github ä»¤äººæ‹æ¡ˆå«ç»çš„Wasserstein GAN A Tensorflow Implementation of WGAN: ä½¿ç”¨ tf.contrib.layersï¼Œä¸€å€‹ higher level çš„ APIï¼Œæ¯”æˆ‘ç¾åœ¨çš„å¯¦ä½œå¯ä»¥ç°¡æ½”å¾ˆå¤šã€‚ A GENTLE GUIDE TO USING BATCH NORMALIZATION IN TENSORFLOW: Batch Normalization, MLP, and CNN examples using tf.contrib.layers","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"generative model","slug":"generative-model","permalink":"http://yoursite.com/tags/generative-model/"}]},{"title":"WGAN Part 1: å…ˆç”¨ GAN é‹ªæ¢—","date":"2017-03-16T13:25:12.000Z","path":"2017/03/16/WGAN-Part-1/","text":"Open.ai é€™å¼µè¡¨é” generative modeling çš„æ„æ€å¾ˆæ¸…æ¥šï¼Œå¿ä¸ä½å°±å€Ÿç”¨äº†ã€‚ ç­†è€…æ‰ç–å­¸æ·ºï¼Œå¦‚æœ‰éŒ¯èª¤ï¼Œé‚„è«‹æŒ‡æ­£ Generative Adversarial Nets æå‡ºäº†ä¸€å€‹ NN çš„ generative modeling æ–¹æ³•ï¼Œåœ¨é€™ä¹‹å‰ï¼ŒNN è¦æˆç‚º p.d.f. å¿…é ˆä¾è³´æ–¼ sigmoid activation çš„ Restricted Boltzmann Machines (RBM) çµæ§‹ã€‚ä¾‹å¦‚ Deep Belief Netï¼Œæ•´å€‹ network æ‰æœƒæ˜¯ä¸€å€‹ p.d.f.ã€‚ç„¶è€Œå­¸ç¿’é€™æ¨£çš„ä¸€å€‹ p.d.f. å¿…é ˆä½¿ç”¨ Contrastive Divergence çš„ MCMC æ–¹æ³•ï¼Œ model è¨“ç·´å®Œå¾Œè¦ç”¢ç”Ÿ sample æ™‚ä¹Ÿé‚„æ˜¯å¿…é ˆä¾è³´ MCMCã€‚åŠ ä¸Šåœ¨å¯¦ç”¨ä¸Šï¼Œåå sigmoid å¾ˆå¤šæ™‚å€™æ•ˆæœä¸å¦‚ ReLu, maxout ç­‰ï¼Œä¾‹å¦‚ sigmoid æœ‰åš´é‡çš„ gradient vanish problemã€‚é€™ä½¿å¾— NN åœ¨ generative modeling åˆæˆ–æ˜¯ unsupervised learning ä¸Šä¸€ç›´å›°é›£é‡é‡ã€‚ GAN ä¸€å‡ºç«‹å³æ‰“ç ´é€™å€‹é›£å ªçš„é™åˆ¶ ! æ€éº¼èªªå‘¢? GAN æ¨æ£„èƒ½å¤ æ˜ç¢ºè¡¨é”å‡º p.d.f.çš„ä½œæ³•ï¼Œå¯«ä¸å‡ºæ˜ç¢ºçš„ p.d.f. ä¸€é»ä¹Ÿæ²’é—œä¿‚ï¼Œåªè¦èƒ½ç”Ÿæˆ å¤ çœŸçš„sampleé»ï¼Œä¸¦ä¸”sampleçš„æ©Ÿç‡è·Ÿtraining dataä¸€æ¨£å°±å¥½ ç„¶è€Œ GAN åœ¨å¯¦ä½œä¸Šå»æœƒé‡ä¸Šä¸€äº›å›°é›£ï¼Œä¾‹å¦‚ç”Ÿæˆçš„ samples å¤šæ¨£æ€§ä¸è¶³ï¼Œè¨“ç·´æµç¨‹/æ¶æ§‹ å’Œ hyper-parameters éœ€è¦å°å¿ƒé¸æ“‡ï¼Œç„¡æ³•æ˜ç¢ºçŸ¥é“è¨“ç·´çš„æ”¶æ–‚ç‹€æ³ï¼Œé€™äº›å•é¡Œç­‰ä¸‹æœƒèªªæ˜ã€‚ æœ¬ç¯‡çš„ä¸»è§’ (äº‹å¯¦ä¸Šä¸‹ä¸€ç¯‡æ‰æœƒç™»å ´) Wasserstein GAN (WGAN)ï¼Œå¾æœ¬è³ªä¸Šæ¢è¨ GAN ç›®æ¨™å‡½å¼ä¸­ä½¿ç”¨çš„ distance measureï¼Œé€²è€Œæ ¹æœ¬åœ°è§£æ±ºä¸Šè¿°ä¸‰å€‹å•é¡Œï¼Œé€™å¤§å¤§é™ä½äº† generative modeling è¨“ç·´é›£åº¦ ! æˆ‘å€‘é‚„æ˜¯ä¾†è«‡è«‡ GAN æ€éº¼ä¸€å›äº‹å…ˆå§ã€‚ Generative Adversarial NetsGAN ä½¿ç”¨ä¸€å€‹ two-player minimax gaming ç­–ç•¥ã€‚å…ˆç”¨ç›´è§€èªªï¼Œæˆ‘å€‘æœ‰ä¸€å€‹ ç”Ÿæˆå™¨ \\(G\\)ï¼Œç”¨ä¾†ç”Ÿæˆå¤ çœŸçš„ sampleï¼Œå¦å¤–é‚„æœ‰ä¸€å€‹ é‘‘åˆ¥å™¨ \\(D\\)ï¼Œç”¨ä¾†åˆ†è¾¨ sample ç©¶ç«Ÿæ˜¯çœŸå¯¦è³‡æ–™ (training data) ä¾†çš„å‘¢ï¼Œé‚„æ˜¯å‡çš„ (\\(G\\)ç”¢ç”Ÿçš„)ã€‚ç•¶é€™å…©å€‹æ¨¡å‹äº’ç›¸ç«¶çˆ­åˆ°ä¸€å€‹å¹³è¡¡é»çš„æ™‚å€™ï¼Œä¹Ÿå°±æ˜¯ \\(G\\) èƒ½å¤ ç”¢ç”Ÿåˆ° \\(D\\) åˆ†è¾¨ä¸å‡ºçœŸå‡çš„ sampleï¼Œæˆ‘å€‘çš„ç”Ÿæˆå™¨ \\(G\\) å°±éŠæˆäº†ã€‚è€Œ GAN ä½œè€…å²å®³çš„åœ°æ–¹å°±åœ¨æ–¼ ä¸€: å°‡é€™å…©å€‹modelçš„ç«¶çˆ­è¦å‰‡è½‰æ›æˆä¸€å€‹æœ€ä½³åŒ–å•é¡ŒäºŒ: ä¸¦ä¸”è­‰æ˜ï¼Œç•¶é”åˆ°è³½å±€çš„å¹³è¡¡é»æ™‚(é”åˆ°æœ€ä½³è§£)ï¼Œç”Ÿæˆå™¨å°±éŠæˆ (å¯ä»¥å®Œç¾è¡¨ç¤º training data çš„ pdfï¼Œä¸¦ä¸”å¯sampling) æˆ‘å€‘é‚„æ˜¯å¿…é ˆæŠŠä¸Šè¿°ç­–ç•¥åš´è¬¹çš„è¡¨é”å‡ºä¾† (å¯«æˆæœ€ä½³åŒ–å•é¡Œ)ï¼Œä¸¦è­‰æ˜ç•¶é”åˆ°æœ€ä½³åŒ–å•é¡Œçš„æœ€ä½³è§£æ™‚ï¼Œå°±å‰›å¥½å®Œæˆç”Ÿæˆå™¨çš„éŠæˆã€‚ Two-player Minimax GameåŸå‰‡ä¸Šæˆ‘å€‘å¸Œæœ›é‘‘åˆ¥å™¨ \\(D\\) èƒ½åˆ†è¾¨å‡ºçœŸå‡ sampleï¼Œå› æ­¤ \\(D(x)\\) å¾ˆè‡ªç„¶åœ°å¯ä»¥è¡¨ç¤ºç‚º sample \\(x\\) ç‚ºçœŸçš„æ©Ÿç‡å¦å¤–ç”Ÿæˆå™¨ \\(G\\) å‰‡æ˜¯è² è²¬ç”¢ç”Ÿå‡ sampleï¼Œä¹Ÿå¯ä»¥å¾ˆè‡ªç„¶åœ°è¡¨é”ç‚º \\(G(z)\\)ï¼Œå…¶ä¸­ \\(z\\) ç‚º latent variablesï¼Œä¸”æˆ‘å€‘å¯ä»¥å‡è¨­è©² latent variables \\(z\\) follow ä¸€å€‹ prior distribution \\(p_z(z)\\)ã€‚ æˆ‘å€‘å¸Œæœ› \\(D(x)\\) å°ä¾†è‡ªæ–¼çœŸå¯¦è³‡æ–™çš„ samples èƒ½å¤ ç›¡é‡å¤§ï¼Œè€Œå°ä¾†è‡ªæ–¼ \\(G\\) ç”¢ç”Ÿçš„è¦ç›¡é‡å°ï¼Œå› æ­¤å°æ–¼é‘‘åˆ¥å™¨ä¾†èªªï¼Œå®ƒçš„ç›®æ¨™å‡½å¼å¯å®šç¾©ç‚ºå¦‚ä¸‹: $$\\begin{align} Maximize: E_{x \\sim p_{data}(x)} [\\log D(x)] + E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ å¦ä¸€æ–¹é¢ï¼Œæˆ‘å€‘å¸Œæœ› \\(G\\) èƒ½å¤ å¼·åˆ°è®“ \\(D\\) ç„¡æ³•åˆ†è¾¨çœŸå½ï¼Œå› æ­¤ç”Ÿæˆå™¨çš„ç›®æ¨™å‡½å¼ç‚º: $$\\begin{align} Minimize: E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ çµåˆä¸Šè¿°å…©å€‹ç›®æ¨™å‡½å¼å°±æ˜¯å¦‚ä¸‹çš„ minmax problemäº† $$\\begin{align} \\min_G{ \\max_D{V(D,G)} } = E_{x \\sim p_{data}(x)} [\\log D(x)] + E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ é€™é‚Šä½œè€…å¾ˆæ¼‚äº®åœ°çµ¦å‡ºäº†ä¸Šè¿°å•é¡Œçš„ç†è«–è­‰æ˜ã€‚è­‰æ˜äº†å…©ä»¶äº‹æƒ…: ä¸Šè¿°æœ€ä½³åŒ–å•é¡Œ (å¼(3)) é”åˆ° global optimum æ™‚, \\( p_g = p_d \\)ã€‚ (ç”Ÿæˆå™¨ç”¢ç”Ÿå‡ºä¾†çš„ pdf æœƒç­‰æ–¼çœŸå¯¦è³‡æ–™çš„ pdfï¼Œå› æ­¤ç”Ÿæˆå™¨éŠæˆ!) ä½¿ç”¨å¦‚ä¸‹çš„æ¼”ç®—æ³•å¯ä»¥æ‰¾åˆ° global optimum æ¥ä¸‹ä¾†æˆ‘å€‘åªè¨è«–ç¬¬ä¸€å€‹äº‹æƒ…çš„è­‰æ˜ï¼Œå› ç‚ºé€™é—œä¿‚åˆ° GAN çš„å¼±é»ï¼Œä¹Ÿå°±æ˜¯ WGAN è¦è§£æ±ºçš„å•é¡Œæ ¹æº! è­‰æ˜ Global optimum ç™¼ç”Ÿæ™‚ï¼ŒéŠæˆç”Ÿæˆå™¨å¤§æ–¹å‘æ˜¯é€™æ¨£çš„ A. å‡å¦‚çµ¦å®š \\(G\\)ï¼Œæˆ‘å€‘éƒ½å¯ä»¥æ‰¾åˆ°ä¸€å€‹ç›¸å°æ‡‰çš„ \\(D_G^*\\) æœ€ä½³åŒ–é‘‘åˆ¥å™¨çš„ç›®æ¨™å‡½å¼ (1)ã€‚B. æ”¹å¯«åŸä¾†çš„ç›®æ¨™å‡½å¼ \\(V(G,D)\\)ï¼Œæ”¹å¯«å¾Œåªè·Ÿ \\(G\\) æœ‰é—œï¼Œæˆ‘å€‘å®šç¾©ç‚º \\(C(G)\\)ï¼Œé€™æ˜¯å› ç‚ºå°æ–¼æ¯ä¸€å€‹ \\(G\\) æˆ‘å€‘å·²ç¶“é…çµ¦å®ƒç›¸å°æ‡‰çš„ \\(D_G^*\\) äº†ï¼Œæ¥è‘—è­‰æ˜æœ€ä½³è§£åªç™¼ç”Ÿåœ¨ \\( p_g = p_d \\) çš„æƒ…æ³ã€‚ æ­¥é©Ÿ A: $$V(G,D)=\\int_{x}{p_d(x)\\log(D(x))dx}+\\int_{z}{p_z(z)\\log(1-D(g(z)))dz} \\\\ =\\int_x[p_d(x)\\log(D(x))+p_g(x)\\log(1-D(x))]dx$$ è€Œä¸€å€‹ function \\(f(x)=a\\log (y)+b\\log (1-y)\\) çš„æœ€ä½³è§£ç‚º \\(y=\\frac{a}{a+b}\\)å› æ­¤æˆ‘å€‘å¾—åˆ° \\( D_G^*(x) = \\frac{p_d(x)}{p_d(x)+p_g(x)} \\) æ­¥é©Ÿ B: $$\\begin{align*} &amp; C(G)=\\max_{D}V(G,D) \\\\ &amp; =E_{x \\sim p_d}[\\log D_G^*(x)]+E_{z \\sim p_z}[\\log(1-D_G^*(G(z)))] \\\\ &amp; =E_{x \\sim p_d}[\\log D_G^*(x)]+E_{x \\sim p_g}[\\log(1-D_G^*(x))] \\\\ &amp; =E_{x \\sim p_d}[\\log{\\frac{p_d(x)}{p_d(x)+p_g(x)}}]+E_{x \\sim p_g}[\\log{\\frac{p_g(x)}{p_d(x)+p_g(x)}}] \\end{align*}$$ ç„¶å¾Œæˆ‘å€‘ç‰¹åˆ¥è§€å¯Ÿå¦‚æœ \\(p_g = p_d\\)ï¼Œä¸Šå¼æœƒ $$\\begin{align} =E_{x \\sim p_d}[-\\log 2]+E_{x \\sim p_g}[-\\log 2]=-\\log4 \\end{align}$$ é‡æ–°æ”¹å¯«ä¸€ä¸‹ \\(C(G)\\) å¦‚ä¸‹ $$\\begin{align} C(G)=-\\log4+KL(p_d\\vert\\frac{p_d+p_g}{2})+KL(p_g\\vert\\frac{p_d+p_g}{2}) \\\\ =-\\log4+2JSD(p_d \\vert p_g) \\end{align}$$ é¦¬ä¸Šè§€å¯Ÿåˆ° \\(JSD\\geq0\\) å’Œ \\(JSD=0 \\Leftrightarrow p_g = p_d \\)é€™è¡¨ç¤º \\(C(G)\\) æœ€ä½³å€¼ç‚º \\(-\\log4\\)ï¼Œä¸”æˆ‘å€‘å·²çŸ¥ç•¶ \\(p_g = p_d\\) æ™‚é”åˆ°æœ€ä½³å€¼ (å¼(4))ï¼Œå› æ­¤ç‚ºæœ€ä½³è§£ çµè«–æ•´å€‹ GAN çš„æµç¨‹:æˆ‘å€‘åŸºæ–¼ä¸€å€‹ç”Ÿæˆå™¨ \\(G\\) å»æœ€ä½³åŒ– \\(D\\) å¾—åˆ° \\(D_G^*\\)ï¼Œæ¥è‘—è¦ç¹¼çºŒæœ€ä½³åŒ–ç”Ÿæˆå™¨çš„æ™‚å€™ï¼Œå•é¡Œå¾ç›®æ¨™å‡½å¼ (3) è®Šæˆç­‰åƒ¹æ–¼è¦æœ€ä½³åŒ–ä¸€å€‹ JSD çš„å•é¡Œ (å¼(5))ã€‚è—‰ç”±æœ€ä½³åŒ– JSD å•é¡Œï¼Œå¾—åˆ°æ–°çš„ \\(G\\)ï¼Œç„¶å¾Œé‡è¤‡ä¸Šé¢æ­¥é©Ÿï¼Œæœ€å¾Œé”åˆ°å¼(3)çš„æœ€ä½³è§£ï¼Œè€Œæˆ‘å€‘å¯ä»¥ä¿è­‰æ­¤æ™‚ç”Ÿæˆå™¨éŠæˆï¼Œ \\(p_g = p_d\\)ã€‚ å•é¡Œå‡ºåœ¨å“ª? å•é¡Œå°±å‡ºåœ¨æœ€ä½³åŒ–ä¸€å€‹ JSD çš„å•é¡Œä¸Šé¢ ! JSD æœ‰ä»€éº¼å•é¡Œ?æˆ‘å€‘é€šéæœ€ä½³åŒ– JSDï¼Œè€Œå°‡ \\(p_g\\) é€æ¼¸æ‹‰å‘ \\(p_d\\)ã€‚ä½†æ˜¯ JSD æœ‰å…©å€‹ä¸»è¦çš„å•é¡Œ: A. åœ¨ å¯¦éš›ç‹€æ³ ä¸‹ï¼Œç„¡æ³•çµ¦åˆé€£çºŒçš„è·é›¢å€¼ï¼Œå°è‡´ gradient å¤§éƒ¨åˆ†éƒ½æ˜¯ 0ï¼Œå› è€Œéå¸¸é›£ä»¥è¨“ç·´B. ç”¢ç”Ÿçš„æ¨£æœ¬å¤šæ¨£æ€§ä¸è¶³ï¼Œcollapse modeã€‚ é€™é‚Šè¦è§£é‡‹ä¸€ä¸‹ å¯¦éš›ç‹€æ³ æ˜¯ä»€éº¼æ„æ€ã€‚ä¸€èˆ¬ä¾†èªªï¼ŒçœŸå¯¦è³‡æ–™æˆ‘å€‘éƒ½æœƒç”¨éå¸¸é«˜çš„ç¶­åº¦å»è¡¨ç¤ºï¼Œç„¶è€Œè³‡æ–™çš„è®ŠåŒ–é€šå¸¸åªè¢«å°‘æ•¸å¹¾ç¨®è®Šå› æ‰€æ§åˆ¶ï¼Œä¹Ÿå°±æ˜¯åªå­˜åœ¨é«˜ç¶­ç©ºé–“ä¸­çš„ local manifoldã€‚ä¾‹å¦‚ä¸€å€‹ swiss roll é›–ç„¶æ˜¯åœ¨ 3 ç¶­ç©ºé–“ä¸­ï¼Œä½†å®ƒæ˜¯åœ¨ä¸€å€‹ 2 ç¶­çš„ manifold ç©ºé–“è£¡ã€‚ é€™æ¨£æœƒé€ æˆä¸€å€‹å•é¡Œå°±æ˜¯ï¼Œ \\(p_d\\) å’Œ \\(p_g\\)ï¼Œä¸æœƒæœ‰äº¤é›†ï¼Œåˆæˆ–è€…äº¤é›†è™•çš„é›†åˆæ¸¬åº¦ç‚º0!é€™æ¨£çš„æƒ…æ³åœ¨JSDè¡¡é‡å…©å€‹æ©Ÿç‡åˆ†å¸ƒçš„æ™‚å€™æœƒæ‚²åŠ‡ã€‚ä½œè€…çµ¦å‡ºäº†ä¸‹é¢ä¸€å€‹ç°¡å–®æ˜“æ‡‚çš„ä¾‹å­: å…©å€‹æ©Ÿç‡åˆ†å¸ƒéƒ½æ˜¯åœ¨ä¸€å€‹ 1 ç¶­çš„ manifold ç›´ç·šä¸Šï¼Œx è»¸çš„è·é›¢ç¶­ \\(\\theta\\)ï¼Œæ­¤æ™‚çš„ JSD å€¼ç‚ºå³åœ–æ‰€ç¤ºï¼Œå…¨éƒ¨éƒ½æ˜¯ \\(\\log2\\)ï¼Œé™¤äº†åœ¨ \\(\\theta\\) é‚£é»çš„å€¼æ˜¯ 0 (pdfå®Œå…¨é‡ç–Š)ã€‚é€™æ¨£è¨ˆç®—å‡ºçš„ Gradients å¹¾ä¹éƒ½æ˜¯ 0ï¼Œé€™ä¹Ÿå°±æ˜¯ç‚ºä»€éº¼ GAN å¾ˆé›£è¨“ç·´çš„åŸå› ã€‚ é€™å•é¡Œåœ¨ WGAN ä¹‹å‰é‚„æ˜¯æœ‰äººæå‡ºè§£æ±ºçš„æ–¹æ³•ï¼Œä¸éå°±å¾ˆåå·¥ç¨‹æ€è€ƒ: åŠ å…¥ noise ä½¿å¾—å…©å€‹æ©Ÿç‡åˆ†éƒ¨æœ‰ä¸å¯å¿½ç•¥çš„é‡ç–Šã€‚å› æ­¤è®“ GAN å…ˆå‹•èµ·ä¾†ï¼Œå‹•èµ·ä¾†ä¹‹å¾Œï¼Œå†æ…¢æ…¢åœ°æŠŠ noise ç¨‹åº¦ä¸‹é™ã€‚é€™æ˜¯è°æ˜å·¥ç¨‹å¸«çš„å²å®³è¾¦æ³•! ä½†çµ‚æ­¸ä¾†èªªé‚„æ˜¯æ²»æ¨™ã€‚çœŸæ­£çš„æ²»æœ¬æ–¹æ³•ï¼Œå¿…é ˆè¦æ›¿æ›æ‰ JSD é€™æ¨£çš„é‡æ¸¬å‡½å¼æ‰å¯ä»¥ã€‚ æœ¬ç¯‡é‹ªæ¢—çµæŸ (é€™æ¢—ä¹Ÿå¤ªé•·äº†)ã€‚ä¸‹ç¯‡çµ‚æ–¼è¼ªåˆ°ä¸»è§’ç™»å ´ï¼Œ WGAN çš„ W !","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"generative model","slug":"generative-model","permalink":"http://yoursite.com/tags/generative-model/"}]},{"title":"Why-Aggregation-Work","date":"2017-03-13T13:29:47.000Z","path":"2017/03/13/Why-Aggregation-Work/","text":"ç‚ºä½•ä¸‰å€‹è‡­çš®åŒ æœƒå‹éä¸€å€‹è«¸è‘›äº®?åœ¨ ML ä¸­æœ‰ä¸€é¡çš„æ¼”ç®—æ³•ç¨±ç‚º Aggregation Methodsï¼Œé€™æ–¹æ³•çš„é‹ä½œæ–¹å¼å…¶å¯¦æˆ‘å€‘å¯èƒ½å¾å°å°±æ¥è§¸åˆ°äº†ã€‚æœ‰æ²’æœ‰é‡éä¸€ç¨®æƒ…æ³å°±æ˜¯ï¼Œç•¶ä¸€ç¾¤äººé‡åˆ°ä¸€å€‹ä¸çŸ¥é“æœ€å¥½ç­”æ¡ˆçš„æ™‚å€™ï¼Œæœ€ç›´æ¥çš„æ–¹å¼å°±æ˜¯å¤§å®¶çš„ç­”æ¡ˆå–å¹³å‡ã€‚è½èµ·ä¾†å¾ˆç›´è¦ºï¼Œä½†å¿ƒè£¡è€è¦ºå¾—æ€ªæ€ªçš„ï¼Œå› ç‚ºæ ¹æœ¬ä¸çŸ¥é“åˆ°åº•å¯ä¸å¯é ã€‚Aggregation methods å°±æ˜¯é€™æ¨£çš„é‹ä½œæ¨¡å¼ï¼Œé€™é‚Šå°±çµ¦å€‹çµè«–ï¼Œå®ƒå¾ˆå¯é ! ä»¥ä¸‹çš„æ¨å°å‡ºè‡ªæ–¼æ—è»’ç”°æ•™æˆçš„è¬›ç¾©ï¼Œé€™è£¡ç”¨è‡ªå·±çš„ç†è§£æ–¹å¼é‡æ–°è¡¨é”ï¼Œä¸»è¦ä½œç­†è¨˜ç”¨ é–‹é ­é‚„æ˜¯çµ¦å…ˆå®šç¾©æ¸…æ¥šä¸€äº› termsï¼Œå°æ–¼ç†è§£å¼å­æ‰ä¸æœƒæ··æ·† å®šç¾©åœ¨å…ˆ Input: \\(x \\in X\\) æ­£ç¢ºç­”æ¡ˆ: \\(f(x)\\) è‡­çš®åŒ : \\(g_t(x),t=1,2,â€¦\\) è‡­çš®åŒ å€‘çš„æ±ºç­–çµæœ: \\(G(x)=avg_t(g_t(x))\\) è¡¡é‡æ–¹æ³• \\(g\\) çš„éŒ¯èª¤ç‡: \\( Error(g)=E_x[(g(x)-f(x))^2]\\) é€™é‚Šè¦ç‰¹åˆ¥èªªçš„æ˜¯è¡¡é‡ä¸€å€‹æ–¹æ³• \\(g\\) çš„éŒ¯èª¤ç‡ï¼Œæ˜¯é‡å°æ‰€æœ‰çš„ input \\(x\\)ï¼Œä¹Ÿå°±æ˜¯é‡å° \\(X\\) domain ä¾†ç®—æœŸæœ›å¹³æ–¹èª¤å·® é‹ç®—ç°¡å–®ä½†æœ‰é»ç¥å¥‡çš„æ¨å° æˆ‘å€‘å…ˆé‡å° ä¸€å€‹å›ºå®šçš„ xï¼Œä¾†çœ‹çœ‹è‡­çš®åŒ å€‘çµ±åˆçš„æ„è¦‹æ˜¯å¦çœŸçš„æœƒå¾—åˆ°è¼ƒå¥½çš„çµæœï¼Œç”±æ–¼inputå·²ç¶“å›ºå®šï¼Œæ‰€ä»¥ä¸‹é¢æœƒå¿½ç•¥ x çš„ termé¦–å…ˆæ˜¯ â€œè‡­çš®åŒ å€‘å„è‡ªçš„å¹³æ–¹éŒ¯èª¤ç‡â€ çš„å¹³å‡å€¼$$avg_t((g_t-f)^2)$$å°‡å¹³æ–¹æ‹†é–‹å¾Œå¾—$$=avg_t(g_t^2-2g_tf+f^2)$$å°‡ avg ç§»å…¥ä¸¦ç”¨ G=avg(gt) å®šç¾©å¾—åˆ°$$=avg_t(g_t^2)-2Gf+f^2$$å†åšå¦‚ä¸‹çš„ç°¡å–®ä»£æ•¸é‹ç®—$$=avg_t(g_t^2)-G^2+(G-f)^2 \\\\=avg_t(g_t^2)-2G^2+G^2+(G-f)^2 \\\\=avg_t(g_t^2-2g_tG+G^2)+(G-f)^2 \\\\=avg_t((g_t-G)^2)+(G-f)^2$$ ç›®å‰ç‚ºæ­¢æ˜¯é‡å° ä¸€å€‹ç‰¹å®šçš„è¼¸å…¥ xï¼Œè€Œæˆ‘å€‘éœ€è¦çŸ¥é“çš„æ˜¯å° æ•´å€‹ domain X çš„éŒ¯èª¤ç‡å› æ­¤çœŸæ­£è¦è¨ˆç®—çš„æ˜¯é€™å€‹ç›®æ¨™éŒ¯èª¤ç‡$$avg_t(Error(g_t))=avg_t(E_x[(g_t(x)-f(x))^2])$$å°‡ Expection for all x ä»£å…¥é€²å»å‰›å‰›ä¸Šé¢é‡å°ä¸€å€‹ x çš„çµæœï¼Œå¾—åˆ°å¦‚ä¸‹å¼å­\\begin{eqnarray}=avg_t(E_x[(g_t(x)-G(x))^2])+E_x[(G(x)-f(x))^2] \\\\=avg_t(E_x[(g_t(x)-G(x))^2])+Error(G) \\\\\\geq Error(G) \\end{eqnarray} æ€éº¼è§£é‡‹?é‡è¤‡ä¸€ä¸‹æœ€å¾Œçš„é‡è¦å¼å­: $$avg_t(Error(g_t)) = avg_t(E_x[(g_t(x)-G(x))^2])+Error(G) \\\\\\geq Error(G)$$ æœ€ç›´æ¥çš„çµè«–å°±æ˜¯: â€œçµ±åˆå‡ºä¾†çš„çµæœâ€çš„éŒ¯èª¤ç‡ æœƒæ¯” â€œå„è‡ªæ±ºå®šâ€çš„å¹³å‡éŒ¯èª¤ç‡ é‚„è¦ä½ å¯ä»¥çœ‹åˆ°é‡å° ä¸€çµ„å›ºå®š çš„è‡­çš®åŒ å€‘ \\({g_t}\\)ï¼Œä¸ç­‰å¼å·¦é‚Š \\(avg_t(Error(g_t))\\) æ˜¯å›ºå®šå€¼ï¼Œå› æ­¤è‹¥è¦æ‰¾ä¸€å€‹çµ±åˆå¤§å®¶æ„è¦‹çš„æ–¹æ³• \\(G\\)ï¼Œè€Œè©²æ–¹æ³•æœ‰æœ€å°çš„éŒ¯èª¤ç‡ (æœ€å°åŒ– \\(Error(G)\\) )ï¼Œå¾ˆæ˜é¡¯å°±æ˜¯è¦æœ€å¤§åŒ– \\(avg_t(E_x(g_t-G)^2)\\)ï¼Œè€Œæ­¤æœ€å¤§åŒ–çš„çµæœ å°±æ˜¯ \\(G\\) æ˜¯ \\({g_t}\\) çš„å¹³å‡å€¼(uniform blending)ï¼Œç¬¦åˆæˆ‘å€‘ä¸€é–‹å§‹èªªçš„æœ€ç›´è¦ºçš„ç­–ç•¥! å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘å€‘é¸åˆ°å…©çµ„ set \\({g_t}\\) and \\({h_t}\\) ä»–å€‘çš„ Error ç›¸åŒ: \\(avg_t(Error(g_t))= avg_t(Error(h_t))\\) ï¼Œé‚£æˆ‘å€‘ç•¶ç„¶æ˜¯è¦é¸æ“‡æ„è¦‹æœ€ä¸åŒçš„é‚£ä¸€çµ„è‡­çš®åŒ å€‘ï¼Œé€™æ˜¯å› ç‚ºæ„è¦‹æ„ˆä¸åŒä»£è¡¨ \\(avg_t(E_x(g_t-G)^2)\\) æ„ˆå¤§ï¼Œå› è€Œå°è‡´ \\(Error(G)\\) æœƒæ„ˆå°ã€‚ å°çµ å‰›å‰›ä¸Šé¢é€™å€‹çµè«–å°±å¾ˆæœ‰è¶£ï¼Œæ„è¦‹é‡ä¸åŒçš„è©±ï¼Œçµ±åˆèµ·ä¾†çš„æ•ˆæœæ„ˆå¥½ï¼Œä¹Ÿå°±æ˜¯ä½ æˆ‘ä¹‹é–“çš„æ„è¦‹æœ‰å¾ˆå¤§çš„åˆ†æ­§æ™‚ï¼Œé€™ä»£è¡¨æ˜¯å¥½äº‹! äº‹å¯¦ä¸Š Adaboost å°±æ˜¯æ¡å–é€™éº¼ä¸€å€‹ç­–ç•¥ï¼Œæ¯ä¸€æ¬¡çš„ iteration æœƒé¸æ“‡è·Ÿä¸Šæ¬¡çµ±åˆå®Œçš„çµæœæ„è¦‹å·®æœ€å¤šé‚£ä¸€ä½è‡­çš®åŒ é€²ä¾†ï¼Œæœ‰æ©Ÿæœƒå†è£œä¸Š Adaboostï¼Œé€™æ˜¯æˆ‘å¾ˆå–œæ­¡çš„ä¸€ç¨® ML æ¼”ç®—æ³•ã€‚ è€Œé€™é‚Šé‚„å¯ä»¥å¼•å‡ºä¸€å€‹æ–¹æ³•, Bootstrap. Bootstrap aggregationæ–¹æ³•å¾ˆç°¡å–®ã€‚å°æˆ‘å€‘çš„datasetæ¯ä¸€æ¬¡é‡æ–°resampling (e.g. å–Nâ€™ç­†ï¼Œæ¯æ¬¡å–çš„dataéƒ½å†æ”¾å›å»ï¼Œå› æ­¤dataå¯ä»¥é‡è¤‡ã€‚å¯é‡è¤‡é€™é»é€ æˆdatasetçš„pointå…·æœ‰weightçš„æ€§è³ªï¼Œé€™åœ¨adaboostæ¯ä¸€æ¬¡iterationçš„re-weightingæœ‰åŒæ¨£æ„æ€) é€™å€‹å«åšbootstrapï¼Œé‡å°è©²æ¬¡çš„dataç®—å‡ºæˆ‘å€‘çš„weak learner gtï¼Œiterateå¾ˆå¤šæ¬¡å¾Œï¼ŒæŠŠæ¯ä¸€æ¬¡çš„gtåšuniform blendingã€‚ æˆ‘èªç‚º aggregation methods å°±ç®—æ”¾åˆ°ç¾åœ¨çš„ Deep Learning ç«ç†±çš„æ™‚ä»£é‚„æ˜¯ç›¸ç•¶æœ‰ç”¨çš„ï¼Œé™¤äº†æœ¬èº«é€™äº›æ–¹æ³•å¦‚ adaboost å¥½ç”¨ä¹‹å¤–ï¼Œå…¶æ¦‚å¿µä¹Ÿç›¸ç•¶æœ‰ç”¨ï¼Œä¾‹å¦‚ Deep Learning çš„ dropout äº‹å¯¦ä¸Šå¯ä»¥ç”¨ bootstrap ä¾†è§£é‡‹ (æœ‰æ©Ÿæœƒå†è£œä¸Šè³‡æ–™)","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"uniform blending","slug":"uniform-blending","permalink":"http://yoursite.com/tags/uniform-blending/"},{"name":"aggregation","slug":"aggregation","permalink":"http://yoursite.com/tags/aggregation/"},{"name":"adaboost","slug":"adaboost","permalink":"http://yoursite.com/tags/adaboost/"},{"name":"bootstrap","slug":"bootstrap","permalink":"http://yoursite.com/tags/bootstrap/"}]},{"title":"Vehicle-Tracking","date":"2017-03-12T14:27:13.000Z","path":"2017/03/12/Vehicle-Tracking/","text":"é€™å€‹ Porject ç›®çš„æ˜¯è¦åµæ¸¬ç•«é¢ä¸­æ‰€æœ‰çš„è»Šå­, å¤§è‡´ä¸Šçš„æµç¨‹æ˜¯å…ˆè¨“ç·´å¥½ car/non-car çš„ classifer, ç„¶å¾Œç”¨ sliding window æ­é…ä¸åŒçš„ window size å»åµæ¸¬, æœ€å¾Œå†æŠŠ bounding boxes åšä¸€äº›å¾Œè™•ç†, ä¾‹å¦‚ merge boxes, å’Œå°æ™‚é–“åºåˆ—çš„è™•ç†ä»¥ä¸‹ç‚º git hub çš„ REAMDE.md The goals / steps of this project are the following: Perform a Histogram of Oriented Gradients (HOG) feature extraction I implement HOG feature extraction and using a subset of training data to search a good settings of parameters. Images are stored in output_images/HOG_with_YCrCb.jpg and output_images/grid_search.jpg Train Classifier I trained a Linear SVM classifier with HOG + color_hist + bin_spatial which achieved 98% accuracy on test set. Sliding Window Search I implemented a sliding window search method with two scales of window. HOG features are extracted once for an given image. Showing Examples so far I showed 4 examples with the pipeline so far. Image is stored in output_images/example_before_post_processing.jpg Video Implementation I showed the results with a short video clip (test_video.mp4) as well as the final result that adopted post-processing below. Further Post-processing A buffer for heat-maps is used for keeping a 6 consecutive heat-maps in frames. This will filtered out some false accepts. Discussion A short discussion is made. â€“ Rubric Points 1. Histogram of Oriented Gradients (HOG) Explain how (and identify where in your code) you extracted HOG features from the training images. Explain how you settled on your final choice of HOG parameters. I randomly selected examples of car and notcar and showed their HOG results in each channel of HLS space: In order to get a good enough setting for those parameters (orientations, pixels_per_cell and cells_per_block), I applied a grid searching method with a linear SVM on a small subset of training data. Grid searching space is defined as follows (24 combinations): 123orient_set = range(9,19,3)pix_per_cell_set = [4,8,16]cell_per_block_set = [1,2] The purpose of this stage is not finding the optimal, but rather, a good enough setting. So I choose orient=15, pix_per_cell=8, cell_per_block=2, cspace=&#39;RGB2YCrCb&#39; 2. Train Classifier Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them). Before training the classifier, dataset should be processed first.Since the vehicles/GTI*/*.png contains time-series data, I manually selected images to avoid train and test sets having identical images. In addition, 20% images in each training folder are treated as test images. The same partition method applied to non-vehicles images too. Then I trianed a Linear SVM model with HOG + color_hist + bin_spatial features which has performance: 1inside-acc=1.0, outside-acc=0.9802036199095022 3. Sliding Window Search Describe how (and identify where in your code) you implemented a sliding window search. How did you decide what scales to search and how much to overlap windows? The course provided a very useful code snippet that can extract HOG features once no matter how much windows are. So I reuse it as the feature extraction function!I used two types of scales, 1.5 and 1.2, which deal with large and small window respectively (car with near and far positions from camera). Also, I found that the overlaping of cells_per_step = 1 (more dense windows) has better results in my implementation. Before going through, it is worth checking the image values. Since feature extraction pipeline processed .png files with mpimg.imread, it reads images with values [0,1]. However, mpimg.imread reads the .jpg file with values within [0,255]. So it is necessary to divide 255 before calling the feature extraction pipeline while reading .jpg images with mpimg.imread. Make sure your images are scaled correctly The training dataset provided for this project ( vehicle and non-vehicle images) are in the .png format. Somewhat confusingly, matplotlib image will read these in on a scale of 0 to 1, but cv2.imread() will scale them from 0 to 255. Be sure if you are switching between cv2.imread() and matplotlib image for reading images that you scale them appropriately! Otherwise your feature vectors can get screwed up. To add to the confusion, matplotlib image will read .jpg images in on a scale of 0 to 255 so if you are testing your pipeline on .jpg images remember to scale them accordingly. And if you take an image that is scaled from 0 to 1 and change color spaces using cv2.cvtColor() youâ€™ll get back an image scaled from 0 to 255. So just be sure to be consistent between your training data features and inference features! 4. Showing Examples Show some examples of test images to demonstrate how your pipeline is working. How did you optimize the performance of your classifier? The followings are some examples. As you can see in the example 2, there exists a false accept. This will be filtered out in the post-processing part. 5. Video Implementation Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.) Following is the final result (combined with post-processing as described below) 6. Further Post-processing Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes. A heat-map to further filtered out some false positives. Moreover, I used a buffer to keep the 6 consecutive frames of heat-maps, and then accumulated those heat-maps in buffer. The accumulated heat-map then thresholded and produced the final results. 7. Discussion Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust? There still have too much parameters that effect the robustness, like ystart, ystop, scale factors, thresholds for heat-maps, and etc. Moreover, with more challanging conditions, those settings might work in one condition but fail in others. I think the most important part in those pipelines is the classifier itself. The linear SVM I used in this project is not good enough as you can see in the video that still has few false accepts. So a deep-learning based classifier might achieve better results and actually helpful to the following pipelines. This would be my future work.","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"CV","slug":"CV","permalink":"http://yoursite.com/tags/CV/"}]},{"title":"Lane-Finding","date":"2017-02-27T02:12:28.000Z","path":"2017/02/27/Lane-Finding/","text":"ä»¥ä¸‹æ˜¯ github ä¸Šçš„ README, å…¨è‹±æ–‡. æ­¤ Project ä¸»è¦éƒ½æ˜¯åœ¨åš Computer Vision ç›¸é—œçš„æ±è¥¿. å­¸åˆ°äº†è¨±å¤šä½¿ç”¨ Python and CV ç›¸é—œçš„æŠ€å·§. æ•´ç†ä¾†èªªæ˜¯å€‹æ»¿æœ‰è¶£çš„ project! The goals / steps of this project are the following: Compute the camera calibration matrix and distortion coefficients given a set of chessboard images. Apply a distortion correction to raw images. Use color transforms, gradients, etc., to create a thresholded binary image. Apply a perspective transform to rectify binary image (â€œbirds-eye viewâ€). Detect lane pixels and fit to find the lane boundary. Determine the curvature of the lane and vehicle position with respect to center. Warp the detected lane boundaries back onto the original image. Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position. Rubric Points1. Camera calibrationThe images for calculating the distortion and 3-D to 2-D mapping matrix are stored in ./camera_cal/calibration*.jpg.Firstly, I used cv2.findChessboardCorners to find out all those corner points (corners) in the images.Then I used cv2.calibrateCamera to calculate the distortion (dist) and mapping matrix (mtx) given the corners pts and their corresponding predifined 3-D pts objp 2. Provide an example of a distortion-corrected imageHere is an example of distortion-corrected image: 3. Create a thresholded binary image and provide exampleI used magnitude of gradients, direction of gradients, and L and S in HLS color space.A combined rule is used: 12combined[((mag_binary == 1) &amp; (dir_binary == 1)) |\\ ((hls_binary == 1) &amp; (dir_binary == 1) &amp; (bright_binary == 1))] = 1 Example masking image is showed: Moreover, I used widgets to help tunning the parameters of those masking functions. It can provide instantaneous binary result that really help for accelarating this step. The widgets codes are list here: 123456789def interactive_mask(ksize, mag_low, mag_high, dir_low, dir_high, hls_low, hls_high, bright_low, bright_high): combined = combined_binary_mask(image,ksize, mag_low, mag_high, dir_low, dir_high,\\ hls_low, hls_high, bright_low, bright_high) plt.figure(figsize=(10,10)) plt.imshow(combined,cmap='gray') interact(interactive_mask, ksize=(1,31,2), mag_low=(0,255), mag_high=(0,255),\\ dir_low=(0, np.pi/2), dir_high=(0, np.pi/2), hls_low=(0,255),\\ hls_high=(0,255), bright_low=(0,255), bright_high=(0,255)) 4. Perspective transformFirst, I defined the source and destination of perspective points as follows: Source Destination 585, 460 320, 0 203, 720 320, 720 1127, 720 960, 720 695, 460 960, 0 Then the perspective_warper function is defined which returns perspective image and the matrix warpM as well.warM is needed for the later step which does the inverse perspective back to the original image. 1perspective_img, warpM = perspective_warper(undist,src,dst) An example is showed here: 5. Lane line pixel and polynomial fittingI applied a windowing approach to identify the lane pixels In this example, I used 9 windows for both lane lines. The window is processed in an order from the buttom to the top. Pixels are detected by the following function 1def identify_lane_pixel(img, lcenter_in, rcenter_in, win_num=9, win_half_width=150, start_from_button=False): lcenter_in and rcenter_inare the centers (in horizontal coordinate) of windows. win_num defines how many windows are used. In this example, 9. win_half_width refers to the half length of window width start_from_button indicates how the initial centers of windows are set. Specifically, Let the current window as j and current frame index as i. If start_from_button=True, the center of window j will be initally set as window j-1. Otherwise, it will be initally set as window j in frame i-1. Then, by using the initial position just set, the lane pixels are identified if the histogram of that window is high enough. Finally, based on those identified pixels, update the center position of current widnow j. Next, a simple second order polynomial fitting is applied to both identified pixels 123# Fit a second order polynomial to eachleft_fit = np.polyfit(lpixely, lpixelx, 2)right_fit = np.polyfit(rpixely, rpixelx, 2) But wait! Since we are assuming â€œbirds-eye viewâ€, both lanes should be parallel! So I first tried a method that ties the polynomial coefficients except the shifting ones! this method results in the following example As can be seen in the figure, curves are indeed parallel. However, when I applied this method to the final video, I found that it wobbling a lot! (see â€œ8. Videoâ€ below) After some investigation, I wonder that this problem is caused by the fixed source points of perspective. Since the pre-defined source points are always at the center of the camera while the lane curves are usually not, the result perspective curves is intrinsically not parellel! Hence, I applied a dynamic source point correction. Idea of method is showed in the follows: mapping inversely from coordinates in perspective images to original images can use the following formula: and results in the following example It works great! Unfortunately, if the lane curves are not stable, the resulting new source points may fail. This is the major difficulty of this method! (see â€œ8. Videoâ€ below) 6. Radius of curvature of the lane and the position of the vehicleThe curvature is calculated based on the following formula. Udacity provides a very good tutorial here ! 1234a1, b1, c1 = left_fit_coefficientsa2, b2, c2 = right_fit_coefficientsr1 = ((1+(2*a1*height*ym_per_pix+b1)**2)**1.5)/(2*np.abs(a1))r2 = ((1+(2*a2*height*ym_per_pix+b2)**2)**1.5)/(2*np.abs(a2)) Thereâ€™s no need to worry about absolute accuracy in this case, but your results should be â€œorder of magnitudeâ€ correct. So I divide my result by 10 to make it seems more reasonable. And of course, the â€œorder of magnitudeâ€ remains intact. 7. Warp the detected lane boundaries back onto the original imageIn order to warp back onto the original image, we need to calculate the inverse of perspective transform matrix warpMjust apply Minv = inv(warpM) which is from numpy.linalg import inv Then, simply apply cv2.warpPerspective with Minv as input. Note: use cv2.putText to print the curvature and position onto images 8. Video Simple poly-fit (Most stable! Simple is better ?!) Shared coefficients of poly-fit (Wobbling problem) Dynamic source points of perspective (Unstable, crash sometimes. If the lane curves are not stable, the resulting new source points may fail) DiscussionBasically, I applied those techniques suggested by Udacity. I did some efforts trying to parallize both curves in the perspective â€œbird eye viewâ€. Two methods are applied Shared coefficients of polynomial fitting Dynamic source points of perspetive Each has its own issue. For (1.), wobbling, and for (2.) unstable. Future works will focus on solving the (2.) unstable issue. Maybe a smoothing method is a good idea. Moreover, for more difficult videos, pixels may not be detected which makes the pipeline crash. One way to overcome this problem is when this issue happens, the lane curve is set to be the same as previous frame. Generelizing this idea, a confidence measure of lane pixels is worth to apply. If the confidence is low, then set the lane curve as the same as previous frame might be a good way to better estimate result. Finally, finding a robust combination of masking rule and tweaking those parameters precisely might help too. é™„ä¸Šä¸­æ–‡å…¶ä»–è¨è«–: Reviewer çµ¦äº†å¾ˆå¤šæœ‰ç”¨çš„ article links! é€™é‚Šé™„ä¸Šåšæœªä¾†åƒè€ƒ Perspective bird eye view:http://www.ijser.org/researchpaper%5CA-Simple-Birds-Eye-View-Transformation-Technique.pdfhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3355419/https://pdfs.semanticscholar.org/4964/9006f2d643c0fb613db4167f9e49462546dc.pdfhttps://pdfs.semanticscholar.org/4074/183ce3b303ac4bb879af8d400a71e27e4f0b.pdf Lane line pixel identification:https://www.researchgate.net/publication/257291768_A_Much_Advanced_and_Efficient_Lane_Detection_Algorithm_for_Intelligent_Highway_Safetyhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5017478/https://chatbotslife.com/robust-lane-finding-using-advanced-computer-vision-techniques-46875bb3c8aa#.l2uxq26sn lane detection with deep learning:http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w3/papers/Gurghian_DeepLanes_End-To-End_Lane_CVPR_2016_paper.pdfhttp://lmb.informatik.uni-freiburg.de/Publications/2016/OB16b/oliveira16iros.pdfhttp://link.springer.com/chapter/10.1007/978-3-319-12637-1_57 (chapter in the book Neural Information Processing)http://ocean.kisti.re.kr/downfile/volume/ieek1/OBDDBE/2016/v11n3/OBDDBE_2016_v11n3_163.pdf (in Korean, but some interesting insights can be found from illustrations)https://github.com/kjw0612/awesome-deep-vision (can be useful in project 5 - vehicle detection)å™å¿ƒåˆ°åè¡€çš„çœŸå¯¦æŒ‘æˆ°: é‚„æ˜¯è€è©±ä¸€å¥, çœŸçš„è¦æˆç‚ºå¯ç”¨çš„ç”¢å“, é›£é“è¶…ç´šç„¡æ•µé«˜é˜¿!!","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"CV","slug":"CV","permalink":"http://yoursite.com/tags/CV/"}]},{"title":"Neural Art","date":"2017-02-13T14:04:36.000Z","path":"2017/02/13/Neural-Art/","text":"Art with Neural Networké¢¨æ ¼, å‰µä½œé€™ç¨®èƒ½åŠ›åœ¨ç¾åœ¨Alpha Goå·²ç¶“ç¨±éœ¸çš„æ™‚ä»£, ç›®å‰è¦ºå¾—é‚„æ˜¯äººé¡ç¨æœ‰çš„ä¸éæœ‰è¶£çš„æ˜¯, å°æ–¼é‚£äº›å·²ç¶“åœ¨ ImageNet è¨“ç·´å¾—éå¸¸å¥½çš„æ¨¡å‹, å¦‚: VGG-19, æˆ‘å€‘é€šå¸¸å·²ç¶“åŒæ„æ¨¡å‹å¯ä»¥è¾¨åˆ¥ä¸€äº›è¼ƒæŠ½è±¡çš„æ¦‚å¿µé‚£éº¼æ˜¯å¦æ¨¡å‹è£¡, ä¹Ÿæœ‰å…·å‚™é¡ä¼¼é¢¨æ ¼å’Œå‰µä½œçš„å…ƒç´ å‘¢? åˆæˆ–è€…é¢¨æ ¼åœ¨æ¨¡å‹è£¡è©²æ€éº¼è¡¨é”? æœ¬ç¯‡æ–‡ç« ä¸»è¦æ˜¯ä»‹ç´¹é€™ç¯‡ A Neural Algorithm of Artistic Style çš„æ¦‚å¿µå’Œå¯¦ä½œ, å¦å¤–ä¸€å€‹å¾ˆå¥½çš„æŠ•å½±ç‰‡ by Mark Chang ä¹Ÿå¾ˆå€¼å¾—åƒè€ƒ å…ˆçµ¦å‡ºç¯„ä¾‹çµæœ, çµæœ = åŸå§‹çš„å…§å®¹ + å¸Œæœ›çš„é¢¨æ ¼ Content Image Style Image Result Image èªªåœ¨å‰é ­çš„æœ€ä½³åŒ–åœ¨è¬›ä¸‹å»ä¹‹å‰, æˆ‘å€‘å…ˆè¬› NN çš„äº‹æƒ…, ä¸€èˆ¬æƒ…æ³, æˆ‘å€‘æ˜¯çµ¦å®š input image x, è€Œåƒæ•¸ w å‰‡æ˜¯è¦æ±‚çš„è®Šæ•¸, åŒæ™‚å° loss (objective function) åš optimize, å¯¦ä½œä¸Šå°±æ˜¯ backprob.ä¸Šé¢è¬›åˆ°çš„ä¸‰ç¨®æ±è¥¿åˆ—å‡ºä¾†: x: input image (given, constant) w: NN parameters (variables) loss: objective function which is correlated to some desired measure äº‹å¯¦ä¸Š, backprob çš„è¨ˆç®— x and w è§’è‰²å¯ä»¥äº’æ›. ä¹Ÿå°±æ˜¯å°‡ w å›ºå®šç‚º constant, è€Œ x è®Šæˆ variables, å¦‚æ­¤ä¸€ä¾†, æˆ‘å€‘ä¸€æ¨£å¯ä»¥ç”¨ backprob å»è¨ˆç®—å‡ºæœ€ä½³çš„ image x.å› æ­¤, å¦‚æœæˆ‘å€‘èƒ½å°‡ loss å®šç¾©å¾—èˆ‡é¢¨æ ¼å’Œå…§å®¹é«˜åº¦ç›¸é—œ, é‚£éº¼æ±‚å¾—çš„æœ€ä½³ image x å°±æœƒæœ‰åŸå§‹çš„å…§å®¹å’Œå¸Œæœ›çš„é¢¨æ ¼äº†!é‚£éº¼å†ä¾†å°±å¾ˆæ˜ç¢ºäº†, æˆ‘å€‘è¦å®šç¾©å‡ºä»€éº¼æ˜¯ Content Loss å’Œ Style Loss äº† Content Lossé‡å°ä¸€å€‹å·²ç¶“è¨“ç·´å¥½çš„ model, æˆ‘å€‘å¸¸å¸¸å°‡å®ƒæ‹¿ä¾†åš feature extraction. ä¾‹å¦‚ä¸€å€‹ DNN æŠŠå®ƒæœ€å¾Œä¸€å±¤è¾¨è­˜çš„ softmax å±¤æ‹¿æ‰, è€Œå®ƒçš„å‰ä¸€å±¤çš„ response (åšforwardçš„çµæœ), å°±æœƒæ˜¯å°æ–¼åŸå§‹ input çš„ä¸€ç¨® encoding. ç†è«–ä¸Šä¹Ÿæœƒæœ‰å¾ˆå¥½çš„é‘‘åˆ¥åŠ› (å› æœ€åªå·®æœ€å¾Œä¸€å±¤çš„softmax). Udacity çš„ traffic-sign detection ä¹Ÿæœ‰æ‹¿ VGG-19, ResNet, å’Œ gooLeNet åš feature extraction, ç„¶å¾Œåªè¨“ç·´é‡æ–°åŠ ä¸Šçš„ softmax layer ä¾†å¾—åˆ°å¾ˆé«˜çš„è¾¨è­˜ç‡. å› æ­¤, æˆ‘å€‘å¯ä»¥å°‡ forward çš„ response image ç•¶ä½œæ˜¯ä¸€ç¨® measure content çš„æŒ‡æ¨™!çŸ¥é“é€™å€‹ç†ç”±å¾Œ, åŸæ–‡å…¬å¼å°±å¾ˆå¥½ç†è§£, å¼•ç”¨å¦‚ä¸‹: So let p and x be the original image and the image that is generated and Pl and Fl their respective feature representation in layer l. We then define the squared-error loss between the two feature representations ç°¡å–®ä¾†èªª Pl æ˜¯ content image P åœ¨ l å±¤çš„ response, è€Œ Fl æ˜¯ input image x (è¨˜å¾—å—? å®ƒæ˜¯è®Šæ•¸å–”) åœ¨ l å±¤çš„ response.é€™å…©å€‹ responses çš„ squared-error å®šç¾©ç‚º content loss, è¦æ„ˆå°æ„ˆå¥½. ç”±æ–¼ response ç‚º input çš„æŸç¨® encoded feature, æ‰€ä»¥å®ƒå€‘å¦‚æœæ„ˆæ¥è¿‘, input å°±æœƒæ„ˆæ¥è¿‘äº† (contentå°±æ„ˆæ¥è¿‘).å¼•ç”¨ Mark Chang çš„æŠ•å½±ç‰‡: Style Losså€‹äººè¦ºå¾—æœ€ç¥å¥‡çš„åœ°æ–¹å°±åœ¨é€™è£¡äº†! ç•¶æ™‚è‡ªå·±æ€éº¼çŒœæ¸¬éƒ½æ²’çŒœåˆ°å¯ä»¥é€™éº¼ formulate.æˆ‘å€‹äººçš„ç†è§£æ˜¯åŸºæ–¼ CNN ä¾†è§£é‡‹å‡è¨­å°æ–¼æŸä¸€å±¤ ConvNet çš„ kernel ç‚º w*h*k (width, hieght, depth), ConvNet çš„ k é€šå¸¸ä»£è¡¨äº†æœ‰å¹¾ç¨® feature mapsèªªç™½ä¸€é», æœ‰ k ç¨® filter responses çš„çµæœ, ä¾‹å¦‚ç¬¬ä¸€ç¨®æ˜¯ç·šæ¢é¡çš„response, ç¬¬äºŒç¨®æ˜¯å¼§å½¢é¡çš„responses â€¦ ç­‰ç­‰è€Œé¢¨æ ¼å°±æ˜¯é€™äº› responses çš„ correlation matrix! (å¯¦éš›ä¸Šç”¨ Gram matrix, ä½†æ„ç¾©é¡ä¼¼)åŸºæ–¼æˆ‘å€‘å°æ–¼ CNN çš„ç†è§£, æ„ˆå¾Œé¢çš„ layers èƒ½è™•ç†æ„ˆæŠ½è±¡çš„æ¦‚å¿µ, å› æ­¤æ„ˆå¾Œé¢çš„ Gram matrix ä¹Ÿå°±æ„ˆèƒ½ä»£è¡¨æŠ½è±¡çš„ style æ¦‚å¿µ.åŸæ–‡å…¬å¼å¼•ç”¨å¦‚ä¸‹: ç¸½ä¹‹å°±æ˜¯è¨ˆç®—åœ¨ l å±¤ä¸Š, sytle image a å’Œ input image x å®ƒå€‘çš„ Gram matrix çš„ L2-norm å€¼ ä¸€æ¨£å†ä¸€æ¬¡å¼•ç”¨ Mark Chang çš„æŠ•å½±ç‰‡:ä¹Ÿå¯ä»¥å»çœ‹çœ‹ä»–çš„æŠ•å½±ç‰‡, æœ‰ä¸åŒè§’åº¦çš„è§£é‡‹ å¯¦æˆ°ä¸»è¦åƒè€ƒæ­¤ gitHubä¸€é–‹å§‹ load VGG-19 model å°±ä¸èªªäº†, ä¸»è¦çš„å…©å€‹ loss, codes å¦‚ä¸‹:123456789101112131415161718def content_loss_func(sess, model): \"\"\" Content loss function as defined in the paper. \"\"\" def _content_loss(p, x): # N is the number of filters (at layer l). N = p.shape[3] # M is the height times the width of the feature map (at layer l). M = p.shape[1] * p.shape[2] # Interestingly, the paper uses this form instead: # # 0.5 * tf.reduce_sum(tf.pow(x - p, 2)) # # But this form is very slow in \"painting\" and thus could be missing # out some constants (from what I see in other source code), so I'll # replicate the same normalization constant as used in style loss. return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(x - p, 2)) return _content_loss(sess.run(model['conv4_2']), model['conv4_2']) 12345678910111213141516171819202122232425262728293031323334353637383940414243# Layers to use. We will use these layers as advised in the paper.# To have softer features, increase the weight of the higher layers# (conv5_1) and decrease the weight of the lower layers (conv1_1).# To have harder features, decrease the weight of the higher layers# (conv5_1) and increase the weight of the lower layers (conv1_1).STYLE_LAYERS = [ ('conv1_1', 0.5), ('conv2_1', 1.0), ('conv3_1', 1.5), ('conv4_1', 3.0), ('conv5_1', 4.0),]def style_loss_func(sess, model): \"\"\" Style loss function as defined in the paper. \"\"\" def _gram_matrix(F, N, M): \"\"\" The gram matrix G. \"\"\" Ft = tf.reshape(F, (M, N)) return tf.matmul(tf.transpose(Ft), Ft) def _style_loss(a, x): \"\"\" The style loss calculation. \"\"\" # N is the number of filters (at layer l). N = a.shape[3] # M is the height times the width of the feature map (at layer l). M = a.shape[1] * a.shape[2] # A is the style representation of the original image (at layer l). A = _gram_matrix(a, N, M) # G is the style representation of the generated image (at layer l). G = _gram_matrix(x, N, M) result = (1 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2)) return result E = [_style_loss(sess.run(model[layer_name]), model[layer_name]) for layer_name, _ in STYLE_LAYERS] W = [w for _, w in STYLE_LAYERS] loss = sum([W[l] * E[l] for l in range(len(STYLE_LAYERS))]) return loss ä¸€é–‹å§‹çµ¦å®š random input image:style image é¸å®šå¦‚ä¸‹:éš¨è‘— iteration å¢åŠ æœƒåƒé€™æ¨£: ç¬¬ä¸€æ¬¡çš„ backprob: 1000 iteration: 2000 iteration: 3000 iteration: 4000 iteration: 5000 iteration: çŸ­ç¯€é€™ä¹‹é–“å¾ˆå¤šåƒæ•¸å¯ä»¥èª¿æ•´å»ç©, æœ‰èˆˆè¶£å¯ä»¥è‡ªå·±ä¸‹è¼‰ gitHub å»æ¸¬ ä¸Šä¸€ç¯‡çš„ â€œGTX 1070 åƒè¦‹â€ æœ‰æåˆ°, åŸä¾†ç”¨ CPU å»è¨ˆç®—, 1000 iteration èŠ±äº†å…­å€‹å°æ™‚! ä½†æ˜¯å¼·å¤§çš„ GTX 1070 åªéœ€è¦ 6 åˆ†é˜! ä¸é, å°±ç®—æ˜¯çµ¦æ‰‹æ©Ÿç”¨ä¸ŠGTX1070å¥½äº† (å“ˆå“ˆç•¶ç„¶ä¸å¯èƒ½), 6åˆ†é˜çš„ä¸€å€‹çµæœä¹Ÿæ˜¯ç„¡æ³•æ¥å—!PRISMA å¯ä»¥åœ¨ä¸€åˆ†é˜å…§è™•ç†å®Œ! é€™å¿…å®šä¸æ˜¯é€™ç¨®è¦ç®— optimization çš„æ–¹æ³•å¯ä»¥é”åˆ°çš„.äº‹å¯¦ä¸Š, æé£›é£›çš„åœ˜éšŠç™¼è¡¨äº†ä¸€ç¯‡è«–æ–‡ â€œPerceptual Losses for Real-Time Style Transfer and Super-Resolutionâ€œè¨“ç·´éå¾Œ, åªéœ€è¦åš forward propagation å³å¯! Standford University çš„ JC Johnson çš„ gitHub æœ‰å®Œæ•´çš„ source code!æ‰¾æ™‚é–“å†ä¾†å¯«é€™ç¯‡å¿ƒå¾—æ–‡å›‰!","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Art","slug":"Art","permalink":"http://yoursite.com/tags/Art/"}]},{"title":"GTX 1070","date":"2017-02-12T13:44:40.000Z","path":"2017/02/12/GTX-1070/","text":"NVIDIA GTX 1070 åƒè¦‹ç¶“éå…©æ¬¡çš„Udacity DNN Projectså¾Œ, æˆ‘å—ä¸äº†ç”¨CPUè¨“ç·´äº†! é€™å¯¦åœ¨æ˜¯å¤ªæ…¢äº†!è€ƒé‡æ‡‰è©²æœƒé•·æœŸä½¿ç”¨GPU, AWSå¯¦åœ¨ä¸æ€éº¼ä¾¿å®œ (1hr=1USD @ Tokyo site), åŠ ä¸Šlocalç«¯è¨“ç·´ä¹Ÿæ¯”è¼ƒæ–¹ä¾¿, å°±æ®ºä¸‹å»äº†!! å®‰è£ CUDA and cuDNNå¤§è‡´ä¸Šçš„å®‰è£æµç¨‹å¦‚ä¸‹, ä¸¦ä¸è¤‡é›œ, æ›´è©³ç´°å¯åƒè€ƒ link å®‰è£ CUDA Driversä¸Šè¿°è¯çµä¸­æœ‰ä¸‹è¼‰è·¯å¾‘, ç„¶å¾Œç…§é é¢ä¸€æ­¥æ­¥é¸æ“‡ (Operating System, Version, Installer Type)Installer Typeå¦‚æœç¶²è·¯ä¸å¥½å»ºè­°é¸æ“‡ exe local, ç„¶å¾Œä¸‹è¼‰å¾ŒåŸ·è¡Œå®‰è£å°±å°äº†Windows ç’°å¢ƒè®Šé‡ä¸­ CUDA_PATH æ˜¯ C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0, ä½†æ˜¯ä»é ˆåŠ ä¸Š bin\\ å’Œ lib\\x64\\, è¨˜å¾—åŠ ä¸Š. å®‰è£ cuDNNè¦ä¸‹è¼‰é€™å€‹é‚„è¦å¡«ä¸€äº›ç™»å…¥è³‡æ–™, éœ€è¦å†Accelerated Computing Developer Programè¨»å†Š, ç¸½ä¹‹è¨»å†Šå¾Œå°±å¯ä¸‹è¼‰è§£å£“å¾Œæœƒæœ‰ä¸€å€‹è³‡æ–™å¤¾ cuda, è£¡é¢ä¸‰å€‹å­è³‡æ–™å¤¾ bin, include, libå°‡ä¸Šè¿°çš„æª”æ¡ˆæ”¾åˆ°ç›¸å°æ‡‰çš„ CUDA Driver çš„å®‰è£è·¯å¾‘å…§, é è¨­æ˜¯åœ¨ C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0 å®‰è£ tensorflow-gpuæœ€ç°¡å–®çš„ä¸€æ­¥pip install tensorflow-gpuç„¶å¾Œå³å¯æ¸¬è©¦, å¦‚æœæœ‰æˆåŠŸæœƒæœ‰ä»¥ä¸‹ç•«é¢, æ³¨æ„ successfully å­—æœ‰ç„¡å‡ºç¾ 1234567&gt;&gt;&gt; import tensorflow as tfI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally&gt;&gt;&gt; æ¸¬è©¦ GTX 1070 å¼·å¤§èƒ½åŠ›ä½¿ç”¨å…©å€‹æ¥µç«¯ä¾‹å­åˆ†åˆ¥æ¸¬è©¦æœ‰ç„¡ä½¿ç”¨GPUé€Ÿåº¦ä¸Šçš„å·®ç•° Neural Art çš„ä¾‹å­: A Neural Algorithm of Artistic Style é€™å€‹ä¾‹å­æ˜¯æ‰€æœ‰çš„æ±è¥¿éƒ½å¯ä»¥ load é€² memory ä¸­, å› æ­¤æ²’æœ‰ä»»ä½• I/O, ç›´æ¥æ¯”æ‹šé‹ç®—èƒ½åŠ›! å› æ­¤å¯ä»¥ç›´æ¥çœ‹å‡º GPU å’Œ CPU çš„è¨ˆç®—èƒ½åŠ›å·®ç•° çµæœ: æ™‚é–“æ²’æœ‰å¾ˆåš´æ ¼è¨ˆç®—, æ˜¯çœ‹ç”¢ç”Ÿçµæœçš„æ™‚é–“ç¨å¾®è¨ˆç®—çš„, ä½†é€™æ•ˆèƒ½å·²ç¶“å¾ˆèª‡è£äº†, 60å€, 60å€, 60å€!è·‘å‡ºä¾†çš„åœ–: Content Image Style Image Result Image ä¸æ˜¯æ‰€æœ‰çš„æƒ…æ³éƒ½èƒ½æŠŠ training data å’Œ model éƒ½ load é€² memory ä¸­, æ‰€ä»¥å‹¢å¿…æœƒæœ‰å…¶ä»–æ‹–æ…¢é€Ÿåº¦çš„ç’°ç¯€, å…¶ä¸­æœ€æ…¢çš„å°±æ˜¯ I/O å‰›å¥½ Udacity çš„ project 3 å°±æ˜¯æ¯ç­† training data éƒ½éœ€è¦å» load image ä¸¦ä¸” on-the-fly é‹ç®—ä¸€å † preprocessing. é€™å€‹æƒ…æ³å‰›å¥½æ˜¯å¦ä¸€ç¨®å¯èƒ½çš„æ¥µç«¯ çµæœè·‘ä¸€å€‹epochæ‰€èŠ±çš„æ™‚é–“ç‚ºé€™ç¨®caseçœ‹ä¾†åªèƒ½åŠ é€Ÿåˆ°ç´„ 2å€. æ²’è¾¦æ³•, å…¶ä»–æ‹–æ²¹ç“¶çš„å‹•ä½œä½”å¤ªå¤šæ¯”ä¾‹äº† çŸ­çµå¤§éƒ¨åˆ†çš„æƒ…æ³ä¸‹, æå‡çš„é€Ÿåº¦ç¯„åœæœƒè½åœ¨ 2~60 å€ ä¹‹é–“, ç¸½ä¹‹æ˜¯å€¼å¾—çš„! å°±ç®—ä¸ç©DNN, é›»å‹•ä¹Ÿè¦æŠŠå®ƒæ‰“åˆ°ç‰©è¶…æ‰€å€¼!","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"NVIDIA","slug":"NVIDIA","permalink":"http://yoursite.com/tags/NVIDIA/"},{"name":"cuDNN","slug":"cuDNN","permalink":"http://yoursite.com/tags/cuDNN/"},{"name":"CUDA","slug":"CUDA","permalink":"http://yoursite.com/tags/CUDA/"}]},{"title":"Driving by Learning Your Style","date":"2017-02-05T13:58:07.000Z","path":"2017/02/05/Driving-by-Learning-Your-Style/","text":"Udacity Self Driving Project 3: behavioral cloningA great simulator is provided that can log your driving data (speed, throttle, brake, steering, and images) and test the driving algorithm.Two modes are provided, Training mode and Atuonomous mode. By using Training mode, you can collect training data to train the model. Then test the model with the Atuonomous mode. For those driving log data, steering and images are the most important features that we are going to use in this project. The goal is, given an image, find out the corresponding steering angle. Some might wonder that speed, throttle, and brake are features that are useful too.Also, driving images are time correlated, not just a given static image.With ignoring so much useful information, does the goal still reasonable? Nvidia just showed it works! and works pretty well!So our first step is to collect the data, and fortunately, Udacity provides data for us and I used it for training. Training Data Analysis8036 data are provided. Each data has 3 positions of images (left, center, right) with 1 corresponding steering angle.Most of angles are 0, and I found that randomly ignoring half of 0-angle data is fine and can speed up. Moreover, I duplicated some samples that has angles within the range +-[0.2, 1] in order to balance the data.Histograms of before/after data selection are shown below: Data AugmentationData augmentation is a practical way to avoid overfit and generalized the model. I used 5 types of augmentations: Flipping â€“ Flipping is a useful way to balance both turns of data. For each data, a 1/2 probability is used to decide wheter to flip. Also, steering angle is multiplied by -1. Horizontal shift â€“ [-20,+20] pixels are randomly selected as the shift value. By doing so, it can help to recover the vehicle when it goes outside the lane.By referencing this article, I added 0.004 steering angle units per pixel shift to the right, and subtracted 0.004 steering angle units per pixel shift to the left.Results in [-0.8~+0.8] steering values adjustment which corresponding to [-2~+2] degrees (steering value * 25 = degree) Brightness â€“ Brightness is done in the â€œHSVâ€ domain. I found that with a ratio of [0.5~1.1] for â€œVâ€ domain works fine. Blurring â€“ A Gaussian blur with kernel size 3 is applied. Not sure how useful of this method helps for robustness. Left/Right camera images â€“ These left/right images are very useful for data augmentation and also help for recovering off-lane driving. Udacity: You also might wonder why there are three cameras on the car: center, left, and right. Thatâ€™s because of the issue of recovering from being off-center.In the simulator, you can weave all over the road and turn recording on and off. In a real car, however, thatâ€™s not really possible. At least not legally.So in a real car, weâ€™ll have multiple cameras on the vehicle, and weâ€™ll map recovery paths from each camera. I adjusted the steering angles for left/right images with a naive method. Following figure shows how I correct the angle of right image: I found that setting offset = 6 or 5 is good enough. For large value, the car starts zig-zagging. An example of correction shows below, where the steering angles are indicated by red lines: Data Normalization Normalization â€“ Images are normalized with (x-128)/128. Cropping â€“ Images are trimmed with 40, 20, 20, and 20 pixels from top, bottom, left, and right respectively. This will cut most of the car hood and sky. Resizing â€“ resized to 66 x 200, same as NVIDIA CNN. Model ArchitectureI adopted NVIDIA CNN with dropout layers: Generator and Training Generator: It is very useful to use a python generator to feed the training data batch-by-batch rather than loading all the data in memory at once.A useful link to learn python iterator/generator list here ( for those who doesnâ€™t familiar with python just like me :) ). In order to further speed up. I tried pre-loading a chunck of data, e.g. 5000 images, into memory, and loaded another chunck if the batch data (required by generator) is outside the chunck in memory. However, it does not speed up! Somewhat weired. For each input images, a position is randomly chosen (left,center,right).Then flipping and shadowing are applied with a random fair coin. Finally, brighteness and horizonal shift are adopted with the corresponding angle adjustment. Training: Some hyper-parameters are listed: epochâ€“50 samples for each epoch â€“ 8896 optimizer â€“ Adam with 1e-4 batch-size â€“ 64 Although Keras did shuffle, it only applies in the batched data. So I shuffled the entire training set for each epoch to get more de-correlated data. Driving PolicyI found that instead of giving a constant throttle, controlling to a constant speed is more stable to drive.So I used a simple policy that tries to keep speed near 20. 123456789speed = float(speed) if speed &gt; 25: throttle = 0.05 elif speed &gt; 20: throttle = 0.2 elif speed &gt; 10: throttle = 0.35 else: throttle = 0.5 ResultsSee below for the track1 drive. However, I failed on track2. Hit a wall during a right turn and still working on it.Hope some tweaks on data selection and model architecture might work~","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"}]},{"title":"Â–traffic-sign-detection","date":"2017-01-18T14:35:21.000Z","path":"2017/01/18/Â–traffic-sign-detection/","text":"å‰è¨€çµ‚æ–¼ä¾†åˆ° project 2 äº†, é€™æ¬¡çš„ä¸»è¦ç›®çš„æ˜¯ç·´ç¿’ä½¿ç”¨ tensorflow åšäº¤é€šè™ŸèªŒè­˜åˆ¥Dataset ç‚º German Traffic Sign Datasetæœ‰43ç¨®äº¤é€šè™ŸèªŒ, æ˜¯ä¸€ç¨®43é¸1çš„æ¦‚å¿µ, å› ç‚ºæ²’æœ‰è€ƒæ…®éƒ½ä¸æ˜¯é€™å€‹é¸é …, ç†è«–ä¸Šé€™é¡å•é¡Œè¼ƒç°¡å–®, æœ‰researcheré”åˆ°99.81%çš„è¾¨è­˜ç‡ å…± 51839 å¼µ training data, è€Œ testing æœ‰ 12630 å¼µ, åˆ†ä½ˆå¦‚ä¸‹, å¯ä»¥çœ‹çš„å‡ºä¾†è³‡æ–™åˆ†ä½ˆä¸å‡æ¯ç¨®é¡åˆ¥ random æŒ‘ä¸€å¼µå‡ºä¾†å¦‚ä¸‹åœ–Udacity å¾ˆå¥½å¿ƒçš„å¹«å¿™æŠŠæ‰€æœ‰çš„ image å¹«ä½ æ‰“åŒ…æˆåªå‰©ä¸‹ traffic sign Download, ä¸” cv2.resize(image,(32,32)) äº†, åªéœ€è¦ pickle.load ä¸‹ä¾†å°±æå®šè€ŒåŸå§‹çš„ data æ˜¯çµ¦ä½ ä¸€å¤§å¼µimage, ç„¶å¾Œå†å‘Šè¨´ä½ é‚£äº›traffic signsåœ¨imageä¸­çš„rectangular windowåº§æ¨™, é‚„è¦å†å¤šè™•ç†è¼ƒéº»ç…© è¦æ³¨æ„çš„ä¸€é»æ˜¯, dataset æ˜¯ç¶“ç”±ä¸€ç§’é˜çš„ video æ“·å–ä¸‹ä¾†, å› æ­¤é„°è¿‘çš„ data æœƒå¾ˆç›¸è¿‘ [1], å¦‚æœä½¿ç”¨ train_test_split æœƒ random é¸æ“‡, å°è‡´ train å’Œ validation æœƒç›¸è¿‘è€Œçœ‹ä¸å‡ºå·®ç•° Input Data PreprocessingUdacity å»ºè­°æˆ‘å€‘å¯ä»¥è™•ç†å¹¾å€‹æ–¹å‘ å°‡ data æ•¸é‡å¼„å¾—è¼ƒ balance NN ç®— loss çš„æ™‚å€™ä¸æœƒæ ¹æ“šæ¯å€‹é¡åˆ¥æ•¸é‡çš„å¤šå¯¡ä½œæ¬Šé‡, å› æ­¤æœ€å–®ç´”çš„æ–¹æ³•æ˜¯å°±æƒ³è¾¦æ³•ç”¢ç”Ÿå‡ºä¸€æ¨£å¤šçš„æ•¸é‡, å¦‚ç¬¬2é» å¯ä»¥å¢åŠ  fake data æˆ‘çš„ image processing å¯¦åœ¨å¾ˆå¼±, åªå–®ç´”çš„ä½¿ç”¨ rotation, è€Œä¸”åªæ•¢ç¨å¾®è®“angleç‚ºæ­£è² 5åº¦, æ€•é‚£ç¨®æœ‰æ–¹å‘ç®­é ­çš„è™ŸèªŒè½‰å£ 1cv2.getRotationMatrix2D(image_center, angle, scale) é€™æ¨£çš„æ–¹å¼æˆ‘å¯¦é©—èµ·ä¾†å…¶å¯¦æ²’å•¥å¹«åŠ©, XD æˆ‘çœ‹åˆ°æœ‰äººé‚„ä½¿ç”¨ cv2.WarpPerspective, æœç„¶å°ˆæ¥­å¤šäº†! æˆ‘ç›¸ä¿¡ç”¢ç”Ÿç¨®é¡å¤ å¤šçš„ fake data ä¸€å®šæœƒæœ‰å¹«åŠ©, ä¾‹å¦‚åŠ  noise, blur ç­‰ç­‰ å°‡ data åš normalization åšèªéŸ³ç¿’æ…£äº†, ç›´è¦ºå°±ç”¨ guassian normalization, mean=0, var=1, çµæœæ•´å€‹å¤§å¤±æ•—! åªæœ‰ä¸åˆ°1%è¾¨è­˜ç‡, why?? å¾Œä¾†ç”¨ mean substraction, ç„¶å¾Œé™¤ abs çš„æœ€å¤§å€¼, æˆ‘åªé¸æ“‡ä½¿ç”¨ YUV çš„ Y channel ç•¶ input CNN æ¶æ§‹è¦è¨­è¨ˆå’Œèª¿æ•´æ¶æ§‹æœ‰é»èŠ±æ™‚é–“, åŠ ä¸Šæˆ‘æ™‚é–“ä¸å¤š(æ‡¶), æ‰€ä»¥æˆ‘ç›´æ¥å°±ç”¨LeNetæ¶æ§‹1234567layer_depth = &#123; 'layer_1': 6, 'layer_2': 16, 'fully_connected_1': 120, 'fully_connected_2': 84, 'out': n_classes,&#125; è‡ªå·±å¤šåŠ äº† dropout å’Œ l2 regularization, åŸå› æ˜¯æ¯æ¬¡è·‘ training çš„ accuracy éƒ½è¦æ¨™åˆ°98 99, ä½†æ˜¯ validation set å§‹çµ‚å¾ˆé›£çªç ´ 93, ä¸€ç›´æœ‰ overfit çš„æ„Ÿè¦ºtensorflow çš„ dropout æ˜¯è¨­å®šè¦ä¿ç•™å¤šå°‘æ¯”ä¾‹ (keep_prob), åœ¨ training çš„æ™‚å€™è¨­å®šåœ¨æœ€å¾Œçš„å…©å±¤ fully connected layers, keep_prob æ„ˆå°åŸºæœ¬ä¸Šæ„ˆé›£è¨“ç·´ä¹Ÿéœ€è¦æ„ˆå¤š epochå¦å¤–è¨˜å¾—åœ¨åš evaluation çš„æ™‚å€™è¦æŠŠ keep_prob è¨­å®šæˆå› 1 [1] çš„æ¶æ§‹æƒ³æ³•ä¸éŒ¯, å°‡è¼ƒä½å±¤çš„ conv. layer å’Œè¼ƒä¸Šå±¤çš„ conv. layer ä¸€ä½µç•¶ä½œ fully connected layer çš„ input, é€™æ¨£åŒæ™‚èƒ½å¤ æœ‰ low-level feature, higher-resolution å’Œ high-level feature, lower-resolution å…©ç¨®è³‡è¨Šä¸€èµ·ç•¶æ±ºç­– å…¶ä»– Hyper-parameters Optimizer: èªªå¯¦è©±, è¦ä¸åœçš„èª¿æ•´å‡ºæœ€å¥½çš„åƒæ•¸å¯¦åœ¨æ²’é‚£å€‹å¿ƒåŠ›, æ‰€ä»¥èˆ‡å…¶ç”¨SGD, æˆ‘å°±ç›´æ¥ç”¨ Adam äº† (Adagradä¹Ÿæ˜¯ä¸€ç¨®æ‡¶äººé¸æ“‡) pooling: æ²’å•¥ç‰¹åˆ¥é¸, å› æ­¤ç”¨ max-pooling batch-size: åŸå…ˆè¨­å®š128, æœ‰ä¸€æ¬¡æ”¹æˆ256å°±å¯¦åœ¨trainä¸å¥½, å°±é€€å›128äº† learning rate: 0.001 l2 weight: 0.01 Learning Performancetest set accuracy = 0.893 è‡ªé¸æ¸¬è©¦åœ–ç‰‡Udacityå¸Œæœ›èƒ½å­¸å“¡è‡ªå·±æ‰¾åœ–ç‰‡ä¾†æ¸¬è©¦, å› æ­¤æˆ‘å°±åœ¨å¾·åœ‹çš„ google map ä¸Šæ‰¾åœ–, (çœ‹è‘—çœ‹è‘—å¿ƒéƒ½é£„éå»äº†)20å¼µåœ–è¾¨è­˜çµæœå¦‚ä¸‹:å‰›å¥½éŒ¯10å€‹, åªæœ‰ 50% æ­£ç¢ºç‡, é€™å¯¦åœ¨æœ‰é»æ‚²åŠ‡å…¶ä¸­æœ‰å…©å€‹éŒ¯èª¤å€¼å¾—æ³¨æ„å³åœ–æ˜¯top5è¾¨è­˜åˆ°çš„é¡åˆ¥åŠæ©Ÿç‡, å¯ä»¥ç™¼ç¾é™¤äº†æ­£ç¢ºç­”æ¡ˆçš„ traffic signal åœ¨ç¬¬äºŒåå¤–, ç¬¬ä¸€åçš„ general causion å…¶å¯¦è·Ÿ traffic signal è¶…åƒçš„ (åªçœ‹ç°éš)çœ‹ä¾†å¿…é ˆæŠŠ input çš„è‰²å½©è³‡è¨Šä¹ŸåŠ é€²å»æ‰èƒ½é€²ä¸€æ­¥æ”¹å–„äº†å¦ä¸€å€‹æ˜¯å¦‚ä¸‹é€™å€‹éŒ¯èª¤è‡ªå·±åˆ†æçš„åŸå› æ˜¯å› ç‚º training data çš„ speed limit éƒ½æ˜¯åœ“çš„å¤–æ¡†, è€Œæ­¤caseå‰›å¥½æ˜¯ä¸€å€‹é•·æ–¹å½¢ç‰Œå­, è£¡é¢æ‰æ˜¯é€€è‰²å¾ˆåš´é‡çš„åœ“å½¢, æ‰€ä»¥å°è‡´è¾¨è­˜å¤±æ•—æˆ–è¨±çœŸçš„ train å¾—å¾ˆå¥½çš„ CNN æœ‰èƒ½åŠ›æ‰¾å‡ºé‡è¦çš„åˆ¤æ–·è³‡è¨Š, å› æ­¤æœƒå»å¿½ç•¥å¤–é¢çš„æ–¹æ¡†, è€Œé¸æ“‡å»â€çœ‹â€å¤–é¢é€€è‰²çš„åœ“å½¢å’Œè£¡é¢çš„æ•¸å­—çµè«–å°±æ˜¯, æ‡‰è©²æ˜¯æˆ‘è‡ªå·±æ²’trainå¥½å§ ?! çŸ­çµå°å°åšéä¸€è¼ªäº¤é€šè™ŸèªŒè¾¨è­˜, æ‰æ¯”è¼ƒæœ‰æ„Ÿè¦ºçœŸå¯¦ç‹€æ³æœƒæœ‰å¤šå›°é›£é˜¿~æ‰¾æ™‚é–“ä¾† visualize ä¸€ä¸‹æ¯å±¤çš„ hidden units å°ä»€éº¼æ¨£çš„ image æœƒæœ‰è¼ƒé«˜çš„ activation! This paper by Zeiler and Fergus with toolbox è¦èƒ½ train å‡ºå¥½ model é™¤äº†åƒè€ƒæ–‡ç»åŸ¹é¤Šå° model æ¶æ§‹çš„å¥½ç›´è¦ºå¤–, engineering çš„è‹¦å·¥ä¹Ÿæœƒæ˜¯å¾ˆå¤§çš„é—œéµ! å¾ŒçºŒå˜—è©¦å°æ–¼ç›®å‰çš„è¾¨è­˜ç‡å¾ˆä¸æ»¿æ„. ä¸æ­»å¿ƒä¸‹å°±å¯¦ä½œ[1]çš„æ¶æ§‹, ç„¶å¾Œå°‡ NN çš„ model size æ“´å¤§, ä¸¦ä¸”å°‡é¡è‰²è³‡è¨Š YUV çš„ U åŠ é€²å»è¨“ç·´ (çµæœä¸Šè¿°å› é¡è‰²éŒ¯èª¤çš„traffic signalå°±åˆ†å°äº†)12345678910111213# Hyper-parametersEPOCHS = 30BATCH_SIZE = 128rate = 0.001drop_out_keep_prob = 0.5layer_depth = &#123; 'layer_1': 16, 'layer_2': 32, 'fully_connected_1': 256, 'fully_connected_2': 128, 'out': n_classes,&#125; å¾—åˆ°äº† Test Accuracy = 0.953 ! ä½†æ˜¯è‡ªé¸åœ–é›–æœ‰é€²æ­¥ä»å¾ˆä½ 65%å¦å¤–, ä¸Šè¿°çš„åƒæ•¸è¨­å®šä¸‹, å¦‚æœåŠ äº† l2_weight = 0.01 çš„è©±, validation åªèƒ½åˆ° 0.91x, å¯¦åœ¨ä¸å¤§å¥½è¨“ç·´, å¾Œä¾†åªå¥½æ”¾æ£„ç¬¬ä¸€æ¬¡çš„ submission, reviewer çµ¦äº†ä¸€äº›ä¸éŒ¯çš„ reference å¦‚ä¸‹: Extra Important MaterialLately on slack few students asked for a good Deep Learning book.So after lot of research found a book which is also recommended by Elon Musk Deep Learning (Adaptive Computation and Machine Learning series) Github and on Amazon Fast.ai A Guide to Deep LearningFew Articles Traffic sign classification using brightness augmentation Dealing with unbalanced dataExtra Materials I noted a linkage here to discuss about how should we choose the batch_size of Stochastic Gradient Decent Since you might be interested into â€œAdam Optimizerâ€, here is a website that talks about it. You might like to learn the whole idea of Dropout Itâ€™s gives a brief analysis of the technique. reviewer å¾ˆç”¨å¿ƒé˜¿!æ£’æ£’! Reference[1.] Traffic Sign Recognition with Multi-Scale Convolutional Networks","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"}]},{"title":"ä½¿ç”¨AWSè¨“ç·´DNNæ­¥é©Ÿ","date":"2017-01-16T13:47:43.000Z","path":"2017/01/16/aws-procedure/","text":"AWS Instanceæ³¨æ„äº‹é …åŠé€£ç·šå»ºç«‹AWS instanceçš„æ™‚å€™, ç”±æ–¼æˆ‘å€‘ä½¿ç”¨jupyteréœ€è¦port 8888, éœ€è¦ Configure the Security Group Running and accessing a Jupyter notebook from AWS requires special configurations. Most of these configurations are already set up on the udacity-carnd AMI. However, you must also configure the security group correctly when you launch the instance. By default, AWS restricts access to most ports on an EC2 instance. In order to access the Jupyter notebook, you must configure the AWS Security Group to allow access to port 8888.Click on â€œEdit security groupsâ€.On the â€œConfigure Security Groupâ€ page:Select â€œCreate a new security groupâ€Set the â€œSecurity group nameâ€ (i.e. â€œJupyterâ€)Click â€œAdd Ruleâ€Set a â€œCustom TCP Ruleâ€Set the â€œPort Rangeâ€ to â€œ8888â€Select â€œAnywhereâ€ as the â€œSourceâ€Click â€œReview and Launchâ€ (again) æˆåŠŸå»ºç«‹AWS instanceä¹‹å¾Œ, é–‹å•Ÿgit bashssh -i â€˜C:\\Users\\bobon\\.ssh\\MyKeyPair.pemâ€™ carnd@54.65.11.64å…¶ä¸­54.65.11.64æ˜¯instanceçš„ip AWSä¸Šé–‹å•Ÿjupyter notebook kernelé¦–å…ˆå…ˆæŠŠproject cloneä¸‹ä¾†, ä¸¦è¨­å®šå¥½conda env1234git clone https://github.com/udacity/CarND-Traffic-Sign-Classifier-Projectcd CarND-Traffic-Sign-Classifier-Projectconda env create -f environment.ymlsource activate CarND-Traffic-Sign-Classifier-Project æ¥è‘—å®‰è£tensorflow-gpu1pip install tensorflow-gpu opencv å®‰è£1conda install -c https://conda.binstar.org/menpo opencv å‰›å‰›å·²ç¶“å»ºç«‹condaçš„ç’°å¢ƒ, ä¸”activate CarND-Traffic-Sign-Classifier-Project, æ‰€ä»¥å¯ä»¥ç›´æ¥é–‹å•Ÿkernel1jupyter notebook åœ¨localç€è¦½å™¨ä¸Šè¼¸å…¥http://[all ip addresses on your system]:8888/ä¾‹å¦‚aws ipç‚º54.65.11.641http://54.65.11.64:8888/ æŠ“å–AWSä¸Šçš„è³‡æ–™ä¸‹ä¾†localç«¯åœ¨è‡ªå·±localçš„terminalä¸Š12scp -i &apos;C:\\Users\\bobon\\.ssh\\MyKeyPair.pem&apos; carnd@54.65.11.64:/home/carnd/Traffic-sign/cnn-traffic-sign* ./models/scp -i &apos;C:\\Users\\bobon\\.ssh\\MyKeyPair.pem&apos; carnd@54.65.11.64:/home/carnd/Traffic-sign/checkpoint ./models/ æ¥è‘—è¼¸å…¥å¯†ç¢¼å³å¯ (carnd)","tags":[{"name":"aws","slug":"aws","permalink":"http://yoursite.com/tags/aws/"}]},{"title":"Hexo ä¸­æ–‡é¡¯ç¤º and Markdown æ¸¬è©¦","date":"2017-01-08T13:47:43.000Z","path":"2017/01/08/chinese-encoding/","text":"é™¤äº†å°‡Hexoçš„_config.yml è¨­å®šæˆ language: zh-tw ä¹‹å¤–æ–‡ç« å¦‚æœç”¨UltraEditç·¨è¼¯çš„è©±çš„è©±, è¦ä½¿ç”¨è½‰æ›ç·¨ç¢¼, å°‡ASCIIè½‰UTF-8(Unicodeç·¨è¼¯), ä¸­æ–‡æ‰èƒ½æ­£å¸¸é¡¯ç¤º å¼•è¨€æ¸¬è©¦åŒä¸€å€‹å€å¡Šçš„å¼•è¨€ å…§å®¹æ–‡å­—, å¼·èª¿ å¼•è¨€æ¸¬è©¦äºŒåŒä¸€å€‹å¼•è¨€æ¸¬è©¦äºŒçš„å€å¡Š ç„¡åºæ¸…å–®, item1 ä»ç„¶æ˜¯item1çš„å…§å®¹ item2 item3 æœ‰åºitem1 item2 ä»ç„¶æ˜¯item2çš„å…§å®¹ item3 1234567891011121314151617181920212223bool text(const string inPath, const string outPath)&#123; ifstream ifs(inPath.c_str()); if (!ifs) return false; ofstream ofs(outPath.c_str()); if (!ofs) return false; string line; while (getline(ifs,line)) &#123; istringstream iss(line); string token; while (iss&gt;&gt;token) &#123; cout &lt;&lt; \"&lt;Token&gt;: \" &lt;&lt; token &lt;&lt; endl; ofs &lt;&lt; \"&lt;Token&gt;: \" &lt;&lt; token &lt;&lt; endl; &#125; &#125; ofs.close(); ifs.close(); return true;&#125; æ–°çš„item? Here is an example of AppleScript: tell application &quot;Foo&quot; beep end tell Normal paragrah ä»¥ä¸‹ç‚ºåˆ†éš”ç·š ä¸Šç·šä½¿ç”¨ä¸‰å€‹*ä¸­é–“æœ‰ç©ºæ ¼ ä¸Šç·šä½¿ç”¨ä¸‰å€‹*ä¸­é–“ç„¡ç©ºæ ¼ ä¸Šç·šä½¿ç”¨5å€‹*ä¸­é–“æœ‰ç©ºæ ¼ ä¸Šç·šä½¿ç”¨ä¸‰å€‹-ä¸­é–“æœ‰ç©ºæ ¼ æ•¸å­¸å…¬å¼æ¸¬è©¦ $$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$\\(x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\ æ–¹æ³•:åœ¨æ–‡ç« è¦æœ‰ &lt;script type=â€text/javascriptâ€ src=â€http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=defaultâ€œ &gt;&lt;/script&gt; é€™è¡ŒæŒ‡ä»¤ç„¶å¾Œå®‰è£…æ’ä»¶ Hexo-math, å®‰è£…æ–¹æ³•å¦‚ä¸‹, ä¾æ¬¡ä¸º1$ npm install hexo-math --save åœ¨ Hexo æ–‡ä»¶å¤¹ä¸­æ‰§è¡Œï¼š1$ hexo math install åœ¨ _config.yml æ–‡ä»¶ä¸­æ·»åŠ ï¼š1plugins: hexo-math","tags":[{"name":"markdown","slug":"markdown","permalink":"http://yoursite.com/tags/markdown/"},{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"Hello World","date":"2017-01-08T13:43:07.943Z","path":"2017/01/08/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]