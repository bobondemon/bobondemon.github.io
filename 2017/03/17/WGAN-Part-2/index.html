<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-tw">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Deep Learning,ML,generative model," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="前情提要GAN 作者設計出一個 Minimax game，讓兩個 players: 生成器 G 和 鑑別器 D 去彼此競爭，並且達到平衡點時，此問題達到最佳解且生成器 G 鍊成。大致上訓練流程為先 optimize 鑑別器 D for some iterations，然後換 optimize 生成器 G (在 optimize G 時，此問題等價於最佳化 JSD 距離)，重複上述 loop">
<meta property="og:type" content="article">
<meta property="og:title" content="WGAN Part 2: 主角 W 登場">
<meta property="og:url" content="http://yoursite.com/2017/03/17/WGAN-Part-2/index.html">
<meta property="og:site_name" content="CS Blog">
<meta property="og:description" content="前情提要GAN 作者設計出一個 Minimax game，讓兩個 players: 生成器 G 和 鑑別器 D 去彼此競爭，並且達到平衡點時，此問題達到最佳解且生成器 G 鍊成。大致上訓練流程為先 optimize 鑑別器 D for some iterations，然後換 optimize 生成器 G (在 optimize G 時，此問題等價於最佳化 JSD 距離)，重複上述 loop">
<meta property="og:image" content="http://yoursite.com/2017/03/17/WGAN-Part-2/EM_exp_data.png">
<meta property="og:image" content="http://yoursite.com/2017/03/17/WGAN-Part-2/EM_exp.png">
<meta property="og:image" content="http://yoursite.com/2017/03/17/WGAN-Part-2/WGAN_algo.png">
<meta property="og:image" content="http://yoursite.com/2017/03/17/WGAN-Part-2/WGAN_exp.png">
<meta property="og:updated_time" content="2017-03-26T02:48:37.856Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="WGAN Part 2: 主角 W 登場">
<meta name="twitter:description" content="前情提要GAN 作者設計出一個 Minimax game，讓兩個 players: 生成器 G 和 鑑別器 D 去彼此競爭，並且達到平衡點時，此問題達到最佳解且生成器 G 鍊成。大致上訓練流程為先 optimize 鑑別器 D for some iterations，然後換 optimize 生成器 G (在 optimize G 時，此問題等價於最佳化 JSD 距離)，重複上述 loop">
<meta name="twitter:image" content="http://yoursite.com/2017/03/17/WGAN-Part-2/EM_exp_data.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/03/17/WGAN-Part-2/"/>





  <title> WGAN Part 2: 主角 W 登場 | CS Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-tw">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">CS Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">讓學習變成一種習慣</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首頁
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            關於
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            歸檔
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            標籤
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/03/17/WGAN-Part-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chih-Sheng Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CS Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                WGAN Part 2: 主角 W 登場
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2017-03-17T21:25:12+08:00">
                2017-03-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分類於</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<hr>
<h3 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h3><p>GAN 作者設計出一個 Minimax game，讓兩個 players: 生成器 G 和 鑑別器 D 去彼此競爭，並且達到平衡點時，此問題達到最佳解且生成器 G 鍊成。大致上訓練流程為先 optimize 鑑別器 D for some iterations，然後換 optimize 生成器 G (在 optimize G 時，此問題等價於最佳化 JSD 距離)，重複上述 loop 直到達到最佳解。<br>但是仔細看看原來的最佳化問題之設計，我們知道在最佳化 G 的時候，等價於最佳化一個 JSD 距離，而 JSD 在遇到真實資料的時會很悲劇。<br>怎麼悲劇呢? 原因是真實資料都存在 local manifold 中，造成 training data 的 p.d.f. 和 生成器的 p.d.f. 彼此之間無交集 (或交集的測度為0)，在這種狀況 JSD = log2 (constant) almost every where。也因此造成 gradients = 0。<br>這是 GAN 很難訓練的一個主因。</p>
<p>也因此 WGAN 的主要治本方式就是換掉 JSD，改用 <strong>Wasserstein (Earth-Mover) distance</strong>，而修改過後的演算法也是簡單得驚人!</p>
<a id="more"></a>
<hr>
<h3 id="Wasserstein-Earth-Mover-distance"><a href="#Wasserstein-Earth-Mover-distance" class="headerlink" title="Wasserstein (Earth-Mover) distance"></a>Wasserstein (Earth-Mover) distance</h3><p>我們先給定義後，再用作者論文上的範例解釋<br>定義如下:<br><span>$$\begin{align} 
W(\mathbb{P}_r,\mathbb{P}_g)=\inf_{\gamma\in\prod(\mathbb{P}_r,\mathbb{P}_g)}E_{(x,y)\sim \gamma}[\Vert x-y \Vert]
\end{align}$$</span><!-- Has MathJax --><br>\(\gamma\)指的是 real data and fake data 的 <strong>joint distribution</strong>，其中 marginal 為各自兩個 distributions。先別被這些符號嚇到，直觀的解釋為: <strong>EM 距離可以理解為將某個機率分佈搬到另一個機率分佈，所要花的最小力氣</strong>。</p>
<p>我們用下面這個例子明確舉例，假設我們有兩個機率分佈 f1 and f2:<br><span>$$\begin{align*} 
f_1(a)=f_1(b)=f_1(c)=1/3 \\\\
f_1(A)=f_1(B)=f_1(C)=1/3
\end{align*}$$</span><!-- Has MathJax --><br>這兩個機率分佈在一個 2 維平面，如下:<br><img src="/2017/03/17/WGAN-Part-2/EM_exp_data.png" width="20%" height="20%"><br>而兩個 \(\gamma\) 對應到兩種 <strong>搬運配對法</strong><br><span>$$\begin{align*} 
\gamma_1(a,A)=\gamma_1(b,B)=\gamma_1(c,C)=1/3 \\\\
\gamma_2(a,B)=\gamma_2(b,C)=\gamma_2(c,A)=1/3
\end{align*}$$</span><!-- Has MathJax --><br>可以很容易知道它們的 marginal distributions 正好符合 f1 and f2 的機率分佈。<br>則這兩種搬運法造成的 EM distance 分別如下:<br><span>$$\begin{align*} 
EM_{\gamma_1}=\gamma_1(a,A)*\Vert a-A \Vert + \gamma_1(b,B)*\Vert b-B \Vert + \gamma_1(c,C)*\Vert c-C \Vert \\\\
EM_{\gamma_2}=\gamma_2(a,B)*\Vert a-B \Vert + \gamma_2(b,C)*\Vert b-C \Vert + \gamma_2(c,A)*\Vert c-A \Vert
\end{align*}$$</span><!-- Has MathJax --><br>明顯知道 <span>$\theta=EM_{\gamma_1}&lt;EM_{\gamma_2}$</span><!-- Has MathJax --><br>而 EM distance 就是在算所有搬運法中，最小的那個，並將那最小的 cost 定義為此兩機率分佈的距離。<br>這個距離如果是兩條平行 1 維的直線 pdf (上面的例子是直線上只有三個離散資料點)，會有如下的 cost:</p>
<p><img src="/2017/03/17/WGAN-Part-2/EM_exp.png" width="30%" height="30%"></p>
<p>對比此圖和上一篇的 JSD 的結果，EM 能夠正確估算兩個沒有交集的機率分佈的距離，直接的結果就是 <strong>gradient 連續且可微 !</strong> 使得 WGAN 訓練上穩定非常多。</p>
<hr>
<h3 id="一個關鍵的好性質-Wasserstein-Earth-Mover-distance-處處連續可微"><a href="#一個關鍵的好性質-Wasserstein-Earth-Mover-distance-處處連續可微" class="headerlink" title="一個關鍵的好性質: Wasserstein (Earth-Mover) distance 處處連續可微"></a>一個關鍵的好性質: Wasserstein (Earth-Mover) distance 處處連續可微</h3><p>原始 EM distance 的定義 (式(1)) 是 intractable<br>一個神奇的數學公式 (Kantorovich-Rubinstein duality) 將 EM distance 轉換如下:<br><span>$$\begin{align} 
W(\mathbb{P}_r,\mathbb{P}_\theta)=\sup_{\Vert f \Vert _L \leq 1}{ E_{x \sim \mathbb{P}_r}[f(x)] - E_{x \sim \mathbb{P}_\theta}[f(x)] }
\end{align}$$</span><!-- Has MathJax --><br>注意到 sup 是針對所有滿足 1-Lipschitz 的 functions f，如果改成滿足 K-Lipschitz 的 functions，則值會相差一個 scale K。<br>但是在實作上我們都使用一個 family of functions，例如使用所有二次式的 functions，或是 Mixture of Gaussians，等等。<br>而經過近幾年深度學習的發展後，我們可以相信，使用 DNN 當作 family of functions 是很洽當的選擇，因此假定我們的 NN 所有參數為 \(W\)，則上式可以表達成:<br><span>$$\begin{align} 
W(\mathbb{P}_r,\mathbb{P}_\theta)\approx\max_{w\in W}{ E_{x \sim \mathbb{P}_r}[f_w(x)] - E_{z \sim p(z)}[f_w(g_{\theta}(z))] }
\end{align}$$</span><!-- Has MathJax --><br>這裡不再是等式，而是逼近，不過 Deep Learning 優異的 Regression 能力是可以很好地逼近的。</p>
<p>我們還是需要保證整個 EM distance 保持處處連續可微分，這樣可以確保我們做 gradient-based 最佳化可以順利，針對這點，WGAN 作者很強大地證明完了，得到結論如下:</p>
<ul>
<li><p>針對生成器 \(g_\theta\)<br>任何 feed-forward NN 皆可</p>
</li>
<li><p>針對鑑別器 \(f_w\)<br>當 \(W\) 是 compact set 時，該 family of functions \(\{f_w\}\) 滿足 K-Lipschitz for some K。<br>具體實現很容易，因為在 \(R^d\) space，compact set 等價於 closed and bounded，因此只需要針對<strong>所有的參數取 bounding box即可!</strong><br>論文裡使用了 [-0.01,0.01] 這個範圍做 clipping。</p>
</li>
</ul>
<blockquote>
<p>與 GAN 第一個不同點為: 鑑別器參數取 clipping。</p>
</blockquote>
<hr>
<h3 id="EM-distance-為目標函式所造成的不同"><a href="#EM-distance-為目標函式所造成的不同" class="headerlink" title="EM distance 為目標函式所造成的不同"></a>EM distance 為目標函式所造成的不同</h3><p>我們將兩者的目標函式列出來做個比較<br><span>$$\begin{align} 
GAN: E_{x \sim \mathbb{P}_r} [\log f_w(x)] + E_{z \sim p(z)}[\log (1-f_w(g_{\theta}(z)))] \\
WGAN: E_{x \sim \mathbb{P}_r}[f_w(x)] - E_{z \sim p(z)}[f_w(g_{\theta}(z))]
\end{align}$$</span><!-- Has MathJax --><br>發現到 WGAN 不取 log，同時對生成器的目標函式也做了修改</p>
<blockquote>
<p>與 GAN 第二個不同點為: WGAN 的目標函式不取 log，同時對生成器的目標函式也做了修改。</p>
</blockquote>
<p>第三個不同點是作者實驗的發現</p>
<blockquote>
<p>與 GAN 第三個不同點為: 使用 Momentum 類的演算法，如 Adam，會不穩定，因此使用 SGD or RMSProp。</p>
</blockquote>
<hr>
<h3 id="WGAN-演算法"><a href="#WGAN-演算法" class="headerlink" title="WGAN 演算法"></a>WGAN 演算法</h3><p>總結一下與 GAN 的修改處</p>
<p>A. 鑑別器參數取 clipping。<br>B. WGAN 的目標函式不取 log，同時對生成器的目標函式也做了修改。<br>C. 使用 SGD or RMSProp。</p>
<p><img src="/2017/03/17/WGAN-Part-2/WGAN_algo.png" width="75%" height="75%"></p>
<hr>
<h3 id="WGAN-的優點"><a href="#WGAN-的優點" class="headerlink" title="WGAN 的優點"></a>WGAN 的優點</h3><p>一: 目標函式與訓練品質高度相關<br>原始的 GAN 沒有這樣的評量指標，因此會在訓練中途用人眼去檢查訓練是否整個壞掉了。 WGAN 解決了這個麻煩。作者的範例如下，可以發現WGAN的目標函式 Loss 愈低，sampling出來的品質愈高。</p>
<p><img src="/2017/03/17/WGAN-Part-2/WGAN_exp.png" width="75%" height="75%"></p>
<p>二: 鑑別器可以直接訓練到最好<br>原始的 GAN 需要小心訓練，不能一下子把鑑別器訓練太強導致導函數壞掉</p>
<p>三: 不需要特別設計 NN 的架構<br>GNN 使用 MLP (Fully connected layers) 難以訓練，較成功的都是 CNN 架構，並搭配 batch normalization。而在 WGAN 演算法下， MLP架構可能穩定訓練 (雖然品質有下降)</p>
<p>四: 沒有 collapse mode (保持生成多樣性)<br>作者自己說在多次實驗的過程都沒有發現這種現象</p>
<hr>
<h3 id="My-Questions"><a href="#My-Questions" class="headerlink" title="My Questions"></a>My Questions</h3><ol>
<li>原先 GAN 會有 collapse mode 看到有人討論是因為 KL divergence 不對稱的關係導致對於 “生成器生出錯誤的 sample” 比 “生成器沒生出所有該對的sample” 逞罰要大很多，不過這邊自己還是有疑問，因為 JSD 已經是對稱的 KL 了，還會有逞罰不同導致 collapse mode 的問題嗎? 需要再多看一下 paper 了解。</li>
<li>如何控制 sample 出來的 output，譬如 mnist 要 sampling 出某個 class。前提是希望不能對 data 有任何標記過，不然就沒有 unsupervised 的條件了。 Conditional GAN? 有空再研究一下這個課題</li>
</ol>
<hr>
<h3 id="Tensorflow-範例測試"><a href="#Tensorflow-範例測試" class="headerlink" title="Tensorflow 範例測試"></a>Tensorflow 範例測試</h3><p>主要參考此 <a href="https://github.com/jiamings/wgan" target="_blank" rel="external">github</a>，用自己的寫法寫一次，並做些測試</p>
<p>用 MNIST dataset 做測試，原始 input 為 28x28，將它 padding 成 32x32，因此 input domain 為 32x32x1</p>
<ol>
<li><p>生成器<br>幾個重點，第一個是生成器用的是 <strong><code>conv2d_transpose</code></strong> (<a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose" target="_blank" rel="external">doc</a>)，這是由於原先的 <code>conv2d</code> 無法將 image 的 size 變大，頂多一樣。因此要用 <code>conv2d_transpose</code>，以 第 15 行舉例。<br>argument <code>wc2</code> 的 shape 為 <code>[3, 3, 256, 512]</code> 分別表示 <code>[filter_h, filter_w, output_depth, input_depth]</code>。argument <code>[batch_size, 8, 8, 256]</code> 表示 output layer 的 shape。後面兩個 argument 就很明顯了，分別是 strides <code>[batch_stride, h_stride, w_stride, channel_stride]</code> 和 padding。<br>第二個重點是最後一層 <code>out_sample = tf.nn.tanh(conv5)</code>，由於我們會將 data 先 normalize 到 <code>[-1,1]</code>，因此使用 <code>tanh</code> 讓 domain 一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">z_dim = <span class="number">128</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_net</span><span class="params">(z)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'generator'</span>):</div><div class="line">        <span class="comment"># Layer 1 - 128 to 4*4*512</span></div><div class="line">        wd1 = tf.get_variable(<span class="string">"wd1"</span>,[z_dim, <span class="number">4</span>*<span class="number">4</span>*<span class="number">512</span>])</div><div class="line">        bd1 = tf.get_variable(<span class="string">"bd1"</span>,[<span class="number">4</span>*<span class="number">4</span>*<span class="number">512</span>])</div><div class="line">        dense1 = tf.add(tf.matmul(z, wd1), bd1)</div><div class="line">        dense1 = tf.nn.relu(dense1)</div><div class="line"></div><div class="line">        <span class="comment"># reshape to 4*4*512</span></div><div class="line">        conv1 = tf.reshape(dense1, (batch_size, <span class="number">4</span>, <span class="number">4</span>, <span class="number">512</span>))</div><div class="line"></div><div class="line">        <span class="comment"># Layer 2 - 4*4*512 to 8*8*256</span></div><div class="line">        wc2 = tf.get_variable(<span class="string">"wc2"</span>,[<span class="number">3</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">512</span>])</div><div class="line">        conv2 = tf.nn.conv2d_transpose(conv1, wc2, [batch_size, <span class="number">8</span>, <span class="number">8</span>, <span class="number">256</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">        conv2 = tf.nn.relu(conv2)</div><div class="line"></div><div class="line">        <span class="comment"># Layer 3 - 8*8*256 to 16*16*128</span></div><div class="line">        wc3 = tf.get_variable(<span class="string">"wc3"</span>,[<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">256</span>])</div><div class="line">        conv3 = tf.nn.conv2d_transpose(conv2, wc3, [batch_size, <span class="number">16</span>, <span class="number">16</span>, <span class="number">128</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">        conv3 = tf.nn.relu(conv3)</div><div class="line"></div><div class="line">        <span class="comment"># Layer 4 - 16*16*128 to 32*32*64</span></div><div class="line">        wc4 = tf.get_variable(<span class="string">"wc4"</span>,[<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>])</div><div class="line">        conv4 = tf.nn.conv2d_transpose(conv3, wc4, [batch_size, <span class="number">32</span>, <span class="number">32</span>, <span class="number">64</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">        conv4 = tf.nn.relu(conv4)</div><div class="line"></div><div class="line">        <span class="comment"># Layer 5 - 32*32*64 to 32*32*1</span></div><div class="line">        wc5 = tf.get_variable(<span class="string">"wc5"</span>,[<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">64</span>])</div><div class="line">        conv5 = tf.nn.conv2d_transpose(conv4, wc5, [batch_size, <span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">        out_sample = tf.nn.tanh(conv5)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> out_sample</div></pre></td></tr></table></figure>
</li>
<li><p>鑑別器<br>這個就是最一般的 CNN，output 最後是一個沒有過 log 的 scaler 且也沒有經過 activation function。比較重要的是變數都是使用 <code>get_variable</code>  和 <code>scope.reuse_variables()</code> (請參考 <a href="https://www.tensorflow.org/versions/r0.12/how_tos/variable_scope/#understanding_tfget_variable" target="_blank" rel="external">Sharing Variables</a>)。<br>具體的原因是因為我們會對 real data 呼叫一次鑑別器，而對於 fake data 也會在呼叫一次。若沒有 share variables，就會導致產生兩組各自的 weights。<br><code>tf.get_variable()</code> 跟 <code>tf.Variable()</code> 差別在於如果已經有名稱一樣的變數時 get_variable() 不會再產生另一個變數，而會 share，但是要真的 share 還必須多一個動作 <code>reuse_variables</code> 確保不是不小心 share 到的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Construct CriticNet</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W, b, strides=<span class="number">1</span>)</span>:</span></div><div class="line">    x = tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, strides, strides, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    x = tf.nn.bias_add(x, b)</div><div class="line">    <span class="keyword">return</span> tf.nn.relu(x)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">critic_net</span><span class="params">(x, reuse=False)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'critic'</span>) <span class="keyword">as</span> scope:</div><div class="line">        size = <span class="number">64</span></div><div class="line">        <span class="keyword">if</span> reuse:</div><div class="line">            scope.reuse_variables()</div><div class="line">        <span class="comment"># Layer 1 - 32*32*1 to 16*16*size</span></div><div class="line">        wc1 = tf.get_variable(<span class="string">"wc1"</span>,[<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, size])</div><div class="line">        bc1 = tf.get_variable(<span class="string">"bc1"</span>,[size])</div><div class="line">        conv1 = conv2d(x, wc1, bc1, strides=<span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Layer 2 - 16*16*size to 8*8*size*2</span></div><div class="line">        wc2 = tf.get_variable(<span class="string">"wc2"</span>,[<span class="number">3</span>, <span class="number">3</span>, size, size*<span class="number">2</span>])</div><div class="line">        bc2 = tf.get_variable(<span class="string">"bc2"</span>,[size*<span class="number">2</span>])</div><div class="line">        conv2 = conv2d(conv1, wc2, bc2, strides=<span class="number">2</span>)</div><div class="line">        </div><div class="line">        <span class="comment"># Layer 3 - 8*8*size*2 to 4*4*size*4</span></div><div class="line">        wc3 = tf.get_variable(<span class="string">"wc3"</span>,[<span class="number">3</span>, <span class="number">3</span>, size*<span class="number">2</span>, size*<span class="number">4</span>])</div><div class="line">        bc3 = tf.get_variable(<span class="string">"bc3"</span>,[size*<span class="number">4</span>])</div><div class="line">        conv3 = conv2d(conv2, wc3, bc3, strides=<span class="number">2</span>)</div><div class="line">        </div><div class="line">        <span class="comment"># Layer 4 - 4*4*size*4 to 2*2*size*8</span></div><div class="line">        wc4 = tf.get_variable(<span class="string">"wc4"</span>,[<span class="number">3</span>, <span class="number">3</span>, size*<span class="number">4</span>, size*<span class="number">8</span>])</div><div class="line">        bc4 = tf.get_variable(<span class="string">"bc4"</span>,[size*<span class="number">8</span>])</div><div class="line">        conv4 = conv2d(conv3, wc4, bc4, strides=<span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Fully connected layer - 2*2*size*8 to 1</span></div><div class="line">        wd5 = tf.get_variable(<span class="string">"wd5"</span>,[<span class="number">2</span>*<span class="number">2</span>*size*<span class="number">8</span>, <span class="number">1</span>])</div><div class="line">        bd5 = tf.get_variable(<span class="string">"bd5"</span>,[<span class="number">1</span>])</div><div class="line">        fc5 = tf.reshape(conv4, [<span class="number">-1</span>, wd5.get_shape().as_list()[<span class="number">0</span>]])</div><div class="line">        logit = tf.add(tf.matmul(fc5, wd5), bd5)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> logit</div></pre></td></tr></table></figure>
</li>
<li><p>Graph<br>這裡有幾個重點，第一個是由於我們在最佳化過程中，會 fix 住一邊的參數，然後最佳化另一邊，接著反過來。此作法參考 <a href="http://stackoverflow.com/questions/35298326/freeze-some-variables-scopes-in-tensorflow-stop-gradient-vs-passing-variables" target="_blank" rel="external">link</a><br>第二個重點是使用 <code>tf.clip_by_value</code>，可以看到我們對於所有透過 <code>tf.get_collection</code> 蒐集到的變數都增加一個 clip op。<br>第三個重點是使用 <code>tf.control_dependencies([opt_c])</code> <a href="https://www.tensorflow.org/api_docs/python/tf/control_dependencies" target="_blank" rel="external">link</a>，這個定義了 op 之間的關聯性，它會等到 argument 內執行完畢後，才會接著執行下去。<br>所以我們可以確保先做完 <code>RMSPropOptimizer</code> 才接著做 <code>clip_by_value</code>。另外 <code>tf.tuple</code> <a href="https://www.tensorflow.org/api_docs/python/tf/tuple" target="_blank" rel="external">link</a> 會等所有的 input arguments 都做完才會真的 return 出去，以確保每個 tensors 都做完 clipping 了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># build graph</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_graph</span><span class="params">()</span>:</span></div><div class="line">    z = tf.placeholder(tf.float32, shape=(batch_size, z_dim))</div><div class="line">    fake_data = generator_net(z)</div><div class="line">    real_data = tf.placeholder(tf.float32, shape=(batch_size, <span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>))</div><div class="line">    <span class="comment"># Define loss and optimizer</span></div><div class="line">    real_logit = critic_net(real_data)</div><div class="line">    fake_logit = critic_net(fake_data, reuse=<span class="keyword">True</span>)</div><div class="line">    </div><div class="line">    c_loss = tf.reduce_mean(fake_logit - real_logit)</div><div class="line">    g_loss = tf.reduce_mean(-fake_logit)</div><div class="line">    </div><div class="line">    <span class="comment"># get the trainable variables list</span></div><div class="line">    theta_g = tf.get_collection(</div><div class="line">        tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="string">'generator'</span>)</div><div class="line">    theta_c = tf.get_collection(</div><div class="line">        tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="string">'critic'</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># freezing or only update designated variables</span></div><div class="line">    opt_g = tf.train.RMSPropOptimizer(learning_rate=lr_generator).minimize(g_loss, var_list=theta_g)</div><div class="line">    opt_c = tf.train.RMSPropOptimizer(learning_rate=lr_critic).minimize(c_loss, var_list=theta_c)</div><div class="line">    </div><div class="line">    <span class="comment"># then pass those trainable variables to clip function</span></div><div class="line">    clipped_var_c = [tf.assign(var, tf.clip_by_value(var, clip_lower, clip_upper)) <span class="keyword">for</span> var <span class="keyword">in</span> theta_c]</div><div class="line">    <span class="comment"># wait until RMSPropOptimizer is done</span></div><div class="line">    <span class="keyword">with</span> tf.control_dependencies([opt_c]):</div><div class="line">        <span class="comment"># fetch the clipped variables and output as op</span></div><div class="line">        opt_c = tf.tuple(clipped_var_c)</div><div class="line">    <span class="keyword">return</span> opt_g, opt_c, z, real_data</div></pre></td></tr></table></figure>
</li>
<li><p>WGAN Algorithm Flow<br>照 paper 上的演算法 flow</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">wgan_train</span><span class="params">()</span>:</span></div><div class="line">    dataset = input_data.read_data_sets(<span class="string">"."</span>, one_hot=<span class="keyword">True</span>)</div><div class="line">    opt_g, opt_c, z, real_data = build_graph()</div><div class="line">    saver = tf.train.Saver()</div><div class="line">    config = tf.ConfigProto(allow_soft_placement=<span class="keyword">True</span>, log_device_placement=<span class="keyword">True</span>)</div><div class="line">    config.gpu_options.allow_growth = <span class="keyword">True</span></div><div class="line">    config.gpu_options.per_process_gpu_memory_fraction = <span class="number">0.8</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_feed_dict</span><span class="params">()</span>:</span></div><div class="line">        train_img = dataset.train.next_batch(batch_size)[<span class="number">0</span>]</div><div class="line">        train_img = <span class="number">2</span>*train_img<span class="number">-1</span></div><div class="line">        train_img = np.reshape(train_img, (<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>))</div><div class="line">        npad = ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>))</div><div class="line">        train_img = np.pad(train_img, pad_width=npad,</div><div class="line">                           mode=<span class="string">'constant'</span>, constant_values=<span class="number">-1</span>)</div><div class="line">        train_img = np.expand_dims(train_img, <span class="number">-1</span>)</div><div class="line">        batch_z = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, [batch_size, z_dim]).astype(np.float32)</div><div class="line">        feed_dict = &#123;real_data: train_img, z: batch_z&#125;</div><div class="line">        <span class="keyword">return</span> feed_dict</div><div class="line">    <span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</div><div class="line">        sess.run(tf.global_variables_initializer())</div><div class="line">        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iter_step):</div><div class="line">            print(<span class="string">"itr = "</span>,i)</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(c_iter):</div><div class="line">                feed_dict = next_feed_dict()</div><div class="line">                sess.run(opt_c, feed_dict=feed_dict)                </div><div class="line">            feed_dict = next_feed_dict()</div><div class="line">            sess.run(opt_g, feed_dict=feed_dict)</div><div class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">999</span>:</div><div class="line">                saver.save(sess, os.path.join(ckpt_dir, <span class="string">"model.ckpt"</span>), global_step=i)</div></pre></td></tr></table></figure>
</li>
<li><p>一點小結論<br>5.1. 上述架構沒有用 <strong>batch normalization</strong>，有用的話效果會好很多，生成器和鑑別器都可用。<br>5.2. 鑑別器換成其他 CNN 架構也可以。<br>5.3. MLP 架構也可以。</p>
</li>
</ol>
<p>整體來說，對於熟悉 tensorflow 的人來說不難實作 (剛好我不是很熟)，尤其 WGAN 從根本上做的改進，讓整個 training 很容易!<br>讓我們期待接下來的發展吧~</p>
<hr>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="external">GAN</a></li>
<li><a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="external">Wasserstein GAN</a>，作者的 <a href="https://github.com/martinarjovsky/WassersteinGAN" target="_blank" rel="external">github</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="external">令人拍案叫绝的Wasserstein GAN</a></li>
<li><a href="https://github.com/jiamings/wgan" target="_blank" rel="external">A Tensorflow Implementation of WGAN</a>: 使用 <code>tf.contrib.layers</code>，一個 higher level 的 API，比我現在的實作可以簡潔很多。</li>
<li><a href="http://ruishu.io/2016/12/27/batchnorm/" target="_blank" rel="external">A GENTLE GUIDE TO USING BATCH NORMALIZATION IN TENSORFLOW</a>: Batch Normalization, MLP, and CNN examples using tf.contrib.layers</li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/ML/" rel="tag"># ML</a>
          
            <a href="/tags/generative-model/" rel="tag"># generative model</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/03/16/WGAN-Part-1/" rel="next" title="WGAN Part 1: 先用 GAN 鋪梗">
                <i class="fa fa-chevron-left"></i> WGAN Part 1: 先用 GAN 鋪梗
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目錄
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            本站概覽
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.png"
               alt="Chih-Sheng Chen" />
          <p class="site-author-name" itemprop="name">Chih-Sheng Chen</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">文章</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">分類</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">標籤</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#前情提要"><span class="nav-number">1.</span> <span class="nav-text">前情提要</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Wasserstein-Earth-Mover-distance"><span class="nav-number">2.</span> <span class="nav-text">Wasserstein (Earth-Mover) distance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一個關鍵的好性質-Wasserstein-Earth-Mover-distance-處處連續可微"><span class="nav-number">3.</span> <span class="nav-text">一個關鍵的好性質: Wasserstein (Earth-Mover) distance 處處連續可微</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EM-distance-為目標函式所造成的不同"><span class="nav-number">4.</span> <span class="nav-text">EM distance 為目標函式所造成的不同</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#WGAN-演算法"><span class="nav-number">5.</span> <span class="nav-text">WGAN 演算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#WGAN-的優點"><span class="nav-number">6.</span> <span class="nav-text">WGAN 的優點</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#My-Questions"><span class="nav-number">7.</span> <span class="nav-text">My Questions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensorflow-範例測試"><span class="nav-number">8.</span> <span class="nav-text">Tensorflow 範例測試</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reference"><span class="nav-number">9.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chih-Sheng Chen</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 強力驅動
</div>

<div class="theme-info">
  主題 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  




	





  





  





  






  





  

  

  

  

</body>
</html>
