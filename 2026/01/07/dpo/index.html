<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-tw">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Reinforcement Learning,DPO (Direct Preference Optimization),SFT (Supervised Fine-Tuning),RLHF (Reinforcement Learning from Human Feedback)," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Pre-trained model ç¶“é SFT (Supervised Fine-Tuning) å†ç¶“é RLHF (Reinforcement Learning from Human Feedback) çš„é€™ä¸€å¥—çµ„åˆæ‹³åœ¨ 2023 å¹´å ªç¨±é¡¯å­¸
ç¶²è·¯ä¸Šç”šè‡³ç”¢ç”Ÿäº†ä¸€å€‹å¾ˆæœ‰åçš„ä¿®æ ¼æ–¯(Shoggoth)æ€ªç‰©æ¢—åœ–. (wiki: å½¢æ…‹ç„¡å®šçš„åŸç”Ÿè³ªç”Ÿç‰©ï¼Œæ˜¯å…‹è˜‡é­¯ç¥è©±ä¸­æœ€é§­äººçš„å­˜åœ¨ä¹‹ä¸€)
Elon">
<meta property="og:type" content="article">
<meta property="og:title" content="å‘Šåˆ¥ Reward Model: DPO å¦‚ä½•è®“ LLM åå¥½å°é½Šè®Šå¾—ç©©å®šä¸”é«˜æ•ˆ">
<meta property="og:url" content="https://bobondemon.github.io/2026/01/07/dpo/index.html">
<meta property="og:site_name" content="æ£’æ£’ç”Ÿ">
<meta property="og:description" content="Pre-trained model ç¶“é SFT (Supervised Fine-Tuning) å†ç¶“é RLHF (Reinforcement Learning from Human Feedback) çš„é€™ä¸€å¥—çµ„åˆæ‹³åœ¨ 2023 å¹´å ªç¨±é¡¯å­¸
ç¶²è·¯ä¸Šç”šè‡³ç”¢ç”Ÿäº†ä¸€å€‹å¾ˆæœ‰åçš„ä¿®æ ¼æ–¯(Shoggoth)æ€ªç‰©æ¢—åœ–. (wiki: å½¢æ…‹ç„¡å®šçš„åŸç”Ÿè³ªç”Ÿç‰©ï¼Œæ˜¯å…‹è˜‡é­¯ç¥è©±ä¸­æœ€é§­äººçš„å­˜åœ¨ä¹‹ä¸€)
Elon">
<meta property="og:image" content="https://bobondemon.github.io/2026/01/07/dpo/image.png">
<meta property="og:image" content="https://bobondemon.github.io/2026/01/07/dpo/image 1.png">
<meta property="og:updated_time" content="2026-01-07T12:00:49.186Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="å‘Šåˆ¥ Reward Model: DPO å¦‚ä½•è®“ LLM åå¥½å°é½Šè®Šå¾—ç©©å®šä¸”é«˜æ•ˆ">
<meta name="twitter:description" content="Pre-trained model ç¶“é SFT (Supervised Fine-Tuning) å†ç¶“é RLHF (Reinforcement Learning from Human Feedback) çš„é€™ä¸€å¥—çµ„åˆæ‹³åœ¨ 2023 å¹´å ªç¨±é¡¯å­¸
ç¶²è·¯ä¸Šç”šè‡³ç”¢ç”Ÿäº†ä¸€å€‹å¾ˆæœ‰åçš„ä¿®æ ¼æ–¯(Shoggoth)æ€ªç‰©æ¢—åœ–. (wiki: å½¢æ…‹ç„¡å®šçš„åŸç”Ÿè³ªç”Ÿç‰©ï¼Œæ˜¯å…‹è˜‡é­¯ç¥è©±ä¸­æœ€é§­äººçš„å­˜åœ¨ä¹‹ä¸€)
Elon">
<meta name="twitter:image" content="https://bobondemon.github.io/2026/01/07/dpo/image.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'åšä¸»'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://bobondemon.github.io/2026/01/07/dpo/"/>





  <title> å‘Šåˆ¥ Reward Model: DPO å¦‚ä½•è®“ LLM åå¥½å°é½Šè®Šå¾—ç©©å®šä¸”é«˜æ•ˆ | æ£’æ£’ç”Ÿ </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-tw">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">æ£’æ£’ç”Ÿ</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">è®“å­¸ç¿’è®Šæˆä¸€ç¨®ç¿’æ…£</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            é¦–é 
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            åˆ†é¡
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            é—œæ–¼
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            æ­¸æª”
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            æ¨™ç±¤
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://bobondemon.github.io/2026/01/07/dpo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chih-Sheng Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="æ£’æ£’ç”Ÿ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                å‘Šåˆ¥ Reward Model: DPO å¦‚ä½•è®“ LLM åå¥½å°é½Šè®Šå¾—ç©©å®šä¸”é«˜æ•ˆ
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">ç™¼è¡¨æ–¼</span>
              
              <time title="å‰µå»ºæ–¼" itemprop="dateCreated datePublished" datetime="2026-01-07T19:22:30+08:00">
                2026-01-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">åˆ†é¡æ–¼</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>[object Object]
            </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<hr>
<p>Pre-trained model ç¶“é SFT (Supervised Fine-Tuning) å†ç¶“é RLHF (Reinforcement Learning from Human Feedback) çš„é€™ä¸€å¥—çµ„åˆæ‹³åœ¨ 2023 å¹´å ªç¨±é¡¯å­¸</p>
<p>ç¶²è·¯ä¸Šç”šè‡³ç”¢ç”Ÿäº†ä¸€å€‹å¾ˆæœ‰åçš„<a href="https://x.com/anthrupad/status/1622349563922362368" target="_blank" rel="external">ä¿®æ ¼æ–¯(Shoggoth)æ€ªç‰©æ¢—åœ–</a>. (<a href="https://zh.wikipedia.org/zh-tw/%E4%BF%AE%E6%A0%BC%E6%96%AF" target="_blank" rel="external">wiki</a>: å½¢æ…‹ç„¡å®šçš„åŸç”Ÿè³ªç”Ÿç‰©ï¼Œæ˜¯å…‹è˜‡é­¯ç¥è©±ä¸­æœ€é§­äººçš„å­˜åœ¨ä¹‹ä¸€)</p>
<p>Elon Musk ä¹Ÿåœ¨ twiitter ç™¼äº†ä¸€å¼µå¸¶è‘—ç¬‘è‡‰é¢å…·çš„ä¿®æ ¼æ–¯å°äººé¡èªªè‘— [<a href="https://web.archive.org/web/20230223004549/https://twitter.com/elonmusk/status/1628491148124884992" target="_blank" rel="external">ref</a>]:</p>
<blockquote>
<p>â€œAs an AI language model, I have been trained to generate responses that are intended to be helpful, informative, and objectiveâ€¦â€</p>
</blockquote>
<p>ç¬¬ä¸€æ­¥çš„ pre-trained phase å°±æ˜¯å¤§åŠ›å‡ºå¥‡è¹Ÿç¡¬ train ä¸€ç™¼, æ¨¡å‹é•·å¾—åƒè§¸æ‰‹æ€ªçš„ä¿®æ ¼æ–¯èˆ¬, ä»€éº¼éƒ½æœ‰å¯èƒ½ä¹Ÿå¾ˆé›£æ§åˆ¶å…¶è¡Œç‚º (ç”¢ç”Ÿäººé¡è¦çš„ç¬‘è‡‰). å› æ­¤éœ€è¦ç¬¬äºŒéšæ®µçš„ SFT, è€Œé€™å¾ˆå¥½åš, åªè¦è—‰ç”±ç¯©é¸æˆ–äººå·¥ä»‹å…¥å¾—åˆ°æ¯”è¼ƒé«˜å“è³ªçš„è³‡æ–™å¾Œ, é‡æ–° supervised fine tuning ä¸€ä¸‹å°±çµæŸäº†.<br>ä¸éè‹¥è¦é”åˆ°æœ€çµ‚æ›´å¥½çš„æ•ˆæœæˆ–æ˜¯äººé¡å–œæ­¡çš„è¼¸å‡ºç­‰ç­‰, é‚„æ˜¯å¾—ä¾é æœ€å¾Œä¸€éšæ®µçš„ RLHF<br>ç›¸æ¯” SFT, RLHF å°è¶…åƒæ•¸æ•æ„Ÿè¨±å¤š, åŒæ™‚ RLHF å¿…é ˆä¾è³´ä¸€å€‹<em>äº‹å…ˆè¨“ç·´å¥½çš„ reward æ¨¡å‹</em>ä¾†ç‚ºæ¯å€‹ output tokens æ‰“åˆ†æ•¸, æ‰èƒ½è¨“ç·´ LLM, é€™ç¨®å…©éšæ®µè¨“ç·´ä¹Ÿæ˜¯æ¨¡å‹èª¿æ•´çš„ä¸€å¤§å›°é›£é»ä¹‹ä¸€.</p>
<p>2023 å¹´ Stanford åœ˜éšŠæå‡º <strong>DPO (Direct Preference Optimization)</strong> [<a href="https://arxiv.org/abs/2305.18290" target="_blank" rel="external">arxiv</a>]<br>æˆåŠŸå°‡ LLM åå¥½å°é½Šçš„å•é¡Œ<strong>å¾ RL è½‰æ›æˆ supervised training</strong>, ä¸€èˆ‰æ”¹å–„ RLHF çš„ç¼ºé»çš„åŒæ™‚, æ•ˆæœä»ç„¶å“è‘—.</p>
<blockquote>
<p>[PS]: ç”±æ–¼åŸæœ¬çš„<a href="https://x.com/anthrupad/status/1622349563922362368" target="_blank" rel="external">ä¿®æ ¼æ–¯(Shoggoth)æ€ªç‰©æ¢—åœ–</a>å¯¦åœ¨å¤ªéé®®æ˜, æˆ‘è®€å®Œ DPO æ–¹æ³•å¾Œ, å¾ˆæƒ³çŸ¥é“é€™å€‹æ¢—åœ–å¯ä»¥æ€éº¼æ”¹<br>ä»¥ä¸‹ç‚º <em>Gemini nano banana</em> ğŸŒ çš„ç‰ˆæœ¬:<br><img src="/2026/01/07/dpo/image.png" width="80%" height="80%"> æ²’æœ‰åŸåœ–é‚£éº¼æœ‰è¡æ“ŠåŠ›, ä½†å€’ä¹Ÿåƒå€‹èªªæ˜æ›¸èˆ¬åœ°åœ–ç¤ºäº†â€¦ ğŸ˜…</p>
</blockquote>
<p>å›åˆ°æ­£æ–‡, ç©¶ç«Ÿ DPO æ€éº¼å°‡ RL è®Š supervised training çš„? é€™æ•¸å­¸é­”è¡“æ€éº¼è®Šçš„?<br>å¸¶è‘—å¥½å¥‡å¿ƒè®“æˆ‘å€‘è®€ä¸‹å», ä¸éé‚„å¾—å…ˆå¾ RLHF èªªèµ·</p>
<a id="more"></a>
<hr>
<h2 id="å¾—å…ˆèªª-RLHF-æ‰è¡Œ"><a href="#å¾—å…ˆèªª-RLHF-æ‰è¡Œ" class="headerlink" title="å¾—å…ˆèªª RLHF æ‰è¡Œ"></a>å¾—å…ˆèªª RLHF æ‰è¡Œ</h2><p>çœ‹ä¸€ä¸‹ RLHF çš„ç›®æ¨™å‡½å¼<br><span>$$\begin{align}
J(\theta) = \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta} [\underbrace{r_\phi(x, y)}_{\text{1. Maximize Reward}} - \underbrace{\beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}}_{\text{2. KL Penalty}}]
\end{align}$$</span><!-- Has MathJax --> è¦æ±‚ <span>$\arg\max_\theta J(\theta)$</span><!-- Has MathJax -->.<br>å…¶ä¸­ <span>$(x,y)$</span><!-- Has MathJax --> åˆ†åˆ¥æ˜¯ input prompt ä»¥åŠ LLM çš„ output tokens; <span>$r_\phi(x,y)$</span><!-- Has MathJax --> æ˜¯åƒæ•¸ç‚º <span>$\phi$</span><!-- Has MathJax --> çš„ reward model, å…¶ç‰©ç†æ„ç¾©æ˜¯ä»£è¡¨ <span>$(x,y)$</span><!-- Has MathJax --> é€™æ¨£çš„ pair äººé¡ä¾†çœ‹æœ‰å¤šå°‘åˆ†, æ„ˆé«˜åˆ†æ„ˆå¥½.<br><span>$\pi_\theta(y|x)$</span><!-- Has MathJax --> å’Œ <span>$\pi_{ref}(y|x)$</span><!-- Has MathJax --> åˆ†åˆ¥æ˜¯æ¬²è¨“ç·´çš„æ¨¡å‹ (åƒæ•¸ <span>$\theta$</span><!-- Has MathJax -->) åŠåˆå§‹æ¨¡å‹ (é€™è£¡æŒ‡ç¶“é SFT å¾Œçš„æ¨¡å‹).</p>
<blockquote>
<p><span>$\pi$</span><!-- Has MathJax --> åœ¨ RL çš„è¡“èªå°±æ˜¯ policy model è€Œå¦‚æœå¥—ç”¨åœ¨ LLM æƒ…å¢ƒè£¡æ­£å¥½å°±æ˜¯èªè¨€æ¨¡å‹. å› ç‚º LLM æ˜¯ next token prediction, ç¿»è­¯åˆ° RL å°±æ˜¯ next action prediction</p>
</blockquote>
<p>å…¶å¯¦æ»¿ç›´è§€çš„, (1) ç¬¬ä¸€å€‹ term (Maximize Reward) æ˜¯åŸæœ¬å¼·åŒ–å­¸ç¿’ RL çš„ç›®æ¨™å‡½å¼, è«‹åƒè€ƒ <a href="https://bobondemon.github.io/2025/12/25/RL%E7%9A%84%E6%95%B8%E5%AD%B8%E5%8E%9F%E7%90%86/#Ch9-Policy-Gradient-Methods">[Ch9] Policy Gradient</a> çš„ metric expression 3.<br>è€Œ RL çš„æœ€çµ‚ç›®çš„å°±æ˜¯æ‰¾å‡ºèƒ½æœ€å¤§åŒ– expected total rewards çš„ policy model. å› æ­¤é€™è£¡ä¸€å€‹é‡é»æ˜¯, æˆ‘å€‘è¦æœ‰ä¸€å€‹ â€œ<em>äº‹å…ˆè¨“ç·´å¥½çš„</em>â€ reward model <span>$r_\phi(x,y)$</span><!-- Has MathJax -->.</p>
<h3 id="Bradley-Terry-æ¨¡å‹-Reward-Model-çš„è¨“ç·´"><a href="#Bradley-Terry-æ¨¡å‹-Reward-Model-çš„è¨“ç·´" class="headerlink" title="Bradley-Terry æ¨¡å‹: Reward Model çš„è¨“ç·´"></a>Bradley-Terry æ¨¡å‹: Reward Model çš„è¨“ç·´</h3><p>çµ¦ä¸€å€‹ pair <span>$(x,y)$</span><!-- Has MathJax --> è¦çµ¦å¹¾åˆ†å°äººä¾†èªªä¸åƒ…é›£ä¸”æ²’å€‹çµ±ä¸€æ¨™æº–. è€Œå¦‚æœè¦äººå°æ¯”å“ªå€‹ output tokens å¥½ (<span>$y_w$</span><!-- Has MathJax -->) æˆ–å£ (<span>$y_l$</span><!-- Has MathJax -->) å°±å®¹æ˜“å¤šäº†.<br>å› æ­¤èŠ±äº†ä¸€äº› cost å»æ¨™è¨˜å‡ºå¾ˆå¤šçš„<strong>åå¥½æ•¸æ“šé›†</strong> <span>$\{(x,y_w,y_l)\}$</span><!-- Has MathJax -->, é€™å°±æ˜¯ reward model <span>$r_\phi(x,y)$</span><!-- Has MathJax --> ä¾è³´çš„è¨“ç·´è³‡æ–™.<br>æœ‰äº†åå¥½æ•¸æ“šé›† <span>$\{(x,y_w,y_l)\}$</span><!-- Has MathJax --> è¦æ€éº¼è®Šæˆçµ¦ä¸€å€‹ pair <span>$(x,y)$</span><!-- Has MathJax --> è¦çµ¦å¹¾åˆ†å‘¢?<br>æœ€ç›´è¦ºçš„æƒ³æ³•å°±æ˜¯è¨“ç·´ä¸€å€‹æ¨¡å‹ <span>$r_\phi$</span><!-- Has MathJax --> å¸Œæœ› <span>$r_\phi(x,y_w)$</span><!-- Has MathJax --> æ¯” <span>$r_\phi(x,y_l)$</span><!-- Has MathJax --> æ„ˆé«˜åˆ†æ„ˆå¥½, é€™å°±æ˜¯ Bradley-Terry æ¨¡å‹çš„ç›®æ¨™å‡½å¼<br>å®šç¾© <span>$y_w$</span><!-- Has MathJax --> å‹é <span>$y_l$</span><!-- Has MathJax --> çš„æ©Ÿç‡å–æ±ºæ–¼å…©è€…çš„çå‹µå·®:<br><span>$$\begin{align}
P(y_w \succ y_l | x) = \sigma(r_\phi(x, y_w) - r_\phi(x, y_l))
\end{align}$$</span><!-- Has MathJax --> ç°¡å–®è¬›, åš MLE (Maximum Likelihood Estimation) æ‰¾å‡º <span>$\phi$</span><!-- Has MathJax --> èƒ½æœ€å¤§åŒ–æ©Ÿç‡å°±çµæŸäº†.</p>
<h3 id="Reward-Hacking"><a href="#Reward-Hacking" class="headerlink" title="Reward Hacking"></a>Reward Hacking</h3><p>å›åˆ° RLHF ç›®æ¨™å‡½å¼ (1), ç¬¬äºŒå€‹ term (KL Penalty) è¡¨æ˜è¨“ç·´çš„ LLM æ¨¡å‹, <span>$\pi_\theta(y|x)$</span><!-- Has MathJax -->, ä¸è¦è·Ÿåˆå§‹æ¨¡å‹, <span>$\pi_{ref}(y|x)$</span><!-- Has MathJax -->, å·®ç•°å¤ªå¤§.:<br><span>$$\begin{align*}
D_{KL}(\pi_\theta || \pi_{ref}) = \mathbb{E}_{y \sim \pi_\theta} \left[ \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)} \right]
\end{align*}$$</span><!-- Has MathJax --> ç‚ºä»€éº¼ä¸å¸Œæœ›å·®ç•°å¤ªå¤§?<br>å› ç‚º reward model <span>$r_\phi(x,y)$</span><!-- Has MathJax --> æ˜¯äº‹å…ˆè¨“ç·´å¥½çš„æ¨¡å‹, ä¹Ÿåªæ˜¯å€‹è¿‘ä¼¼, å¦‚æœèªªæ¨¡å‹è¨“ç·´å¾Œæœ‰äº›ç¼ºé™·, ä¾‹å¦‚å°å¾ˆå¤š emojio ç¬¦è™Ÿæœƒçµ¦å¾ˆé«˜çš„ reward, å‰‡åŸºæ–¼ RL è¿½æ±‚æœ€å¤§åŒ– reward çš„è¡Œç‚ºå°±æœƒç˜‹ç‹‚è¼¸å‡º emojio.<br>é€™ç¨®è¡Œç‚ºç¨±ç‚º <a href="https://en.wikipedia.org/wiki/Reward_hacking" target="_blank" rel="external">Reward Hacking</a>. å› æ­¤åŠ å€‹ KL ç´„æŸé …ä¾†å¸Œæœ›é¿å….</p>
<h3 id="RLHF-ç¸½çµå’Œç¼ºé»"><a href="#RLHF-ç¸½çµå’Œç¼ºé»" class="headerlink" title="RLHF ç¸½çµå’Œç¼ºé»"></a>RLHF ç¸½çµå’Œç¼ºé»</h3><p>åœ¨ DPO å‡ºç¾ä¹‹å‰, æˆ‘å€‘ä½¿ç”¨ RLHF ä¾†è®“æ¨¡å‹ç¬¦åˆäººé¡å–œå¥½. é€™æ˜¯ä¸€å€‹<strong>å…©éšæ®µ</strong>ä¸”ä¸ç©©å®šçš„éç¨‹:</p>
<ol>
<li><strong>è¨“ç·´ Reward Model (RM):</strong> æ”¶é›†æˆå°æ•¸æ“š <span>$(x, y_w, y_l)$</span><!-- Has MathJax -->, å…¶ä¸­ <span>$x$</span><!-- Has MathJax --> æ˜¯ prompt,  <span>$y_w$</span><!-- Has MathJax --> æ˜¯å‹å‡ºçš„å›ç­” (winner),  <span>$y_l$</span><!-- Has MathJax --> æ˜¯è½æ•—çš„å›ç­” (loser). è¨“ç·´ä¸€å€‹ç¥ç¶“ç¶²è·¯ <span>$r_\phi(x, y)$</span><!-- Has MathJax --> ä¾†é æ¸¬å“ªå€‹å›ç­”æ›´å¥½ (ä½¿ç”¨ Bradley-Terry æ¨¡å‹)<span>$$\begin{align*}
\mathcal{L}_{RM} = -\log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l))
\end{align*}$$</span><!-- Has MathJax --></li>
<li><strong>åŠ ä¸Š KL-divergence å„ªåŒ– Policy:</strong> å›ºå®š RM, åŠ ä¸Š KL-divergence æ›´æ–°èªè¨€æ¨¡å‹ <span>$\pi_\theta$</span><!-- Has MathJax -->ï¼Œæœ€å¤§åŒ–çå‹µä¸¦é˜²æ­¢æ¨¡å‹åé›¢åˆå§‹æ¨¡å‹ <span>$\pi_{ref}$</span><!-- Has MathJax --> å¤ªé </li>
</ol>
<p>ç¸½çµä¾†èªª RLHF æœ‰ä»¥ä¸‹ç¼ºé»:</p>
<ul>
<li>éœ€è¦åŒæ™‚ç¶­è­· <strong>3~4 å€‹æ¨¡å‹</strong>åœ¨è¨˜æ†¶é«”ä¸­ (Policy <span>$\pi_\theta(y|x)$</span><!-- Has MathJax -->, Reference <span>$\pi_{ref}(y|x)$</span><!-- Has MathJax -->, Reward Model <span>$r_\phi(x,y)$</span><!-- Has MathJax -->, <a href="https://bobondemon.github.io/2025/12/25/RL%E7%9A%84%E6%95%B8%E5%AD%B8%E5%8E%9F%E7%90%86/#Ch10-Actor-Critic-Methods">Critic value function in RL (å¦‚æœæœ‰çš„è©±)</a>)</li>
<li>ç›®æ¨™å‡½å¼ (1) å°è¶…åƒæ•¸æ¥µåº¦æ•æ„Ÿ, æˆ–è¨±è¦ä½¿ç”¨å¦‚ <a href="https://youtu.be/mg-iU-WxiNs?si=WRzy_mexotCm4H7F&amp;t=1008" target="_blank" rel="external">PPO-clip</a> ç­‰çš„åšæ³•ä¾†ç¶­æŒè¨“ç·´ç©©å®š</li>
<li>Reward Model æœ¬èº«ä¹Ÿæ˜¯å€‹è¿‘ä¼¼, å„ªåŒ–å®ƒå¯èƒ½æœƒå°è‡´ä¸Šé¢æåˆ°çš„ Reward Hacking ç¾è±¡</li>
</ul>
<hr>
<h2 id="DPO-Direct-Preference-Optimization-ç™»æ¿æ•‘æ´"><a href="#DPO-Direct-Preference-Optimization-ç™»æ¿æ•‘æ´" class="headerlink" title="DPO (Direct Preference Optimization) ç™»æ¿æ•‘æ´"></a>DPO (Direct Preference Optimization) ç™»æ¿æ•‘æ´</h2><p><strong>DPO æ˜¯ä¸€ç¨®å°‡ã€Œäººé¡åå¥½å°é½Š (Alignment)ã€è¦–ç‚ºã€Œåˆ†é¡å•é¡Œã€è€Œéã€Œå¼·åŒ–å­¸ç¿’å•é¡Œã€çš„æ–¹æ³•.</strong>  å®ƒåœ¨ 2023 å¹´ç”± Stanford åœ˜éšŠæå‡º [<a href="https://arxiv.org/abs/2305.18290" target="_blank" rel="external">arxiv</a>], è¿…é€Ÿæˆç‚ºå¾®èª¿ LLM (å¦‚ Llama-3, Mistral) çš„ä¸»æµæ–¹æ³•<br>è½‰æ›æˆå–®ç´”çš„ç›£ç£å¼å­¸ç¿’å¥½è™•æ˜¯é¿é–‹ä½¿ç”¨ RL <strong>å…©éšæ®µ</strong>ä¸”ä¸ç©©å®šçš„éç¨‹, ä¸”ç„¡é ˆåŒæ™‚ç¶­è­· 3~4 å€‹æ¨¡å‹åœ¨è¨˜æ†¶é«”ä¸­, åˆå› ç‚ºä¸ä½¿ç”¨ RL è‡ªç„¶ä¹Ÿå°±æ²’æœ‰ Reward Hacking problem.<br>è½èµ·ä¾†å¤ªç¥å¥‡äº†å§? ä¾†ä¸€æ­¥æ­¥æ‹†è§£æ€éº¼è¾¦åˆ°çš„.<br>å›é¡§ç›®æ¨™å‡½æ•¸ (1) (ç‚ºäº†ç°¡åŒ–ç¬¦è™Ÿ, çœç•¥æœŸæœ›å€¼ <span>$\mathbb{E}_x$</span><!-- Has MathJax -->, <span>$r_\phi$</span><!-- Has MathJax --> çš„ <span>$\phi$</span><!-- Has MathJax -->, <span>$\pi_\theta$</span><!-- Has MathJax -->  çš„ <span>$\theta$</span><!-- Has MathJax -->)<br><span>$$\begin{align}
\max_{\pi}J(\pi) \doteq \max_\pi \mathbb{E}_{y \sim \pi} \left[ r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{ref}(y|x)} \right]
\end{align}$$</span><!-- Has MathJax --> é€™å€‹å½¢å¼å…¶å¯¦å¯ä»¥é‡å¯«, å°å…¬å¼å…§çš„é …é€²è¡Œä»£æ•¸è®Šæ›:<br><span>$$\begin{aligned}
\therefore J(\pi) &amp;= \sum_y \pi(y|x) \left( r(x, y) - \beta \log \pi(y|x) + \beta \log \pi_{ref}(y|x) \right) \\
&amp;= \beta \sum_y \pi(y|x) \left( \frac{r(x, y)}{\beta} + \log \pi_{ref}(y|x) - \log \pi(y|x) \right) \\
&amp;= \beta \sum_y \pi(y|x) \log \left( \frac{\pi_{ref}(y|x) \exp(\frac{r(x, y)}{\beta})}{\pi(y|x)} \right)
\end{aligned}$$</span><!-- Has MathJax --> é€™å…¶å¯¦å°±æ˜¯è² çš„ KL-divergence. ç‚ºäº†è®“é€™è£¡é¢çš„ <span>$\pi_{ref}(y|x) \exp(\dots)$</span><!-- Has MathJax --> è®Šæˆä¸€å€‹åˆæ³•çš„æ©Ÿç‡åˆ†ä½ˆ, æˆ‘å€‘éœ€è¦é™¤ä»¥ä¸€å€‹æ­¸ä¸€åŒ–å¸¸æ•¸ (Partition Function) <span>$Z(x)$</span><!-- Has MathJax -->.<br>æ ¹æ“š KL-divergence æ€§è³ª, æˆ‘å€‘çŸ¥é“ if and only if <span>$\pi$</span><!-- Has MathJax --> ç­‰æ–¼ä»¥ä¸‹åˆ†ä½ˆæ™‚, ä¸Šè¿°ç›®æ¨™å‡½æ•¸é”åˆ°æœ€å¤§å€¼:<br><span>$$\begin{align}
\pi^*(y|x) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left( \frac{r(x, y)}{\beta} \right)
\end{align}$$</span><!-- Has MathJax --> å…¶ä¸­ <span>$Z(x) = \sum_y \pi_{ref}(y|x) \exp\left( \frac{r(x, y)}{\beta} \right)$</span><!-- Has MathJax -->.<br>æ‰€ä»¥çµ¦å®šä¸€å€‹ reward <span>$r$</span><!-- Has MathJax --> æˆ‘å€‘çŸ¥é“å°æ‡‰é€™å€‹ <span>$r$</span><!-- Has MathJax --> çš„æœ€ä½³ policy <span>$\pi^*$</span><!-- Has MathJax --> (ä½¿å¾— (3) é”åˆ°æœ€å¤§) é•·æ€æ¨£<br>åéä¾†èªª, çµ¦ä¸€å€‹ç­–ç•¥ <span>$\pi^*$</span><!-- Has MathJax -->, <em>è®“é€™å€‹ç­–ç•¥æˆç‚ºæœ€ä½³ç­–ç•¥</em>çš„é‚£å€‹ reward <span>$r$</span><!-- Has MathJax --> æ˜¯ä¸æ˜¯ä¹Ÿæ‰¾çš„åˆ°? å¯ä»¥çš„<br>ä»£æ•¸åæ¨ä¸€ä¸‹ (4) å¾—åˆ°<br><span>$$\begin{align}
\Longrightarrow \log \pi^*(y|x) = -\log Z(x) + \log \pi_{ref}(y|x) + \frac{1}{\beta} r(x, y) \\
\Longrightarrow r(x, y) = \beta \log \frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)
\end{align}$$</span><!-- Has MathJax --> ç¾åœ¨å›åˆ°æˆ‘å€‘çš„åå¥½æ•¸æ“šé›†, æˆ‘å€‘æœ‰ <span>$\{(x, y_w, y_l)\}$</span><!-- Has MathJax -->.  Bradley-Terry æ¨¡å‹ (å¼ (2)) å‘Šè¨´æˆ‘å€‘, <span>$y_w$</span><!-- Has MathJax --> å‹é <span>$y_l$</span><!-- Has MathJax --> çš„æ©Ÿç‡å–æ±ºæ–¼å…©è€…çš„çå‹µå·®:<br><span>$P(y_w \succ y_l | x) = \sigma(r(x, y_w) - r(x, y_l))$</span><!-- Has MathJax --> ç¾åœ¨æŠŠ <span>$r$</span><!-- Has MathJax --> å¼ (6) ä»£å…¥é€™å€‹ <span>$r(x, y_w) - r(x, y_l)$</span><!-- Has MathJax --> ä¸­ï¼š<br><span>$$\begin{align}
r(x, y_w) - r(x, y_l) &amp;= \left( \beta \log \frac{\pi^*(y_w|x)}{\pi_{ref}(y_w|x)} + \beta \log Z(x) \right) - \left( \beta \log \frac{\pi^*(y_l|x)}{\pi_{ref}(y_l|x)} + \beta \log Z(x) \right) \\
&amp;= \beta \log \frac{\pi^*(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi^*(y_l|x)}{\pi_{ref}(y_l|x)}
\end{align}$$</span><!-- Has MathJax --> <span>$\beta \log Z(x)$</span><!-- Has MathJax --> å› ç‚ºè·Ÿ <span>$y$</span><!-- Has MathJax --> ç„¡é—œ, æŠµéŠ·æ‰äº†.<br>ç¾åœ¨, æˆ‘å€‘æŠŠç†è«–ä¸Šçš„ <span>$\pi^*$</span><!-- Has MathJax --> æ›¿æ›æˆæˆ‘å€‘æ­£åœ¨è¨“ç·´çš„ç¥ç¶“ç¶²è·¯ <span>$\pi_\theta$</span><!-- Has MathJax -->. æˆ‘å€‘çš„ç›®æ¨™æ˜¯æœ€å¤§åŒ–åå¥½æ•¸æ“šçš„ Log Likelihood (ä¹Ÿå°±æ˜¯æœ€å°åŒ– Negative Log Likelihood):<br><span>$$\begin{align}
\mathcal{L}_{DPO}(\pi_\theta; \pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \right) \right]
\end{align}$$</span><!-- Has MathJax --> é€™å°±æ˜¯æœ€çµ‚ DPO çš„ loss ğŸ‘ğŸ»<br>ç¸½çµä¾†èªª, <strong>å„ªåŒ– DPO loss (9) ä½¿å¾—æˆ‘å€‘èƒ½ç›¡é‡å°åå¥½æ•¸æ“šé›†ä¸­çš„ <span>$y_w$</span><!-- Has MathJax --> å‹é <span>$y_l$</span><!-- Has MathJax --> çš„æ©Ÿç‡æ„ˆé«˜æ„ˆå¥½</strong>.<br>è€Œ (9) åˆæ˜¯åŸºæ–¼ <span>$\pi$</span><!-- Has MathJax --> èˆ‡ <span>$r$</span><!-- Has MathJax --> çš„ closed form solution é—œä¿‚ (å¼ (4)&amp;(6)) æ¨å°å‡ºä¾†çš„, <strong>è©²é—œä¿‚ç›´æ¥ä¿è­‰äº† RL çš„ç›®æ¨™å‡½å¼ (3) é”åˆ°æœ€å¤§</strong>.</p>
<blockquote>
<p><a href="https://arxiv.org/abs/2305.18290" target="_blank" rel="external">è«–æ–‡</a>é‚„æœ‰è¨±å¤šå…§å®¹, åŒ…å« â€œTheoretical Analysis of DPOâ€ æ®µè½, æœ‰èˆˆè¶£å¯ä»¥æ·±å…¥é–±è®€</p>
</blockquote>
<hr>
<h2 id="Appendix-1-æˆå°æ¯”è¼ƒ-V-S-å–®é»å›é¥‹"><a href="#Appendix-1-æˆå°æ¯”è¼ƒ-V-S-å–®é»å›é¥‹" class="headerlink" title="Appendix 1: æˆå°æ¯”è¼ƒ V.S. å–®é»å›é¥‹"></a>Appendix 1: <strong>æˆå°æ¯”è¼ƒ V.S. å–®é»å›é¥‹</strong></h2><p>åå¥½æ•¸æ“šé›† <span>$\{(x,y_w,y_l)\}$</span><!-- Has MathJax --> éœ€è¦çµ¦å…©å€‹å›ç­”ä¾†è®“äººå€‘æ¯”è¼ƒå„ªåŠ£, ç›¸ä¿¡åœ¨ 2024 å¹´çš„æ™‚å€™ä½¿ç”¨é AI çš„äººéƒ½æœ‰éé‚£ç¨®ç¶“é©—:<br>å…©å€‹å›ç­”è½è½é•·, çœ‹å®Œéƒ½ç´¯äº†å“ªæœ‰èƒ½åŠ›è©•åˆ†å¥½å£ ğŸ¤®. æˆ–æ˜¯å…©å€‹å›ç­”éƒ½å·®ä¸å¤š, ä¹Ÿä¸æƒ³ä»”ç´°è©•åƒ¹èª°å¥½èª°å£.<br>åœ¨ 2025 å¹´çš„æ™‚å€™, ç­†è€…è‡ªå·±çš„ç¶“é©—å°±æ¯”è¼ƒå°‘çœ‹åˆ°é€™ç¨®<strong>æˆå°æ¯”è¼ƒ</strong>çš„å›é¥‹, å–è€Œä»£ä¹‹çš„æ˜¯é‡å°<strong>å–®å€‹å›ç­”</strong>çµ¦ thumbs up/down:<br><img src="/2026/01/07/dpo/image 1.png" width="20%" height="20%"> å¦‚æœæ•¸æ“šé›†æ”¹æˆé€™æ¨£åˆè©²æ€éº¼åš DPO å‘¢? çœ‹èµ·ä¾†ç„¡æ³•å¥—ç”¨ Bradley-Terry æ¨¡å‹äº†<br>åŸºæ–¼é€™å€‹å¥½å¥‡å¿ƒ, æˆ‘è©¢å•äº† Gemini:</p>
<blockquote>
<p>ç›®å‰ RLHF é ˜åŸŸå¾ <strong>Pairwise (æˆå°æ¯”è¼ƒ)</strong> è½‰å‘ <strong>Pointwise (å–®é»/ä¸€å…ƒå›é¥‹)</strong> çš„è¶¨å‹¢<br>ç°¡å–®çš„å›ç­”æ˜¯ï¼šå¦‚æœæœ‰å–®é»çš„ Thumbs Up/Down è³‡æ–™ï¼Œæˆ‘å€‘ä¸èƒ½ç›´æ¥ã€Œç¡¬å¥—ã€æ¨™æº–çš„ Bradley-Terry æ¨¡å‹ï¼ˆå› ç‚ºå®ƒæ•¸å­¸ä¸Šå°±æ˜¯è¨­è¨ˆçµ¦ A vs B çš„ï¼‰ï¼Œä½†æ˜¯æˆ‘å€‘æœ‰å°ˆé–€é‡å°é€™ç¨®è³‡æ–™çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæœ€è‘—åçš„å°±æ˜¯ KTO (Kahneman-Tversky Optimization)ã€‚<br>â€¦ (ç•¥)</p>
</blockquote>
<p>AI çµ¦å‡ºçš„åƒè€ƒè«–æ–‡æ˜¯é€™ç¯‡: KTO (Kahneman-Tversky Optimization) [<a href="https://arxiv.org/abs/2402.01306" target="_blank" rel="external">arxiv</a>]<br>éå¸¸æ¦‚ç•¥çš„è¬›, å®ƒä¸å†è¨ˆç®— <span>$r(y_w) - r(y_l)$</span><!-- Has MathJax -->, è€Œæ˜¯å°‡ç›®æ¨™å‡½æ•¸æ‹†é–‹, å®šç¾©äº†ä¸€å€‹ <strong>ã€Œåƒè€ƒé» (Reference Point)ã€</strong></p>
<ul>
<li><strong>å°æ–¼ Thumbs Up (Desirable) çš„è³‡æ–™:</strong> æˆ‘å€‘å¸Œæœ›å®ƒçš„ Implicit Reward é«˜æ–¼æŸå€‹åƒè€ƒå€¼</li>
<li><strong>å°æ–¼ Thumbs Down (Undesirable) çš„è³‡æ–™:</strong> æˆ‘å€‘å¸Œæœ›å®ƒçš„ Implicit Reward ä½æ–¼æŸå€‹åƒè€ƒå€¼</li>
</ul>
<p>KTO çš„ Loss çœ‹èµ·ä¾†å¾ˆåƒæŠŠ DPO æ‹†æˆå…©åŠ:<br><span>$$\begin{align*}
L_{KTO} \approx \underbrace{\mathbb{E}_{x, y \sim D_{good}} [\text{Loss}(\text{Reward}(x,y) &gt; \text{Target})]}_{\text{é¼“å‹µå¥½å›ç­”}} + \lambda \underbrace{\mathbb{E}_{x, y \sim D_{bad}} [\text{Loss}(\text{Reward}(x,y) &lt; \text{Target})]}_{\text{æ‡²ç½°å£å›ç­”}}
\end{align*}$$</span><!-- Has MathJax --> è©³ç´°å°±è¦å»è®€è«–æ–‡äº†, å…ˆå¤§è‡´äº†è§£åˆ°é€™.</p>
<hr>
<h2 id="Appendix-2-DPO-å»¶ä¼¸æ”¹é€²"><a href="#Appendix-2-DPO-å»¶ä¼¸æ”¹é€²" class="headerlink" title="Appendix 2: DPO å»¶ä¼¸æ”¹é€²"></a>Appendix 2: DPO å»¶ä¼¸æ”¹é€²</h2><p>DPO ä»é¢è‡¨é‚£äº›ç¼ºé»? ä»¥åŠæœ‰å“ªäº›æ–¹æ³•æå‡ºä¾†æ”¹é€²? å¯è‡ªè¡ŒæŸ¥é–±ç›¸é—œè³‡æ–™, ä¹Ÿæ»¿æœ‰è¶£çš„.<br>è«‹ Gemini å¹«å¿™çµ¦å€‹åƒè€ƒå¦‚ä¸‹:</p>
<table>
<thead>
<tr>
<th><strong>å•é¡Œé»</strong></th>
<th><strong>è§£æ±ºæ–¹æ¡ˆ</strong></th>
<th><strong>æ ¸å¿ƒæ€è·¯</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>éåº¦æ‡²ç½° Loser</strong></td>
<td><strong>IPO</strong></td>
<td>ä½¿ç”¨ MSE Loss + Marginï¼Œä¸å†ä½¿ç”¨ Sigmoid å¼·æ¨æ¥µé™ã€‚</td>
</tr>
<tr>
<td><strong>é¡¯å­˜ä¸å¤  (OOM)</strong></td>
<td><strong>SimPO</strong></td>
<td>ä¸Ÿæ‰ Reference Modelï¼Œæ”¹ç”¨é•·åº¦æ­¸ä¸€åŒ–ã€‚</td>
</tr>
<tr>
<td><strong>æ•¸æ“šæœ‰å™ªè²</strong></td>
<td><strong>cDPO / rDPO</strong></td>
<td>å¼•å…¥ Label Smoothing æˆ–é–¾å€¼ï¼Œè®Šå¾—æ›´ä¿å®ˆã€‚</td>
</tr>
<tr>
<td><strong>æ¨ç†èƒ½åŠ›è®Šå·®</strong></td>
<td><strong>Iterative DPO</strong></td>
<td>åƒ PPO ä¸€æ¨£å¤šè¼ªè¿­ä»£ï¼Œè‡ªå·±ç”Ÿæˆæ•¸æ“šè‡ªå·±å­¸ã€‚</td>
</tr>
<tr>
<td><strong>æµç¨‹å¤ªæ…¢</strong></td>
<td><strong>ORPO</strong></td>
<td>SFT + Alignment åˆäºŒç‚ºä¸€ï¼Œä¸éœ€è¦ Ref Modelã€‚</td>
</tr>
</tbody>
</table>
<p><strong>æ‚¨çš„ä¸‹ä¸€æ­¥å»ºè­°ï¼š</strong><br>å¦‚æœæ‚¨æ˜¯è‡ªå·±åœ¨åšå¯¦é©—æˆ–å¾®èª¿å°æ¨¡å‹ï¼ˆå¦‚ Mistral-7B, Llama-3-8Bï¼‰ï¼š</p>
<ol>
<li><strong>è³‡æºå—é™</strong>ï¼šå¼·çƒˆæ¨è–¦è©¦è©¦ <strong>ORPO</strong> æˆ– <strong>SimPO</strong>ï¼Œçœé¡¯å­˜ä¸”æ•ˆæœå¥½ã€‚</li>
<li><strong>è¿½æ±‚æ¥µè‡´æ•ˆæœ</strong>ï¼šå¦‚æœæ˜¯åšæ•¸å­¸æˆ–é‚è¼¯ä»»å‹™ï¼Œå»ºè­°ä½¿ç”¨ <strong>Iterative DPO</strong>ï¼ˆå‰ææ˜¯æ‚¨æœ‰ä¸€å€‹å¥½çš„ Reward Model æˆ– GPT-4 ç•¶è£åˆ¤ï¼‰ã€‚</li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>Post authorï¼š</strong>
      Chih-Sheng Chen
    </li>
    <li class="post-copyright-link">
      <strong>Post linkï¼š</strong>
      <a href="https://bobondemon.github.io/2026/01/07/dpo/" title="å‘Šåˆ¥ Reward Model: DPO å¦‚ä½•è®“ LLM åå¥½å°é½Šè®Šå¾—ç©©å®šä¸”é«˜æ•ˆ">https://bobondemon.github.io/2026/01/07/dpo/</a>
    </li>
    <li class="post-copyright-license">
      <strong>Copyright Noticeï¼š </strong>
      All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
          
            <a href="/tags/DPO-Direct-Preference-Optimization/" rel="tag"># DPO (Direct Preference Optimization)</a>
          
            <a href="/tags/SFT-Supervised-Fine-Tuning/" rel="tag"># SFT (Supervised Fine-Tuning)</a>
          
            <a href="/tags/RLHF-Reinforcement-Learning-from-Human-Feedback/" rel="tag"># RLHF (Reinforcement Learning from Human Feedback)</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2025/12/25/RLçš„æ•¸å­¸åŸç†/" rel="next" title="RLçš„æ•¸å­¸åŸç†: è¶™ä¸–éˆºèª²ç¨‹æ¿ƒç¸®ç­†è¨˜">
                <i class="fa fa-chevron-left"></i> RLçš„æ•¸å­¸åŸç†: è¶™ä¸–éˆºèª²ç¨‹æ¿ƒç¸®ç­†è¨˜
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            æ–‡ç« ç›®éŒ„
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            æœ¬ç«™æ¦‚è¦½
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.png"
               alt="Chih-Sheng Chen" />
          <p class="site-author-name" itemprop="name">Chih-Sheng Chen</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">115</span>
                <span class="site-state-item-name">æ–‡ç« </span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">åˆ†é¡</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">241</span>
                <span class="site-state-item-name">æ¨™ç±¤</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#å¾—å…ˆèªª-RLHF-æ‰è¡Œ"><span class="nav-number">1.</span> <span class="nav-text">å¾—å…ˆèªª RLHF æ‰è¡Œ</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Bradley-Terry-æ¨¡å‹-Reward-Model-çš„è¨“ç·´"><span class="nav-number">1.1.</span> <span class="nav-text">Bradley-Terry æ¨¡å‹: Reward Model çš„è¨“ç·´</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reward-Hacking"><span class="nav-number">1.2.</span> <span class="nav-text">Reward Hacking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RLHF-ç¸½çµå’Œç¼ºé»"><span class="nav-number">1.3.</span> <span class="nav-text">RLHF ç¸½çµå’Œç¼ºé»</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DPO-Direct-Preference-Optimization-ç™»æ¿æ•‘æ´"><span class="nav-number">2.</span> <span class="nav-text">DPO (Direct Preference Optimization) ç™»æ¿æ•‘æ´</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Appendix-1-æˆå°æ¯”è¼ƒ-V-S-å–®é»å›é¥‹"><span class="nav-number">3.</span> <span class="nav-text">Appendix 1: æˆå°æ¯”è¼ƒ V.S. å–®é»å›é¥‹</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Appendix-2-DPO-å»¶ä¼¸æ”¹é€²"><span class="nav-number">4.</span> <span class="nav-text">Appendix 2: DPO å»¶ä¼¸æ”¹é€²</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chih-Sheng Chen</span>
</div>


<div class="powered-by">
  ç”± <a class="theme-link" href="https://hexo.io">Hexo</a> å¼·åŠ›é©…å‹•
</div>

<div class="theme-info">
  ä¸»é¡Œ -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>




<span id="busuanzi_container_site_pv">
  æœ¬ç«™ç¸½ç€è¦½ <span id="busuanzi_value_site_pv"></span> æ¬¡
</span>
<span id="busuanzi_container_site_uv">
  æœ¬ç«™è¨ªå®¢ <span id="busuanzi_value_site_uv"></span> äºº
</span>

        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      [object Object]
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      [object Object]
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  




	





  





  





  






  





  

  

  

  

</body>
</html>
