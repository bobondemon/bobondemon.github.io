<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-tw">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Reinforcement Learning,TD Learning,Q Learning,DQN,Actor Critic," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="ğŸ“” ç­†è¨˜å…§å®¹: èµµä¸–é’°è€å¸« â€œå¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†â€ èª²ç¨‹ (ä¹Ÿæ¨è–¦ç”¨ Notionç‰ˆæœ¬ ä¾†é–±è®€)å¾ˆé«˜èˆˆæˆ‘åˆæ¬¡å­¸ç¿’ RL å°±æ˜¯é€éé€™é–€èª², è®“æˆ‘æ‰å¯¦ç†è§£å…¶èƒŒå¾Œçš„æ•¸å­¸å’Œé‚è¼¯è€å¯¦èªª RL æ–¹æ³•å¾ˆå¤šå¾ˆé›œ, å¥—å¥è¶™è€å¸«çš„è©±, RL æ•¸å­¸å¾ˆæ·±, çµæ§‹æ€§åˆå¾ˆå¼·, ä¸€ç’°æ‰£ä¸€ç’°å¦‚æœæ²’æœ‰é€™å ‚èª²é€™æ¨£å¾ªåºå‰–æå’Œå¾æ•¸å­¸å‡ºç™¼è§£é‡‹çš„è©±, æˆ‘è‡ªå·±æ‡‰è©²å¾ˆé›£å…¥é–€, è¬è¬é€™é–€èª²çš„èµµä¸–é’°è€å¸«! ğŸ™ğŸ»
æœ¬ç¯‡ç­†è¨˜æ–¹å¼ç›¡é‡æ¿ƒç¸®,">
<meta property="og:type" content="article">
<meta property="og:title" content="RLçš„æ•¸å­¸åŸç†: è¶™ä¸–éˆºèª²ç¨‹æ¿ƒç¸®ç­†è¨˜">
<meta property="og:url" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/index.html">
<meta property="og:site_name" content="æ£’æ£’ç”Ÿ">
<meta property="og:description" content="ğŸ“” ç­†è¨˜å…§å®¹: èµµä¸–é’°è€å¸« â€œå¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†â€ èª²ç¨‹ (ä¹Ÿæ¨è–¦ç”¨ Notionç‰ˆæœ¬ ä¾†é–±è®€)å¾ˆé«˜èˆˆæˆ‘åˆæ¬¡å­¸ç¿’ RL å°±æ˜¯é€éé€™é–€èª², è®“æˆ‘æ‰å¯¦ç†è§£å…¶èƒŒå¾Œçš„æ•¸å­¸å’Œé‚è¼¯è€å¯¦èªª RL æ–¹æ³•å¾ˆå¤šå¾ˆé›œ, å¥—å¥è¶™è€å¸«çš„è©±, RL æ•¸å­¸å¾ˆæ·±, çµæ§‹æ€§åˆå¾ˆå¼·, ä¸€ç’°æ‰£ä¸€ç’°å¦‚æœæ²’æœ‰é€™å ‚èª²é€™æ¨£å¾ªåºå‰–æå’Œå¾æ•¸å­¸å‡ºç™¼è§£é‡‹çš„è©±, æˆ‘è‡ªå·±æ‡‰è©²å¾ˆé›£å…¥é–€, è¬è¬é€™é–€èª²çš„èµµä¸–é’°è€å¸«! ğŸ™ğŸ»
æœ¬ç¯‡ç­†è¨˜æ–¹å¼ç›¡é‡æ¿ƒç¸®,">
<meta property="og:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/image.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/Bellman_equations.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/value_and_policy_iteration_algo.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 3.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/unifiy_view_of_TD_algo.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 6.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 7.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 9.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 10.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 11.png">
<meta property="og:updated_time" content="2025-12-25T07:41:42.070Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RLçš„æ•¸å­¸åŸç†: è¶™ä¸–éˆºèª²ç¨‹æ¿ƒç¸®ç­†è¨˜">
<meta name="twitter:description" content="ğŸ“” ç­†è¨˜å…§å®¹: èµµä¸–é’°è€å¸« â€œå¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†â€ èª²ç¨‹ (ä¹Ÿæ¨è–¦ç”¨ Notionç‰ˆæœ¬ ä¾†é–±è®€)å¾ˆé«˜èˆˆæˆ‘åˆæ¬¡å­¸ç¿’ RL å°±æ˜¯é€éé€™é–€èª², è®“æˆ‘æ‰å¯¦ç†è§£å…¶èƒŒå¾Œçš„æ•¸å­¸å’Œé‚è¼¯è€å¯¦èªª RL æ–¹æ³•å¾ˆå¤šå¾ˆé›œ, å¥—å¥è¶™è€å¸«çš„è©±, RL æ•¸å­¸å¾ˆæ·±, çµæ§‹æ€§åˆå¾ˆå¼·, ä¸€ç’°æ‰£ä¸€ç’°å¦‚æœæ²’æœ‰é€™å ‚èª²é€™æ¨£å¾ªåºå‰–æå’Œå¾æ•¸å­¸å‡ºç™¼è§£é‡‹çš„è©±, æˆ‘è‡ªå·±æ‡‰è©²å¾ˆé›£å…¥é–€, è¬è¬é€™é–€èª²çš„èµµä¸–é’°è€å¸«! ğŸ™ğŸ»
æœ¬ç¯‡ç­†è¨˜æ–¹å¼ç›¡é‡æ¿ƒç¸®,">
<meta name="twitter:image" content="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/image.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'åšä¸»'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/"/>





  <title> RLçš„æ•¸å­¸åŸç†: è¶™ä¸–éˆºèª²ç¨‹æ¿ƒç¸®ç­†è¨˜ | æ£’æ£’ç”Ÿ </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-tw">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">æ£’æ£’ç”Ÿ</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">è®“å­¸ç¿’è®Šæˆä¸€ç¨®ç¿’æ…£</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            é¦–é 
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            åˆ†é¡
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            é—œæ–¼
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            æ­¸æª”
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            æ¨™ç±¤
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chih-Sheng Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="æ£’æ£’ç”Ÿ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                RLçš„æ•¸å­¸åŸç†: è¶™ä¸–éˆºèª²ç¨‹æ¿ƒç¸®ç­†è¨˜
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">ç™¼è¡¨æ–¼</span>
              
              <time title="å‰µå»ºæ–¼" itemprop="dateCreated datePublished" datetime="2025-12-25T10:37:34+08:00">
                2025-12-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">åˆ†é¡æ–¼</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>[object Object]
            </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<hr>
<p>ğŸ“” ç­†è¨˜å…§å®¹: <strong>èµµä¸–é’°è€å¸« â€œ<a href="https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" target="_blank" rel="external">å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†</a>â€</strong> èª²ç¨‹ (ä¹Ÿæ¨è–¦ç”¨ <a href="https://bobondemon.notion.site/notes-mathematical-foundation-of-reinforcement-learning" target="_blank" rel="external">Notionç‰ˆæœ¬</a> ä¾†é–±è®€)<br>å¾ˆé«˜èˆˆæˆ‘åˆæ¬¡å­¸ç¿’ RL å°±æ˜¯é€éé€™é–€èª², è®“æˆ‘æ‰å¯¦ç†è§£å…¶èƒŒå¾Œçš„æ•¸å­¸å’Œé‚è¼¯<br>è€å¯¦èªª RL æ–¹æ³•å¾ˆå¤šå¾ˆé›œ, å¥—å¥è¶™è€å¸«çš„è©±, RL æ•¸å­¸å¾ˆæ·±, çµæ§‹æ€§åˆå¾ˆå¼·, ä¸€ç’°æ‰£ä¸€ç’°<br>å¦‚æœæ²’æœ‰é€™å ‚èª²é€™æ¨£<em>å¾ªåºå‰–æ</em>å’Œå¾<em>æ•¸å­¸å‡ºç™¼</em>è§£é‡‹çš„è©±, æˆ‘è‡ªå·±æ‡‰è©²å¾ˆé›£å…¥é–€, è¬è¬é€™é–€èª²çš„èµµä¸–é’°è€å¸«! ğŸ™ğŸ»</p>
<p>æœ¬ç¯‡ç­†è¨˜æ–¹å¼ç›¡é‡æ¿ƒç¸®, è€Œæ¯å€‹ç« ç¯€æ›´è©³ç´°çš„ç­†è¨˜åƒè€ƒ:<br>&emsp;$\circ$ <a href="https://bobondemon.notion.site/RL-1-Fundamentals-of-Reinforcement-Learning-196edc3d531d801d8c73d0d771abb3f9?source=copy_link" target="_blank" rel="external">RL(1): Fundamentals of Reinforcement Learning</a><br>&emsp;$\circ$ <a href="https://bobondemon.notion.site/RL-2-Sample-based-Learning-Methods-1c7edc3d531d80ecaaa5cd6e4a8b55f7?source=copy_link" target="_blank" rel="external">RL(2): Sample-based Learning Methods</a><br>&emsp;$\circ$ <a href="https://bobondemon.notion.site/RL-3-Prediction-and-Control-with-Function-Approximation-1efedc3d531d80d395f4d1edc6f4523c?source=copy_link" target="_blank" rel="external">RL(3): Prediction and Control with Function Approximation</a></p>
<a id="more"></a>
<p><img src="/2025/12/25/RLçš„æ•¸å­¸åŸç†/image.png" width="70%" height="70%"></p>
<blockquote>
<p>æŒ–å‘çµ¦è‡ªå·±: ä¸‹ä¸€æ¬¡è®€ Kevin Murphy çš„ reinforcement learning [<a href="https://arxiv.org/abs/2412.05265" target="_blank" rel="external">arxiv</a>]</p>
</blockquote>
<hr>
<h2 id="Ch1-Basic-Concepts"><a href="#Ch1-Basic-Concepts" class="headerlink" title="[Ch1]: Basic Concepts"></a>[Ch1]: Basic Concepts</h2><p>RL çš„ç’°å¢ƒè¨­å®š reward <span>$r$</span><!-- Has MathJax -->, action <span>$a$</span><!-- Has MathJax -->, state <span>$s$</span><!-- Has MathJax -->, policy <span>$\pi(a|s)$</span><!-- Has MathJax --> ç­‰çš„åˆæ­¥ä»‹ç´¹è‡ªè¡ŒæŸ¥è³‡æ–™å³å¯<br>é‡é»æ˜¯å¥—ç”¨ Markov Decision Process (MDP) çš„æ¡†æ¶.<br>Markov Decision Process (MDP) çµ¦å®šå‰ä¸€æ¬¡ state <span>$s$</span><!-- Has MathJax --> å’Œå‰ä¸€æ¬¡ action <span>$a$</span><!-- Has MathJax -->, æˆ‘å€‘å¯ä»¥å®šç¾© state å’Œ reward çš„ distribution:<br><span>$p(s&apos;,r|s,a)\triangleq Pr\{S_t=s&apos;,R_t=r|S_{t-1}=s,A_{t-1}=a\}$</span><!-- Has MathJax --> For all <span>$s&apos;,s\in\mathcal{S}$</span><!-- Has MathJax -->, <span>$r\in\mathcal{R}$</span><!-- Has MathJax --> , and <span>$a\in\mathcal{A}(s)$</span><!-- Has MathJax -->. Action space æ˜¯å¾ state å®šç¾©å‡ºä¾†çš„.<br>æ‰€ä»¥ <span>$p:\mathcal{S}\times\mathcal{R}\times\mathcal{S}\times\mathcal{A}\rightarrow[0,1]$</span><!-- Has MathJax -->. (<span>$p$</span><!-- Has MathJax --> ç¨± <strong>dynamics function of MDP</strong>)</p>
<blockquote>
<p>çŸ¥é“é€™å€‹å››å…ƒ <span>$p(s&apos;,r|s,a)$</span><!-- Has MathJax --> çš„æ©Ÿç‡å°±ç¨± with model çš„ RL æ–¹æ³•, åä¹‹å°±æ˜¯ without model çš„æ–¹æ³•</p>
</blockquote>
<p>Return <span>$G_t$</span><!-- Has MathJax --> å®šç¾©ç‚º (æœ‰äº›äººç¨± total rewards)<br><span>$$\begin{align*}
G_t\triangleq R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots \\
=\sum_{k=0}^\infty\gamma^kR_{t+k+1}=R_{t+1}+\gamma G_{t+1}
\end{align*}$$</span><!-- Has MathJax --> Return å–æœŸæœ›å€¼çš„è©± (expected return) å°±ç¨± Value (åƒ¹å€¼) â† <strong>RL ç›®æ¨™å°±æ˜¯æ‰¾åˆ°èƒ½æœ€å¤§åŒ– value çš„ policy <span>$\pi(a|s)$</span><!-- Has MathJax -->.</strong></p>
<hr>
<h2 id="Ch2-Bellman-Equation"><a href="#Ch2-Bellman-Equation" class="headerlink" title="[Ch2]: Bellman Equation"></a>[Ch2]: Bellman Equation</h2><p><img src="/2025/12/25/RLçš„æ•¸å­¸åŸç†/Bellman_equations.png" width="100%" height="100%"> State value and action value ä¸åŒå½¢å¼çš„ Bellman equations æ¨å°ä¾†æº:</p>
<ul>
<li>State Value <span>$v_\pi(s)$</span><!-- Has MathJax --> Elementwised form [<a href="https://bobondemon.notion.site/Week-3-Value-Functions-Bellman-Equations-1adedc3d531d80e9ade3cb1f22f6472e#1adedc3d531d8034ad30d001ac869113" target="_blank" rel="external">ref</a>]</li>
<li>State Value <span>$v_\pi(s)$</span><!-- Has MathJax --> Expectation form [<a href="https://www.notion.so/bobondemon/Ch-7-Temporal-Difference-Learning-Methods-for-Prediction-Control-1c7edc3d531d802aa64cf9e5c8f81ca0?source=copy_link#1dcedc3d531d80beb45efe4d4b6f2a3d" target="_blank" rel="external">ref</a>]</li>
<li>State Value <span>$v_\pi(s)$</span><!-- Has MathJax --> Matrix Vector form [<a href="https://www.notion.so/bobondemon/Week-3-Value-Functions-Bellman-Equations-1adedc3d531d80e9ade3cb1f22f6472e?source=copy_link#1aeedc3d531d8025b48be6a4d70b8749" target="_blank" rel="external">ref</a>]</li>
<li>Action Value <span>$q_\pi(s,a)$</span><!-- Has MathJax --> Elementwised form [<a href="https://www.notion.so/bobondemon/Week-3-Value-Functions-Bellman-Equations-1adedc3d531d80e9ade3cb1f22f6472e?source=copy_link#1aeedc3d531d809fbb20c1089ea3f8e7" target="_blank" rel="external">ref</a>]</li>
<li>Action Value <span>$q_\pi(s,a)$</span><!-- Has MathJax --> Expectation form [<a href="https://www.notion.so/bobondemon/Ch-7-Temporal-Difference-Learning-Methods-for-Prediction-Control-1c7edc3d531d802aa64cf9e5c8f81ca0?source=copy_link#1e1edc3d531d80fa825adcf1c5f686c3" target="_blank" rel="external">ref</a>]</li>
</ul>
<p>Bellman equation å°±æ˜¯åœ¨åš <strong>PE (Policy Evaluation)</strong>, å³çµ¦å®šä¸€å€‹ policy <span>$\pi$</span><!-- Has MathJax --> è©•ä¼°æ¯å€‹ state <span>$s$</span><!-- Has MathJax --> çš„ expected return (value)<br>å¯ä»¥ç”¨ iterative æ–¹æ³•æ±‚è§£: <span>$v_{k+1}=r_\pi+\gamma P_\pi v_k,\quad k=0,1,2,...$</span><!-- Has MathJax --> [<a href="https://bobondemon.notion.site/Week-3-Value-Functions-Bellman-Equations-1adedc3d531d80e9ade3cb1f22f6472e#1afedc3d531d806abdf3f03c08257ce2" target="_blank" rel="external">ref</a>]</p>
<hr>
<h2 id="Ch3-Bellman-Optimality-Equation"><a href="#Ch3-Bellman-Optimality-Equation" class="headerlink" title="[Ch3]: Bellman Optimality Equation"></a>[Ch3]: Bellman Optimality Equation</h2><p>åŸå…ˆçš„ Bellman equation éƒ½æ˜¯çµ¦å®šä¸€å€‹ policy <span>$\pi$</span><!-- Has MathJax --> æƒ…æ³ä¸‹æ±‚è§£çš„ state value or action value. ä½† RL ç›®çš„æ˜¯è¦æ‰¾å‡ºä»€éº¼æ¨£çš„ policy èƒ½å¾—åˆ°æœ€å¤§çš„ value (optimal policy), å…‰è©•ä¼°çµ¦å®šçš„ä¸€å€‹ policy çš„ value æ˜¯ä¸å¤ çš„.<br>BOE ç¾åœ¨å°‡ <span>$\pi$</span><!-- Has MathJax --> ä¹Ÿç•¶æˆè¦æ±‚è§£çš„åƒæ•¸, ç›´æ¥å° Bellman equation åšæœ€ä½³åŒ– (å¤šäº† <span>$\max_\pi$</span><!-- Has MathJax -->), å› æ­¤ BOE:<br><span>$$\begin{align*}
{\color{orange}{v(s)}}={\color{red}{\max_{\pi}}}\left[\sum_a {\color{orange}{\pi(a|s)}}\left(\sum_{r}p(r|s,a)r + \gamma\sum_{s&apos;}p(s&apos;|s,a){\color{orange}{v(s&apos;)}}\right)\right],\quad\forall s\in\mathcal{S} \\
=\max_{\pi}
\sum_a\pi(a|s)q_\pi(s,a)
,\quad\forall s\in\mathcal{S}\\
\text{Matrix Vector form}\quad\Longrightarrow v=f(v)\triangleq \max_\pi(r_\pi+\gamma P_\pi v)
\end{align*}$$</span><!-- Has MathJax --> è¦æ±‚è§£çš„ variables ç‚º <span>${\color{orange}{v(s)}}$</span><!-- Has MathJax --> å’Œ <span>${\color{orange}{\pi(a|s)}}$</span><!-- Has MathJax -->.<br>å¯ä»¥è­‰æ˜ <span>$f$</span><!-- Has MathJax --> æ˜¯ contraction mapping, å‰‡ <span>$\exists!$</span><!-- Has MathJax --> fixed point <span>$v_\ast$</span><!-- Has MathJax -->: <span>$v_\ast=f(v_\ast)=\max_\pi(r_\pi+\gamma P_\pi v_\ast)$</span><!-- Has MathJax --> å› æ­¤ <span>$v_\ast$</span><!-- Has MathJax --> å¯é€é <a href="https://www.notion.so/Contraction-Mapping-Principle-1afedc3d531d80d58122dfff5f716fec?pvs=21" target="_blank" rel="external">contraction mapping theorem</a> çš„ç–Šä»£æ–¹æ³•æ‰¾åˆ°: <span>$v_{k+1}=f(v_k)=\max_\pi(r_\pi+\gamma P_\pi v_k),\quad k=0,1,2,...$</span><!-- Has MathJax --> (å…¶ä¸­ initial å¯ä»¥æ˜¯ä»»æ„)<br>æ‰¾åˆ° <span>$v_\ast$</span><!-- Has MathJax --> å¾Œ, ä»¤ <span>$\pi_\ast\triangleq\arg\max_\pi(r_\pi+\gamma P_\pi v_\ast)$</span><!-- Has MathJax --> å³ç‚º optimal policy äº†<br>åœ¨ç–Šä»£éç¨‹ä¸­æ±‚è§£é€™å€‹ <span>$\max_\pi(r_\pi+\gamma P_\pi v_k)$</span><!-- Has MathJax --> çš„æœ€ä½³çš„ <span>$\pi_{k+1}$</span><!-- Has MathJax --> ä½¿ç”¨ <a href="https://bobondemon.notion.site/Week-3-Value-Functions-Bellman-Equations-1adedc3d531d80e9ade3cb1f22f6472e#1afedc3d531d80ad9606ed7c8f7e24be" target="_blank" rel="external">Greedy Optimal Policy</a>:<br><span>$$\pi_{k+1}(a|s)=\left\{
\begin{array}{cl}
1, &amp;a=a_k^\ast(s) \\
0, &amp;a\neq a_k^\ast(s)
\end{array}
\right. \\
,\text{where }a_k^\ast(s)=\arg\max_a q_k(s,a)$$</span><!-- Has MathJax --> <span>$\pi_{k+1}$</span><!-- Has MathJax --> æ˜¯ deterministic çš„ (one-hot vector distribution)<br>æ³¨æ„åˆ° BOE æ˜¯ Bellman equation çš„å…¶ä¸­ä¸€ç¨®æƒ…æ³è€Œå·², æ»¿è¶³ BOE çš„ policy ç‚º optimal policy<br>å¦‚æœçœ‹ action value çš„è©±, å…¶ <strong>**</strong>BOE ç‚º:<br><span>$$\begin{align*}
{\color{orange}{q(s,a)=\mathbb{E}\left[\left.
R_{t+1}+\gamma\max_{a&apos;}q(S_{t+1},a&apos;)\right|S_t=s,A_t=a
\right]}}
\end{align*}$$</span><!-- Has MathJax --></p>
<details><br>   <summary>â–¶ï¸(é»æ“Šå±•é–‹) Action Value BOE æ¨å° [<a href="https://www.notion.so/bobondemon/Ch-7-Temporal-Difference-Learning-Methods-for-Prediction-Control-1c7edc3d531d802aa64cf9e5c8f81ca0#1ebedc3d531d80189647f3c2b4f566f1" target="_blank" rel="external">ref</a>]</summary><br>    <span>$$\begin{align*}
    q(s,a)=\mathbb{E}\left[\left.
    R_{t+1}+\gamma\max_{a&apos;}q(S_{t+1},a&apos;)\right|S_t=s,A_t=a
    \right] \\
    =\sum_r p(r|s,a)r + \gamma\sum_{s&apos;}p(s&apos;|s,a)\max_{a&apos;\in\mathcal{A}(s&apos;)}q(s&apos;,a&apos;) \\
    \end{align*}$$</span><!-- Has MathJax --> å°å…©é‚Šå° action <span>$a$</span><!-- Has MathJax --> å– <span>$\max$</span><!-- Has MathJax --> è®Šæˆ<br>    <span>$$\begin{align*}
    \max_{a\in\mathcal{A}(s)}q(s,a) = \max_{a\in\mathcal{A}(s)}\left[
    \sum_r p(r|s,a)r + \gamma\sum_{s&apos;}p(s&apos;|s,a)\max_{a&apos;\in\mathcal{A}(s&apos;)}q(s&apos;,a&apos;)
    \right]
    \end{align*}$$</span><!-- Has MathJax --> å®šç¾© <span>$v(s)\doteq\max_{a\in\mathcal{A}(s)}q(s,a)$</span><!-- Has MathJax -->, æ”¹å¯«ä¸Šå¼<br>    <span>$$\begin{align*}
    v(s)=\max_{a\in\mathcal{A}(s)}\left[
    \sum_r p(r|s,a)r + \gamma\sum_{s&apos;}p(s&apos;|s,a)v(s&apos;)
    \right] \\
    =\max_\pi\sum_{a\in\mathcal{A}(s)}\pi(a|s)\left[
    \sum_r p(r|s,a)r + \gamma\sum_{s&apos;}p(s&apos;|s,a)v(s&apos;)
    \right]
    \end{align*}$$</span><!-- Has MathJax --> é€™æ­£å¥½å°±æ˜¯ State Value çš„ BOE<br></details>

<hr>
<h2 id="Ch4-Value-Iteration-amp-Policy-Iteration"><a href="#Ch4-Value-Iteration-amp-Policy-Iteration" class="headerlink" title="[Ch4]: Value Iteration &amp; Policy Iteration"></a>[Ch4]: Value Iteration &amp; Policy Iteration</h2><p>æ±‚è§£ BOE å¯ä»¥ç›´æ¥å¹«æˆ‘å€‘æ‰¾å‡º optimal state value æ˜¯å¤šå°‘, ä»¥åŠå°æ‡‰çš„ optimal policy æ˜¯ä»€éº¼<br>æ ¹æ“š <a href="https://www.notion.so/Contraction-Mapping-Principle-1afedc3d531d80d58122dfff5f716fec?pvs=21" target="_blank" rel="external">contraction mapping theorem</a> å¯ä»¥å¾ä»»æ„çš„ initial value <span>$v_0$</span><!-- Has MathJax --> å‡ºç™¼, åªè¦ä¸€ç›´ç–Šä»£: <span>$v_{k+1}=f(v_k)=\max_\pi(r_\pi+\gamma P_\pi v_k) ,$</span><!-- Has MathJax --> <span>$k=0,1,2,...$</span><!-- Has MathJax -->, å°±èƒ½è§£<br>å› æ­¤æœ‰äº† Value Iteration ç®—æ³•, å…¶ä¸­ policy update ç”¨ <a href="https://bobondemon.notion.site/Week-3-Value-Functions-Bellman-Equations-1adedc3d531d80e9ade3cb1f22f6472e#1afedc3d531d80ad9606ed7c8f7e24be" target="_blank" rel="external">Greedy Optimal Policy</a>.<br>é †åºåéä¾†çš„è©±, Policy iteration å…ˆå¾ä¸€å€‹ä»»æ„çš„ policy <span>$\pi_0$</span><!-- Has MathJax --> å‡ºç™¼, åˆ©ç”¨ä¸Šé¢ Ch2 çš„ <a href="https://www.notion.so/bobondemon/RL-2cdedc3d531d80deb68de5606084a717?source=copy_link#2cdedc3d531d8078be22ea78df87d9bf" target="_blank" rel="external">PE (Policy Evaluation)</a> æˆ‘å€‘å¯ä»¥è©•ä¼°åœ¨è©² policy ä¸‹çš„ state value <span>$v_{\pi_0}$</span><!-- Has MathJax -->.<br>(æ³¨æ„åˆ°é€™æ˜¯åˆæ³•çš„ state value, è·Ÿ value iteration çš„ <span>$v_0$</span><!-- Has MathJax --> ä¸ä¸€æ¨£)<br>æœ‰äº†å°æ‡‰çš„ state value <span>$v_{\pi_0}$</span><!-- Has MathJax --> ä¸€æ¨£åˆ©ç”¨ <a href="https://bobondemon.notion.site/Week-3-Value-Functions-Bellman-Equations-1adedc3d531d80e9ade3cb1f22f6472e#1afedc3d531d80ad9606ed7c8f7e24be" target="_blank" rel="external">Greedy Optimal Policy</a> æ›´æ–° policy, å³ policy improvement (PI) æ‰¾åˆ°æ›´å„ªçš„ policy. é‡è¤‡ iteration å°±æ˜¯ç­–ç•¥è¿­ä»£ç®—æ³•.<br>ä¸‹åœ–å·¦å’Œå³åˆ†åˆ¥æ˜¯ Value &amp; Policy iteration ç®—æ³•<br><img src="/2025/12/25/RLçš„æ•¸å­¸åŸç†/value_and_policy_iteration_algo.png" width="100%" height="100%"> Policy iteration çš„æ”¶æ–‚æ€§è­‰æ˜éœ€è¦ç”¨åˆ° Value iteration æ”¶æ–‚çš„æ€§è³ª. å…¶ä»–è­‰æ˜å¦‚èƒ½ç¢ºä¿æ‰¾åˆ°çš„ policy or value æ˜¯ optimal çš„è«‹åƒè€ƒ <a href="https://www.notion.so/Week-4-Dynamic-Programming-Value-and-Policy-Iteration-1b0edc3d531d80f8aea4e720938ea1e0?pvs=21" target="_blank" rel="external">Week4 ç­†è¨˜</a>.<br>æœ€å¾Œå®ƒå€‘éƒ½æ˜¯ truncated policy iteration çš„ç‰¹ä¾‹<br>æ³¨æ„åˆ°, Policy iteration çš„ PE ç†è«–ä¸Šè¦æ”¶æ–‚åˆ°ç„¡çª®å¤šæ­¥æ‰èƒ½æ±‚å‡ºæ­£ç¢ºçš„ value å€¼ <span>$v_{\pi_k}$</span><!-- Has MathJax -->, ä½†å¦‚æœåªåšä¸€æ­¥ update å…¶å¯¦å°±æ˜¯ value iteration çš„åšæ³•, æ›´å¤šè«‹åƒè€ƒ <a href="https://www.notion.so/Week-4-Dynamic-Programming-Value-and-Policy-Iteration-1b0edc3d531d80f8aea4e720938ea1e0?pvs=21" target="_blank" rel="external">Week4 ç­†è¨˜</a>.<br>åœ¨åš policy or value iteration çš„æ™‚å€™, éƒ½æœƒå°æ‰€æœ‰ state åš for loop<br>äº‹å¯¦ä¸Šå¯ä»¥å¾ˆå½ˆæ€§, å¯ä»¥ random å° state åš, ä¹Ÿå¯ä»¥é¦¬ä¸Š inplace æ›´æ–° state value, ä¹Ÿå¯ä»¥æ¯æ¬¡è®Šå‹• truncated <span>$j$</span><!-- Has MathJax --> æ­¥. é€™æ¨£åšçš„å¥½è™•æ˜¯ç•¶ states æ•¸é‡å¾ˆå¤šçš„æ™‚å€™ (ç”šè‡³é€£æƒéä¸€æ¬¡æ‰€æœ‰ states éƒ½å¾ˆé›£) é‚„æ˜¯èƒ½åš RL.<br>é€™ç¨®æ›´ general çš„æ–¹æ³• Sutton and Barto èª²æœ¬ç¨± Generalized Policy Iteration (GPI).</p>
<hr>
<h2 id="Ch5-Monte-Carlo-Methods-for-Prediction-amp-Control"><a href="#Ch5-Monte-Carlo-Methods-for-Prediction-amp-Control" class="headerlink" title="[Ch5]: Monte Carlo Methods for Prediction &amp; Control"></a>[Ch5]: Monte Carlo Methods for Prediction &amp; Control</h2><p>åœ¨ Policy iteration ç®—æ³•ä¸­çš„ PI step è§€å¯Ÿå…¶ Elementwised form ç™¼ç¾å°±æ˜¯æ ¹æ“šç¾åœ¨ä¼°è¨ˆçš„ action value <span>$q_k(s,a)$</span><!-- Has MathJax --> ä¾†æ‰¾æœ€ä½³çš„policy<br><img src="/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 3.png" width="70%" height="70%"> é‚£éº¼é€™å€‹ action value <span>$q_k(s,a)$</span><!-- Has MathJax --> åœ¨æ²’æœ‰æ¨¡å‹ (<a href="https://www.notion.so/bobondemon/RL-2cdedc3d531d80deb68de5606084a717#2cdedc3d531d801e83fdfc5b9304b88d" target="_blank" rel="external">dynamic function</a>) çš„æƒ…æ³ä¸‹å°±ç„¡æ³•ç”¨ Bellman equation ä¾†è¨ˆç®—<br>å› æ­¤æ”¹æ¡æœ€åŸå§‹çš„å®šç¾©, é‚£å°±å¯ä»¥é€éæ¡æ¨£ä¾†ä¼°è¨ˆ: <span>$q_{\pi_k}(s,a)=\mathbb{E}\left[G_t|S_t=s,A_t=a\right]\approx\frac{1}{N}\sum_{i=1}^Ng^{(i)}(s,a)$</span><!-- Has MathJax -->.<br>ç°¡å–®è¬›, å°æ¯ä¸€å€‹ <span>$s\in S,a\in A$</span><!-- Has MathJax --> ä¾†èªªè¦æ‰¾å‡º <span>$N$</span><!-- Has MathJax --> æ¢å¾ <span>$(s,a)$</span><!-- Has MathJax --> å‡ºç™¼çš„ trajectory (episode), æ¯ä¸€æ¢éƒ½ç®—å‡º return, ç„¶å¾Œå¹³å‡ return å¾Œå°±ç•¶ä½œæ˜¯æœŸæœ›å€¼.<br>ä½†é€™æ¨£å°æ¯ä¸€å€‹ episode çš„æ•¸æ“šæ²’æœ‰å¾ˆå¥½åˆ©ç”¨, å¾ˆæ²’æ•ˆç‡.<br>ä¾‹å¦‚å°æŸä¸€å€‹ state-action pair <span>$(s_i,a_i)$</span><!-- Has MathJax --> é–‹å§‹çš„é•·åº¦ <span>$T$</span><!-- Has MathJax --> çš„ episode åªæ‹¿ä¾†ä¼°è¨ˆ <span>$q_{\pi_k}(s_i,a_i)$</span><!-- Has MathJax -->.<br>å› æ­¤æœ‰ä¸€äº›å¾ˆæ˜é¡¯çš„æ”¹é€²æ–¹æ³•, first visit, every visit, å€’éä¾†ç®— return, åœ¨ PI éšæ®µæ”¹æˆç”¨ <span>$\varepsilon$</span><!-- Has MathJax -->-greedy æ–¹æ³•è§£é™¤ exploring starts é™åˆ¶ç­‰ç­‰, è©³ç´°çœ‹ <a href="https://www.notion.so/Ch-5-Monte-Carlo-Methods-for-Prediction-Control-1c7edc3d531d800e923dea3217327660?pvs=21" target="_blank" rel="external">Week 5 ç­†è¨˜</a>.<br>åæ­£ä¸€æ¨£æ˜¯ PE &amp; PI å…©éšæ®µ (è·Ÿ Policy Iteration ç®—æ³•ä¸€æ¨£), åªæ˜¯ PE æ¡ç”¨æœ‰æ•ˆç‡çš„ MC æ–¹æ³•ä¼°è¨ˆ</p>
<hr>
<h2 id="Ch6-Robbins-Monro-Algorithm-å’Œ-Dvoretzkyâ€™s-Convergence-Theorem-ç­†è¨˜"><a href="#Ch6-Robbins-Monro-Algorithm-å’Œ-Dvoretzkyâ€™s-Convergence-Theorem-ç­†è¨˜" class="headerlink" title="[Ch6]: Robbins-Monro Algorithm å’Œ Dvoretzkyâ€™s Convergence Theorem ç­†è¨˜"></a>[Ch6]: <a href="https://bobondemon.github.io/2025/05/02/RM-algo/">Robbins-Monro Algorithm å’Œ Dvoretzkyâ€™s Convergence Theorem ç­†è¨˜</a></h2><p>ç›´æ¥åƒè€ƒ <a href="https://bobondemon.github.io/2025/05/02/RM-algo/">blog</a></p>
<hr>
<h2 id="Ch7-Temporal-Difference-Learning-Methods-for-Prediction-Control"><a href="#Ch7-Temporal-Difference-Learning-Methods-for-Prediction-Control" class="headerlink" title="[Ch7]: Temporal Difference Learning Methods for Prediction/Control"></a>[Ch7]: Temporal Difference Learning Methods for Prediction/Control</h2><p>[State Value çš„ TD algorithm ä¸€å¥è©±]: <strong>Model-free çš„ incremental Policy Evaluation</strong><br>é€™å¥è©±é•·ä¸€é»æ˜¯é€™æ¨£: åœ¨æ²’æœ‰ dynamic (model) function æƒ…æ³ä¸‹çµ¦å®šä¸€å€‹ policy <span>$\pi$</span><!-- Has MathJax --> ç”¨ incremental çš„æ–¹å¼ (ä¸ç”¨ç­‰åˆ° episode çµæŸ) æ±‚ state value (è§£ Bellman equation).</p>
<blockquote>
<p>æ‰€ä»¥é‚„æ˜¯åœ¨åš policy evaluation (å¦‚ä½• incremental ä¼°è¨ˆ state/action value), å¯ä»¥æƒ³æˆ <a href="https://www.notion.so/bobondemon/RL-2cdedc3d531d80deb68de5606084a717#2cdedc3d531d8098b896f0135674964f" target="_blank" rel="external">Policy iteration ç®—æ³•</a>çš„ PE éšæ®µæ›æˆ TD method.</p>
</blockquote>
<p><details><br>   <summary>â–¶ï¸(é»æ“Šå±•é–‹) D ç®—æ³•é‚è¼¯æ¨å°è§£é‡‹ (RM è§£ root, è©² root ç‚º Bellman equation è§£)</summary><br>   å›é¡§ <a href="https://www.notion.so/bobondemon/RL-2cdedc3d531d80deb68de5606084a717#2cdedc3d531d80dfbf51e84f94d0643a" target="_blank" rel="external">Bellman Expectation equation</a>: <span>$v_\pi(s)=\mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s],\quad s\in\mathcal{S}$</span><!-- Has MathJax --> <strong>å°æŸå€‹ç‰¹å®šçš„ state <span>$s$</span><!-- Has MathJax -->, ä»¤æˆ‘å€‘è¦æ±‚è§£çš„ <span>$v_\pi(s)$</span><!-- Has MathJax --> ç‚ºè®Šæ•¸ <span>$w$</span><!-- Has MathJax --></strong>, æˆ‘å€‘å®šç¾© function <span>$$\begin{align*}
    g({\color{orange}{w}})\doteq {\color{orange}{w}} - \mathbb{E}[R_{t+1}+\gamma v_{\color{red}{\pi}}(S_{t+1})|S_t=s]
    \end{align*}$$</span><!-- Has MathJax --> å‰‡å° <span>$g$</span><!-- Has MathJax --> æ±‚è§£ root ç­‰åƒ¹æ–¼è§£å‡º Bellman Expectation equation.<br>    å¥—ç”¨ RM ç®—æ³• (åƒè€ƒ <a href="https://bobondemon.github.io/2025/05/02/RM-algo/">Robbins-Monro algorithm</a>), å®šç¾©è§€æ¸¬å‡½æ•¸ <span>$\tilde{g}$</span><!-- Has MathJax --> ç‚ºä¸€æ¬¡çš„è§€æ¸¬ sample: <span>$\tilde{g}(w,\eta_t)\doteq w-\left[r+\gamma v_\pi(s&apos;)\right]$</span><!-- Has MathJax --> å…¶ä¸­ <span>$R_{t+1}=r$</span><!-- Has MathJax --> å’Œ <span>$S_{t+1}=s&apos;$</span><!-- Has MathJax -->. é€™æ˜¯å› ç‚ºåœ¨ model-free çš„è¨­å®šä¸‹, æˆ‘å€‘å¯ä»¥å¾ˆå®¹æ˜“æ‹¿åˆ° <span>$(S_t,R_{t+1},S_{t+1})$</span><!-- Has MathJax --> çš„å¾ˆå¤šè§€æ¸¬å€¼ <span>$\{(s,r,s&apos;)\}$</span><!-- Has MathJax -->, é€™å« experience sample.<br>    å‰‡ RM ç®—æ³•ç‚º: <span>$$\begin{align*}
    w_{k+1}=w_{k}-a_k\tilde{g}(w_{k},\eta_k),\quad k=1,2,3,... \\
    \Longrightarrow w_{k+1}=w_{k}-a_k\left(w_{k}-\left[r+\gamma v_{\color{red}{\pi}}(s&apos;)\right]\right),\quad k=1,2,3,...
    \end{align*}$$</span><!-- Has MathJax --> æ»¿è¶³ RM ç®—æ³•çš„æ”¶æ–‚æ¢ä»¶ä¸‹, æœ€å¾Œ <span>$w_\infty$</span><!-- Has MathJax --> ç‚º root, å³ <span>$g(w_\infty)=0$</span><!-- Has MathJax -->: <span>$w_\infty=\mathbb{E}[R_{t+1}+\gamma v_{\color{red}{\pi}}(S_{t+1})|S_t=s]$</span><!-- Has MathJax -->. é€™æ¨£å°±æŠŠ Bellman equation è§£å‡ºä¾†äº†.<br>    æ³¨æ„åˆ°ä¸Šè¿°çš„è§£ <span>$w_\infty$</span><!-- Has MathJax --> çš„éç¨‹éœ€è¦ç”¨åˆ° ground truth state value <span>$v_{\color{red}{\pi}}$</span><!-- Has MathJax -->, å¯¦å‹™ä¸Šä½¿ç”¨ç›®å‰æ­£åœ¨ä¼°è¨ˆçš„ <span>$w_k$</span><!-- Has MathJax --> æ›¿ä»£, å› æ­¤æ”¶æ–‚éç¨‹é‚„éœ€åš´æ ¼è­‰æ˜, é€™è£¡ç•¥é<br></details><br>å› æ­¤ TD ç®—æ³•:<br><span>$$\begin{align*}
{\color{orange}{
\underbrace{v_{t+1}(s)}_{\text{new est.}} = \underbrace{v_{t}(s)}_{\text{current est.}} -\alpha_t(s)
[
\overbrace{ v_t(s)-(
\underbrace{r+\gamma v_t(s&apos;)}_{\text{TD target }\bar{v}_t}) }^{\text{TD error }\delta_t}
]
}} ,\\
v_{t+1}(s)=v_t(s),\quad\forall s\neq s_t,
\end{align*}$$</span><!-- Has MathJax --> å…©å€‹é‡è¦çš„ terms, <strong>TD target <span>$\bar{v}_t$</span><!-- Has MathJax --></strong> å’Œ T<strong>D error <span>$\delta_t$</span><!-- Has MathJax --></strong> éœ€è¦è¨˜ä½.<br>æ¯æ¬¡ iteration éƒ½æœƒç¸®çŸ­èˆ‡ TD target <span>$\bar{v}_t$</span><!-- Has MathJax --> çš„è·é›¢, è€Œ TD error åæ‡‰äº†ç›®å‰ state value çš„ä¼°è¨ˆ <span>$v_t$</span><!-- Has MathJax --> èˆ‡çœŸå¯¦ state value <span>$v_\pi$</span><!-- Has MathJax --> çš„ â€æœŸæœ›â€ å·®ç•°. æ›´å¤š<a href="https://bobondemon.notion.site/Ch-7-Temporal-Difference-Learning-Methods-for-Prediction-Control-1c7edc3d531d802aa64cf9e5c8f81ca0#1dcedc3d531d805bbd4cdb5771aaa711" target="_blank" rel="external">åƒé–±å®ƒå€‘çš„ç‰¹æ€§</a>.<br>å¦‚æœ TD æ–¹æ³•æ”¹ç‚ºä¼°è¨ˆ action value, ä¸€å€‹ç¶“å…¸çš„æ–¹æ³•ç¨±ç‚º Sarsa<br>çµ¦å®š experience samples <span>$\{(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})\}_t$</span><!-- Has MathJax --> (æ‰€ä»¥ s a r s a åå­—å°±é€™éº¼ä¾†)<br>å‰‡ <strong>Sarsa algorithm</strong> ç‚º :<br><span>$$\begin{align*}
{\color{orange}{q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\alpha_t(s_t,a_t)\left[q_t(s_t,a_t)-(r_{t+1}+\gamma q_t(s_{t+1},a_{t+1}))\right]}}, \\
q_{t+1}(s,a)
=q_t(s,a),\quad \forall (s,a)\neq(s_t,a_t)
\end{align*}$$</span><!-- Has MathJax --> å¯ä»¥çœ‹åˆ°è·Ÿ state value çš„ TD algorithm å¹¾ä¹ä¸€æ¨£, æŠŠ <span>$(s,a)$</span><!-- Has MathJax --> ç•¶æˆæ˜¯æ–°çš„ state é€™æ¨£çœ‹å°±ä¸€æ¨£äº†, æ‰€ä»¥æ”¶æ–‚æ€§è³ªä¸€æ¨£.<br>é‚„æœ‰ expected Sarsa, n-step Sarsa çš„è®Šå½¢. ä¹Ÿä¸€æ¨£åƒè€ƒ <a href="https://bobondemon.notion.site/Ch-7-Temporal-Difference-Learning-Methods-for-Prediction-Control-1c7edc3d531d802aa64cf9e5c8f81ca0?pvs=25" target="_blank" rel="external">Week 7 çš„ç­†è¨˜</a>.</p>
<p>æœ€å¾Œä¸€å€‹é¼é¼å¤§åçš„æ–¹æ³•, Q-learning<br>[<strong>Q-learning</strong> ä¸€å¥è©±]: <strong>Model-free çš„ incremental è§£ BOE (Bellman Optimality Equation) æ–¹æ³•</strong><br>å…ˆç›´æ¥çµ¦å‡º Q-learning ç®—æ³•<br><span>$$\begin{align*}
q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\alpha_t(s_t,a_t)\left[q_t(s_t,a_t)-\left[  {\color{orange}{r_{t+1}+\gamma \max_{a\in\mathcal{A}}{q_t(s_{t+1},a)} }}  \right]\right], \\
q_{t+1}(s,a)
=q_t(s,a),\quad \forall (s,a)\neq(s_t,a_t)
\end{align*}$$</span><!-- Has MathJax --> å¯ä»¥ç™¼ç¾åŒæ¨£åªæœ‰ TD target ä¸åŒ, æ‰€ä»¥ç­‰åŒæ–¼åœ¨è§£æ±ºå¦‚ä¸‹çš„æ•¸å­¸å•é¡Œ (é€™å‰›å¥½æ˜¯ <a href="https://bobondemon.notion.site/notes-mathematical-foundation-of-reinforcement-learning#2cdedc3d531d80a1b893ff4ae9c6b18f" target="_blank" rel="external">action value çš„ BOE</a>):<br><span>$$\begin{align*}

{\color{orange}{q(s,a)=\mathbb{E}\left[\left.
R_{t+1}+\gamma\max_{a&apos;}q(S_{t+1},a&apos;)\right|S_t=s,A_t=a
\right]}}

\end{align*}$$</span><!-- Has MathJax --></p>
<blockquote>
<p>ä½¿ç”¨ <a href="https://bobondemon.github.io/2025/05/02/RM-algo/">RM ç®—æ³•å»è§£ root</a> å°±å¾—åˆ°ä¸Šè¿°çš„ Q-learning æ¼”ç®—æ³•</p>
</blockquote>
<p>æœ€å¾Œæœ‰å€‹çµ±ä¸€çš„è¦–è§’<br><img src="/2025/12/25/RLçš„æ•¸å­¸åŸç†/unifiy_view_of_TD_algo.png" width="100%" height="100%"> ç®—æ³•è§’åº¦ä¸Š, åªæ˜¯ TD target çš„ä¸åŒ<br>å› æ­¤æ•¸å­¸ä¸Šå°æ‡‰æ±‚è§£çš„ç›®æ¨™å‡½å¼ä¹Ÿå°±è·Ÿè‘—ä¸åŒ</p>
<hr>
<h2 id="Ch8-Value-Function-with-Function-Approximation-Methods"><a href="#Ch8-Value-Function-with-Function-Approximation-Methods" class="headerlink" title="[Ch8]: Value Function with Function Approximation Methods"></a>[Ch8]: Value Function with Function Approximation Methods</h2><p>ä¹‹å‰çš„æ–¹æ³•åªèƒ½è™•ç† state/action value å’Œ policy éƒ½æ˜¯ tabular çš„å½¢å¼ (åªèƒ½æ˜¯ finite discrete function)</p>
<ul>
<li>Ch8 å°‡ state/action value æ¨å»£æˆ continuous å½¢å¼</li>
<li>Ch9 å°‡ policy æ¨å»£æˆ continuous å½¢å¼</li>
<li>Ch10 çµåˆ Ch8&amp;9 ç¸½çµå‡º Actor-Critic ç®—æ³•<br>æœ¬ç« ç¯€ä»‹ç´¹çš„æ–¹æ³•éƒ½æ˜¯åœ¨åš PE (Policy Evaluation), ä¹Ÿå°±æ˜¯é‚„è¦çµåˆ Policy Improvement (é™¤äº† Q-learning&amp;DQN) æ‰æ˜¯å®Œæ•´çš„ <a href="https://bobondemon.notion.site/notes-mathematical-foundation-of-reinforcement-learning#2cdedc3d531d8098b896f0135674964f" target="_blank" rel="external">policy iteration</a> ç®—æ³• (PE+PI).</li>
</ul>
<h3 id="State-value-with-function-approximation"><a href="#State-value-with-function-approximation" class="headerlink" title="State value with function approximation"></a>State value with function approximation</h3><p>å…ˆæ¨å»£çœ‹çœ‹ state value, æˆ‘å€‘ç”¨ continuous function ä¾†ä»£è¡¨, <span>$\hat{v}(s,w)$</span><!-- Has MathJax -->, å…¶ä¸­ <span>$w$</span><!-- Has MathJax --> è¡¨ç¤ºè©² function çš„åƒæ•¸<br>ä¸€èˆ¬ä¾†èªªæˆ‘å€‘æœƒæœ‰é€™æ¨£çš„(å„ªåŒ–)ç›®æ¨™å‡½æ•¸, å¸Œæœ›çš„ä¼°è¨ˆ function <span>$\hat{v}$</span><!-- Has MathJax --> è·Ÿç›®æ¨™ <span>$v_\pi$</span><!-- Has MathJax --> åœ¨ MSE è¦–è§’ä¸‹ â€œæœŸæœ›å€¼â€ æ„ˆå°æ„ˆå¥½<br><span>$$\begin{align*}
J(w)=\frac{1}{2}\mathbb{E}_{s\sim {\color{orange}{d_\pi}}}[(v_\pi(S)-\hat{v}(S,w))^2]
\end{align*}$$</span><!-- Has MathJax --> æœŸæœ›å€¼çš„è¨ˆç®—æ¡ç”¨ steady state <span>$d_\pi$</span><!-- Has MathJax --> çš„åˆ†å¸ƒ, é€™æ˜¯å› ç‚º MDP çµ¦å®š policy <span>$\pi$</span><!-- Has MathJax --> çš„æƒ…æ³ä¸‹æœƒ reduce æˆ MC (Markov Process)<br>MC åœ¨æ»¿è¶³ä¸€äº›æ¢ä»¶ä¸‹æœƒå­˜åœ¨ steady state åˆ†å¸ƒ, <span>$d_\pi$</span><!-- Has MathJax -->, [<a href="https://bobondemon.github.io/2021/12/12/Stochastic-Processes-Week-3-Markov-Chains/#Week-3-6-7-Ergodic-chains-Ergodic-theorem">Ergodic Theorem</a>]</p>
<blockquote>
<p>æˆ‘å€‘èªª <span>$J(w)$</span><!-- Has MathJax --> é€™å€‹ç›®æ¨™å‡½å¼, å…¶å¯¦ç­‰åƒ¹æ–¼åœ¨è§£ state value çš„ Bellman equation çš„ expectation form. å› ç‚º <span>$=0$</span><!-- Has MathJax --> çš„æƒ…æ³å°±æ˜¯æ»¿è¶³å¹³æ–¹å‘è£¡é¢ç‚º <span>$0$</span><!-- Has MathJax -->.</p>
</blockquote>
<p>è€Œç”¨ policy <span>$\pi$</span><!-- Has MathJax --> ç›´æ¥æ¡æ¨£ trajectories å¾Œ, æœé›†åˆ°çš„ <span>$\{s_t\}_t$</span><!-- Has MathJax --> æœå¾ <span>$d_\pi$</span><!-- Has MathJax --> çš„åˆ†ä½ˆ, å› æ­¤å¥— SGD å°±å¾ˆå®¹æ˜“äº†</p>
<span>$$\begin{align*}
w_{t+1}=w_t+\alpha_t( {\color{red}{v_\pi(s_t)}} -\hat{v}(s_t,w_t))\nabla_{w}\hat{v}(s_t,w_t)
\end{align*}$$</span><!-- Has MathJax -->
<p>ground truth çš„ <span>$v_\pi$</span><!-- Has MathJax --> æˆ‘å€‘ä¸çŸ¥é“, å› æ­¤æœ‰å…©ç¨®æ›¿ä»£æ–¹å¼:</p>
<ol>
<li>Monte Carlo learning with function approximation<br> è¨­å®š <span>$v_\pi(s_t)$</span><!-- Has MathJax --> ç‚º <span>$g_t$</span><!-- Has MathJax -->, å…¶ä¸­ <span>$g_t$</span><!-- Has MathJax --> æ˜¯å¾ <span>$s_t$</span><!-- Has MathJax --> å‡ºç™¼ (æ¡æ¨£) ç›´åˆ°å®Œæˆä¸€å€‹ episode çš„ return. å‰‡è®Šæˆ <span>$$\begin{align*}
    w_{t+1}=w_t+\alpha_t( {\color{red}{g_t}} -\hat{v}(s_t,w_t))\nabla_{w}\hat{v}(s_t,w_t)
    \end{align*}$$</span><!-- Has MathJax -->    </li>
<li>TD learning with function approximation<br> è¨­å®š <span>$v_\pi(s_t)$</span><!-- Has MathJax --> ç‚º TD target <span>$r_{t+1}+\gamma\hat{v}(s_{t+1},w_t)$</span><!-- Has MathJax -->. å‰‡è®Šæˆ <span>$$\begin{align*}
    w_{t+1}=w_t+\alpha_t( {\color{red}{r_{t+1}+\gamma\hat{v}(s_{t+1},w_t)}} -\hat{v}(s_t,w_t))\nabla_{w}\hat{v}(s_t,w_t)
    \end{align*}$$</span><!-- Has MathJax --> å…¶å¯¦å°±è®Šæˆä¸­é–“æ˜¯ TD-error äº† (åƒè€ƒ <a href="https://bobondemon.notion.site/Ch-7-Temporal-Difference-Learning-Methods-for-Prediction-Control-1c7edc3d531d802aa64cf9e5c8f81ca0#1dcedc3d531d8063bda9f1b943cc0441" target="_blank" rel="external">TD-learning çš„å…¬å¼ (7)</a>)<br> é€²ä¸€æ­¥çš„å¦‚æœä½¿ç”¨ linear function <span>$\hat{v}(s,w)=\phi(s)^Tw$</span><!-- Has MathJax --> ä¾†è¿‘ä¼¼, å…¶ä¸­ gradient ç‚º <span>$\nabla_w\hat{v}(s,w)=\phi(s)$</span><!-- Has MathJax -->, å‰‡è®Šæˆ<br> <span>$$\begin{align*}
    w_{t+1}=w_t+\alpha_t({\color{red}{r_{t+1}+\gamma\phi^T(s_{t+1})w_t}}-\phi^T(s_t)w_t)\phi(s_t)
    \end{align*}$$</span><!-- Has MathJax --> ç¨±ç‚º TD-linear ç®—æ³•, æˆ‘å€‘å¯ä»¥è­‰æ˜ Ch7 å­¸çš„ tabular TD ç®—æ³•æ˜¯é€™å€‹ TD-linear çš„ä¸€å€‹ç‰¹ä¾‹ [<a href="https://bobondemon.notion.site/Ch8-Value-Function-with-Function-Approximation-Methods-1efedc3d531d8069a1bae8a51aee01d1#1f2edc3d531d80239f4cefcba3a1c0d7" target="_blank" rel="external">ref</a>]</li>
</ol>
<blockquote>
<p>TD learning with function approximation çš„ç®—æ³•æ›´æ–°ç©¶ç«Ÿåœ¨å„ªåŒ–ä»€éº¼ç›®æ¨™å‡½å¼, ç²—æš´é»ç†è§£æ˜¯ä¸€æ¨£åœ¨è§£ Bellman equation, è©³ç´°è¦‹<a href="https://bobondemon.notion.site/Ch8-Value-Function-with-Function-Approximation-Methods-1efedc3d531d8069a1bae8a51aee01d1#2d2edc3d531d803499edc8a01c5af34f" target="_blank" rel="external">ç­†è¨˜é€™éƒ¨åˆ†</a>.</p>
</blockquote>
<h3 id="Action-value-with-function-approximation"><a href="#Action-value-with-function-approximation" class="headerlink" title="Action value with function approximation"></a>Action value with function approximation</h3><p>å†ä¾†å¦‚æœæ˜¯ action value åš continuous æ¨å»£çš„è©±, å…¶å¯¦æ–¹æ³•ä¸€æ¨£<br>Sarsa çš„ action value with function approximation çš„ç®—æ³•å…¬å¼:<br><span>$$\begin{align*}
w_{t+1}=w_t+\alpha_t( r_{t+1}+\gamma {\color{orange}{\hat{q}(s_{t+1},a_{t+1},w_t)}} - {\color{orange}{\hat{q}(s_t,a_t,w_t)}} ) {\color{orange}{\nabla_{w}\hat{q}(s_t,a_t,w_t)}}
\end{align*}$$</span><!-- Has MathJax --> å°æ¯”æ©˜è‰²çš„éƒ¨åˆ†, å¯ä»¥çœ‹åˆ°è·Ÿ state value æ–¹æ³•å°æ¯”åªæ˜¯æŠŠ <span>$\hat{v}$</span><!-- Has MathJax --> æ›æˆ <span>$\hat{q}$</span><!-- Has MathJax --> è€Œå·²</p>
<h3 id="Q-learning-with-function-approximation"><a href="#Q-learning-with-function-approximation" class="headerlink" title="Q-learning with function approximation"></a>Q-learning with function approximation</h3><p>åŒæ¨£çš„ Q-learning with function approximation çš„ç®—æ³•å…¬å¼å¦‚ä¸‹:<br><span>$$\begin{align*}
w_{t+1}=w_t+\alpha_t \left[ r_{t+1}+\gamma {\color{orange}{\max_{a\in\mathcal{A}(s_{t+1})}\hat{q}(s_{t+1},a,w_t)}} - \hat{q}(s_t,a_t,w_t) \right] \nabla_{w}\hat{q}(s_t,a_t,w_t)
\end{align*}$$</span><!-- Has MathJax --> åŸºæœ¬ä¸Šåªæ˜¯ TD target æ”¹æˆ Q-learning çš„ TD target (å¯ä»¥å»å›é¡§<a href="https://bobondemon.notion.site/Ch-7-Temporal-Difference-Learning-Methods-for-Prediction-Control-1c7edc3d531d802aa64cf9e5c8f81ca0#1ebedc3d531d80b79f36ff7da898c3a1" target="_blank" rel="external">å‰é¢çš„æ•´ç†</a>)<br><img src="/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 6.png" width="80%" height="80%"> ä¸Šåœ–çš„ pseudocode æ˜¯ on-policy (behavior ç­–ç•¥è·Ÿ target ç­–ç•¥ç›¸åŒ), æ‰€ä»¥é‚„æ˜¯ä½¿ç”¨ <a href="https://bobondemon.notion.site/Ch-5-Monte-Carlo-Methods-for-Prediction-Control-1c7edc3d531d800e923dea3217327660#1ccedc3d531d804885d7e9d15e4255e0" target="_blank" rel="external"><span>$\epsilon$</span><!-- Has MathJax -->-greedy</a> æ›´æ–°ç›®æ¨™ç­–ç•¥ä»¥é¿å… <a href="https://bobondemon.notion.site/Ch-5-Monte-Carlo-Methods-for-Prediction-Control-1c7edc3d531d800e923dea3217327660#1ccedc3d531d80da97b4e352db62df36" target="_blank" rel="external">exploring starts</a> ç¼ºé»<br>ä½†åˆ¥å¿˜äº†, <a href="https://bobondemon.notion.site/Ch-7-Temporal-Difference-Learning-Methods-for-Prediction-Control-1c7edc3d531d802aa64cf9e5c8f81ca0#1ebedc3d531d809a87bdebe68dae186d" target="_blank" rel="external">Q-learning å¯¦éš›ä¸Šæ˜¯ç›´æ¥æ±‚è§£ BOE</a>, å› æ­¤å¯ä»¥ç›´æ¥è¨ˆç®—æœ€ä½³ç­–ç•¥<br>å…¶ä¸­ä¸€å€‹æœ€å¤§çš„å¥½è™•æ˜¯ <a href="https://bobondemon.notion.site/Ch-7-Temporal-Difference-Learning-Methods-for-Prediction-Control-1c7edc3d531d802aa64cf9e5c8f81ca0#1ebedc3d531d801183ccf9486af2d4fd" target="_blank" rel="external">Q-learning æ˜¯ off-policy</a> çš„, æ‰€ä»¥å¯ä»¥ä¸ç”¨ policy improvement (or called update policy) é€™ä¸€æ­¥<br>Off-policy çš„ Q-learning æœƒåœ¨ä¸‹é¢ç”¨ Deep Q-learning (DQN) çµ¦å‡º</p>
<h3 id="Deep-Q-learning-or-Deep-Q-network-DQN"><a href="#Deep-Q-learning-or-Deep-Q-network-DQN" class="headerlink" title="Deep Q-learning or Deep Q-network (DQN)"></a>Deep Q-learning or Deep Q-network (DQN)</h3><p>Deep Q-learning or DQN å¯ä»¥è¦–ç‚º Q-learning (<a href="https://bobondemon.notion.site/Ch8-Value-Function-with-Function-Approximation-Methods-1efedc3d531d8069a1bae8a51aee01d1#1f2edc3d531d8000a0fdfc6184565459" target="_blank" rel="external">function approximation</a>) çš„æ¨å»£ä½œæ³•, æ•¸å­¸(å„ªåŒ–)ç›®æ¨™å‡½å¼ç‚º:<br><span>$$\begin{align*}
J(w)=\mathbb{E}\left[\left(
 \underbrace{\color{orange}{R+\gamma\max_{a\in\mathcal{A}(S&apos;)}\hat{q}(S&apos;,a,w)}}_{\text{TD Target}}
- \underbrace{\hat{q}(S,A,w)}_{\text{Predicted Q-value}}
\right)^2\right]
\end{align*}$$</span><!-- Has MathJax --> DQN çš„ç›®æ¨™å‡½å¼è·Ÿ <a href="https://bobondemon.notion.site/notes-mathematical-foundation-of-reinforcement-learning#2cdedc3d531d803cb4d7ca4f3556856e" target="_blank" rel="external">Q-learning (tabular) çš„ BOE</a> ç­‰åƒ¹, éƒ½æ˜¯ç›´æ¥æ±‚è§£ BOE [<a href="https://bobondemon.notion.site/Ch8-Value-Function-with-Function-Approximation-Methods-1efedc3d531d8069a1bae8a51aee01d1#200edc3d531d80189e5dd51842bfdd2a" target="_blank" rel="external">è­‰æ˜</a>]<br>å¯¦å‹™ä¸Š DQN ä½¿ç”¨äº†å…©å€‹ NN models, ç¨± main network <span>${\color{orange}{\hat{q}(s,a,w)}}$</span><!-- Has MathJax --> å’Œ target network <span>${\color{green}{\hat{q}(s,a,w_T)}}$</span><!-- Has MathJax -->:<br><span>$$\begin{align*}
J(w)=\mathbb{E}\left[\left(
R+\gamma\max_{a\in\mathcal{A}(S&apos;)} {\color{green}{\hat{q}(S&apos;,a,w_T)}} - {\color{orange}{\hat{q}(S,A,w)}}
\right)^2\right]
\end{align*}$$</span><!-- Has MathJax --> ç„¶å¾Œ update main NN <span>${\color{orange}{\hat{q}(s,a,w)}}$</span><!-- Has MathJax --> æ™‚ target network <span>${\color{green}{\hat{q}(s,a,w_T)}}$</span><!-- Has MathJax --> è¦–ç‚ºå¸¸æ•¸ä¸å‹•, æ‰€ä»¥ main network çš„ gradient å¾ˆå¥½è¨ˆç®—<br>ç„¶å¾Œå¹¾æ¬¡ iteration å¾Œ, æŠŠæ›´æ–°å¾Œçš„ main NN <span>${\color{orange}{\hat{q}(s,a,w)}}$</span><!-- Has MathJax --> copy éå»çµ¦ target network <span>${\color{green}{\hat{q}(s,a,w_T)}}$</span><!-- Has MathJax -->,  DQN å°±æ˜¯é€éé€™æ¨£çš„æŠ€å·§å¾ˆæˆåŠŸçš„æŠŠ DNN å¼•å…¥åˆ° RL ä¸­<br>æ›´å¤šç´°ç¯€, è­¬å¦‚é€™æ¨£åšçš„å¥½è™•, ä»¥åŠ mini-batch æ¡ç”¨ uniform distribution æŠ“å– examples ç­‰, è«‹åƒè€ƒ <a href="https://www.notion.so/Ch8-Value-Function-with-Function-Approximation-Methods-1efedc3d531d8069a1bae8a51aee01d1?pvs=21" target="_blank" rel="external">Week8 ç­†è¨˜</a><br>DQN off-policy pseudocode:<br><img src="/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 7.png" width="80%" height="80%"></p>
<hr>
<h2 id="Ch9-Policy-Gradient-Methods"><a href="#Ch9-Policy-Gradient-Methods" class="headerlink" title="[Ch9]: Policy Gradient Methods"></a>[Ch9]: Policy Gradient Methods</h2><p>è®“ actions ä¸åªé™æ–¼ tabular form, ä¹Ÿå¯ä»¥æ˜¯ continous form, å³ <span>$\pi(a,\theta)$</span><!-- Has MathJax -->, å…¶ä¸­ <span>$\theta$</span><!-- Has MathJax --> ç‚ºåƒæ•¸.<br>ä»¥å‰ tabular å½¢å¼çš„ optimal policy å¯ä»¥ç›´æ¥æ±‚è§£ BOE å¾—åˆ°. ä¸”æ˜ç¢ºçŸ¥é“ optimal policy ä»£è¡¨è©² policy åœ¨ä»»ä½• state éƒ½èƒ½å¾—åˆ°æœ€å¤§çš„ value.<br>ä½†ä»€éº¼æ˜¯ optimal çš„ continous policy å°±éœ€è¦é€éä¸€äº› metrics ä¾†å®šç¾©å„ªåŠ£.<br>è—‰ç”±é€™äº› metrics ç”¨ SGD ä¾†å„ªåŒ–æ‰¾å‡ºæœ€ä½³ policy, é€™ç¨®æ–¹æ³•ç¨± policy gradient method.<br>Metrics æœ‰å…©ç¨®å¸¸è¦‹çš„å®šç¾©æ–¹å¼</p>
<ul>
<li>[Metric 1] Average state value <span>$\bar{v}_\pi$</span><!-- Has MathJax -->: æ¯ä¸€å€‹ state çš„ value <span>$v_\pi(s)$</span><!-- Has MathJax --> æ ¹æ“š state åˆ†å¸ƒ <span>$d(s)$</span><!-- Has MathJax --> åšå€‹å¹³å‡, åˆ†å…©ç¨®æƒ…æ³<ul>
<li><span>$d(s)$</span><!-- Has MathJax --> è·Ÿ policy <span>$\pi$</span><!-- Has MathJax --> ç„¡é—œ: ç”¨ <span>$\bar{v}_\pi^0$</span><!-- Has MathJax --> å’Œ <span>$d_0$</span><!-- Has MathJax --> ä¾†è¡¨ç¤º. ä¾‹å¦‚ <span>$d(s)=1/|\mathcal{S}|$</span><!-- Has MathJax --> æ˜¯ä¸€å€‹ uniform. æˆ–ç•¶æˆ‘å€‘åªé—œå¿ƒæŸä¸€å€‹ state çš„è©±, <span>$d_0$</span><!-- Has MathJax --> å°±æ˜¯ä¸€å€‹ one-hot vector.</li>
<li><span>$d(s)$</span><!-- Has MathJax --> è·Ÿ policy <span>$\pi$</span><!-- Has MathJax --> æœ‰é—œ: é€šå¸¸è¨­å®šç‚º <a href="https://bobondemon.notion.site/Ch8-Value-Function-with-Function-Approximation-Methods-1efedc3d531d8069a1bae8a51aee01d1#1f0edc3d531d80988640cbdf5789a688" target="_blank" rel="external">stationary distributino</a> <span>$d_\pi(s)$</span><!-- Has MathJax -->, å…¶æ»¿è¶³ <span>$d_\pi=P_\pi d_\pi$</span><!-- Has MathJax -->, å…¶ä¸­ <span>$P_\pi$</span><!-- Has MathJax --> ç‚ºç‹€æ…‹è½‰ç§»çŸ©é™£.</li>
</ul>
</li>
<li>[Metric 2] Average <em>one-step</em> reward <span>$\bar{r}_\pi$</span><!-- Has MathJax -->: ç›®å‰ç‹€æ…‹æ˜¯ <span>$s$</span><!-- Has MathJax --> çš„æƒ…æ³ä¸‹, <em>èµ°ä¸€æ­¥çš„æ¢ä»¶ä¸‹</em>å¾—åˆ°çš„å¹³å‡ immediate rewards<ul>
<li>å…¶ä¸­ <span>$r_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s,\theta){\sum_r}rp(r|s,a)$</span><!-- Has MathJax -->, å‰‡ <span>$\bar{r}_\pi=\sum_{s\in\mathcal{S}}d_\pi(s)r_\pi(s)$</span><!-- Has MathJax -->.</li>
</ul>
</li>
</ul>
<p>ä»¥ä¸‹æœ‰ä¸‰ç¨®ç­‰åƒ¹çš„ expressions éƒ½å¾ˆé‡è¦ (è©³ç´°åƒè€ƒ <a href="https://bobondemon.notion.site/Ch9-Policy-Gradient-Methods-1f1edc3d531d80b6adeeee5796ed9867?pvs=25" target="_blank" rel="external">Week9</a>)</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Expression 1</th>
<th>Expression 2</th>
<th>Expression 3</th>
</tr>
</thead>
<tbody>
<tr>
<td><span>$\bar{v}_\pi$</span><!-- Has MathJax --></td>
<td><span>$\sum_{s\in\mathcal{S}}d(s)v_\pi(s)$</span><!-- Has MathJax --></td>
<td><span>$\mathbb{E}_{S\sim d}[v_\pi(S)]$</span><!-- Has MathJax --></td>
<td><span>$\lim_{n\rightarrow\infty}\mathbb{E}\left[\sum_{t=0}^n{\gamma^t R_{t+1}}\right]$</span><!-- Has MathJax --></td>
</tr>
<tr>
<td><span>$\bar{r}_\pi$</span><!-- Has MathJax --></td>
<td><span>$\sum_{s\in\mathcal{S}}d_\pi(s)r_\pi(s)$</span><!-- Has MathJax --></td>
<td><span>$\mathbb{E}_{S\sim d_\pi}[r_\pi(S)]$</span><!-- Has MathJax --></td>
<td><span>$\lim_{n\rightarrow\infty}\frac{1}{n}\mathbb{E}\left[\sum_{t=0}^{n-1}{R_{t+1}}\right]$</span><!-- Has MathJax --></td>
</tr>
</tbody>
</table>
<p>ç•¶ä½¿ç”¨ stationary <span>$d_\pi$</span><!-- Has MathJax --> æ™‚, å…©è€…ç­‰åƒ¹ <span>${\color{orange}{\bar{r}_\pi=(1-\gamma)\bar{v}_\pi}}$</span><!-- Has MathJax -->.<br>æœ€å¤§åŒ–æ¡ç”¨ gradient ascent, ä½† gradient çš„è¨ˆç®—éå¸¸è¤‡é›œ, é‚„è¦è€ƒæ…®ä¸åŒ cases çš„æƒ…æ³, e.g. discounted or undiscounted cases, ä¸åŒ metrics <span>$\bar{v}_\pi$</span><!-- Has MathJax -->, <span>$\bar{v}_\pi^0$</span><!-- Has MathJax -->, <span>$\bar{r}_\pi$</span><!-- Has MathJax -->.<br>å› æ­¤èª²ç¨‹ä¸Šè€å¸«åªåˆ—å‡º gradient é€šç”¨çš„å½¢å¼:<br><span>$$\begin{align*}
{\color{orange}{\nabla_\theta J(\theta)=\sum_{s\in\mathcal{S}}\eta(s)\sum_{a\in\mathcal{A}}\nabla_\theta\pi(a|s,\theta)q_\pi(s,a)}} \\
{\color{orange}{=\mathbb{E}_{S\sim\eta,A\sim\pi(S,\theta)}\left[\nabla_\theta\ln\pi(A|S,\theta)q_\pi(S,A)\right]}}
\end{align*}$$</span><!-- Has MathJax --> å…¶ä¸­ <span>$J(\theta)$</span><!-- Has MathJax --> å¯ä»¥æ˜¯ <span>$\bar{v}_\pi$</span><!-- Has MathJax -->, <span>$\bar{v}_\pi^0$</span><!-- Has MathJax -->, <span>$\bar{r}_\pi$</span><!-- Has MathJax -->, æœƒå°è‡´ <span>$\eta$</span><!-- Has MathJax --> æœƒå°æ‡‰åˆ°ä¸åŒçš„ state distributions å®šç¾©æˆ–è®Šæˆ weightings, åŒæ™‚ â€œ<span>$=$</span><!-- Has MathJax -->â€ ä¹Ÿæœ‰å¯èƒ½è®Šæˆ â€œ<span>$\approx$</span><!-- Has MathJax -->â€.<br>ç¸½çµå¦‚ä¸‹:<br><span>$$\begin{align*}
\nabla_\theta\bar{r}_\pi\approx \sum_sd_\pi(s)\sum_a \nabla_\theta\pi(a|s,\theta)q_\pi(s,a),\\
\nabla_\theta\bar{v}_\pi=\frac{1}{1-\gamma}\nabla_\theta\bar{r}_\pi,\\
\nabla_\theta\bar{v}_\pi^0=\sum_s\rho_\pi(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_\pi(s,a)
\end{align*}$$</span><!-- Has MathJax --> å…¶ä¸­ <span>$\rho_\pi(s)$</span><!-- Has MathJax --> [<a href="https://bobondemon.notion.site/Ch9-Policy-Gradient-Methods-1f1edc3d531d80b6adeeee5796ed9867#206edc3d531d80828167e908ed6860a7" target="_blank" rel="external">è©³ç´°å®šç¾©</a>] ç‚ºåœ¨ policy <span>$\pi$</span><!-- Has MathJax --> ä¸‹çš„ discounted total probability of transitioning from <span>$s&apos;$</span><!-- Has MathJax --> to <span>$s$</span><!-- Has MathJax -->.<br>æ‰€ä»¥ (Stochastic) Gradient Ascent (GA/SGA) æµç¨‹:<br><span>$$\begin{align*}
\theta_{t+1}=\theta_t + \alpha\nabla_\theta J(\theta_t) \\
\text{(GA)}\quad=\theta_t + \alpha\mathbb{E}\left[\nabla_\theta\ln\pi(A|S,\theta_t)q_\pi(S,A)\right] \\
\text{(SGA)}\quad=\theta_t + \alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t)q_{\color{red}{\pi}}(s_t,a_t)
\end{align*}$$</span><!-- Has MathJax --> æ³¨æ„åˆ°æˆ‘å€‘ç„¡æ³•å¾—çŸ¥ ground truth çš„ <span>$q_{\color{red}{\pi}}(s_t,a_t)$</span><!-- Has MathJax -->, å› æ­¤ç›´æ¥ç”¨ä¼°è¨ˆçš„ <span>$q_{\color{red}{t}}(s_t,a_t)$</span><!-- Has MathJax --> æ›¿ä»£<br>ä¼°è¨ˆçš„ <span>$q_{\color{red}{t}}(s_t,a_t)$</span><!-- Has MathJax --> å¯ä»¥ç”¨å¤šç¨®æ–¹æ³•, å¸¸è¦‹çš„æœ‰ä½¿ç”¨ MC (Monte Carlo) æ–¹æ³•å’Œ TD æ–¹æ³•å…©ç¨® (TD çš„æ–¹æ³•ç‚ºä¸‹ä¸€ç«  Actor-Critic)<br>å…¶ä¸­ <strong>MC æ–¹æ³•çš„ policy gradient åˆç¨±ç‚º REINFORCE</strong> æ–¹æ³•, å…¶ä¼°è¨ˆçš„ <span>$q_{\color{red}{t}}(s_t,a_t)$</span><!-- Has MathJax --> ä½¿ç”¨ <span>$q_t(s_t,a_t)=\sum_{k=t+1}^T\gamma^{k-t-1}r_k$</span><!-- Has MathJax -->:<br>å› æ­¤ REINFORCE æ¼”ç®—æ³•ç‚º<br><img src="/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 9.png" width="80%" height="80%"> é€™ç®—æ˜¯æœ€æ—©ä¸”æœ€ç°¡å–®çš„ policy gradient æ–¹æ³•<br>æ³¨æ„åˆ° GA (13) æ¡æ¨£çš„æ™‚å€™æˆ‘å€‘æ˜¯ follow <span>$S\sim\eta$</span><!-- Has MathJax -->, <span>$A\sim\pi(S,\theta)$</span><!-- Has MathJax -->. <span>$S\sim\eta$</span><!-- Has MathJax --> æ˜¯ä¸€å€‹ long run çš„ distribution å¾Œçš„æ¡æ¨£, é€šå¸¸äººå€‘ä¸å¤ªé—œå¿ƒ<br>ä¸»è¦æ˜¯ <span>$A\sim\pi(S,\theta)$</span><!-- Has MathJax -->, é€™ä½¿å¾— <span>$a_t$</span><!-- Has MathJax --> å¿…é ˆå¾ <span>$\pi(S,\theta_t)$</span><!-- Has MathJax --> æ¡æ¨£å‡ºä¾†, ä¹Ÿå› æ­¤æ›´æ–°çš„ policy åŒæ™‚ä¹Ÿæ˜¯æ¡æ¨£æ™‚å€™çš„ policy, è¡¨ç¤º <strong>REINFORCE æ˜¯ on-policy</strong> ç®—æ³•.</p>
<hr>
<h2 id="Ch10-Actor-Critic-Methods"><a href="#Ch10-Actor-Critic-Methods" class="headerlink" title="[Ch10]: Actor-Critic Methods"></a>[Ch10]: Actor-Critic Methods</h2><h3 id="QAC-Q-function-actor-critic"><a href="#QAC-Q-function-actor-critic" class="headerlink" title="QAC (Q-function actor-critic)"></a>QAC (Q-function actor-critic)</h3><p>ä¸Šé¢çš„ MC policy gradient (REINFORCE) ç®—æ³•, æŠŠ value update çš„åœ°æ–¹æ¡ç”¨ <a href="https://bobondemon.notion.site/notes-mathematical-foundation-of-reinforcement-learning#2cfedc3d531d80d2bd67da5d0625c97a" target="_blank" rel="external">action value with function approximation</a> çš„æ–¹æ³•å°±å¾—åˆ°æœ€ç°¡å–®çš„ QAC äº†<br><img src="/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 10.png" width="70%" height="70%"> æ³¨æ„åˆ°æ”¹æˆ TD è®Šæˆæœ‰ incremental update çš„å¥½è™•, ä¸ç”¨åƒ MC ä¸€æ¨£ç­‰åˆ°ä¸€æ¢ trajectory æ¡æ¨£å®Œæ‰èƒ½åš.</p>
<h3 id="On-policy-A2C-Advantage-actor-critic"><a href="#On-policy-A2C-Advantage-actor-critic" class="headerlink" title="On-policy A2C (Advantage actor-critic)"></a>On-policy A2C (Advantage actor-critic)</h3><p>å…¶å¯¦ A2C å°±æ˜¯ä½¿ç”¨: <a href="https://bobondemon.github.io/2025/06/04/REINFORCE-estimator/">https://bobondemon.github.io/2025/06/04/REINFORCE-estimator/</a> æåˆ°çš„ baseline + control-variates æŠ€å·§, å»æ”¹é€² QAC çš„çµæœ.<br>é‡è¤‡ä¸€æ¬¡ policy gradient çš„å…¬å¼ <span>$\nabla_\theta J(\theta_t)=\mathbb{E}_{S\sim\eta,A\sim\pi}\left[\nabla_\theta\ln\pi(A|S,\theta_t)q_\pi(S,A)\right]$</span><!-- Has MathJax -->.<br>SGA æ¯æ¬¡æ¡æ¨£å‡ºä¸€å€‹ <span>$\nabla_\theta\ln\pi(A|S,\theta_t)q_\pi(S,A)$</span><!-- Has MathJax --> ä¾†æ›´æ–°, è§€å¯Ÿåˆ°å¦‚æœåŠ ä¸Šä¸€å€‹èˆ‡ <span>$\mathcal{A}$</span><!-- Has MathJax --> ç„¡é—œçš„è®Šæ•¸ <span>$b(S)$</span><!-- Has MathJax --> å°æ–¼è¨ˆç®—æœŸæœ›å€¼ä¸æœƒæœ‰ä»»ä½•å½±éŸ¿<br><span>$$\begin{align*}
X\doteq\nabla_\theta\ln\pi(A|S,\theta_t)(q_\pi(S,A)-{\color{orange}{b(S)}}) \\
\Longrightarrow \mathbb{E}_{S\sim\eta,A\sim\pi}[X]=\nabla_\theta J(\theta_t)
\end{align*}$$</span><!-- Has MathJax --> ä½†æ˜¯ <span>$Var(X)$</span><!-- Has MathJax --> ä»æœƒè¢« <span>$b(S)$</span><!-- Has MathJax --> å½±éŸ¿, å³ <span>$b(S)$</span><!-- Has MathJax --> ä¸åŒå‰‡ç®—å‡ºä¾†çš„ <span>$Var(X)$</span><!-- Has MathJax --> ä¸åŒ, å› æ­¤ç›®æ¨™å°±æ˜¯æ‰¾å‡ºä¸€å€‹æœ€ä½³çš„ baseline <span>$b(S)$</span><!-- Has MathJax --> ä½¿å¾— <span>$Var(X)$</span><!-- Has MathJax --> æœ€å°<br>é€™æ¨£åš SGA æ¡æ¨£å‡ºä¾†çš„ sample ä¹‹é–“ variance å°±è®Šå°, æ”¶æ–‚å¿«åˆç©©å®š. è€Œç†è«–ä¸Šæœ€ä½³è§£ç‚º:<br><span>$$\begin{align*}
b^\ast(s)=\frac{\mathbb{E}_{A\sim\pi}[ {\color{orange}{\|\nabla_\theta\ln\pi(A|s,\theta_t)\|^2}} q_\pi(s,A) ]}
{\mathbb{E}_{A\sim\pi}[ {\color{orange}{\|\nabla_\theta\ln\pi(A|s,\theta_t)\|^2}}]}
\end{align*}$$</span><!-- Has MathJax --> é›–ç„¶é€™æ˜¯æœ€ä½³è§£, ä½†æ¯”è¼ƒè¤‡é›œé€šå¸¸ä¸ç”¨, æˆ‘å€‘ç›´æ¥æŠŠæ©˜è‰²çš„æ¬Šé‡æ‹”æ‰è®Šæˆ <span>${\color{orange}{b(s)}}=\mathbb{E}_{A\sim\pi}[q_\pi(s,A)]{\color{orange}{=v_\pi(s)}}$</span><!-- Has MathJax -->. Baseline <span>$b(s)$</span><!-- Has MathJax --> å°±è¨­å®šæˆ state value <span>$v(s)$</span><!-- Has MathJax --> å³å¯!<br>åŠ ä¸Š basline term çš„ policy gradient è®Šæˆ:<br><span>$$\begin{align*}
{\color{orange}{\nabla_\theta J(\theta_t)}} = \mathbb{E}_{S\sim\eta,A\sim\pi}
\left[\nabla_\theta\ln\pi(A|S,\theta_t) {\color{orange}{\delta_\pi(S,A)}}\right] \\
{\color{orange}{\delta_\pi(S,A)\doteq q_\pi(S,A)-{v_\pi(S)}}}
\end{align*}$$</span><!-- Has MathJax --> <span>$\delta_\pi(S,A)$</span><!-- Has MathJax --> ç¨±ç‚º advantage function.<br>æ‰€ä»¥ä¹‹å‰çš„ <a href="https://bobondemon.notion.site/Ch9-Policy-Gradient-Methods-1f1edc3d531d80b6adeeee5796ed9867#206edc3d531d8050ac4ede2a5ac70165" target="_blank" rel="external">policy gradient å…¶ gradient ascent å…¬å¼</a>æ”¹ç‚º:<br><span>$$\begin{align*}
\theta_{t+1}=\theta_t + \alpha\nabla_\theta J(\theta_t) \\
\text{(GA)}\quad=\theta_t + \alpha\mathbb{E}\left[\nabla_\theta\ln\pi(A|S,\theta_t){\color{orange}{\delta_\pi(S,A)}}\right] \\
\text{(SGA)}\quad=\theta_t + \alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t) {\color{orange}{\delta_{\color{red}{t}}(s_t,a_t)}}
\end{align*}$$</span><!-- Has MathJax --> å¾ GA åˆ° SGA æˆ‘å€‘ä¹ŸæŠŠç„¡æ³•å¾—çŸ¥çš„ <span>$\delta_\pi(s_t,a_t)$</span><!-- Has MathJax --> ç”¨ä¼°è¨ˆçš„ <span>$\delta_t(s_t,a_t)$</span><!-- Has MathJax --> ä¾†æ›¿ä»£<br>å…¶ä¸­ <strong><span>$\delta_t(s_t,a_t)$</span><!-- Has MathJax --> å¯ä»¥ç”¨ temporal difference</strong> çš„æƒ³æ³•ä¾†é€¼è¿‘:<br><span>$$\begin{align*}
\delta_t(s_t,a_t)\doteq {\color{orange}{q_t(s_t,a_t)}}-v_t(s_t) \approx {\color{orange}{r_{t+1}+\gamma v_t(s_{t+1})}} -v_t(s_t)
\end{align*}$$</span><!-- Has MathJax --> é€™å¸¶ä¾†çš„å¥½è™•æ˜¯åªéœ€è¦ç”¨ä¸€å€‹ NN å­¸ <span>$v_t$</span><!-- Has MathJax --> å³å¯.</p>
<h3 id="Off-policy-A2C-Advantage-actor-critic"><a href="#Off-policy-A2C-Advantage-actor-critic" class="headerlink" title="Off-policy A2C (Advantage actor-critic)"></a>Off-policy A2C (Advantage actor-critic)</h3><p>å†çµåˆ <a href="https://bobondemon.notion.site/Ch10-Actor-Critic-Methods-207edc3d531d80beaf1afd55a86b78a2#209edc3d531d8078a7bbd6b8d8441c82" target="_blank" rel="external">Important Sampling (IS)</a> å°‡ on-policy çš„ A2C è®Šæˆ off-policy, å‰‡ policy gradient å…¬å¼è®Šæˆ:<br><span>$$\begin{align*}
\nabla_\theta J(\theta_t)=\mathbb{E}_{S\sim\eta,A\sim{\color{orange}{\pi}}}
\left[\nabla_\theta\ln\pi(A|S,\theta_t) \delta_\pi(S,A)\right] \\
=\mathbb{E}_{S\sim\eta,A\sim{\color{orange}{\beta}}}
\left[
{\color{orange}{\frac{\pi(A|S,\theta)}{\beta(A|S)}}}
\nabla_\theta\ln\pi(A|S,\theta_t) \delta_\pi(S,A)
\right] \\
=\mathbb{E}_{S\sim\eta,A\sim{\color{orange}{\beta}}}
\left[
{\color{orange}{\frac{\delta_\pi(S,A)}{\beta(A|S)}}}
\nabla_\theta\pi(A|S,\theta_t)
\right]
\end{align*}$$</span><!-- Has MathJax --> <span>$\beta$</span><!-- Has MathJax --> æ˜¯ behavior policy, user å®šç¾©åœ°æ‰€ä»¥å¯ä»¥è¨ˆç®— <span>$\beta(A|S)$</span><!-- Has MathJax -->. å› æ­¤<br><span>$$\begin{align*}
\theta_{t+1}=\theta_t + \alpha\nabla_\theta J(\theta_t) \\
\text{(GA)}\quad=\theta_t + \alpha\mathbb{E}_{S\sim\eta,A\sim{\color{orange}{\beta}}}
\left[
{\color{orange}{\frac{\delta_\pi(S,A)}{\beta(A|S)}}}
\nabla_\theta\pi(A|S,\theta_t)
\right] \\
\text{(SGA)}\quad=\theta_t + \alpha
{\color{orange}{\frac{\delta_{\color{red}t}(s_t,a_t)}{\beta(a_t|s_t)}}}
\nabla_\theta\pi(a_t|s_t,\theta_t)
\end{align*}$$</span><!-- Has MathJax --> æ³¨æ„åˆ°å¾ GA è®Šæˆ SGA çš„æ™‚å€™, æˆ‘å€‘ä¹Ÿå°‡ç„¡æ³•å¾—çŸ¥çš„ advantage function <span>$\delta_\pi$</span><!-- Has MathJax --> æ›¿é‚„æˆç›®å‰æ­£åœ¨ä¼°è¨ˆçš„ <span>$\delta_t$</span><!-- Has MathJax --> äº†. å› æ­¤ important weight <span>$\delta_t/\beta$</span><!-- Has MathJax --> å¾ˆå®¹æ˜“è¨ˆç®—.</p>
<blockquote>
<p>ğŸ’¡ ç”¨ PyTorch çš„ auto-diff å¯¦ä½œçš„æ™‚å€™ä¸æœƒè‡ªå·±åˆ» gradient, æˆ‘å€‘åªéœ€è¦å®šç¾©å¥½ç­‰åƒ¹çš„ loss function<br>è€Œå°æ‡‰çµ¦ PyTorch çš„ loss function ç‚º: (åŒæ¨£ä¹ŸæŠŠ advantage function <span>$\delta_\pi$</span><!-- Has MathJax --> æ›¿é‚„æˆé‚„åœ¨ä¼°è¨ˆçš„ <span>$\delta_t$</span><!-- Has MathJax -->)<br><span>$$\begin{align*}
 J(\theta)=\mathbb{E}_{S\sim\eta,A\sim{\color{orange}{\beta}}}\left[\frac{\pi(A|S)}{\beta(A|S)}\delta_{\color{red}t}(S,A)\right]\end{align*}$$</span><!-- Has MathJax --> (ç”¨ log derivative trick å¾ˆå®¹æ˜“é©—è­‰, or <a href="https://youtu.be/mg-iU-WxiNs?si=ki2-fJGXwTBia0R8&amp;t=930" target="_blank" rel="external">see here</a>) é€™å€‹ç›®æ¨™å‡½å¼é€šå¸¸ç¨±ç‚º <strong>surrogate objective</strong> ä¹Ÿæ‰æ˜¯æœ€å¾Œ NN è¦åš training çš„ loss.</p>
</blockquote>
<p>çµè«–, A2C çš„ off-policy psuedo code å¦‚ä¸‹:<br><img src="/2025/12/25/RLçš„æ•¸å­¸åŸç†/image 11.png" width="80%" height="80%"><br>æ³¨æ„åˆ°é€™é‚Šæˆ‘å€‘å° state value <span>$v_t$</span><!-- Has MathJax --> æ¡ç”¨ function approximation æ–¹æ³•, åƒè€ƒ <a href="https://bobondemon.notion.site/Ch8-Value-Function-with-Function-Approximation-Methods-1efedc3d531d8069a1bae8a51aee01d1#1f0edc3d531d807db9d5e0ba226580bb" target="_blank" rel="external">Ch8 é€™è£¡</a>. ä½†æ˜¯è¦å†åŠ ä¸Š importance weight ä¿®æ­£.</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>Post authorï¼š</strong>
      Chih-Sheng Chen
    </li>
    <li class="post-copyright-link">
      <strong>Post linkï¼š</strong>
      <a href="https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/" title="RLçš„æ•¸å­¸åŸç†: è¶™ä¸–éˆºèª²ç¨‹æ¿ƒç¸®ç­†è¨˜">https://bobondemon.github.io/2025/12/25/RLçš„æ•¸å­¸åŸç†/</a>
    </li>
    <li class="post-copyright-license">
      <strong>Copyright Noticeï¼š </strong>
      All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
          
            <a href="/tags/TD-Learning/" rel="tag"># TD Learning</a>
          
            <a href="/tags/Q-Learning/" rel="tag"># Q Learning</a>
          
            <a href="/tags/DQN/" rel="tag"># DQN</a>
          
            <a href="/tags/Actor-Critic/" rel="tag"># Actor Critic</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2025/12/11/IndexTTS2-ç­†è¨˜-by-AI/" rel="next" title="IndexTTS2 ç­†è¨˜ by AI">
                <i class="fa fa-chevron-left"></i> IndexTTS2 ç­†è¨˜ by AI
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            æ–‡ç« ç›®éŒ„
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            æœ¬ç«™æ¦‚è¦½
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.png"
               alt="Chih-Sheng Chen" />
          <p class="site-author-name" itemprop="name">Chih-Sheng Chen</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">114</span>
                <span class="site-state-item-name">æ–‡ç« </span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">åˆ†é¡</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">238</span>
                <span class="site-state-item-name">æ¨™ç±¤</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch1-Basic-Concepts"><span class="nav-number">1.</span> <span class="nav-text">[Ch1]: Basic Concepts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch2-Bellman-Equation"><span class="nav-number">2.</span> <span class="nav-text">[Ch2]: Bellman Equation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch3-Bellman-Optimality-Equation"><span class="nav-number">3.</span> <span class="nav-text">[Ch3]: Bellman Optimality Equation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch4-Value-Iteration-amp-Policy-Iteration"><span class="nav-number">4.</span> <span class="nav-text">[Ch4]: Value Iteration & Policy Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch5-Monte-Carlo-Methods-for-Prediction-amp-Control"><span class="nav-number">5.</span> <span class="nav-text">[Ch5]: Monte Carlo Methods for Prediction & Control</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch6-Robbins-Monro-Algorithm-å’Œ-Dvoretzkyâ€™s-Convergence-Theorem-ç­†è¨˜"><span class="nav-number">6.</span> <span class="nav-text">[Ch6]: Robbins-Monro Algorithm å’Œ Dvoretzkyâ€™s Convergence Theorem ç­†è¨˜</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch7-Temporal-Difference-Learning-Methods-for-Prediction-Control"><span class="nav-number">7.</span> <span class="nav-text">[Ch7]: Temporal Difference Learning Methods for Prediction/Control</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch8-Value-Function-with-Function-Approximation-Methods"><span class="nav-number">8.</span> <span class="nav-text">[Ch8]: Value Function with Function Approximation Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#State-value-with-function-approximation"><span class="nav-number">8.1.</span> <span class="nav-text">State value with function approximation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Action-value-with-function-approximation"><span class="nav-number">8.2.</span> <span class="nav-text">Action value with function approximation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-learning-with-function-approximation"><span class="nav-number">8.3.</span> <span class="nav-text">Q-learning with function approximation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Q-learning-or-Deep-Q-network-DQN"><span class="nav-number">8.4.</span> <span class="nav-text">Deep Q-learning or Deep Q-network (DQN)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch9-Policy-Gradient-Methods"><span class="nav-number">9.</span> <span class="nav-text">[Ch9]: Policy Gradient Methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch10-Actor-Critic-Methods"><span class="nav-number">10.</span> <span class="nav-text">[Ch10]: Actor-Critic Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#QAC-Q-function-actor-critic"><span class="nav-number">10.1.</span> <span class="nav-text">QAC (Q-function actor-critic)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#On-policy-A2C-Advantage-actor-critic"><span class="nav-number">10.2.</span> <span class="nav-text">On-policy A2C (Advantage actor-critic)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Off-policy-A2C-Advantage-actor-critic"><span class="nav-number">10.3.</span> <span class="nav-text">Off-policy A2C (Advantage actor-critic)</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chih-Sheng Chen</span>
</div>


<div class="powered-by">
  ç”± <a class="theme-link" href="https://hexo.io">Hexo</a> å¼·åŠ›é©…å‹•
</div>

<div class="theme-info">
  ä¸»é¡Œ -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>




<span id="busuanzi_container_site_pv">
  æœ¬ç«™ç¸½ç€è¦½ <span id="busuanzi_value_site_pv"></span> æ¬¡
</span>
<span id="busuanzi_container_site_uv">
  æœ¬ç«™è¨ªå®¢ <span id="busuanzi_value_site_uv"></span> äºº
</span>

        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      [object Object]
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      [object Object]
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  




	





  





  





  






  





  

  

  

  

</body>
</html>
