<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-tw">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="CNN,PyTorch,Batch Normalization,Fake Quantization,Quantization Aware Training (QAT)," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="常見的 NN blocks: Convolution (Conv) —&amp;gt; Batch Normalization (BN) —&amp;gt; ReLU這 3 個 OPs 在量化後 inference 的時候可以直接融合成一個 OP:&amp;emsp; - Conv —&amp;gt; BN 可以融合是因為 BN 可以視為一個 1x1 convolution, 所以兩者的 weights 可以合併&amp;ems">
<meta property="og:type" content="article">
<meta property="og:title" content="Convolution 與 BatchNorm 的融合大法：從推論、QAT 到 PyTorch 的加速">
<meta property="og:url" content="https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/index.html">
<meta property="og:site_name" content="棒棒生">
<meta property="og:description" content="常見的 NN blocks: Convolution (Conv) —&amp;gt; Batch Normalization (BN) —&amp;gt; ReLU這 3 個 OPs 在量化後 inference 的時候可以直接融合成一個 OP:&amp;emsp; - Conv —&amp;gt; BN 可以融合是因為 BN 可以視為一個 1x1 convolution, 所以兩者的 weights 可以合併&amp;ems">
<meta property="og:image" content="https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/Untitled.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/image.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/image 1.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/image 2.png">
<meta property="og:image" content="https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/image 3.png">
<meta property="og:updated_time" content="2025-05-04T02:00:59.998Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Convolution 與 BatchNorm 的融合大法：從推論、QAT 到 PyTorch 的加速">
<meta name="twitter:description" content="常見的 NN blocks: Convolution (Conv) —&amp;gt; Batch Normalization (BN) —&amp;gt; ReLU這 3 個 OPs 在量化後 inference 的時候可以直接融合成一個 OP:&amp;emsp; - Conv —&amp;gt; BN 可以融合是因為 BN 可以視為一個 1x1 convolution, 所以兩者的 weights 可以合併&amp;ems">
<meta name="twitter:image" content="https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/Untitled.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/"/>





  <title> Convolution 與 BatchNorm 的融合大法：從推論、QAT 到 PyTorch 的加速 | 棒棒生 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-tw">

  




<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '[object Object]', 'auto');
  ga('send', 'pageview');
</script>











  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">棒棒生</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">讓學習變成一種習慣</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首頁
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分類
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            關於
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            歸檔
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            標籤
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chih-Sheng Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="棒棒生">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Convolution 與 BatchNorm 的融合大法：從推論、QAT 到 PyTorch 的加速
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2025-05-04T09:04:59+08:00">
                2025-05-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分類於</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>[object Object]
            </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<hr>
<p>常見的 NN blocks: Convolution (Conv) —&gt; Batch Normalization (BN) —&gt; ReLU<br>這 3 個 OPs 在量化後 inference 的時候可以直接融合成一個 OP:<br>&emsp; - Conv —&gt; BN 可以融合是因為 BN 可以視為一個 1x1 convolution, 所以兩者的 weights 可以合併<br>&emsp; - ReLU 可以合併是因為在 QAT 時, 可以被 fake quant 的 quantization parameter 處理掉</p>
<p>本文筆記 Conv+BN 的合併, 分 3 部分:<br>&emsp; - 先討論 inference 階段怎麼合併 (已經都 train 好的情況下) [<a href="https://nenadmarkus.com/p/fusing-batchnorm-and-conv/" target="_blank" rel="external">來源</a>]<br>&emsp; - 再來討論做 QAT 時, 怎麼插 <a href="https://github.com/bobondemon/quantization_study/blob/main/pic/quantize_ops.png" target="_blank" rel="external">fake quant</a> 效果才會好 [<a href="https://arxiv.org/abs/1806.08342" target="_blank" rel="external">來源</a>]<br>&emsp; - 最後看看 PyTorch 怎麼實作, 更重要的是, 怎麼加速?</p>
<a id="more"></a>
<hr>
<h2 id="Inference-階段融合-Conv-BN"><a href="#Inference-階段融合-Conv-BN" class="headerlink" title="Inference 階段融合 Conv+BN"></a>Inference 階段融合 Conv+BN</h2><p>這段筆記來源: <a href="https://nenadmarkus.com/p/fusing-batchnorm-and-conv/" target="_blank" rel="external">https://nenadmarkus.com/p/fusing-batchnorm-and-conv/</a><br>回顧一下, 給定一個 minibatch $(N,C,H,W)$, BN 對 $(N,H,W)$ 做 normalization: [<a href="https://arxiv.org/abs/1903.10520v2" target="_blank" rel="external">圖來源</a>]<br><img src="/2025/05/04/Convolution-與-BatchNorm-的融合大法/Untitled.png" width="50%" height="50%"><br>也就是對每一個 channel $c$ 計算出 mean $\mu_c$ 和 standard deviation $\sigma_c$:</p>
<blockquote>
<p>例如 input tensor <code>x_in = torch.randn(n, c, h, w)</code> , <code>mu_c = torch.einsum(&#39;nchw-&gt;c&#39;, x_in) / (n * h * w)</code><br>所以 <code>mu_c.shape == c</code>.</p>
</blockquote>
<p>對 batch 裡每一筆 feature map $x=(C,H,W)$, BN 對每一個 $c\in[1,C]$ 作如下的 normalization:<br><span>$$\hat{x}_c=\gamma\frac{x_c-\mu_c}{\sigma_c}+\beta$$</span><!-- Has MathJax --> 其中 $\gamma,\beta$ 是 BN 的參數, 學出來的. 另外通常為了數值穩定不會直接除 $\sigma_c$, 而是會除 <span>$\sigma_c+\epsilon$</span><!-- Has MathJax -->.<br>這個操作可以寫成一個 1x1 conv:<br><span>$$\begin{align}

\left[\begin{array}{c}
\hat{x}_{1,i,j}\\\hat{x}_{2,i,j}\\\vdots\\\hat{x}_{C-1,i,j} \\\hat{x}_{C,i,j}
\end{array}\right] =

\underbrace{
\left[\begin{array}{ccccc}
\frac{\gamma_1}{\sigma_1} &amp; 0 &amp; \cdots &amp;  &amp; 0 \\
0 &amp; \frac{\gamma_2}{\sigma_2} &amp; &amp; &amp; \\
\vdots &amp; &amp; \ddots &amp; &amp; \vdots \\
&amp; &amp; &amp; \frac{\gamma_{C-1}}{\sigma_{C-1}} &amp; 0 \\
0 &amp; \cdots &amp; &amp; &amp; \frac{\gamma_C}{\sigma_C} \\
\end{array}\right]
}_{W_{bn}}

\left[\begin{array}{c}
x_{1,i,j} \\ x_{2,i,j} \\ \vdots \\ x_{C-1,i,j} \\ x_{C,i,j}
\end{array}\right] + 

\underbrace{
\left[\begin{array}{c}
\beta_1-\gamma_1\frac{\mu_1}{\sigma_1} \\ \beta_2-\gamma_2\frac{\mu_2}{\sigma_2} \\ \vdots \\ \beta_{C-1}-\gamma_{C-1}\frac{\mu_{C-1}}{\sigma_{C-1}} \\ \beta_C-\gamma_C\frac{\mu_C}{\sigma_C}
\end{array}\right]
}_{b_{bn}}

\end{align}$$</span><!-- Has MathJax --> 其中 <span>$W_{bn}\in\mathbb{R}^{C\times C}$</span><!-- Has MathJax -->, <span>$b_{bn}\in\mathbb{R}^{C\times 1}$</span><!-- Has MathJax -->.<br>我們假設前一層 kernel size $k\times k$ 的 convolution 參數為 <span>$W_{conv}\in\mathbb{R}^{C\times(C_{prev}\cdot k^2)}$</span><!-- Has MathJax -->, <span>$b_{conv}\in\mathbb{R}^{C\times 1}$</span><!-- Has MathJax -->, <span>$C_{prev}$</span><!-- Has MathJax --> 表示 convolution 的 input channel<br>對 input feature map <span>$\mathbf{f}_{i,j}$</span><!-- Has MathJax --> 來說, 根據 convolution 做法的定義, 知道 <span>$\mathbf{f}_{i,j}\in\mathbb{R}^{(C_{prev}\cdot k^2)\times1}$</span><!-- Has MathJax -->,<br>則 Conv—&gt;BN:<br><span>$$\begin{align*}
\hat{\mathbf{f}}_{i,j}=W_{bn}\cdot(W_{conv}\cdot\mathbf{f}_{i,j}+b_{conv})+b_{bn} \\
\Longrightarrow \hat{\mathbf{f}}_{i,j}=(W_{bn}\cdot W_{conv})\cdot\mathbf{f}_{i,j} + (W_{bn}\cdot b_{conv} + b_{bn})
\end{align*}$$</span><!-- Has MathJax --> 所以合併後的 weight and bias:<br><span>$$\begin{align}
W_{fused}=W_{bn}\cdot W_{conv} \\
b_{fused}=W_{bn}\cdot b_{conv} + b_{bn}
\end{align}$$</span><!-- Has MathJax --> 改寫一下:<br><span>$$\begin{align}
W_{fused}=\frac{\gamma W_{conv}}{\sigma} \\
b_{fused}=\frac{\gamma b_{conv}}{\sigma}+\beta-\frac{\gamma\mu}{\sigma}=\beta-\gamma\frac{\mu-b_{conv}}{\sigma}
\end{align}$$</span><!-- Has MathJax --> Python exmple codes 可參考<a href="https://nenadmarkus.com/p/fusing-batchnorm-and-conv/" target="_blank" rel="external">來源</a></p>
<hr>
<h2 id="QAT-對-Conv-BN-插-Fake-quant"><a href="#QAT-對-Conv-BN-插-Fake-quant" class="headerlink" title="QAT 對 Conv+BN 插 Fake-quant"></a>QAT 對 Conv+BN 插 Fake-quant</h2><p>這段筆記來自於論文 “Quantizing deep convolutional networks for efficient inference: A whitepaper” [<a href="https://arxiv.org/abs/1806.08342" target="_blank" rel="external">arxiv</a>], 圖為還原論文內容只是我重新畫而以.<br>觀察 (3), 可以發現如果 convolution 的 bias 為 0, 融合後仍有 bias 項 (由 BN 提供), 所以在 Conv+BN 情況下, Conv 可以不用設定 bias.<br>因此以下討論的 Conv 只有 weight 沒有 bias. 對 (4), (5) 重新命名改寫:<br><span>$$\begin{align}
W_{train}=\frac{\gamma W}{\sigma_B}, \quad b_{train}=\beta-\gamma\frac{\mu_B}{\sigma_B} \\
W_{inf}=\frac{\gamma W}{\sigma}, \quad b_{inf}=\beta-\gamma\frac{\mu}{\sigma}
\end{align}$$</span><!-- Has MathJax --> 其中 $\mu_B,\sigma_B$ 為針對一個 batch 統計出來的 mean 和 std (training用的), 而 $\mu,\sigma$ 則是他們的 EMA, 即 exponential moving average (inference 用的).<br>$W$ 直接就是 Conv 的 weight, <span>$W_{train},W_{inf}$</span><!-- Has MathJax --> 分別表示 training 和 inference 時的 fused weight, 同理 <span>$b_{train},b_{inf}$</span><!-- Has MathJax -->.</p>
<h3 id="Baseline-插-fake-quant"><a href="#Baseline-插-fake-quant" class="headerlink" title="Baseline 插 fake-quant"></a>Baseline 插 fake-quant</h3><p>分開看 training 和 inference 怎麼插 fake quant.</p>
<p><strong>[Inference]</strong> (下圖左)<strong>:</strong><br>假設參數都已經訓練好了, 我們直接使用 (7) 融合 Conv 和 BN 得到 <span>$W_{inf}$</span><!-- Has MathJax --> 和 <span>$b_{inf}$</span><!-- Has MathJax --> 並插 fake quant 即可. (當然真正 inference 要再轉 integer)<br>注意到 inference 使用的是 EMA 的 $\mu$ 和 $\sigma$.</p>
<p><strong>[Training]</strong> QAT(下圖右)<strong>:</strong><br>由於 BN 訓練的時候要使用 batch 的 $\mu_B$ 和 $\sigma_B$, 因此相比於 inference 時多了要計算 $\mu_B$ 和 $\sigma_B$ 的運算. 看下圖右可以發現, 多了一次 convolution 只為了得到 $\mu_B$ 和 $\sigma_B$.<br>然後使用 (6) 得到 <span>$W_{train}$</span><!-- Has MathJax --> 和 <span>$b_{train}$</span><!-- Has MathJax -->.<br><img src="/2025/05/04/Convolution-與-BatchNorm-的融合大法/image.png" width="100%" height="100%"><br>但是注意到, 這麼做 QAT 效果不好<br>因為 $\mu_B,\sigma_B$ 是針對一個 batch 去統計出來的本身就會變化劇烈, 如果再加上 fake quant 的 error (含 STE 的 gradient error) 會讓整個訓練很不穩定<br>論文實驗顯示 training loss 有 jitter 現象 (見下圖綠色 curve), 更詳細見<a href="https://arxiv.org/abs/1806.08342" target="_blank" rel="external">論文</a>的 Fig14 and 15.<br><img src="/2025/05/04/Convolution-與-BatchNorm-的融合大法/image 1.png" width="60%" height="60%"><br>但如果 training 時使用 EMA $\mu,\sigma$ 這又不對, 會失去 BN 的效果.<br>所以這就面臨了兩難. 因此論文一個重要的貢獻就是解決此問題.</p>
<hr>
<h3 id="Fake-Quant-With-Correction-Term"><a href="#Fake-Quant-With-Correction-Term" class="headerlink" title="Fake Quant With Correction Term"></a>Fake Quant With Correction Term</h3><p><img src="/2025/05/04/Convolution-與-BatchNorm-的融合大法/image 2.png" width="75%" height="75%"></p>
<p><strong>[Training]:</strong><br>最大的改動就是對 $W$ 做 fake quant $fq(\cdot)$  的時候使用的是 <span>$W_{inf}$</span><!-- Has MathJax -->, 這樣就能避免上面提到的訓練不穩定現象 (jitter). 看上圖能知道:<br><span>$$fq\left(\frac{\sigma_B}{\sigma}\cdot W\cdot \frac{\gamma}{\sigma_B}\right) 
= fq\left(\frac{\gamma W} {\sigma}\right) = fq(W_{inf})$$</span><!-- Has MathJax --> 但是我們希望 training 的時候仍然使用 <span>$W_{train}$</span><!-- Has MathJax --> (即使用 batch 的統計結果 $\mu_B,\sigma_B$ ), 所以乘上一個 correction term $\sigma/\sigma_B$ 還原, 意思是:<br><span>$$\begin{align*}
\frac{\sigma}{\sigma_B}\cdot fq(W_{inf})\approx fq\left(\frac{\sigma}{\sigma_B}\cdot\frac{\gamma W} {\sigma}\right) \\
=fq\left(\frac{\gamma W}{\sigma_B}\right)=fq(W_{train})
\end{align*}$$</span><!-- Has MathJax --> 這樣我們就能在 training 的時候就能使用 <span>$W_{train}$</span><!-- Has MathJax -->, 且又能避免 fake quant 造成的不穩定.<br>觀察一下 bias term, 注意到此時<strong><em>不能</em></strong> freeze BN status 所以圖中的邏輯閘為 False:<br><span>$$0 + \beta - \frac{\gamma\mu_B}{\sigma_B} = \beta-\frac{\mu_B}{\sigma_B} = b_{train}$$</span><!-- Has MathJax --></p>
<p><strong>[Inference]:</strong><br>此時要 freeze BN status 所以邏輯閘為 True<br>因此<strong><em>不用</em></strong>乘上 correction term $\sigma/\sigma_B$, 所以用的是 <span>$W_{inf}$</span><!-- Has MathJax --> 去做 fake quant.<br>觀察一下 bias term, 設定邏輯閘為 True:<br><span>$$\gamma\left(\frac{\mu_B}{\sigma_B}-\frac{\mu}{\sigma}\right) + \beta - \frac{\gamma\mu_B}{\sigma_B} = \beta-\frac{\mu}{\sigma} = b_{inf}$$</span><!-- Has MathJax --></p>
<hr>
<h2 id="PyTorch-作法"><a href="#PyTorch-作法" class="headerlink" title="PyTorch 作法"></a>PyTorch 作法</h2><p>上面提到的作法 “fake quant with correction term” 就是 PyTorch <code>_ConvBnNd</code> 這個 class 的 <code>_forward_slow</code> 作法 [<a href="https://github.com/pytorch/pytorch/blob/main/torch/ao/nn/intrinsic/qat/modules/conv_fused.py#L162" target="_blank" rel="external">code link</a>]<br>名字上都有 slow 這個字眼了, 但為什麼是 slow 呢?<br>其實我們上面有提過, 為了計算一個 batch 的 $\mu_B$ 和 $\sigma_B$ 要多一次 convolution 運算.<br>PyTorch 做了 <code>_forward_approximate</code> [<a href="https://github.com/pytorch/pytorch/blob/main/torch/ao/nn/intrinsic/qat/modules/conv_fused.py#L133" target="_blank" rel="external">code link</a>] 來加速, 但注意到如同函數名字一樣, 雖然加速, 但這是 approximate 作法. (也是預設做法)<br>我們來分析看看 PyTorch 怎麼避掉那個多的 covolution 吧…<br><img src="/2025/05/04/Convolution-與-BatchNorm-的融合大法/image 3.png" width="60%" height="60%"><br>跟論文做法 (<code>_forward_slow</code>) 最主要差別是在套用 BN 的時候, $\mu_B$ 和 $\sigma_B$ 的統計是<strong><em>已經經過 fake quant 後</em></strong>的值去統計出來的<br>注意到原本論文作法, $\mu_B$ 和 $\sigma_B$ 使用的是最精準的 float 結果 (無 fake quant 損失) 去統計的.</p>
<blockquote>
<p>8-bit quantize 經驗上做這樣的 approximate 影響不大, 或許在 lower bit rate, e.g. &lt;4-bits, 情況下才可能要注意?!</p>
</blockquote>
<p>雖然是 approximate 作法, 但少了一次 convolution 運算就快不少.<br>對應的 <a href="https://github.com/pytorch/pytorch/blob/main/torch/ao/nn/intrinsic/qat/modules/conv_fused.py#L133" target="_blank" rel="external">PyTorch 官方實作</a>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_forward_approximate</span><span class="params">(self, input)</span>:</span></div><div class="line">        <span class="string">"""Approximated method to fuse conv and bn. It requires only one forward pass.</span></div><div class="line">        conv_orig = conv / scale_factor where scale_factor = bn.weight / running_std</div><div class="line">        """</div><div class="line">        <span class="keyword">assert</span> self.bn.running_var <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></div><div class="line">        running_std = torch.sqrt(self.bn.running_var + self.bn.eps)</div><div class="line">        scale_factor = self.bn.weight / running_std</div><div class="line">        weight_shape = [<span class="number">1</span>] * len(self.weight.shape)</div><div class="line">        weight_shape[<span class="number">0</span>] = <span class="number">-1</span></div><div class="line">        bias_shape = [<span class="number">1</span>] * len(self.weight.shape)</div><div class="line">        bias_shape[<span class="number">1</span>] = <span class="number">-1</span></div><div class="line">        scaled_weight = self.weight_fake_quant(</div><div class="line">            self.weight * scale_factor.reshape(weight_shape)</div><div class="line">        )</div><div class="line">        <span class="comment"># using zero bias here since the bias for original conv</span></div><div class="line">        <span class="comment"># will be added later</span></div><div class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            zero_bias = torch.zeros_like(self.bias, dtype=input.dtype)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            zero_bias = torch.zeros(</div><div class="line">                self.out_channels, device=scaled_weight.device, dtype=input.dtype</div><div class="line">            )</div><div class="line">        conv = self._conv_forward(input, scaled_weight, zero_bias)</div><div class="line">        conv_orig = conv / scale_factor.reshape(bias_shape)</div><div class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            conv_orig = conv_orig + self.bias.reshape(bias_shape)</div><div class="line">        conv = self.bn(conv_orig)</div><div class="line">        <span class="keyword">return</span> conv</div></pre></td></tr></table></figure></p>
<hr>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>總結來說幾個要點:</p>
<ul>
<li>對 convolution weight $W$ 做 fake quant 的時候要採用 EMA mean/std $\mu,\sigma$.</li>
<li>QAT 訓練的時候, 給 ReLU 的 input activation 仍然要使用 $\mu_B,\sigma_B$, 這是因為 BN 訓練時就是根據 batch 去計算的</li>
<li>PyTorch 實作了 <code>_forward_approximat</code> 藉此避掉因為要統計最精確的 $\mu_B,\sigma_B$ 而多出來的一個 convolution 運算, 雖然加快, 但代價是稍微不精確 (8-bit quant 經驗上還好, 但更低的 quant 可能會有影響)</li>
</ul>
<hr>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li>Fusing batch normalization and convolution in runtime [<a href="https://nenadmarkus.com/p/fusing-batchnorm-and-conv/" target="_blank" rel="external">blog</a>]</li>
<li>Quantizing deep convolutional networks for efficient inference: A whitepaper [<a href="https://arxiv.org/abs/1806.08342" target="_blank" rel="external">arxiv</a>]</li>
<li>Pytorch <code>_forward_approximate</code> and <code>_forward_slow</code> (in <a href="https://github.com/pytorch/pytorch/blob/main/torch/ao/nn/intrinsic/qat/modules/conv_fused.py#L133" target="_blank" rel="external">torch.ao.nn.intrinsic.qat.modules.conv_fused.py</a>)</li>
</ol>
<p>本文圖檔文件: <a href="ConvBN_fusion.drawio">ConvBN_fusion.drawio</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>Post author：</strong>
      Chih-Sheng Chen
    </li>
    <li class="post-copyright-link">
      <strong>Post link：</strong>
      <a href="https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/" title="Convolution 與 BatchNorm 的融合大法：從推論、QAT 到 PyTorch 的加速">https://bobondemon.github.io/2025/05/04/Convolution-與-BatchNorm-的融合大法/</a>
    </li>
    <li class="post-copyright-license">
      <strong>Copyright Notice： </strong>
      All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/CNN/" rel="tag"># CNN</a>
          
            <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          
            <a href="/tags/Batch-Normalization/" rel="tag"># Batch Normalization</a>
          
            <a href="/tags/Fake-Quantization/" rel="tag"># Fake Quantization</a>
          
            <a href="/tags/Quantization-Aware-Training-QAT/" rel="tag"># Quantization Aware Training (QAT)</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2025/05/02/RM-algo/" rel="next" title="Robbins-Monro Algorithm 和 Dvoretzky's Convergence Theorem 筆記">
                <i class="fa fa-chevron-left"></i> Robbins-Monro Algorithm 和 Dvoretzky's Convergence Theorem 筆記
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2025/06/04/REINFORCE-estimator/" rel="prev" title="REINFORCE Estimator">
                REINFORCE Estimator <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目錄
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            本站概覽
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.png"
               alt="Chih-Sheng Chen" />
          <p class="site-author-name" itemprop="name">Chih-Sheng Chen</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">110</span>
                <span class="site-state-item-name">文章</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">分類</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">229</span>
                <span class="site-state-item-name">標籤</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Inference-階段融合-Conv-BN"><span class="nav-number">1.</span> <span class="nav-text">Inference 階段融合 Conv+BN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#QAT-對-Conv-BN-插-Fake-quant"><span class="nav-number">2.</span> <span class="nav-text">QAT 對 Conv+BN 插 Fake-quant</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Baseline-插-fake-quant"><span class="nav-number">2.1.</span> <span class="nav-text">Baseline 插 fake-quant</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fake-Quant-With-Correction-Term"><span class="nav-number">2.2.</span> <span class="nav-text">Fake Quant With Correction Term</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PyTorch-作法"><span class="nav-number">3.</span> <span class="nav-text">PyTorch 作法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">4.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">5.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chih-Sheng Chen</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 強力驅動
</div>

<div class="theme-info">
  主題 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      [object Object]
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      [object Object]
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  




	





  





  





  






  





  

  

  

  

</body>
</html>
