<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-tw">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Ordinary Differential Equations,ODE,Gradient Descent,Stochastic Gradient Descent,SGD," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Sharp V.S. Flat Local Minimum çš„æ³›åŒ–èƒ½åŠ›å…ˆç°¡å–®ä»‹ç´¹é€™ç¯‡æ–‡ç« :On large-batch training for deep learning: Generalization gap and sharp minimaè€ƒæ…®ä¸‹åœ–å…©å€‹ minimum, å°æ–¼ training loss ä¾†èªªå…¶ losses ä¸€æ¨£. å¾åœ–å¯ä»¥å®¹æ˜“ç†è§£åˆ°, å¦‚æœæ‰¾åˆ°å¤ª sharp çš„é»">
<meta property="og:type" content="article">
<meta property="og:title" content="SGD æ³›åŒ–èƒ½åŠ›çš„ç­†è¨˜">
<meta property="og:url" content="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/index.html">
<meta property="og:site_name" content="æ£’æ£’ç”Ÿ">
<meta property="og:description" content="Sharp V.S. Flat Local Minimum çš„æ³›åŒ–èƒ½åŠ›å…ˆç°¡å–®ä»‹ç´¹é€™ç¯‡æ–‡ç« :On large-batch training for deep learning: Generalization gap and sharp minimaè€ƒæ…®ä¸‹åœ–å…©å€‹ minimum, å°æ–¼ training loss ä¾†èªªå…¶ losses ä¸€æ¨£. å¾åœ–å¯ä»¥å®¹æ˜“ç†è§£åˆ°, å¦‚æœæ‰¾åˆ°å¤ª sharp çš„é»">
<meta property="og:image" content="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/Untitled.png">
<meta property="og:image" content="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/Untitled 1.png">
<meta property="og:image" content="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/Untitled 2.png">
<meta property="og:image" content="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/Untitled 3.png">
<meta property="og:image" content="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/Untitled 4.png">
<meta property="og:image" content="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/Untitled 5.png">
<meta property="og:image" content="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/Untitled 6.png">
<meta property="og:image" content="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/Untitled 7.png">
<meta property="og:updated_time" content="2022-07-20T14:06:40.963Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SGD æ³›åŒ–èƒ½åŠ›çš„ç­†è¨˜">
<meta name="twitter:description" content="Sharp V.S. Flat Local Minimum çš„æ³›åŒ–èƒ½åŠ›å…ˆç°¡å–®ä»‹ç´¹é€™ç¯‡æ–‡ç« :On large-batch training for deep learning: Generalization gap and sharp minimaè€ƒæ…®ä¸‹åœ–å…©å€‹ minimum, å°æ–¼ training loss ä¾†èªªå…¶ losses ä¸€æ¨£. å¾åœ–å¯ä»¥å®¹æ˜“ç†è§£åˆ°, å¦‚æœæ‰¾åˆ°å¤ª sharp çš„é»">
<meta name="twitter:image" content="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/Untitled.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'åšä¸»'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/"/>





  <title> SGD æ³›åŒ–èƒ½åŠ›çš„ç­†è¨˜ | æ£’æ£’ç”Ÿ </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-tw">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">æ£’æ£’ç”Ÿ</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">è®“å­¸ç¿’è®Šæˆä¸€ç¨®ç¿’æ…£</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            é¦–é 
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            åˆ†é¡
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            é—œæ–¼
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            æ­¸æª”
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            æ¨™ç±¤
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chih-Sheng Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="æ£’æ£’ç”Ÿ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                SGD æ³›åŒ–èƒ½åŠ›çš„ç­†è¨˜
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">ç™¼è¡¨æ–¼</span>
              
              <time title="å‰µå»ºæ–¼" itemprop="dateCreated datePublished" datetime="2022-05-28T22:15:58+08:00">
                2022-05-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">åˆ†é¡æ–¼</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Optimization/" itemprop="url" rel="index">
                    <span itemprop="name">Optimization</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>[object Object]
            </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<hr>
<h2 id="Sharp-V-S-Flat-Local-Minimum-çš„æ³›åŒ–èƒ½åŠ›"><a href="#Sharp-V-S-Flat-Local-Minimum-çš„æ³›åŒ–èƒ½åŠ›" class="headerlink" title="Sharp V.S. Flat Local Minimum çš„æ³›åŒ–èƒ½åŠ›"></a>Sharp V.S. Flat Local Minimum çš„æ³›åŒ–èƒ½åŠ›</h2><p>å…ˆç°¡å–®ä»‹ç´¹é€™ç¯‡æ–‡ç« :<br><a href="https://arxiv.org/abs/1609.04836" target="_blank" rel="external">On large-batch training for deep learning: Generalization gap and sharp minima</a><br>è€ƒæ…®ä¸‹åœ–å…©å€‹ minimum, å°æ–¼ training loss ä¾†èªªå…¶ losses ä¸€æ¨£.<br><img src="/2022/05/28/SGD-Ggeneralization-Notes/Untitled.png" width="70%" height="70%"> å¾åœ–å¯ä»¥å®¹æ˜“ç†è§£åˆ°, å¦‚æœæ‰¾åˆ°å¤ª sharp çš„é», ç”±æ–¼ test and train çš„ mismatch, æœƒå°è‡´æ¸¬è©¦çš„æ™‚å€™ data ä¸€é»åç§»å°±æœƒå° model output å½±éŸ¿å¾ˆå¤§.<br>è«–æ–‡ç”¨å¯¦é©—çš„æ–¹å¼, å»è©•é‡ä¸€å€‹ local minimum çš„ sharpness ç¨‹åº¦, ç°¡å–®èªªåˆ©ç”¨ random perturb åˆ°é™„è¿‘å…¶ä»–é», ç„¶å¾Œçœ‹çœ‹è©²é» loss è®ŠåŒ–çš„ç¨‹åº¦å¦‚ä½•, è®ŠåŒ–æ„ˆå¤§, ä»£è¡¨è©² local minimum å¯èƒ½æ„ˆ sharp.<br>ç„¶å¾Œæ‰¾å…©å€‹ local minimums, ä¸€å€‹ä¼°å‡ºä¾†æ¯”è¼ƒ sharp å¦ä¸€å€‹æ¯”è¼ƒ flat. æ¥è‘—å°é€™å…©é»é€£æˆçš„ç·š, ç·šä¸Šçš„åƒæ•¸å€¼å°æ‡‰çš„ loss åŠƒå‡ºåœ–ä¾†, é•·ç›¸å¦‚ä¸‹:<br><img src="/2022/05/28/SGD-Ggeneralization-Notes/Untitled 1.png" width="100%" height="100%"> é€™ä¹Ÿæ˜¯ç›®å‰ä¸€å€‹æ™®éçš„èªçŸ¥: flat çš„ local minimum æ³›åŒ–èƒ½åŠ›è¼ƒå¥½.<br>æ‰€ä»¥å¯ä»¥æƒ³åƒ, step size (learning rate) å¦‚æœæ„ˆå¤§, æ„ˆæœ‰å¯èƒ½è·³å‡º sharp minimum.<br>è€Œ batch size æ„ˆå°, è¡¨ç¤º gradient å› ç‚º mini-batch é€ æˆçš„ noise æ„ˆå¤§, ç›¸ç•¶æ–¼æ„ˆæœ‰å¯èƒ½â€äº‚è·‘â€è·‘å‡º sharp minimum.<br>ä½†é€™ç¯‡æ–‡ç« <em>åƒ…æ­¢æ–¼å¯¦é©—æ€§è³ªä¸Šçš„é©—è­‰</em>. Step size and batch size å°æ–¼æ³›åŒ–èƒ½åŠ›, æˆ–æ˜¯èªªå°æ–¼æ‰¾åˆ°æ¯”è¼ƒ flat optimum çš„æ©Ÿç‡æœƒä¸æœƒæ¯”è¼ƒé«˜? å…©è€…æœ‰ä»€éº¼é—œè¯å‘¢?<br><strong>DeepMind çš„è¿‘æœŸ (2021) å…©ç¯‡æ–‡ç« çµ¦å‡ºäº†å¾ˆæ¼‚äº®çš„ç†è«–åˆ†æ.</strong></p>
<a id="more"></a>
<h2 id="Full-Batch-Gradient-Steepest-Descent"><a href="#Full-Batch-Gradient-Steepest-Descent" class="headerlink" title="Full-Batch Gradient (Steepest) Descent"></a>Full-Batch Gradient (Steepest) Descent</h2><hr>
<p>å†ä¾†ä»‹ç´¹é€™ç¯‡: <strong><a href="https://arxiv.org/abs/2009.11162" target="_blank" rel="external">Implicit Gradient Regularization</a></strong>, DeepMind å‡ºå“.<br>æƒ³æ¢è¨ç‚ºä»€éº¼ NN çš„æ³›åŒ–èƒ½åŠ›é€™éº¼å¥½? çµè«–å°±æ˜¯è·Ÿ Gradient Descent æœ¬èº«ç®—æ³•ç‰¹æ€§æœ‰é—œ.<br>ä¸€èˆ¬æˆ‘å€‘å° cost (loss) function åš gradient (steepest) descent å…¬å¼å¦‚ä¸‹:</p>
<span>$$\begin{align}
\omega_{n+1}=\omega_n-h\nabla C(\omega)
\end{align}$$</span><!-- Has MathJax -->
<p>å…¶ä¸­ $h$ ç‚º step size (learning rate), <span>$\omega\in\mathbb{R}^d$</span><!-- Has MathJax --> è¡¨ç¤º parameters.<br>ç•¶ $h\rightarrow 0$, $n$ è®Šæˆé€£çºŒçš„æ™‚é–“ $t$, å‰‡å¯è¦–ç‚ºä¸€å€‹ Ordinary Differential Equation (ODE) system, æ•´ç†å¦‚ä¸‹:<br><span>$$\begin{align}
\text{Cost Function}: C(\omega) \\
\text{ODE}: \dot{\omega}=f(\omega)=-\nabla C(\omega)
\end{align}$$</span><!-- Has MathJax --><br>çµ¦å®š initial point $\omega_0$, ä¸Šé¢çš„ ODE æ±‚è§£å°±æ˜¯ä¸€æ¢é€£çºŒçš„ trajectory.</p>
<blockquote>
<p>ğŸ’¡ æˆ‘å€‘åœ¨ <a href="https://bobondemon.github.io/2022/05/15/Numerical-Methods-for-Ordinary-Differential-Equations/">Numerical Methods for Ordinary Differential Equations</a> æœ‰ä»‹ç´¹å„ç¨®æ•¸å€¼æ–¹æ³•, å¯ä»¥çŸ¥é“ gradient descent å°±æ˜¯ Euler method, è€Œé€™æ¨£çš„ error æ˜¯ $O(h^2)$.</p>
</blockquote>
<p>ç”¨å¼ (1) gradient descent ($h$ å›ºå®š) æ±‚è§£, æœƒä½¿å¾— trajectory è·Ÿé€£çºŒçš„ ODE (3) çš„ä¸åŒ.<br><img src="/2022/05/28/SGD-Ggeneralization-Notes/Untitled 2.png" width="30%" height="30%"> æ³¨æ„åˆ°é€™è£¡æ²’æœ‰ä½¿ç”¨ mini-batch, ç”¨çš„æ˜¯ full-batch, æ‰€ä»¥ä¸æ˜¯ Stochastic gradient descent (SGD).</p>
<p>å¦‚æœæˆ‘å€‘èƒ½å° gradient descent çš„ trajectory ç”¨<strong><em>å¦ä¸€å€‹ ODE</em></strong> system çš„ trajectory ä»£è¡¨çš„è©± (æ€éº¼æ‰¾ç­‰ç­‰å†èªª), åˆ†æä¿®æ”¹éå¾Œçš„ ODE å’ŒåŸä¾†çš„ ODE systems èªªä¸å®šèƒ½çœ‹åˆ°ä»€éº¼é—œè¯. é€™æ­£æ˜¯é€™ç¯‡è«–æ–‡çš„é‡è¦ç™¼ç¾.<br>å…ˆä¾†çœ‹çœ‹ä¿®æ”¹éå¾Œçš„ ODE é•·ä»€éº¼æ¨£:<br><span>$$\begin{align}
\text{Cost Function}: \tilde{C}_{gd}(\omega)=C(\omega)+\frac{h}{4}\|\nabla C(\omega)\|^2 \\
\text{ODE}: \dot{\omega}=\tilde{f}(\omega)=-\nabla\tilde{C}_{gd}(\omega)
\end{align}$$</span><!-- Has MathJax --></p>
<p>æ³¨æ„åˆ°æœ€ä½³è§£èˆ‡åŸä¾†çš„ ODE system ä¸€æ¨£: $C(\omega)$ å’Œ <span>$\tilde{C}_{gd}(\omega)$</span><!-- Has MathJax --> æœ€ä½³è§£ç›¸åŒ. (å¾ˆå®¹æ˜“å¯ä»¥çœ‹å‡ºä¾†å› ç‚º minimal points å…¶ gradient å¿…å®šç‚º $0$)<br>å°‡ä¸‰æ¢ trajectories ç”¨åœ–ä¾†è¡¨ç¤ºçš„è©±å¦‚ä¸‹:<br>&emsp;- Gradient descent çš„ trajectory å¼ (1): ç¶ è‰²ç®­è™Ÿç·š<br>&emsp;- ODE çš„ trajectory å¼ (3): é»‘è‰²ç·š<br>&emsp;- ä¿®æ”¹å¾Œçš„ ODE çš„ trajectory å¼ (5): é»ƒè‰²ç·š, å¯ä»¥ç”¨ä¾†ä»£è¡¨ gradient descent çš„ trajectory<br>(åƒè€ƒè‡ª inFERENCe blog æ–‡ç« : <a href="https://www.inference.vc/notes-on-the-origin-of-implicit-regularization-in-stochastic-gradient-descent/" target="_blank" rel="external">Notes on the Origin of Implicit Regularization in SGD</a>)<br><img src="/2022/05/28/SGD-Ggeneralization-Notes/Untitled 3.png" width="50%" height="50%"><br>ç‚ºä»€éº¼å¯ä»¥ç”¨ä¿®æ”¹å¾Œçš„ ODE ä»£è¡¨ gradient descent çš„ trajectory å‘¢?<br>å› ç‚ºå…©è€…å·®ç•°å¤ å°, ç‚º $O(h^3)$, æ¯” gradient descent å’ŒåŸæœ¬ ODE ä¹‹é–“çš„ error $O(h^2)$ æ›´å°.<br>(ç¶ è‰²ç®­è™Ÿç·šæ¯”èµ·é»‘è‰²ç·šæ›´æ¥è¿‘é»ƒè‰²ç·š)</p>
<p>å†ä¾†æˆ‘å€‘å›ç­”é€™å€‹å•é¡Œ: æ€éº¼æ‰¾åˆ° (4) (5) é€™æ¨£çš„ ODE å¯ä»¥ç”¨ä¾†ä»£è¡¨ gradient descent çš„ trajectory å‘¢?<br>ğŸ’¡ éœ€åˆ©ç”¨ backward error analysis, é€™è£¡ç•¥é, è«‹åƒè€ƒ [<a href="https://www.unige.ch/~hairer/poly_geoint/week3.pdf" target="_blank" rel="external">ref1</a>] [<a href="https://webspace.science.uu.nl/~frank011/Classes/numwisk/ch17.pdf" target="_blank" rel="external">ref2</a>]</p>
<blockquote>
<p>å…¶ä¸­ ref2 è£¡çš„äºŒéš Taylor expansion è£œå……æ¨å°:<br><span>$$\left.\frac{d^2}{dt^2}\tilde{y}(t)\right|_{t=t_n}=\left.\frac{d}{dt}\left[
f(\tilde{y}(t))+hf_1(\tilde{y}(t))
\right]\right|_{t=t_n} \\
=\left.\left[
f&apos;(\tilde{y}(t))\frac{d\tilde{y}(t)}{dt}+hf_1&apos;(\tilde{y}(t))\frac{d\tilde{y}(t)}{dt}
\right]\right|_{t=t_n} \\
=\left.\left[
f&apos;(\tilde{y}(t))\tilde{f}(\tilde{y}(t))+hf_1&apos;(\tilde{y}(t))\tilde{f}(\tilde{y}(t))
\right]\right|_{t=t_n} \\
=\left.\left[
\left( f&apos;(\tilde{y}(t))+hf_1&apos;(\tilde{y}(t)) \right)\tilde{f}(\tilde{y}(t))
\right]\right|_{t=t_n} \\
=(f&apos;(\tilde{y}_n)+hf_1&apos;(\tilde{y}_n))\tilde{f}(\tilde{y}_n)$$</span><!-- Has MathJax --></p>
</blockquote>
<p>è§€å¯Ÿ (4) çš„ <span>$\tilde{C}_{gd}(\omega)$</span><!-- Has MathJax -->, å¯ä»¥ç™¼ç¾ç›¸ç•¶æ–¼åœ¨åŸä¾†çš„ cost function $C(\omega)$ åŠ ä¸Šä¸€å€‹æ­£å‰‡é …. è€Œè©²é …æ­£æ¯”æ–¼ gradient norm çš„å¹³æ–¹.<br>ç™½è©±å°±æ˜¯å¦‚æœ gradient æ„ˆå¤§, penalty æ„ˆå¤§, æ‰€ä»¥å„ªåŒ–çš„æ™‚å€™æœƒå‚¾å‘æ–¼æ‰¾ gradient å°çš„å€åŸŸ. ç›¸ç•¶æ–¼æ‰¾æ¯”è¼ƒ flat çš„å€åŸŸ. é€™æ¨£æœ‰ä»€éº¼å¥½è™•å‘¢? å¦‚åŒä¸€é–‹å§‹èªªçš„, èƒ½æé«˜æ³›åŒ–èƒ½åŠ›!<br>å¦å¤–æ­£å‰‡é …ä¹Ÿæ­£æ¯”æ–¼ step size $h$, æ‰€ä»¥å¦‚æœ step size æ„ˆå¤§, è¡¨ç¤ºå° sharp å€åŸŸçš„ penalty æ„ˆå¤§, å› æ­¤æ›´åŠ å‚¾å‘æ‰¾ flat å€åŸŸ. é€™ä¹Ÿç¬¦åˆæˆ‘å€‘ä¹‹å‰æåˆ°æ„ˆæœ‰å¯èƒ½è·³å‡º sharp minimum çš„è§€é». å¦å¤–ä½œè€…çš„ <a href="https://youtu.be/pZnZSxOttN0?t=230" target="_blank" rel="external">presentation</a> é–‹é ­ä¹Ÿç”¨ä»¥ä¸‹ä¾‹å­èªªæ˜é€™å€‹ç¾è±¡:<br><img src="/2022/05/28/SGD-Ggeneralization-Notes/Untitled 4.png" width="40%" height="40%"> å¤§çš„ learning rate å‚¾å‘æ‰¾æ¯”è¼ƒ flat çš„ minimum, ä¹Ÿå°±æ˜¯æ³›åŒ–èƒ½åŠ›è¼ƒå¥½. æ‰€ä»¥å°æ‡‰åˆ°ä¸Šåœ–é¡¯ç¤ºçš„ Test æƒ…æ³ä¸‹æœ€å¥½çš„ learning rate æ¯” training çš„è¦å¤§.<br>ç¸½çµä¾†èªªæä¾›äº†ä¸€å€‹çœ‹æ³•, èªªæ˜ç‚ºä»€éº¼ NN çš„è¡¨ç¾é€™éº¼å¥½, ç‰¹åˆ¥æ˜¯æ³›åŒ–èƒ½åŠ›. å¾ˆæ„å¤–çš„æ˜¯, å…¶å¯¦è·Ÿæˆ‘å€‘ç”¨çš„ gradient descent å¤©ç”Ÿçš„ç‰¹æ€§æœ‰é—œ.</p>
<h2 id="Mini-Batch-Stochastic-Gradient-Descent"><a href="#Mini-Batch-Stochastic-Gradient-Descent" class="headerlink" title="Mini-Batch Stochastic Gradient Descent"></a>Mini-Batch Stochastic Gradient Descent</h2><hr>
<p>ä¸Šä¸€æ®µéƒ½é‚„æ²’è€ƒæ…® mini-batch çš„æƒ…æ³. å› ç‚ºä¸€æ—¦è®Šæˆ mini-batch ç›¸ç•¶æ–¼ gradient è¢«åŠ ä¸Šäº† random noise è®Šçš„æ›´é›£åˆ†æ. å› æ­¤ DeepMind ä»–å€‘ç™¼äº†ä¸€ç¯‡å¾ŒçºŒæ–‡ç« : <strong><a href="https://arxiv.org/abs/2101.12176" target="_blank" rel="external">On the Origin of Implicit Regularization in Stochastic Gradient Descent</a></strong>, å°‡ mini-batch è€ƒé‡é€²å», ç›¸ç•¶æ–¼åˆ†æ SGD ç®—æ³•.<br>ç”±æ–¼ mini-batches åœ¨ä¸€å€‹ epoch å¯èƒ½çš„é †åºä¸ä¸€æ¨£, æ‰€ä»¥ä¸€æ¢ trajectory å°æ‡‰åˆ°ä¸€å€‹é †åº.<br>(åƒè€ƒè‡ª inFERENCe blog æ–‡ç« : <a href="https://www.inference.vc/notes-on-the-origin-of-implicit-regularization-in-stochastic-gradient-descent/" target="_blank" rel="external">Notes on the Origin of Implicit Regularization in SGD</a>)<br><img src="/2022/05/28/SGD-Ggeneralization-Notes/Untitled 5.png" width="50%" height="50%"> æˆ‘å€‘è®Šæˆè¦è€ƒé‡çš„æ˜¯ <strong>â€œmeanâ€</strong> trajectory. é¡ä¼¼åœ°, mean trajectory ä¸€æ¨£å¯ä»¥ç”¨ä¸€å€‹ä¿®æ”¹å¾Œçš„ ODE system ä¾†ä»£è¡¨å®ƒ:<br><span>$$\begin{align}
\text{Mean Trajectory}: \mathbb{E}(\omega_m)=\omega(mh)+O(m^3h^3)\\
\text{Cost Function}:\tilde{C}_{sgd}(\omega)= \tilde{C}_{gd}(\omega)  +
\underbrace{\frac{h}{4m}\sum_{i=0}^{m-1}\|\nabla \hat{C}_i(\omega)-\nabla C(\omega)\|^2}_\text{additional regularizer} \\
\text{ODE}: \dot{\omega}=-\nabla\tilde{C}_{sgd}(\omega)
\end{align}$$</span><!-- Has MathJax --><br>å…¶ä¸­ $m$ è¡¨ç¤ºæ•´å€‹ training data å¯ä»¥åˆ†æˆ $m$ å€‹ mini-batches. <span>$\nabla \hat{C}_i(\omega)$</span><!-- Has MathJax --> è¡¨ç¤º i-th mini-batch çš„ gradient.<br>å¯ä»¥çœ‹åˆ°å¤šäº†ä¸€é …æ­£å‰‡é …: mini-batches çš„ gradients æ¸›æ‰ full-batch gradient çš„ variance.<br>æˆ‘å€‘å°±å…ˆç•¶ $\omega$ å·²ç¶“æ˜¯ local minimum å¥½äº† (<span>$\nabla C(\omega)=0$</span><!-- Has MathJax -->). æ‰€ä»¥è©²æ­£å‰‡é …ç°¡åŒ–æˆ mini-batches gradients çš„ variance.<br>ç›¸ç•¶æ–¼å‘Šè¨´æˆ‘å€‘, å¦‚æœ mini-batches çš„é‚£äº› gradients å·®ç•°éƒ½å¾ˆå¤§çš„è©±, penalty æœƒæ¯”è¼ƒå¤§, æ¯”è¼ƒä¸æœƒæ˜¯ SGD æœƒæ‰¾åˆ°çš„è§£.<br>é€™æ¨£çš„ç‰¹æ€§å°æ–¼æ³›åŒ–èƒ½åŠ›æœ‰ä»€éº¼é—œè¯? <a href="https://www.inference.vc/notes-on-the-origin-of-implicit-regularization-in-stochastic-gradient-descent/" target="_blank" rel="external">inFERENCe æ–‡ç« </a>çµ¦äº†ä¸€å€‹å¾ˆæ¸…æ¥šçš„èªªæ˜:<br>x-è»¸æ˜¯ parameter $\omega$, y-è»¸æ˜¯ loss $C(\omega)$.<br><img src="/2022/05/28/SGD-Ggeneralization-Notes/Untitled 6.png" width="100%" height="100%"><br>Variance of mini-batchesâ€™ gradients å·¦åœ–æ¯”å³åœ–å°, å› è€Œé€ æˆå³åœ–çš„ penalty æ¯”è¼ƒå¤§, æ‰€ä»¥ (8) æœƒå‚¾å‘é¸æ“‡å·¦åœ–. æ˜é¡¯çš„, å°æ–¼ test data ä¾†èªªå·¦åœ–çš„è§£æœƒæ¯”å³åœ– robust, å› ç‚º test data å¯ä»¥çœ‹æˆä¸Šé¢ä¸åŒ batches çš„è¡¨ç¾.<br>å¯ä»¥å¾ (7) çœ‹å‡ºä¾†, ç”±æ–¼ additional regularizer çš„é—œä¿‚, SGD æœ€ä½³è§£æœƒè·ŸåŸä¾† full-batch çš„æœ€ä½³è§£ä¸åŒäº†. é™¤éæ‰€æœ‰ mini-batches çš„ gradients ä¹Ÿéƒ½æ˜¯ $0$.<br>å¦å¤– (7) åœ¨è«–æ–‡ä¸­ä¹Ÿæ¨å°æˆå¦ä¸€å€‹å½¢å¼ (å°æ¯”(7)ç‚º additional regularizer æ”¹å¯«äº†):<br><span>$$\mathbb{E}(\tilde{C}_{sgd}(\omega))=\tilde{C}_{gd}(\omega)+\frac{N-B}{N-1}\frac{\color{orange}{h}}{4\color{orange}{B}}\Gamma(\omega) \\
\Gamma(\omega)=\frac{1}{N}\sum_{i=1}^N \|\nabla C_i(\omega)-\nabla C(\omega)\|^2$$</span><!-- Has MathJax --> å¯ä»¥çœ‹å‡º learning rate and batch size çš„é—œä¿‚, $h/B$ å¦‚æœç¶­æŒä¸€å®šæ¯”ä¾‹, å‰‡æ­£å‰‡é …çš„å½±éŸ¿åŠ›å¤§ç´„ç›¸åŒ.</p>
<blockquote>
<p>ä½œè€… presentation èªª, ç¶“é©—ä¸Š batch size double, learning rate ä¹Ÿè¦ double. [<a href="https://youtu.be/pZnZSxOttN0?t=2151" target="_blank" rel="external">YouTube time</a>]<br>å°æ‡‰åˆ° $h/B$ æ¯”ä¾‹ä¸è®Š, æ‰€ä»¥ performance æ‡‰è©²ä¹Ÿç¶­æŒä¸€æ¨£ (åœ¨ $B$ ä¸å¤§çš„æƒ…æ³ä¸‹). è«–æ–‡åšäº†å¯¦é©—çµæœå¦‚ä¸‹:<br><img src="/2022/05/28/SGD-Ggeneralization-Notes/Untitled 7.png" width="50%" height="50%"></p>
</blockquote>
<h2 id="çµè«–"><a href="#çµè«–" class="headerlink" title="çµè«–"></a>çµè«–</h2><hr>
<p>é›–ç„¶å­˜åœ¨ä¸€äº›å‡è¨­æ‰æœƒä½¿ SGD çš„ä¼°è¨ˆæ­£ç¢º</p>
<blockquote>
<p>âš ï¸ è«–æ–‡æ¨å°çš„å‡è¨­:</p>
<ol>
<li>batch shuffle çš„æ–¹å¼å– data, ä¹Ÿå°±æ˜¯ä¸€å€‹ epoch æœƒä¾åºè·‘å®Œ shuffle å¾Œçš„æ‰€æœ‰ batches</li>
<li>learning rate is finite (å°±æ˜¯æœ‰ lower bound)</li>
<li>åªåˆ†æ SGD, å…¶ä»–æ›´å¤šè®Šå½¢ä¾‹å¦‚ Adam, Adagrad, RMSProp, ç­‰çš„è¡Œç‚ºä¸çŸ¥é“</li>
<li>$m^3h^3$ å¿…é ˆè¦å¤ å°, SGD çš„ â€œmeanâ€ trajectory æ‰æœƒç¬¦åˆ (7), (8) çš„ ODE çµæœ. ä¸€èˆ¬ dataset éƒ½å¾ˆå¤§ ($m$ å¾ˆå¤§), æ‰€ä»¥è¦æŠŠ $h$ éƒ½è¨­å®šå¾ˆå°, æ„Ÿè¦ºä¹Ÿæœ‰é»é›£ç¬¦åˆ (?). å½±ç‰‡: [<a href="https://youtu.be/pZnZSxOttN0?t=1766" target="_blank" rel="external">here</a>]</li>
</ol>
</blockquote>
<p>ä½†ç¸½çµä¾†èªª, åœ¨ full-batch è¨­å®šä¸‹, å¯¦å‹™ä¸Šä½¿ç”¨ steepest descent å¾é€£çºŒè®Šæˆé›¢æ•£çš„è·¯å¾‘, æœ¬èº«å°±æä¾›äº†æ³›åŒ–èƒ½åŠ›çš„å¥½è™•. åŠ ä¸Š mini-batch çš„è¨­å®š, ä½¿å¾—æ³›åŒ–èƒ½åŠ›æ›´å¥½äº†.</p>
<p>æ²’æƒ³åˆ°å·²ç¶“ç¿’ä»¥ç‚ºå¸¸çš„ SGD æ–¹æ³•, èƒŒå¾Œç«Ÿç„¶è—äº†é€™æ¨£çš„è§€é», å¤ªå²å®³äº†!</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><hr>
<ol>
<li><a href="https://arxiv.org/abs/2009.11162" target="_blank" rel="external">Implicit Gradient Regularization</a></li>
<li><a href="https://arxiv.org/abs/2101.12176" target="_blank" rel="external">On the Origin of Implicit Regularization in Stochastic Gradient Descent</a></li>
<li>inFERENCe: <a href="https://www.inference.vc/notes-on-the-origin-of-implicit-regularization-in-stochastic-gradient-descent/" target="_blank" rel="external">Notes on the Origin of Implicit Regularization in SGD</a></li>
<li><a href="https://bobondemon.github.io/2022/05/15/Numerical-Methods-for-Ordinary-Differential-Equations/">Numerical Methods for Ordinary Differential Equations</a></li>
<li><a href="https://arxiv.org/abs/1609.04836" target="_blank" rel="external">On large-batch training for deep learning: Generalization gap and sharp minima</a></li>
<li>Paper presentation by author: <a href="https://www.youtube.com/watch?v=pZnZSxOttN0" target="_blank" rel="external">On the Origin of Implicit Regularization in Stochastic Gradient Descent</a></li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>Post authorï¼š</strong>
      Chih-Sheng Chen
    </li>
    <li class="post-copyright-link">
      <strong>Post linkï¼š</strong>
      <a href="https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/" title="SGD æ³›åŒ–èƒ½åŠ›çš„ç­†è¨˜">https://bobondemon.github.io/2022/05/28/SGD-Ggeneralization-Notes/</a>
    </li>
    <li class="post-copyright-license">
      <strong>Copyright Noticeï¼š </strong>
      All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Ordinary-Differential-Equations/" rel="tag"># Ordinary Differential Equations</a>
          
            <a href="/tags/ODE/" rel="tag"># ODE</a>
          
            <a href="/tags/Gradient-Descent/" rel="tag"># Gradient Descent</a>
          
            <a href="/tags/Stochastic-Gradient-Descent/" rel="tag"># Stochastic Gradient Descent</a>
          
            <a href="/tags/SGD/" rel="tag"># SGD</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2022/05/15/Numerical-Methods-for-Ordinary-Differential-Equations/" rel="next" title="Numerical Methods for Ordinary Differential Equations">
                <i class="fa fa-chevron-left"></i> Numerical Methods for Ordinary Differential Equations
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2022/07/20/Why-Stochastic-Weight-Averaging-averaging-results-V-S-averaging-weights/" rel="prev" title="Why Stochastic Weight Averaging? averaging results V.S. averaging weights">
                Why Stochastic Weight Averaging? averaging results V.S. averaging weights <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            æ–‡ç« ç›®éŒ„
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            æœ¬ç«™æ¦‚è¦½
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.png"
               alt="Chih-Sheng Chen" />
          <p class="site-author-name" itemprop="name">Chih-Sheng Chen</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">81</span>
                <span class="site-state-item-name">æ–‡ç« </span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">åˆ†é¡</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">171</span>
                <span class="site-state-item-name">æ¨™ç±¤</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sharp-V-S-Flat-Local-Minimum-çš„æ³›åŒ–èƒ½åŠ›"><span class="nav-number">1.</span> <span class="nav-text">Sharp V.S. Flat Local Minimum çš„æ³›åŒ–èƒ½åŠ›</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Full-Batch-Gradient-Steepest-Descent"><span class="nav-number">2.</span> <span class="nav-text">Full-Batch Gradient (Steepest) Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mini-Batch-Stochastic-Gradient-Descent"><span class="nav-number">3.</span> <span class="nav-text">Mini-Batch Stochastic Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#çµè«–"><span class="nav-number">4.</span> <span class="nav-text">çµè«–</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">5.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chih-Sheng Chen</span>
</div>


<div class="powered-by">
  ç”± <a class="theme-link" href="https://hexo.io">Hexo</a> å¼·åŠ›é©…å‹•
</div>

<div class="theme-info">
  ä¸»é¡Œ -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      [object Object]
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      [object Object]
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  




	





  





  





  






  





  

  

  

  

</body>
</html>
